commit,old_file,new_file,old_contents,new_contents,subject,message,lang,license,repos,length,code
f8bc5893ee875a309361c26b93996917dbef3ba8,silk/webdoc/html/__init__.py,silk/webdoc/html/__init__.py,"
from .common import *
","
from .common import (  # noqa
    A,
    ABBR,
    ACRONYM,
    ADDRESS,
    APPLET,
    AREA,
    ARTICLE,
    ASIDE,
    AUDIO,
    B,
    BASE,
    BASEFONT,
    BDI,
    BDO,
    BIG,
    BLOCKQUOTE,
    BODY,
    BR,
    BUTTON,
    Body,
    CANVAS,
    CAPTION,
    CAT,
    CENTER,
    CITE,
    CODE,
    COL,
    COLGROUP,
    COMMENT,
    CONDITIONAL_COMMENT,
    DATALIST,
    DD,
    DEL,
    DETAILS,
    DFN,
    DIALOG,
    DIR,
    DIV,
    DL,
    DT,
    EM,
    EMBED,
    FIELDSET,
    FIGCAPTION,
    FIGURE,
    FONT,
    FOOTER,
    FORM,
    FRAME,
    FRAMESET,
    Form,
    H1,
    H2,
    H3,
    H4,
    H5,
    H6,
    HEAD,
    HEADER,
    HR,
    HTML,
    HTMLDoc,
    Hyper,
    I,
    IFRAME,
    IMG,
    INPUT,
    INS,
    Image,
    Javascript,
    KBD,
    KEYGEN,
    LABEL,
    LEGEND,
    LI,
    LINK,
    MAIN,
    MAP,
    MARK,
    MENU,
    MENUITEM,
    META,
    METER,
    NAV,
    NBSP,
    NOFRAMES,
    NOSCRIPT,
    OBJECT,
    OL,
    OPTGROUP,
    OPTION,
    OUTPUT,
    P,
    PARAM,
    PRE,
    PROGRESS,
    Q,
    RP,
    RT,
    RUBY,
    S,
    SAMP,
    SCRIPT,
    SECTION,
    SELECT,
    SMALL,
    SOURCE,
    SPAN,
    STRIKE,
    STRONG,
    STYLE,
    SUB,
    SUMMARY,
    SUP,
    TABLE,
    TBODY,
    TD,
    TEXTAREA,
    TFOOT,
    TH,
    THEAD,
    TIME,
    TITLE,
    TR,
    TRACK,
    TT,
    U,
    UL,
    VAR,
    VIDEO,
    WBR,
    XML,
    XMLEntity,
    XMLNode,
    XMP,
    xmlescape,
    xmlunescape,
)
",Replace import * with explicit names,"Replace import * with explicit names
",Python,bsd-3-clause,orbnauticus/silk,141,"```python

from .common import (  # noqa
    A,
    ABBR,
    ACRONYM,
    ADDRESS,
    APPLET,
    AREA,
    ARTICLE,
    ASIDE,
    AUDIO,
    B,
    BASE,
    BASEFONT,
    BDI,
    BDO,
    BIG,
    BLOCKQUOTE,
    BODY,
    BR,
    BUTTON,
    Body,
    CANVAS,
    CAPTION,
    CAT,
    CENTER,
    CITE,
    CODE,
    COL,
    COLGROUP,
    COMMENT,
    CONDITIONAL_COMMENT,
    DATALIST,
    DD,
    DEL,
    DETAILS,
    DFN,
    DIALOG,
    DIR,
    DIV,
    DL,
    DT,
    EM,
    EMBED,
    FIELDSET,
    FIGCAPTION,
    FIGURE,
    FONT,
    FOOTER,
    FORM,
    FRAME,
    FRAMESET,
    Form,
    H1,
    H2,
    H3,
    H4,
    H5,
    H6,
    HEAD,
    HEADER,
    HR,
    HTML,
    HTMLDoc,
    Hyper,
    I,
    IFRAME,
    IMG,
    INPUT,
    INS,
    Image,
    Javascript,
    KBD,
    KEYGEN,
    LABEL,
    LEGEND,
    LI,
    LINK,
    MAIN,
    MAP,
    MARK,
    MENU,
    MENUITEM,
    META,
    METER,
    NAV,
    NBSP,
    NOFRAMES,
    NOSCRIPT,
    OBJECT,
    OL,
    OPTGROUP,
    OPTION,
    OUTPUT,
    P,
    PARAM,
    PRE,
    PROGRESS,
    Q,
    RP,
    RT,
    RUBY,
    S,
    SAMP,
    SCRIPT,
    SECTION,
    SELECT,
    SMALL,
    SOURCE,
    SPAN,
    STRIKE,
    STRONG,
    STYLE,
    SUB,
    SUMMARY,
    SUP,
    TABLE,
    TBODY,
    TD,
    TEXTAREA,
    TFOOT,
    TH,
    THEAD,
    TIME,
    TITLE,
    TR,
    TRACK,
    TT,
    U,
    UL,
    VAR,
    VIDEO,
    WBR,
    XML,
    XMLEntity,
    XMLNode,
    XMP,
    xmlescape,
    xmlunescape,
)

```"
caeb76cbcb6cdd49138e41f57144573598b722ba,source/clique/__init__.py,source/clique/__init__.py,"# :coding: utf-8
# :copyright: Copyright (c) 2013 Martin Pengelly-Phillips
# :license: See LICENSE.txt.

from ._version import __version__

","# :coding: utf-8
# :copyright: Copyright (c) 2013 Martin Pengelly-Phillips
# :license: See LICENSE.txt.

import re
from collections import defaultdict

from ._version import __version__
from .collection import Collection
from .error import CollectionError


#: Pattern for matching an index with optional padding.
DIGITS_PATTERN = '(?P<index>(?P<padding>0*)\d+)'

_DIGITS_REGEX = re.compile(DIGITS_PATTERN)

#: Common patterns that can be passed to :py:func:`~clique.assemble`.
PATTERNS = {
    'frames': '\.{0}\.\D+\d?$'.format(DIGITS_PATTERN),
    'versions': 'v{0}'.format(DIGITS_PATTERN)
}


def assemble(iterable, patterns=None, minimum_items=2):
    '''Assemble items in *iterable* into discreet collections.

    *patterns* may be specified as a list of regular expressions to limit
    the returned collection possibilities. Use this when interested in
    collections that only match specific patterns. Each pattern must contain
    the expression from :py:data:`DIGITS_PATTERN` exactly once.

    A selection of common expressions are available in :py:data:`PATTERNS`.

    .. note::

        If a pattern is supplied as a string it will be automatically compiled
        to a regular expression for convenience.

    When *patterns* is not specified, collections are formed by examining all
    possible groupings of the items in *iterable* based around common numerical
    components.

    *minimum_items* dictates the minimum number of items a collection must have
    in order to be included in the result. The default is 2, filtering out
    single item collections.

    Return list of assembled :py:class:`~clique.collection.Collection`
    instances.

    '''
    collection_map = defaultdict(set)
    collections = []

    # Compile patterns.
    compiled_patterns = []

    if patterns is not None:
        if not patterns:
            return collections

        for pattern in patterns:
            if isinstance(pattern, basestring):
                compiled_patterns.append(re.compile(pattern))
            else:
                compiled_patterns.append(pattern)

    else:
        compiled_patterns.append(_DIGITS_REGEX)

    # Process iterable.
    for item in iterable:
        for pattern in compiled_patterns:
            for match in pattern.finditer(item):
                index = match.group('index')

                head = item[:match.start('index')]
                tail = item[match.end('index'):]

                padding = match.group('padding')
                if padding:
                    padding = len(index)
                else:
                    padding = 0

                key = (head, tail, padding)
                collection_map[key].add(int(index))

    # Form collections, filtering out those that do not have at least
    # as many indexes as minimum_items
    for (head, tail, padding), indexes in collection_map.items():
        if len(indexes) >= minimum_items:
            collections.append(
                Collection(head, tail, padding, indexes)
            )

    return collections

",Add top level function to help assemble collections from arbitrary items.,"Add top level function to help assemble collections from arbitrary items.
",Python,apache-2.0,4degrees/clique,99,"```python
# :coding: utf-8
# :copyright: Copyright (c) 2013 Martin Pengelly-Phillips
# :license: See LICENSE.txt.

import re
from collections import defaultdict

from ._version import __version__
from .collection import Collection
from .error import CollectionError


#: Pattern for matching an index with optional padding.
DIGITS_PATTERN = '(?P<index>(?P<padding>0*)\d+)'

_DIGITS_REGEX = re.compile(DIGITS_PATTERN)

#: Common patterns that can be passed to :py:func:`~clique.assemble`.
PATTERNS = {
    'frames': '\.{0}\.\D+\d?$'.format(DIGITS_PATTERN),
    'versions': 'v{0}'.format(DIGITS_PATTERN)
}


def assemble(iterable, patterns=None, minimum_items=2):
    '''Assemble items in *iterable* into discreet collections.

    *patterns* may be specified as a list of regular expressions to limit
    the returned collection possibilities. Use this when interested in
    collections that only match specific patterns. Each pattern must contain
    the expression from :py:data:`DIGITS_PATTERN` exactly once.

    A selection of common expressions are available in :py:data:`PATTERNS`.

    .. note::

        If a pattern is supplied as a string it will be automatically compiled
        to a regular expression for convenience.

    When *patterns* is not specified, collections are formed by examining all
    possible groupings of the items in *iterable* based around common numerical
    components.

    *minimum_items* dictates the minimum number of items a collection must have
    in order to be included in the result. The default is 2, filtering out
    single item collections.

    Return list of assembled :py:class:`~clique.collection.Collection`
    instances.

    '''
    collection_map = defaultdict(set)
    collections = []

    # Compile patterns.
    compiled_patterns = []

    if patterns is not None:
        if not patterns:
            return collections

        for pattern in patterns:
            if isinstance(pattern, basestring):
                compiled_patterns.append(re.compile(pattern))
            else:
                compiled_patterns.append(pattern)

    else:
        compiled_patterns.append(_DIGITS_REGEX)

    # Process iterable.
    for item in iterable:
        for pattern in compiled_patterns:
            for match in pattern.finditer(item):
                index = match.group('index')

                head = item[:match.start('index')]
                tail = item[match.end('index'):]

                padding = match.group('padding')
                if padding:
                    padding = len(index)
                else:
                    padding = 0

                key = (head, tail, padding)
                collection_map[key].add(int(index))

    # Form collections, filtering out those that do not have at least
    # as many indexes as minimum_items
    for (head, tail, padding), indexes in collection_map.items():
        if len(indexes) >= minimum_items:
            collections.append(
                Collection(head, tail, padding, indexes)
            )

    return collections


```"
199f9ace071b95822a9a0fb53c9becfb0ab4abd2,tests/pytests/unit/modules/test_win_servermanager.py,tests/pytests/unit/modules/test_win_servermanager.py,"import os

import pytest
import salt.modules.win_servermanager as win_servermanager
from tests.support.mock import MagicMock, patch


@pytest.fixture
def configure_loader_modules():
    return {win_servermanager: {}}


def test_install():
    mock_out = {
        ""FeatureResult"": {

        }
    }

    with patch.object(win_servermanager, ""_pshell_json"", return_value=""""):

","import os

import pytest
import salt.modules.win_servermanager as win_servermanager
from tests.support.mock import MagicMock, patch


@pytest.fixture
def configure_loader_modules():
    return {
        win_servermanager: {
            ""__grains__"": {""osversion"": ""6.2""}
        }
    }


def test_install():
    mock_out = {
        'Success': True,
        'RestartNeeded': 1,
        'FeatureResult': [
            {
                'Id': 338,
                'Name': 'XPS-Viewer',
                'DisplayName': 'XPS Viewer',
                'Success': True,
                'RestartNeeded': False,
                'Message': '',
                'SkipReason': 0
            }
        ],
        'ExitCode': 0
    }
    expected = {
        ""ExitCode"": 0,
        ""RestartNeeded"": False,
        ""Restarted"": False,
        ""Features"": {
            ""XPS-Viewer"": {
                ""DisplayName"": ""XPS Viewer"",
                ""Message"": """",
                ""RestartNeeded"": False,
                ""SkipReason"": 0,
                ""Success"": True
            }
        },
        ""Success"": True}

    mock_reboot = MagicMock(return_value=True)
    with patch.object(win_servermanager, ""_pshell_json"", return_value=mock_out), \
        patch.dict(win_servermanager.__salt__, {""system.reboot"": mock_reboot}):
        result = win_servermanager.install(""XPS-Viewer"")
        assert result == expected


def test_install_restart():
    mock_out = {
        'Success': True,
        'RestartNeeded': 1,
        'FeatureResult': [
            {
                'Id': 338,
                'Name': 'XPS-Viewer',
                'DisplayName': 'XPS Viewer',
                'Success': True,
                'RestartNeeded': True,
                'Message': '',
                'SkipReason': 0
            }
        ],
        'ExitCode': 0
    }
    expected = {
        ""ExitCode"": 0,
        ""RestartNeeded"": True,
        ""Restarted"": True,
        ""Features"": {
            ""XPS-Viewer"": {
                ""DisplayName"": ""XPS Viewer"",
                ""Message"": """",
                ""RestartNeeded"": True,
                ""SkipReason"": 0,
                ""Success"": True
            }
        },
        ""Success"": True}

    mock_reboot = MagicMock(return_value=True)
    with patch.object(win_servermanager, ""_pshell_json"", return_value=mock_out), \
            patch.dict(win_servermanager.__salt__, {""system.reboot"": mock_reboot}):
        result = win_servermanager.install(""XPS-Viewer"", restart=True)
        mock_reboot.assert_called_once()
        assert result == expected
",Add some unit tests for install,"Add some unit tests for install
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",94,"```python
import os

import pytest
import salt.modules.win_servermanager as win_servermanager
from tests.support.mock import MagicMock, patch


@pytest.fixture
def configure_loader_modules():
    return {
        win_servermanager: {
            ""__grains__"": {""osversion"": ""6.2""}
        }
    }


def test_install():
    mock_out = {
        'Success': True,
        'RestartNeeded': 1,
        'FeatureResult': [
            {
                'Id': 338,
                'Name': 'XPS-Viewer',
                'DisplayName': 'XPS Viewer',
                'Success': True,
                'RestartNeeded': False,
                'Message': '',
                'SkipReason': 0
            }
        ],
        'ExitCode': 0
    }
    expected = {
        ""ExitCode"": 0,
        ""RestartNeeded"": False,
        ""Restarted"": False,
        ""Features"": {
            ""XPS-Viewer"": {
                ""DisplayName"": ""XPS Viewer"",
                ""Message"": """",
                ""RestartNeeded"": False,
                ""SkipReason"": 0,
                ""Success"": True
            }
        },
        ""Success"": True}

    mock_reboot = MagicMock(return_value=True)
    with patch.object(win_servermanager, ""_pshell_json"", return_value=mock_out), \
        patch.dict(win_servermanager.__salt__, {""system.reboot"": mock_reboot}):
        result = win_servermanager.install(""XPS-Viewer"")
        assert result == expected


def test_install_restart():
    mock_out = {
        'Success': True,
        'RestartNeeded': 1,
        'FeatureResult': [
            {
                'Id': 338,
                'Name': 'XPS-Viewer',
                'DisplayName': 'XPS Viewer',
                'Success': True,
                'RestartNeeded': True,
                'Message': '',
                'SkipReason': 0
            }
        ],
        'ExitCode': 0
    }
    expected = {
        ""ExitCode"": 0,
        ""RestartNeeded"": True,
        ""Restarted"": True,
        ""Features"": {
            ""XPS-Viewer"": {
                ""DisplayName"": ""XPS Viewer"",
                ""Message"": """",
                ""RestartNeeded"": True,
                ""SkipReason"": 0,
                ""Success"": True
            }
        },
        ""Success"": True}

    mock_reboot = MagicMock(return_value=True)
    with patch.object(win_servermanager, ""_pshell_json"", return_value=mock_out), \
            patch.dict(win_servermanager.__salt__, {""system.reboot"": mock_reboot}):
        result = win_servermanager.install(""XPS-Viewer"", restart=True)
        mock_reboot.assert_called_once()
        assert result == expected

```"
de15315b95f70e56d424d54637e3ac0d615ea0f0,proto/ho.py,proto/ho.py,"from board import Board, BoardCanvas


b = Board(19, 19)
c = BoardCanvas(b)
","#!/usr/bin/env python

import platform
import subprocess
import sys
from copy import deepcopy

from board import Board, BoardCanvas


def clear():
    subprocess.check_call('cls' if platform.system() == 'Windows' else 'clear', shell=True)


class _Getch:
    """"""
    Gets a single character from standard input.  Does not echo to the
    screen.
    """"""
    def __init__(self):
        try:
            self.impl = _GetchWindows()
        except ImportError:
            self.impl = _GetchUnix()

    def __call__(self):
        return self.impl()


class _GetchUnix:
    def __call__(self):
        import tty
        import termios
        fd = sys.stdin.fileno()
        old_settings = termios.tcgetattr(fd)
        try:
            tty.setraw(sys.stdin.fileno())
            ch = sys.stdin.read(1)
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
            return ch


class _GetchWindows:
    def __init__(self):
        import msvcrt  # NOQA

    def __call__(self):
        import msvcrt
        return msvcrt.getch()


getch = _Getch()


WIDTH = 19
HEIGHT = 19


def trunc_width(v):
    return max(1, min(WIDTH, v))


def trunc_height(v):
    return max(1, min(HEIGHT, v))


def move_up(x, y):
    return trunc_width(x), trunc_height(y - 1)


def move_down(x, y):
    return trunc_width(x), trunc_height(y + 1)


def move_left(x, y):
    return trunc_width(x - 1), trunc_height(y)


def move_right(x, y):
    return trunc_width(x + 1), trunc_height(y)


KEYS = {
    'w': move_up,
    'r': move_down,
    'a': move_left,
    's': move_right,
}


def main():
    board = Board(WIDTH, HEIGHT)
    canvas = BoardCanvas(board)

    cur_x, cur_y = (1, 1)

    while True:
        clear()

        # Print board
        select_board = deepcopy(canvas)
        select_board.set(cur_x, cur_y, 'X')
        print select_board
        print 'Make your move... '

        # Get char
        c = getch()

        # Escape terminates
        if c == '\x1b':
            break

        # Move cursor
        try:
            cur_x, cur_y = KEYS[c](cur_x, cur_y)
        except KeyError:
            pass

if __name__ == '__main__':
    main()
",Add game loop to prototype,"Add game loop to prototype
",Python,mit,davesque/go.py,122,"```python
#!/usr/bin/env python

import platform
import subprocess
import sys
from copy import deepcopy

from board import Board, BoardCanvas


def clear():
    subprocess.check_call('cls' if platform.system() == 'Windows' else 'clear', shell=True)


class _Getch:
    """"""
    Gets a single character from standard input.  Does not echo to the
    screen.
    """"""
    def __init__(self):
        try:
            self.impl = _GetchWindows()
        except ImportError:
            self.impl = _GetchUnix()

    def __call__(self):
        return self.impl()


class _GetchUnix:
    def __call__(self):
        import tty
        import termios
        fd = sys.stdin.fileno()
        old_settings = termios.tcgetattr(fd)
        try:
            tty.setraw(sys.stdin.fileno())
            ch = sys.stdin.read(1)
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)
            return ch


class _GetchWindows:
    def __init__(self):
        import msvcrt  # NOQA

    def __call__(self):
        import msvcrt
        return msvcrt.getch()


getch = _Getch()


WIDTH = 19
HEIGHT = 19


def trunc_width(v):
    return max(1, min(WIDTH, v))


def trunc_height(v):
    return max(1, min(HEIGHT, v))


def move_up(x, y):
    return trunc_width(x), trunc_height(y - 1)


def move_down(x, y):
    return trunc_width(x), trunc_height(y + 1)


def move_left(x, y):
    return trunc_width(x - 1), trunc_height(y)


def move_right(x, y):
    return trunc_width(x + 1), trunc_height(y)


KEYS = {
    'w': move_up,
    'r': move_down,
    'a': move_left,
    's': move_right,
}


def main():
    board = Board(WIDTH, HEIGHT)
    canvas = BoardCanvas(board)

    cur_x, cur_y = (1, 1)

    while True:
        clear()

        # Print board
        select_board = deepcopy(canvas)
        select_board.set(cur_x, cur_y, 'X')
        print select_board
        print 'Make your move... '

        # Get char
        c = getch()

        # Escape terminates
        if c == '\x1b':
            break

        # Move cursor
        try:
            cur_x, cur_y = KEYS[c](cur_x, cur_y)
        except KeyError:
            pass

if __name__ == '__main__':
    main()

```"
a9f55a57559a6647c451d38893624be4109be23b,Spiders.py,Spiders.py,"'''
Created on 2 сент. 2016 г.

@author: garet
'''


class BaseSpider():
    
    def __init__(self):
        pass

    def AddUrls(self, urls):
        pass

    def Routing(self, url):
        pass

    def SaveCache(self, url, data=None):
        pass

    def GetCache(self, url):
        pass

    def Run(self):
        pass

","'''
Created on 2 сент. 2016 г.

@author: garet
'''

import queue
import sqlite3

class BaseSpider():
    
    def __init__(self):
        pass

    def AddUrls(self, urls):
        pass

    def Routing(self, url):
        pass

    def SaveCache(self, url, data=None):
        pass

    def GetCache(self, url):
        pass

    def Run(self):
        pass


class QueueUrls():

    def __init__(self):
        self._urls_queue = queue.Queue()
        self._urls_set = set()

    def AddUrls(self, urls):
        for url in urls:
            if url not in self._urls_set:
                self._urls_queue.put(url)
                self._urls_set.add(url)
        pass

    def ExistUrl(self, url):
        if url in self._urls_set:
            return True
        return False

    def GetUrl(self):
        return self._urls_queue.get()


class SqliteCache():

    def __init__(self, db_name):
        self.db_name = db_name

    def InitDB(self):
        file = self.db_name + '.sqlite'
        self._db = sqlite3.connect(file)
        self._cursor = self._db.cursor()
        # Create table
        sql = """"""
            CREATE TABLE IF NOT EXISTS tbl_urls
            ( 
                url text primary key not null, 
                html text, 
                time timestamp DEFAULT CURRENT_TIMESTAMP
            );""""""
        self._cursor.execute(sql)

    def Get(self, url):
        if self._cursor == None:
            self.InitDB()
        sql = """"""SELECT * FROM tbl_urls WHERE url=?;""""""
        self._cursor.execute(sql, (url,))
        return self._cursor.fetchone()

    def Set(self, url, data):
        if self._cursor == None:
            self.InitDB()
        sql = """"""INSERT OR REPLACE INTO tbl_urls(url, html)
        VALUES (?,?);""""""
        self._cursor.execute(sql, (url, data) )
        self._db.commit()





",Add SqliteCache for html raw data. Add QueueUrls for list urls.,Add SqliteCache for html raw data. Add QueueUrls for list urls.,Python,bsd-3-clause,SaltusVita/ReoGrab,91,"```python
'''
Created on 2 сент. 2016 г.

@author: garet
'''

import queue
import sqlite3

class BaseSpider():
    
    def __init__(self):
        pass

    def AddUrls(self, urls):
        pass

    def Routing(self, url):
        pass

    def SaveCache(self, url, data=None):
        pass

    def GetCache(self, url):
        pass

    def Run(self):
        pass


class QueueUrls():

    def __init__(self):
        self._urls_queue = queue.Queue()
        self._urls_set = set()

    def AddUrls(self, urls):
        for url in urls:
            if url not in self._urls_set:
                self._urls_queue.put(url)
                self._urls_set.add(url)
        pass

    def ExistUrl(self, url):
        if url in self._urls_set:
            return True
        return False

    def GetUrl(self):
        return self._urls_queue.get()


class SqliteCache():

    def __init__(self, db_name):
        self.db_name = db_name

    def InitDB(self):
        file = self.db_name + '.sqlite'
        self._db = sqlite3.connect(file)
        self._cursor = self._db.cursor()
        # Create table
        sql = """"""
            CREATE TABLE IF NOT EXISTS tbl_urls
            ( 
                url text primary key not null, 
                html text, 
                time timestamp DEFAULT CURRENT_TIMESTAMP
            );""""""
        self._cursor.execute(sql)

    def Get(self, url):
        if self._cursor == None:
            self.InitDB()
        sql = """"""SELECT * FROM tbl_urls WHERE url=?;""""""
        self._cursor.execute(sql, (url,))
        return self._cursor.fetchone()

    def Set(self, url, data):
        if self._cursor == None:
            self.InitDB()
        sql = """"""INSERT OR REPLACE INTO tbl_urls(url, html)
        VALUES (?,?);""""""
        self._cursor.execute(sql, (url, data) )
        self._db.commit()






```"
17f14574a35d985571e71023587ddb858a8b3ba2,tests/test_engines.py,tests/test_engines.py,,"#!/usr/bin/env python

from __future__ import print_function

import unittest

try:
    from unittest import mock
except ImportError:
    import mock

import imp
import os.path


import engines


class TestInit(unittest.TestCase):

    def test_init(self):
        mock_engines = {}

        mock_listdir = mock.Mock(return_value=(
                'text_engine.txt',
                '0_engine.py',
                '.period_engine.py',
                '@at_engine.py',
                '__init__.py',
                '_underscore_engine.py',
                'normal_engine.py',
                'compiled_engine.pyc',
                'optimized_engine.pyo',
            ))

        valid_engines = {
                '_underscore_engine':   ('UnderscoreEngine', 'underscore'),
                'normal_engine':        ('NormalEngine',     'normal'),
                'compiled_engine':      ('CompiledEngine',   'compiled'),
                'optimized_engine':     ('OptimizedEngine',  'optimized'),
            }

        result_dict = {handle: None for __, (__, handle) in valid_engines.items()}

        def mock_import_module(name, package):
            self.assertEqual(package, 'engines')

            assert name.startswith('.')
            name = name[1:]

            module = imp.new_module('%s.%s' % (package, name))

            if name == '__init__':
                module.Engine = engines.Engine
            else:
                self.assertIn(name, valid_engines)
                class_name, engine_handle = valid_engines[name]

                class MockEngine(engines.Engine):
                    handle = engine_handle

                MockEngine.__name__ = class_name
                MockEngine.__qualname__ = class_name

                result_dict[engine_handle] = MockEngine
                setattr(module, class_name, MockEngine)

            return module

        with mock.patch.object(engines, 'engines', mock_engines), \
             mock.patch('os.listdir', mock_listdir), \
             mock.patch('importlib.import_module', mock_import_module):
            engines._init()

        mock_listdir.assert_called_once_with(os.path.dirname(engines.__file__))
        self.assertDictEqual(mock_engines, result_dict)


class TestEngine(unittest.TestCase):

    def test_not_instantiable(self):
        self.assertRaises(AssertionError, engines.Engine, dirname='/tmp/', tolerant=False)

    def test_handle_is_none(self):
        class TestEngine(engines.Engine):
            pass

        engine = TestEngine(dirname='/tmp/', tolerant=False)

        self.assertIsNone(engine.handle)

    def test_apply_not_implemented(self):
        class TestEngine(engines.Engine):
            pass

        engine = TestEngine(dirname='/tmp/', tolerant=False)

        self.assertRaises(NotImplementedError, engine.apply, {})


if __name__ == '__main__':
    unittest.main()
",Add tests for engines package.,"Add tests for engines package.
",Python,mit,blubberdiblub/eztemplate,103,"```python
#!/usr/bin/env python

from __future__ import print_function

import unittest

try:
    from unittest import mock
except ImportError:
    import mock

import imp
import os.path


import engines


class TestInit(unittest.TestCase):

    def test_init(self):
        mock_engines = {}

        mock_listdir = mock.Mock(return_value=(
                'text_engine.txt',
                '0_engine.py',
                '.period_engine.py',
                '@at_engine.py',
                '__init__.py',
                '_underscore_engine.py',
                'normal_engine.py',
                'compiled_engine.pyc',
                'optimized_engine.pyo',
            ))

        valid_engines = {
                '_underscore_engine':   ('UnderscoreEngine', 'underscore'),
                'normal_engine':        ('NormalEngine',     'normal'),
                'compiled_engine':      ('CompiledEngine',   'compiled'),
                'optimized_engine':     ('OptimizedEngine',  'optimized'),
            }

        result_dict = {handle: None for __, (__, handle) in valid_engines.items()}

        def mock_import_module(name, package):
            self.assertEqual(package, 'engines')

            assert name.startswith('.')
            name = name[1:]

            module = imp.new_module('%s.%s' % (package, name))

            if name == '__init__':
                module.Engine = engines.Engine
            else:
                self.assertIn(name, valid_engines)
                class_name, engine_handle = valid_engines[name]

                class MockEngine(engines.Engine):
                    handle = engine_handle

                MockEngine.__name__ = class_name
                MockEngine.__qualname__ = class_name

                result_dict[engine_handle] = MockEngine
                setattr(module, class_name, MockEngine)

            return module

        with mock.patch.object(engines, 'engines', mock_engines), \
             mock.patch('os.listdir', mock_listdir), \
             mock.patch('importlib.import_module', mock_import_module):
            engines._init()

        mock_listdir.assert_called_once_with(os.path.dirname(engines.__file__))
        self.assertDictEqual(mock_engines, result_dict)


class TestEngine(unittest.TestCase):

    def test_not_instantiable(self):
        self.assertRaises(AssertionError, engines.Engine, dirname='/tmp/', tolerant=False)

    def test_handle_is_none(self):
        class TestEngine(engines.Engine):
            pass

        engine = TestEngine(dirname='/tmp/', tolerant=False)

        self.assertIsNone(engine.handle)

    def test_apply_not_implemented(self):
        class TestEngine(engines.Engine):
            pass

        engine = TestEngine(dirname='/tmp/', tolerant=False)

        self.assertRaises(NotImplementedError, engine.apply, {})


if __name__ == '__main__':
    unittest.main()

```"
d9a522df5827867897e4a2bbaf680db563fb983e,scripts/tile_images.py,scripts/tile_images.py,,"""""""Tile images.""""""

import os
import random
import argparse

from collections import defaultdict

import dtoolcore

import numpy as np

from jicbioimage.core.image import Image

from skimage.transform import downscale_local_mean

from dtoolutils import (
    temp_working_dir,
    stage_outputs
)

from image_utils import join_horizontally, join_vertically


def ensure_uri(path_or_uri):

    if ':' in path_or_uri:
        return path_or_uri
    else:
        return ""disk:{}"".format(path_or_uri)


def tile_plots(dataset, working_dir):

    is_jpeg_overlay = dataset.get_overlay(""is_jpeg"")
    date_overlay = dataset.get_overlay(""date"")
    ordering_overlay = dataset.get_overlay(""ordering"")

    dates = set(date_overlay.values())

    fnames = []
    dates = ['2016-06-28']

    # print [date_overlay[identifier] for identifier in dataset.identifiers]

    for date in dates:

        selected = {}
        for identifier in dataset.identifiers:
            if is_jpeg_overlay[identifier] and date_overlay[identifier] == date:
                try:
                    selected[ordering_overlay[identifier]] = identifier
                except TypeError:
                    pass

        print(selected.keys())

        def generate_column(numbers):
            images = []
            for i in numbers:
                i = selected[i]
                image_fpath = dataset.item_content_abspath(i)
                images.append(
                    downscale_local_mean(Image.from_file(image_fpath), (5, 5, 1))
                )

            column = join_horizontally(images)

            return column

        columns = []
        for o in range(12):
            numbers = range(59-o, 10-o, -12)
            columns.append(generate_column(numbers))

        tiled = join_vertically(columns)

        fname = 'tiled-{}.png'.format(date)
        output_fpath = os.path.join(working_dir, fname)
        with open(output_fpath, 'wb') as fh:
            fh.write(tiled.png())

        fnames.append((fname, {}))

    return fnames


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset-uri')
    parser.add_argument('--output-uri')

    args = parser.parse_args()

    dataset_uri = ensure_uri(args.dataset_uri)
    output_uri = ensure_uri(args.output_uri)
    dataset = dtoolcore.DataSet.from_uri(dataset_uri)
    output_dataset = dtoolcore.ProtoDataSet.from_uri(output_uri)

    with temp_working_dir() as working_dir:
        outputs = tile_plots(dataset, working_dir)
        stage_outputs(
            outputs,
            working_dir,
            dataset,
            output_dataset,
            [],
            None
        )

if __name__ == '__main__':
    main()
",Add script to tile images,"Add script to tile images
",Python,mit,"JIC-Image-Analysis/senescence-in-field,JIC-Image-Analysis/senescence-in-field,JIC-Image-Analysis/senescence-in-field",113,"```python
""""""Tile images.""""""

import os
import random
import argparse

from collections import defaultdict

import dtoolcore

import numpy as np

from jicbioimage.core.image import Image

from skimage.transform import downscale_local_mean

from dtoolutils import (
    temp_working_dir,
    stage_outputs
)

from image_utils import join_horizontally, join_vertically


def ensure_uri(path_or_uri):

    if ':' in path_or_uri:
        return path_or_uri
    else:
        return ""disk:{}"".format(path_or_uri)


def tile_plots(dataset, working_dir):

    is_jpeg_overlay = dataset.get_overlay(""is_jpeg"")
    date_overlay = dataset.get_overlay(""date"")
    ordering_overlay = dataset.get_overlay(""ordering"")

    dates = set(date_overlay.values())

    fnames = []
    dates = ['2016-06-28']

    # print [date_overlay[identifier] for identifier in dataset.identifiers]

    for date in dates:

        selected = {}
        for identifier in dataset.identifiers:
            if is_jpeg_overlay[identifier] and date_overlay[identifier] == date:
                try:
                    selected[ordering_overlay[identifier]] = identifier
                except TypeError:
                    pass

        print(selected.keys())

        def generate_column(numbers):
            images = []
            for i in numbers:
                i = selected[i]
                image_fpath = dataset.item_content_abspath(i)
                images.append(
                    downscale_local_mean(Image.from_file(image_fpath), (5, 5, 1))
                )

            column = join_horizontally(images)

            return column

        columns = []
        for o in range(12):
            numbers = range(59-o, 10-o, -12)
            columns.append(generate_column(numbers))

        tiled = join_vertically(columns)

        fname = 'tiled-{}.png'.format(date)
        output_fpath = os.path.join(working_dir, fname)
        with open(output_fpath, 'wb') as fh:
            fh.write(tiled.png())

        fnames.append((fname, {}))

    return fnames


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset-uri')
    parser.add_argument('--output-uri')

    args = parser.parse_args()

    dataset_uri = ensure_uri(args.dataset_uri)
    output_uri = ensure_uri(args.output_uri)
    dataset = dtoolcore.DataSet.from_uri(dataset_uri)
    output_dataset = dtoolcore.ProtoDataSet.from_uri(output_uri)

    with temp_working_dir() as working_dir:
        outputs = tile_plots(dataset, working_dir)
        stage_outputs(
            outputs,
            working_dir,
            dataset,
            output_dataset,
            [],
            None
        )

if __name__ == '__main__':
    main()

```"
9f5e11f789c01e3a6da0ff2c7376c4ead2741a6a,pyramid_authsanity/tests/test_includeme.py,pyramid_authsanity/tests/test_includeme.py,,"import pytest

from pyramid.authorization import ACLAuthorizationPolicy
import pyramid.testing

from zope.interface import (
    Interface,
    implementedBy,
    providedBy,
)

from zope.interface.verify import (
        verifyClass,
        verifyObject
        )

from pyramid_services import IServiceClassifier

from pyramid_authsanity.interfaces import (
        IAuthSourceService,
        )

class TestAuthServicePolicyIntegration(object):
    @pytest.fixture(autouse=True)
    def pyramid_config(self, request):
        from pyramid.interfaces import IDebugLogger
        self.config = pyramid.testing.setUp()
        self.config.set_authorization_policy(ACLAuthorizationPolicy())

        def finish():
            del self.config
            pyramid.testing.tearDown()
        request.addfinalizer(finish)

    def _makeOne(self, settings):
        self.config.registry.settings.update(settings)
        self.config.include('pyramid_authsanity')

    def test_include_me(self):
        from pyramid_authsanity.policy import AuthServicePolicy
        self._makeOne({})
        self.config.commit()
        introspector = self.config.registry.introspector
        auth_policy = introspector.get('authentication policy', None)

        assert isinstance(auth_policy['policy'], AuthServicePolicy)

        with pytest.raises(ValueError):
            find_service_factory(self.config, IAuthSourceService)

    def test_include_me_cookie_no_secret(self):
        settings = {'authsanity.source': 'cookie'}
        
        with pytest.raises(RuntimeError):
            self._makeOne(settings)

    def test_include_me_cookie_with_secret(self):
        from pyramid_authsanity.policy import AuthServicePolicy
        settings = {'authsanity.source': 'cookie', 'authsanity.secret': 'sekrit'}

        self._makeOne(settings)
        self.config.commit()
        introspector = self.config.registry.introspector
        auth_policy = introspector.get('authentication policy', None)

        assert isinstance(auth_policy['policy'], AuthServicePolicy)
        assert verifyClass(IAuthSourceService, find_service_factory(self.config, IAuthSourceService))

    def test_include_me_session(self):
        from pyramid_authsanity.policy import AuthServicePolicy
        settings = {'authsanity.source': 'session'}

        self._makeOne(settings)
        self.config.commit()
        introspector = self.config.registry.introspector
        auth_policy = introspector.get('authentication policy', None)

        assert isinstance(auth_policy['policy'], AuthServicePolicy)
        assert verifyClass(IAuthSourceService, find_service_factory(self.config, IAuthSourceService))


def find_service_factory(
    config,
    iface=Interface,
    ):

    context_iface = providedBy(None)
    svc_types = (IServiceClassifier, context_iface)

    adapters = config.registry.adapters
    svc_factory = adapters.lookup(svc_types, iface, name='')
    if svc_factory is None:
        raise ValueError('could not find registered service')
    return svc_factory
",Add tests for the includeme/settings,"Add tests for the includeme/settings
",Python,isc,usingnamespace/pyramid_authsanity,95,"```python
import pytest

from pyramid.authorization import ACLAuthorizationPolicy
import pyramid.testing

from zope.interface import (
    Interface,
    implementedBy,
    providedBy,
)

from zope.interface.verify import (
        verifyClass,
        verifyObject
        )

from pyramid_services import IServiceClassifier

from pyramid_authsanity.interfaces import (
        IAuthSourceService,
        )

class TestAuthServicePolicyIntegration(object):
    @pytest.fixture(autouse=True)
    def pyramid_config(self, request):
        from pyramid.interfaces import IDebugLogger
        self.config = pyramid.testing.setUp()
        self.config.set_authorization_policy(ACLAuthorizationPolicy())

        def finish():
            del self.config
            pyramid.testing.tearDown()
        request.addfinalizer(finish)

    def _makeOne(self, settings):
        self.config.registry.settings.update(settings)
        self.config.include('pyramid_authsanity')

    def test_include_me(self):
        from pyramid_authsanity.policy import AuthServicePolicy
        self._makeOne({})
        self.config.commit()
        introspector = self.config.registry.introspector
        auth_policy = introspector.get('authentication policy', None)

        assert isinstance(auth_policy['policy'], AuthServicePolicy)

        with pytest.raises(ValueError):
            find_service_factory(self.config, IAuthSourceService)

    def test_include_me_cookie_no_secret(self):
        settings = {'authsanity.source': 'cookie'}
        
        with pytest.raises(RuntimeError):
            self._makeOne(settings)

    def test_include_me_cookie_with_secret(self):
        from pyramid_authsanity.policy import AuthServicePolicy
        settings = {'authsanity.source': 'cookie', 'authsanity.secret': 'sekrit'}

        self._makeOne(settings)
        self.config.commit()
        introspector = self.config.registry.introspector
        auth_policy = introspector.get('authentication policy', None)

        assert isinstance(auth_policy['policy'], AuthServicePolicy)
        assert verifyClass(IAuthSourceService, find_service_factory(self.config, IAuthSourceService))

    def test_include_me_session(self):
        from pyramid_authsanity.policy import AuthServicePolicy
        settings = {'authsanity.source': 'session'}

        self._makeOne(settings)
        self.config.commit()
        introspector = self.config.registry.introspector
        auth_policy = introspector.get('authentication policy', None)

        assert isinstance(auth_policy['policy'], AuthServicePolicy)
        assert verifyClass(IAuthSourceService, find_service_factory(self.config, IAuthSourceService))


def find_service_factory(
    config,
    iface=Interface,
    ):

    context_iface = providedBy(None)
    svc_types = (IServiceClassifier, context_iface)

    adapters = config.registry.adapters
    svc_factory = adapters.lookup(svc_types, iface, name='')
    if svc_factory is None:
        raise ValueError('could not find registered service')
    return svc_factory

```"
e303425602e7535fb16d97a2f24a326791ef646d,tests/test_job.py,tests/test_job.py,,"import pytest
import multiprocessing
from copy import deepcopy
from pprint import pprint

import virtool.job


class TestProcessor:

    def test(self, test_job):
        """"""
        Test that the processor changes the ``_id`` field to ``job_id``.
         
        """"""
        processed = virtool.job.processor(deepcopy(test_job))

        test_job[""job_id""] = test_job.pop(""_id"")

        assert processed == test_job


class TestDispatchProcessor:

    def test(self, test_job):
        """"""
        Test that the dispatch processor properly formats a raw job document into a dispatchable format.
         
        """"""
        assert virtool.job.dispatch_processor(test_job) == {
            ""added"": ""2017-03-24T13:20:35.780926"",
            ""args"": {
                ""algorithm"": ""nuvs"",
                ""analysis_id"": ""e410429b"",
                ""index_id"": ""465428b0"",
                ""name"": None,
                ""sample_id"": ""1e01a382"",
                ""username"": ""igboyes""
            },
            ""job_id"": ""4c530449"",
            ""mem"": 16,
            ""proc"": 10,
            ""progress"": 1.0,
            ""stage"": ""import_results"",
            ""state"": ""complete"",
            ""task"": ""nuvs"",
            ""user_id"": ""igboyes""
        }


class TestJob:

    def test(self, mocker):
        job_id = ""foobar""

        settings = {
            ""db_name"": ""test"",
            ""db_host"": ""localhost"",
            ""db_port"": 27017
        }

        queue = multiprocessing.Queue()

        task = ""foobar""
        task_args = dict()
        proc = 1
        mem = 1

        m = mocker.patch(""setproctitle.setproctitle"")

        job = virtool.job.Job(job_id, settings, queue, task, task_args, proc, mem)

        assert job._job_id == job_id

        job.start()

        print(queue.get())
        print(queue.get())

        assert 0


class TestTermination:

    def test(self):
        """"""
        Test that the :class:`virtool.job.Termination` exception works properly.

        """"""
        with pytest.raises(virtool.job.Termination) as err:
            raise virtool.job.Termination

        assert ""Termination"" in str(err)


class TestJobError:

    def test(self):
        """"""
        Test that the :class:`virtool.job.JobError` exception works properly.
         
        """"""
        with pytest.raises(virtool.job.JobError) as err:
            raise virtool.job.JobError

        assert ""JobError"" in str(err)


class TestStageMethod:

    def test(self):
        """"""
        Test the the stage_method decorator adds a ``is_stage_method`` attribute to a function with a value of ``True``.
         
        """"""
        @virtool.job.stage_method
        def func():
            return ""Hello world""

        assert func.is_stage_method is True
",Add first job module tests,"Add first job module tests

",Python,mit,"igboyes/virtool,igboyes/virtool,virtool/virtool,virtool/virtool",121,"```python
import pytest
import multiprocessing
from copy import deepcopy
from pprint import pprint

import virtool.job


class TestProcessor:

    def test(self, test_job):
        """"""
        Test that the processor changes the ``_id`` field to ``job_id``.
         
        """"""
        processed = virtool.job.processor(deepcopy(test_job))

        test_job[""job_id""] = test_job.pop(""_id"")

        assert processed == test_job


class TestDispatchProcessor:

    def test(self, test_job):
        """"""
        Test that the dispatch processor properly formats a raw job document into a dispatchable format.
         
        """"""
        assert virtool.job.dispatch_processor(test_job) == {
            ""added"": ""2017-03-24T13:20:35.780926"",
            ""args"": {
                ""algorithm"": ""nuvs"",
                ""analysis_id"": ""e410429b"",
                ""index_id"": ""465428b0"",
                ""name"": None,
                ""sample_id"": ""1e01a382"",
                ""username"": ""igboyes""
            },
            ""job_id"": ""4c530449"",
            ""mem"": 16,
            ""proc"": 10,
            ""progress"": 1.0,
            ""stage"": ""import_results"",
            ""state"": ""complete"",
            ""task"": ""nuvs"",
            ""user_id"": ""igboyes""
        }


class TestJob:

    def test(self, mocker):
        job_id = ""foobar""

        settings = {
            ""db_name"": ""test"",
            ""db_host"": ""localhost"",
            ""db_port"": 27017
        }

        queue = multiprocessing.Queue()

        task = ""foobar""
        task_args = dict()
        proc = 1
        mem = 1

        m = mocker.patch(""setproctitle.setproctitle"")

        job = virtool.job.Job(job_id, settings, queue, task, task_args, proc, mem)

        assert job._job_id == job_id

        job.start()

        print(queue.get())
        print(queue.get())

        assert 0


class TestTermination:

    def test(self):
        """"""
        Test that the :class:`virtool.job.Termination` exception works properly.

        """"""
        with pytest.raises(virtool.job.Termination) as err:
            raise virtool.job.Termination

        assert ""Termination"" in str(err)


class TestJobError:

    def test(self):
        """"""
        Test that the :class:`virtool.job.JobError` exception works properly.
         
        """"""
        with pytest.raises(virtool.job.JobError) as err:
            raise virtool.job.JobError

        assert ""JobError"" in str(err)


class TestStageMethod:

    def test(self):
        """"""
        Test the the stage_method decorator adds a ``is_stage_method`` attribute to a function with a value of ``True``.
         
        """"""
        @virtool.job.stage_method
        def func():
            return ""Hello world""

        assert func.is_stage_method is True

```"
8c2aa6409358b4b5830e9a6242194030307f2aa6,makemigrations.py,makemigrations.py,,"#!/usr/bin/env python
import decimal
import os
import sys

import django

from django.conf import settings


DEFAULT_SETTINGS = dict(
    DEBUG=True,
    USE_TZ=True,
    TIME_ZONE='UTC',
    DATABASES={
        ""default"": {
            ""ENGINE"": ""django.db.backends.sqlite3"",
        }
    },
    MIDDLEWARE_CLASSES=[
        ""django.contrib.sessions.middleware.SessionMiddleware"",
        ""django.contrib.auth.middleware.AuthenticationMiddleware"",
        ""django.contrib.messages.middleware.MessageMiddleware""
    ],
    ROOT_URLCONF=""pinax.stripe.urls"",
    INSTALLED_APPS=[
        ""django.contrib.auth"",
        ""django.contrib.contenttypes"",
        ""django.contrib.sessions"",
        ""django.contrib.sites"",
        ""django_forms_bootstrap"",
        ""jsonfield"",
        ""pinax.stripe"",
    ],
    SITE_ID=1,
    PINAX_STRIPE_PUBLIC_KEY="""",
    PINAX_STRIPE_SECRET_KEY="""",
    PINAX_STRIPE_PLANS={
        ""free"": {
            ""name"": ""Free Plan""
        },
        ""entry"": {
            ""stripe_plan_id"": ""entry-monthly"",
            ""name"": ""Entry ($9.54/month)"",
            ""description"": ""The entry-level monthly subscription"",
            ""price"": 9.54,
            ""interval"": ""month"",
            ""currency"": ""usd""
        },
        ""pro"": {
            ""stripe_plan_id"": ""pro-monthly"",
            ""name"": ""Pro ($19.99/month)"",
            ""description"": ""The pro-level monthly subscription"",
            ""price"": 19.99,
            ""interval"": ""month"",
            ""currency"": ""usd""
        },
        ""premium"": {
            ""stripe_plan_id"": ""premium-monthly"",
            ""name"": ""Gold ($59.99/month)"",
            ""description"": ""The premium-level monthly subscription"",
            ""price"": decimal.Decimal(""59.99""),
            ""interval"": ""month"",
            ""currency"": ""usd""
        }
    },
    PINAX_STRIPE_SUBSCRIPTION_REQUIRED_EXCEPTION_URLS=[""pinax_stripe_subscribe""],
    PINAX_STRIPE_SUBSCRIPTION_REQUIRED_REDIRECT=""pinax_stripe_subscribe"",
    PINAX_STRIPE_HOOKSET=""pinax.stripe.tests.hooks.TestHookSet""
)


def run(*args):
    if not settings.configured:
        settings.configure(**DEFAULT_SETTINGS)

    django.setup()

    parent = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, parent)

    django.core.management.call_command(
        ""makemigrations"",
        ""pinax_stripe"",
        *args
    )


if __name__ == ""__main__"":
    run(*sys.argv[1:])
",Add a helper script for creating migrations,"Add a helper script for creating migrations
",Python,mit,pinax/django-stripe-payments,91,"```python
#!/usr/bin/env python
import decimal
import os
import sys

import django

from django.conf import settings


DEFAULT_SETTINGS = dict(
    DEBUG=True,
    USE_TZ=True,
    TIME_ZONE='UTC',
    DATABASES={
        ""default"": {
            ""ENGINE"": ""django.db.backends.sqlite3"",
        }
    },
    MIDDLEWARE_CLASSES=[
        ""django.contrib.sessions.middleware.SessionMiddleware"",
        ""django.contrib.auth.middleware.AuthenticationMiddleware"",
        ""django.contrib.messages.middleware.MessageMiddleware""
    ],
    ROOT_URLCONF=""pinax.stripe.urls"",
    INSTALLED_APPS=[
        ""django.contrib.auth"",
        ""django.contrib.contenttypes"",
        ""django.contrib.sessions"",
        ""django.contrib.sites"",
        ""django_forms_bootstrap"",
        ""jsonfield"",
        ""pinax.stripe"",
    ],
    SITE_ID=1,
    PINAX_STRIPE_PUBLIC_KEY="""",
    PINAX_STRIPE_SECRET_KEY="""",
    PINAX_STRIPE_PLANS={
        ""free"": {
            ""name"": ""Free Plan""
        },
        ""entry"": {
            ""stripe_plan_id"": ""entry-monthly"",
            ""name"": ""Entry ($9.54/month)"",
            ""description"": ""The entry-level monthly subscription"",
            ""price"": 9.54,
            ""interval"": ""month"",
            ""currency"": ""usd""
        },
        ""pro"": {
            ""stripe_plan_id"": ""pro-monthly"",
            ""name"": ""Pro ($19.99/month)"",
            ""description"": ""The pro-level monthly subscription"",
            ""price"": 19.99,
            ""interval"": ""month"",
            ""currency"": ""usd""
        },
        ""premium"": {
            ""stripe_plan_id"": ""premium-monthly"",
            ""name"": ""Gold ($59.99/month)"",
            ""description"": ""The premium-level monthly subscription"",
            ""price"": decimal.Decimal(""59.99""),
            ""interval"": ""month"",
            ""currency"": ""usd""
        }
    },
    PINAX_STRIPE_SUBSCRIPTION_REQUIRED_EXCEPTION_URLS=[""pinax_stripe_subscribe""],
    PINAX_STRIPE_SUBSCRIPTION_REQUIRED_REDIRECT=""pinax_stripe_subscribe"",
    PINAX_STRIPE_HOOKSET=""pinax.stripe.tests.hooks.TestHookSet""
)


def run(*args):
    if not settings.configured:
        settings.configure(**DEFAULT_SETTINGS)

    django.setup()

    parent = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, parent)

    django.core.management.call_command(
        ""makemigrations"",
        ""pinax_stripe"",
        *args
    )


if __name__ == ""__main__"":
    run(*sys.argv[1:])

```"
93642ed71a2a0ec8fe96858b20c319c061706f39,cleanup_repo.py,cleanup_repo.py,,"#!/usr/bin/env python

import subprocess
import os.path
import sys
import re

from contextlib import contextmanager

TAGS_RE = re.compile('.+/tags/(.+)')


def git(*args):
    p = subprocess.Popen(['git'] + list(args), stdout=subprocess.PIPE)
    output = p.communicate()[0]

    if p.returncode != 0:
        raise Exception('git failed, err code {}'.format(p.returncode))

    return output


@contextmanager
def chdir(path):
    cwd = os.path.abspath(os.getcwd())

    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(cwd)


def name_of(remote):
    return remote.rsplit('/', 1)[1]


def get_branches_and_tags():
    output = git('branch', '-r')

    branches, tags = [], []

    for line in output.splitlines():
        line = line.strip()
        m = TAGS_RE.match(line)

        t = tags if m is not None else branches
        t.append(line)

    return branches, tags


@contextmanager
def checkout(branch, remote=None):
    # if remote is not None -> create local branch from remote
    if remote is not None:
        git('checkout', '-b', branch, remote)
    else:
        git('checkout', branch)
    yield
    git('checkout', 'master')


def branchtag_to_tag(tag_name, remote_tag):
    with checkout(tag_name, remote_tag):
        pass

    git('tag', tag_name, tag_name)
    git('branch', '-D', tag_name)


def cleanup(repo):
    with chdir(repo):
        branches, tags = get_branches_and_tags()

        for branch in branches:
            # trunk is automatically remapped to master by git svn
            if name_of(branch) == 'trunk':
                continue

            with checkout(name_of(branch), branch):
                pass

        for tag in tags:
            branchtag_to_tag(name_of(tag), tag)


if __name__ == '__main__':
    cleanup(sys.argv[1])
",Add little script to do basic cleanup after git svn clone,"Add little script to do basic cleanup after git svn clone
",Python,mit,"develersrl/git-externals,develersrl/git-externals,develersrl/git-externals",90,"```python
#!/usr/bin/env python

import subprocess
import os.path
import sys
import re

from contextlib import contextmanager

TAGS_RE = re.compile('.+/tags/(.+)')


def git(*args):
    p = subprocess.Popen(['git'] + list(args), stdout=subprocess.PIPE)
    output = p.communicate()[0]

    if p.returncode != 0:
        raise Exception('git failed, err code {}'.format(p.returncode))

    return output


@contextmanager
def chdir(path):
    cwd = os.path.abspath(os.getcwd())

    try:
        os.chdir(path)
        yield
    finally:
        os.chdir(cwd)


def name_of(remote):
    return remote.rsplit('/', 1)[1]


def get_branches_and_tags():
    output = git('branch', '-r')

    branches, tags = [], []

    for line in output.splitlines():
        line = line.strip()
        m = TAGS_RE.match(line)

        t = tags if m is not None else branches
        t.append(line)

    return branches, tags


@contextmanager
def checkout(branch, remote=None):
    # if remote is not None -> create local branch from remote
    if remote is not None:
        git('checkout', '-b', branch, remote)
    else:
        git('checkout', branch)
    yield
    git('checkout', 'master')


def branchtag_to_tag(tag_name, remote_tag):
    with checkout(tag_name, remote_tag):
        pass

    git('tag', tag_name, tag_name)
    git('branch', '-D', tag_name)


def cleanup(repo):
    with chdir(repo):
        branches, tags = get_branches_and_tags()

        for branch in branches:
            # trunk is automatically remapped to master by git svn
            if name_of(branch) == 'trunk':
                continue

            with checkout(name_of(branch), branch):
                pass

        for tag in tags:
            branchtag_to_tag(name_of(tag), tag)


if __name__ == '__main__':
    cleanup(sys.argv[1])

```"
238c550d8131f3b35dae437182c924191ff08b72,tools/find_scan_roots.py,tools/find_scan_roots.py,,"#!/usr/bin/env python
# Copyright (C) 2018 The Android Open Source Project
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Takes output of
# adb shell 'find /data -print0 | xargs -0 ls -ldZ' | awk '{print $5 "" "" $9}'
# in standard input and generates list of directories we need to scan to cover
# the labels given on the command line.

import sys
import argparse

class Node(object):
  def __init__(self, name, label=None):
    self.name = name
    self.label = label
    self.marked = False
    self.children = {}

  def Find(self, components):
    if not components:
      return self

    child = components[0]
    if child in self.children:
      return self.children[child].Find(components[1:])

    n = Node(child)
    self.children[child] = n
    return n

  def __iter__(self):
    for child in self.children.itervalues():
      yield self.name + '/' + child.name, child
      for p, ch in child:
        yield self.name + '/' + p, ch

  def Mark(self, labels):
    # Either incorrect label or already marked, we do not need to scan
    # this path.
    if self.marked or self.label not in labels:
      return False

    self.marked = True

    for child in self.children.itervalues():
      child.Mark(labels)

    return True


def BuildTree(stream=sys.stdin):
  root = Node("""")

  for line in stream:
    line = line.strip()
    if not line:
      continue

    label, path = line.split(' ', 1)
    #  u:object_r:system_data_file:s0  -> system_data_file.
    sanitized_label = label.split(':')[2]
    # Strip leading slash.
    components = path[1:].split('/')

    n = root.Find(components)
    n.label = sanitized_label
  return root


def main():
  parser = argparse.ArgumentParser()
  parser.add_argument('labels', metavar='L', type=str, nargs='+',
                      help='labels we want to find')
  args = parser.parse_args()
  root = BuildTree()
  for fullpath, elem in root:
    if elem.Mark(args.labels):
      print fullpath


if __name__ == '__main__':
  main()
",Add tool to find scan roots.,"Add tool to find scan roots.

Bug: 73625480
Change-Id: I93b405feb999aaed9675d2ea52712663fa83c9e0",Python,apache-2.0,"google/perfetto,google/perfetto,google/perfetto,google/perfetto,google/perfetto,google/perfetto,google/perfetto,google/perfetto",95,"```python
#!/usr/bin/env python
# Copyright (C) 2018 The Android Open Source Project
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Takes output of
# adb shell 'find /data -print0 | xargs -0 ls -ldZ' | awk '{print $5 "" "" $9}'
# in standard input and generates list of directories we need to scan to cover
# the labels given on the command line.

import sys
import argparse

class Node(object):
  def __init__(self, name, label=None):
    self.name = name
    self.label = label
    self.marked = False
    self.children = {}

  def Find(self, components):
    if not components:
      return self

    child = components[0]
    if child in self.children:
      return self.children[child].Find(components[1:])

    n = Node(child)
    self.children[child] = n
    return n

  def __iter__(self):
    for child in self.children.itervalues():
      yield self.name + '/' + child.name, child
      for p, ch in child:
        yield self.name + '/' + p, ch

  def Mark(self, labels):
    # Either incorrect label or already marked, we do not need to scan
    # this path.
    if self.marked or self.label not in labels:
      return False

    self.marked = True

    for child in self.children.itervalues():
      child.Mark(labels)

    return True


def BuildTree(stream=sys.stdin):
  root = Node("""")

  for line in stream:
    line = line.strip()
    if not line:
      continue

    label, path = line.split(' ', 1)
    #  u:object_r:system_data_file:s0  -> system_data_file.
    sanitized_label = label.split(':')[2]
    # Strip leading slash.
    components = path[1:].split('/')

    n = root.Find(components)
    n.label = sanitized_label
  return root


def main():
  parser = argparse.ArgumentParser()
  parser.add_argument('labels', metavar='L', type=str, nargs='+',
                      help='labels we want to find')
  args = parser.parse_args()
  root = BuildTree()
  for fullpath, elem in root:
    if elem.Mark(args.labels):
      print fullpath


if __name__ == '__main__':
  main()

```"
0f39968ee0cfd4b38a599b6fc9dc6d8369392513,benchmarks/benchmark3.py,benchmarks/benchmark3.py,,"from time import clock
from random import choice, randint, seed
from sys import stdout

import ahocorasick


def write(str):
    stdout.write(str)
    stdout.flush()


def writeln(str):
    stdout.write(str)
    stdout.write('\n')


class ElapsedTime:
    def __init__(self, msg):
        self.msg = msg

    def __enter__(self):
        write(""%-40s: "" % self.msg)
        self.start = clock()

    def __exit__(self, a1, a2, a3):
        self.stop = clock()
        writeln(""%0.3f s"" % self.get_time())

    def get_time(self):
        return self.stop - self.start


class Test:

    def __init__(self, max_word_length, count):
        self.min_word_length = 3
        self.max_word_length = max_word_length
        self.count = count
        self.words = []
        self.inexisting = []
        self.input = """"

        self.automaton = None
        seed(0) # make sure that tests will be repeatable

    def init_data(self):
        
        def random_word(length):
            chars = ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789""
            return ''.join(choice(chars) for _ in range(length))

        for i in range(self.count):
            length = randint(self.min_word_length, self.max_word_length)
            self.words.append(random_word(length))

            length = randint(self.min_word_length, self.max_word_length)
            self.inexisting.append(random_word(length))


        self.input = random_word(self.count)

        assert(len(self.words) == len(self.inexisting))

    def add_words(self):
        
        self.automaton = ahocorasick.Automaton()
        A = self.automaton
        for word in self.words:
            A.add_word(word, word)

    def build(self):
        
        self.automaton.make_automaton()

    def lookup(self):

        n = len(self.words)

        A = self.automaton
        for i in range(n):
            A.get(self.words[i])
            A.get(self.inexisting[i], ""unknown"")
    

    def search(self):

        A = self.automaton
        n = 0
        for item in A.iter(self.input):
            n += 1


    def run(self):
        
        with ElapsedTime(""Generating data (%d words)"" % self.count):
            self.init_data()
        
        with ElapsedTime(""Add words""):
            self.add_words()
        
        with ElapsedTime(""Building automaton""):
            self.build()

        with ElapsedTime(""Look up""):
            self.lookup()

        with ElapsedTime(""Search""):
            self.search()

def main():
    
    test = Test(32, 1000000)
    test.run()

if __name__ == '__main__':
    main()
",Add benchmark script for python 3,"Add benchmark script for python 3
",Python,bsd-3-clause,"WojciechMula/pyahocorasick,WojciechMula/pyahocorasick,pombredanne/pyahocorasick,WojciechMula/pyahocorasick,pombredanne/pyahocorasick,pombredanne/pyahocorasick,pombredanne/pyahocorasick,WojciechMula/pyahocorasick,woakesd/pyahocorasick,woakesd/pyahocorasick",118,"```python
from time import clock
from random import choice, randint, seed
from sys import stdout

import ahocorasick


def write(str):
    stdout.write(str)
    stdout.flush()


def writeln(str):
    stdout.write(str)
    stdout.write('\n')


class ElapsedTime:
    def __init__(self, msg):
        self.msg = msg

    def __enter__(self):
        write(""%-40s: "" % self.msg)
        self.start = clock()

    def __exit__(self, a1, a2, a3):
        self.stop = clock()
        writeln(""%0.3f s"" % self.get_time())

    def get_time(self):
        return self.stop - self.start


class Test:

    def __init__(self, max_word_length, count):
        self.min_word_length = 3
        self.max_word_length = max_word_length
        self.count = count
        self.words = []
        self.inexisting = []
        self.input = """"

        self.automaton = None
        seed(0) # make sure that tests will be repeatable

    def init_data(self):
        
        def random_word(length):
            chars = ""abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789""
            return ''.join(choice(chars) for _ in range(length))

        for i in range(self.count):
            length = randint(self.min_word_length, self.max_word_length)
            self.words.append(random_word(length))

            length = randint(self.min_word_length, self.max_word_length)
            self.inexisting.append(random_word(length))


        self.input = random_word(self.count)

        assert(len(self.words) == len(self.inexisting))

    def add_words(self):
        
        self.automaton = ahocorasick.Automaton()
        A = self.automaton
        for word in self.words:
            A.add_word(word, word)

    def build(self):
        
        self.automaton.make_automaton()

    def lookup(self):

        n = len(self.words)

        A = self.automaton
        for i in range(n):
            A.get(self.words[i])
            A.get(self.inexisting[i], ""unknown"")
    

    def search(self):

        A = self.automaton
        n = 0
        for item in A.iter(self.input):
            n += 1


    def run(self):
        
        with ElapsedTime(""Generating data (%d words)"" % self.count):
            self.init_data()
        
        with ElapsedTime(""Add words""):
            self.add_words()
        
        with ElapsedTime(""Building automaton""):
            self.build()

        with ElapsedTime(""Look up""):
            self.lookup()

        with ElapsedTime(""Search""):
            self.search()

def main():
    
    test = Test(32, 1000000)
    test.run()

if __name__ == '__main__':
    main()

```"
2c5dbf8fb147b9d9494de5f8abff52aeedf1cabf,src/bilor/core/handler.py,src/bilor/core/handler.py,,"import datetime
import logging
import sys
import traceback

import requests

from werkzeug.debug.tbtools import get_current_traceback


class BilorHandler(logging.Handler):
    """"""Logging handler for bilor.

    Based on raven.
    """"""

    def __init__(self, host, *args, **kwargs):
        super(BilorHandler, self).__init__(level=kwargs.get('level', logging.NOTSET))
        self.host = host

    def can_record(self, record):
        return not (
            record.name == 'bilor' or
            record.name.startswith('bilor.')
        )

    def emit(self, record):
        try:
            self.format(record)

            if not self.can_record(record):
                print(to_string(record.message), file=sys.stderr)
                return

            return self._emit(record)
        except Exception:
            if self.client.raise_send_errors:
                raise
            print(""Top level Bilor exception caught - failed creating log record"", file=sys.stderr)
            print(to_string(record.msg), file=sys.stderr)
            print(to_string(traceback.format_exc()), file=sys.stderr)

    def _emit(self, record, **kwargs):
        data = {}

        extra = getattr(record, 'data', None)
        if not isinstance(extra, dict):
            if extra:
                extra = {'data': extra}
            else:
                extra = {}

        stack = getattr(record, 'stack', None)
        if stack is True:
            stack = get_current_traceback(skip=1)

        date = datetime.datetime.utcfromtimestamp(record.created)
        handler_kwargs = {
            'params': record.args,
        }
        try:
            handler_kwargs['message'] = six.text_type(record.msg)
        except UnicodeDecodeError:
            # Handle binary strings where it should be unicode...
            handler_kwargs['message'] = repr(record.msg)[1:-1]

        try:
            handler_kwargs['formatted'] = six.text_type(record.message)
        except UnicodeDecodeError:
            # Handle binary strings where it should be unicode...
            handler_kwargs['formatted'] = repr(record.message)[1:-1]

        # If there's no exception being processed, exc_info may be a 3-tuple of None
        # http://docs.python.org/library/sys.html#sys.exc_info
        if record.exc_info and all(record.exc_info):
            # capture the standard message first so that we ensure
            # the event is recorded as an exception, in addition to having our
            # message interface attached
            handler = self.client.get_handler(event_type)
            data.update(handler.capture(**handler_kwargs))

            handler_kwargs = {'exc_info': record.exc_info}

        data['level'] = record.levelno
        data['logger'] = record.name

        if hasattr(record, 'tags'):
            kwargs['tags'] = record.tags

        kwargs.update(handler_kwargs)

        #requests.post()
","Add temporary, not yet working client","Add temporary, not yet working client
",Python,bsd-3-clause,"EnTeQuAk/bilor,EnTeQuAk/bilor,EnTeQuAk/bilor,EnTeQuAk/bilor",93,"```python
import datetime
import logging
import sys
import traceback

import requests

from werkzeug.debug.tbtools import get_current_traceback


class BilorHandler(logging.Handler):
    """"""Logging handler for bilor.

    Based on raven.
    """"""

    def __init__(self, host, *args, **kwargs):
        super(BilorHandler, self).__init__(level=kwargs.get('level', logging.NOTSET))
        self.host = host

    def can_record(self, record):
        return not (
            record.name == 'bilor' or
            record.name.startswith('bilor.')
        )

    def emit(self, record):
        try:
            self.format(record)

            if not self.can_record(record):
                print(to_string(record.message), file=sys.stderr)
                return

            return self._emit(record)
        except Exception:
            if self.client.raise_send_errors:
                raise
            print(""Top level Bilor exception caught - failed creating log record"", file=sys.stderr)
            print(to_string(record.msg), file=sys.stderr)
            print(to_string(traceback.format_exc()), file=sys.stderr)

    def _emit(self, record, **kwargs):
        data = {}

        extra = getattr(record, 'data', None)
        if not isinstance(extra, dict):
            if extra:
                extra = {'data': extra}
            else:
                extra = {}

        stack = getattr(record, 'stack', None)
        if stack is True:
            stack = get_current_traceback(skip=1)

        date = datetime.datetime.utcfromtimestamp(record.created)
        handler_kwargs = {
            'params': record.args,
        }
        try:
            handler_kwargs['message'] = six.text_type(record.msg)
        except UnicodeDecodeError:
            # Handle binary strings where it should be unicode...
            handler_kwargs['message'] = repr(record.msg)[1:-1]

        try:
            handler_kwargs['formatted'] = six.text_type(record.message)
        except UnicodeDecodeError:
            # Handle binary strings where it should be unicode...
            handler_kwargs['formatted'] = repr(record.message)[1:-1]

        # If there's no exception being processed, exc_info may be a 3-tuple of None
        # http://docs.python.org/library/sys.html#sys.exc_info
        if record.exc_info and all(record.exc_info):
            # capture the standard message first so that we ensure
            # the event is recorded as an exception, in addition to having our
            # message interface attached
            handler = self.client.get_handler(event_type)
            data.update(handler.capture(**handler_kwargs))

            handler_kwargs = {'exc_info': record.exc_info}

        data['level'] = record.levelno
        data['logger'] = record.name

        if hasattr(record, 'tags'):
            kwargs['tags'] = record.tags

        kwargs.update(handler_kwargs)

        #requests.post()

```"
a1a552857498206b6684681eb457978dd5adf710,nap/rest/mapper.py,nap/rest/mapper.py,,"'''
Mixins for using Mappers with Publisher
'''
from django.core.exceptions import ValidationError

from nap import http
from nap.utils import flatten_errors


class MapperListMixin(object):

    def list_get_default(self, request, action, object_id):
        '''
        Replace the default list handler with one that returns a flat list.
        '''
        object_list = self.get_object_list()
        object_list = self.filter_object_list(object_list)
        object_list = self.sort_object_list(object_list)

        mapper = self.mapper()

        data = [
            mapper << obj
            for obj in object_list
        ]
        return self.create_response(data)


class MapperDetailMixin(object):

    def object_get_default(self, request, action, object_id):
        obj = self.get_object(object_id)

        mapper = self.mapper(obj)

        return self.create_response(mapper._reduce())


class MapperPostMixin(object):
    '''
    Generic handling of POST-to-create
    '''
    def list_post_default(self, request, action, object_id):
        data = self.get_request_data({})

        mapper = self.mapper(self.model())
        try:
            obj = mapper._apply(data, full=True)
        except ValidationError as e:
            return self.post_invalid(e.error_dict)
        else:
            return self.post_valid(obj)

    def post_valid(self, obj):
        obj.save()
        return http.Created()

    def post_invalid(self, errors):
        return self.create_response(flatten_errors(errors),
            response_class=http.BadRequest)


class PutMixin(object):
    '''
    Generic handling of PUT-to-update
    '''
    def object_put_default(self, request, action, object_id):
        data = self.get_request_data({})

        obj = self.get_object(object_id)

        mapper = self.mapper(obj)
        try:
            mapper._apply(data)
        except ValidationError as e:
            return self.put_invalid(obj, e.error_dict)

        return self.put_valid(obj, data)

        return http.Accepted()

    def put_valid(self, obj, data):
        '''
        Hook to control updating of objects.

        Will be passes the unsaved updated model instance.
        Default: save the object.
        '''
        obj.save()
        return obj

    def put_invalid(self, obj, errors):
        return self.create_response(flatten_errors(errors),
            response_class=http.BadRequest)


class DeleteMixin(object):
    '''
    Generic handling of DELETE-to-disable
    '''
    def object_delete_default(self, request, action, object_id):
        obj = self.get_object(object_id)

        self.delete_object(obj)
        return http.ResetContent()

    def delete_object(self, obj):
        '''
        Hook to allow control of how to delete an instance.
        '''
        obj.delete()
",Add Publisher mixins for using Mappers,"Add Publisher mixins for using Mappers
",Python,bsd-3-clause,"limbera/django-nap,MarkusH/django-nap",112,"```python
'''
Mixins for using Mappers with Publisher
'''
from django.core.exceptions import ValidationError

from nap import http
from nap.utils import flatten_errors


class MapperListMixin(object):

    def list_get_default(self, request, action, object_id):
        '''
        Replace the default list handler with one that returns a flat list.
        '''
        object_list = self.get_object_list()
        object_list = self.filter_object_list(object_list)
        object_list = self.sort_object_list(object_list)

        mapper = self.mapper()

        data = [
            mapper << obj
            for obj in object_list
        ]
        return self.create_response(data)


class MapperDetailMixin(object):

    def object_get_default(self, request, action, object_id):
        obj = self.get_object(object_id)

        mapper = self.mapper(obj)

        return self.create_response(mapper._reduce())


class MapperPostMixin(object):
    '''
    Generic handling of POST-to-create
    '''
    def list_post_default(self, request, action, object_id):
        data = self.get_request_data({})

        mapper = self.mapper(self.model())
        try:
            obj = mapper._apply(data, full=True)
        except ValidationError as e:
            return self.post_invalid(e.error_dict)
        else:
            return self.post_valid(obj)

    def post_valid(self, obj):
        obj.save()
        return http.Created()

    def post_invalid(self, errors):
        return self.create_response(flatten_errors(errors),
            response_class=http.BadRequest)


class PutMixin(object):
    '''
    Generic handling of PUT-to-update
    '''
    def object_put_default(self, request, action, object_id):
        data = self.get_request_data({})

        obj = self.get_object(object_id)

        mapper = self.mapper(obj)
        try:
            mapper._apply(data)
        except ValidationError as e:
            return self.put_invalid(obj, e.error_dict)

        return self.put_valid(obj, data)

        return http.Accepted()

    def put_valid(self, obj, data):
        '''
        Hook to control updating of objects.

        Will be passes the unsaved updated model instance.
        Default: save the object.
        '''
        obj.save()
        return obj

    def put_invalid(self, obj, errors):
        return self.create_response(flatten_errors(errors),
            response_class=http.BadRequest)


class DeleteMixin(object):
    '''
    Generic handling of DELETE-to-disable
    '''
    def object_delete_default(self, request, action, object_id):
        obj = self.get_object(object_id)

        self.delete_object(obj)
        return http.ResetContent()

    def delete_object(self, obj):
        '''
        Hook to allow control of how to delete an instance.
        '''
        obj.delete()

```"
1663cb8557de85b1f3ccf5822fa01758e679ccd7,TestScript/multi_client.py,TestScript/multi_client.py,,"# -*- coding: utf-8 -*-
__author__ = 'sm9'

import asyncore, socket
import string, random
import struct, time

HOST = '192.168.0.11'
PORT = 9001


PKT_CS_LOGIN = 1
PKT_SC_LOGIN = 2
PKT_CS_CHAT	 = 3
PKT_SC_CHAT	 = 4


def str_generator(size=128, chars=string.ascii_uppercase + string.digits):
    return ''.join(random.choice(chars) for x in range(size))


class PacketDecoder(object):

    def __init__(self):
        self._stream = ''

    def feed(self, buff):
        self._stream += buff

    def decode(self):

        while len(self._stream) > 4:
            pkt_size = struct.unpack('h', self._stream[:2])[0]
            if pkt_size > len(self._stream):
                break

            packet = self._stream[:pkt_size]
            yield packet
            self._stream = self._stream[pkt_size:]



class Client(asyncore.dispatcher):

    def __init__(self, pid):
        asyncore.dispatcher.__init__(self)
        self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
        self.connect( (HOST, PORT) )
        self.decoder = PacketDecoder()
        self.islogin = False
        self.pid = pid
        self.buffer = struct.pack('hhi', 8, PKT_CS_LOGIN, pid)


    def handle_connect(self):
        pass

    def handle_close(self):
        self.close()

    def handle_read(self):
        data = self.recv(8192)
        if not data:
            #print 'NOT DATA'
            self.close()
        else:
            self.decoder.feed(data)
            for packet in self.decoder.decode():
                self.process(packet)

            if self.islogin:
                self.buffer = struct.pack('<hhi1024s', 1032, PKT_CS_CHAT, self.pid, str_generator(1000))

    def writable(self):
        return (len(self.buffer) > 0)

    def handle_write(self):
        sent = self.send(self.buffer)
        self.buffer = self.buffer[sent:]

    def process(self, packet):
        pkt_len, pkt_type = struct.unpack('hh', packet[:4])

        if pkt_type == PKT_SC_LOGIN:
            self.islogin = True
            playerId, posX, posY, posZ, playerName = struct.unpack('<iddd30s', packet[4:])
            print playerId, posX, posY, posZ, playerName

        elif pkt_type == PKT_SC_CHAT:
            playerId, playerName, chat = struct.unpack('<i30s1024s', packet[4:])
            #print playerId, playerName, chat
        else:
            print 'PKT_TYPE ERROR'


for pid in range(1000, 1100):
    client = Client(pid)
    time.sleep(0.02)

asyncore.loop()
",Test script for the stress test,*added: Test script for the stress test,Python,mit,"zrma/EasyGameServer,zeliard/EasyGameServer,zeliard/EasyGameServer,zrma/EasyGameServer,Lt-Red/EasyGameServer,Lt-Red/EasyGameServer",101,"```python
# -*- coding: utf-8 -*-
__author__ = 'sm9'

import asyncore, socket
import string, random
import struct, time

HOST = '192.168.0.11'
PORT = 9001


PKT_CS_LOGIN = 1
PKT_SC_LOGIN = 2
PKT_CS_CHAT	 = 3
PKT_SC_CHAT	 = 4


def str_generator(size=128, chars=string.ascii_uppercase + string.digits):
    return ''.join(random.choice(chars) for x in range(size))


class PacketDecoder(object):

    def __init__(self):
        self._stream = ''

    def feed(self, buff):
        self._stream += buff

    def decode(self):

        while len(self._stream) > 4:
            pkt_size = struct.unpack('h', self._stream[:2])[0]
            if pkt_size > len(self._stream):
                break

            packet = self._stream[:pkt_size]
            yield packet
            self._stream = self._stream[pkt_size:]



class Client(asyncore.dispatcher):

    def __init__(self, pid):
        asyncore.dispatcher.__init__(self)
        self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
        self.connect( (HOST, PORT) )
        self.decoder = PacketDecoder()
        self.islogin = False
        self.pid = pid
        self.buffer = struct.pack('hhi', 8, PKT_CS_LOGIN, pid)


    def handle_connect(self):
        pass

    def handle_close(self):
        self.close()

    def handle_read(self):
        data = self.recv(8192)
        if not data:
            #print 'NOT DATA'
            self.close()
        else:
            self.decoder.feed(data)
            for packet in self.decoder.decode():
                self.process(packet)

            if self.islogin:
                self.buffer = struct.pack('<hhi1024s', 1032, PKT_CS_CHAT, self.pid, str_generator(1000))

    def writable(self):
        return (len(self.buffer) > 0)

    def handle_write(self):
        sent = self.send(self.buffer)
        self.buffer = self.buffer[sent:]

    def process(self, packet):
        pkt_len, pkt_type = struct.unpack('hh', packet[:4])

        if pkt_type == PKT_SC_LOGIN:
            self.islogin = True
            playerId, posX, posY, posZ, playerName = struct.unpack('<iddd30s', packet[4:])
            print playerId, posX, posY, posZ, playerName

        elif pkt_type == PKT_SC_CHAT:
            playerId, playerName, chat = struct.unpack('<i30s1024s', packet[4:])
            #print playerId, playerName, chat
        else:
            print 'PKT_TYPE ERROR'


for pid in range(1000, 1100):
    client = Client(pid)
    time.sleep(0.02)

asyncore.loop()

```"
6af695bdceaabdd4144def6a11f930e9d02a2d5c,src/assistant_library_with_local_commands_demo.py,src/assistant_library_with_local_commands_demo.py,,"#!/usr/bin/env python3
# Copyright 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Run a recognizer using the Google Assistant Library.

The Google Assistant Library has direct access to the audio API, so this Python
code doesn't need to record audio. Hot word detection ""OK, Google"" is supported.

The Google Assistant Library can be installed with:
    env/bin/pip install google-assistant-library==0.0.2

It is available for Raspberry Pi 2/3 only; Pi Zero is not supported.
""""""

import logging
import subprocess
import sys

import aiy.assistant.auth_helpers
import aiy.audio
import aiy.voicehat
from google.assistant.library import Assistant
from google.assistant.library.event import EventType

logging.basicConfig(
    level=logging.INFO,
    format=""[%(asctime)s] %(levelname)s:%(name)s:%(message)s""
)


def power_off_pi():
	aiy.audio.say('Good bye!')
	subprocess.call('sudo shutdown now', shell=True)


def reboot_pi():
	aiy.audio.say('See you in a bit!')
	subprocess.call('sudo reboot', shell=True)


def process_event(assistant, event):
    status_ui = aiy.voicehat.get_status_ui()
    if event.type == EventType.ON_START_FINISHED:
        status_ui.status('ready')
        if sys.stdout.isatty():
            print('Say ""OK, Google"" then speak, or press Ctrl+C to quit...')

    elif event.type == EventType.ON_CONVERSATION_TURN_STARTED:
        status_ui.status('listening')
        
    elif event.type == EventType.ON_RECOGNIZING_SPEECH_FINISHED and event.args:
        text = event.args['text']
        print('You said:', text)
        if text == 'power off':
            assistant.stop_conversation()
            power_off_pi()
        elif text == 'reboot':
            assistant.stop_conversation()
            reboot_pi()

    elif event.type == EventType.ON_END_OF_UTTERANCE:
        status_ui.status('thinking')

    elif event.type == EventType.ON_CONVERSATION_TURN_FINISHED:
        status_ui.status('ready')

    elif event.type == EventType.ON_ASSISTANT_ERROR and event.args and event.args['is_fatal']:
        sys.exit(1)


def main():
    credentials = aiy.assistant.auth_helpers.get_assistant_credentials()
    with Assistant(credentials) as assistant:
        for event in assistant.start():
            process_event(assistant, event)


if __name__ == '__main__':
    main()
",Add a demo to showcase how to handle local commands with Assistant Library.,"Add a demo to showcase how to handle local commands with Assistant Library.
",Python,apache-2.0,"google/aiyprojects-raspbian,t1m0thyj/aiyprojects-raspbian,google/aiyprojects-raspbian,google/aiyprojects-raspbian,t1m0thyj/aiyprojects-raspbian,google/aiyprojects-raspbian,google/aiyprojects-raspbian",92,"```python
#!/usr/bin/env python3
# Copyright 2017 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Run a recognizer using the Google Assistant Library.

The Google Assistant Library has direct access to the audio API, so this Python
code doesn't need to record audio. Hot word detection ""OK, Google"" is supported.

The Google Assistant Library can be installed with:
    env/bin/pip install google-assistant-library==0.0.2

It is available for Raspberry Pi 2/3 only; Pi Zero is not supported.
""""""

import logging
import subprocess
import sys

import aiy.assistant.auth_helpers
import aiy.audio
import aiy.voicehat
from google.assistant.library import Assistant
from google.assistant.library.event import EventType

logging.basicConfig(
    level=logging.INFO,
    format=""[%(asctime)s] %(levelname)s:%(name)s:%(message)s""
)


def power_off_pi():
	aiy.audio.say('Good bye!')
	subprocess.call('sudo shutdown now', shell=True)


def reboot_pi():
	aiy.audio.say('See you in a bit!')
	subprocess.call('sudo reboot', shell=True)


def process_event(assistant, event):
    status_ui = aiy.voicehat.get_status_ui()
    if event.type == EventType.ON_START_FINISHED:
        status_ui.status('ready')
        if sys.stdout.isatty():
            print('Say ""OK, Google"" then speak, or press Ctrl+C to quit...')

    elif event.type == EventType.ON_CONVERSATION_TURN_STARTED:
        status_ui.status('listening')
        
    elif event.type == EventType.ON_RECOGNIZING_SPEECH_FINISHED and event.args:
        text = event.args['text']
        print('You said:', text)
        if text == 'power off':
            assistant.stop_conversation()
            power_off_pi()
        elif text == 'reboot':
            assistant.stop_conversation()
            reboot_pi()

    elif event.type == EventType.ON_END_OF_UTTERANCE:
        status_ui.status('thinking')

    elif event.type == EventType.ON_CONVERSATION_TURN_FINISHED:
        status_ui.status('ready')

    elif event.type == EventType.ON_ASSISTANT_ERROR and event.args and event.args['is_fatal']:
        sys.exit(1)


def main():
    credentials = aiy.assistant.auth_helpers.get_assistant_credentials()
    with Assistant(credentials) as assistant:
        for event in assistant.start():
            process_event(assistant, event)


if __name__ == '__main__':
    main()

```"
49046745e9000281c4cf225836ad988b04a59517,judge/migrations/0112_language_extensions.py,judge/migrations/0112_language_extensions.py,,"from django.db import migrations


def update_language_extensions(apps, schema_editor):
    Language = apps.get_model('judge', 'Language')

    extension_mapping = {
        'ADA': 'adb',
        'AWK': 'awk',
        'BASH': 'sh',
        'BF': 'c',
        'C': 'c',
        'C11': 'c',
        'CBL': 'cbl',
        'CLANG': 'c',
        'CLANGX': 'cpp',
        'COFFEE': 'coffee',
        'CPP03': 'cpp',
        'CPP11': 'cpp',
        'CPP14': 'cpp',
        'CPP17': 'cpp',
        'D': 'd',
        'DART': 'dart',
        'F95': 'f95',
        'FORTH': 'fs',
        'GAS32': 'asm',
        'GAS64': 'asm',
        'GASARM': 'asm',
        'GO': 'go',
        'GROOVY': 'groovy',
        'HASK': 'hs',
        'ICK': 'i',
        'JAVA10': 'java',
        'JAVA11': 'java',
        'JAVA8': 'java',
        'JAVA9': 'java',
        'KOTLIN': 'kt',
        'LUA': 'lua',
        'MONOCS': 'cs',
        'MONOFS': 'fs',
        'MONOVB': 'vb',
        'NASM': 'asm',
        'NASM64': 'asm',
        'OBJC': 'm',
        'OCAML': 'ml',
        'PAS': 'pas',
        'PERL': 'pl',
        'PHP': 'php',
        'PIKE': 'pike',
        'PRO': 'pl',
        'PY2': 'py',
        'PY3': 'py',
        'PYPY': 'py',
        'PYPY3': 'py',
        'RKT': 'rkt',
        'RUBY18': 'rb',
        'RUBY2': 'rb',
        'RUST': 'rs',
        'SBCL': 'cl',
        'SCALA': 'scala',
        'SCM': 'scm',
        'SED': 'sed',
        'SWIFT': 'swift',
        'TCL': 'tcl',
        'TEXT': 'txt',
        'TUR': 't',
        'V8JS': 'js',
        'ZIG': 'zig',
    }

    languages = Language.objects.all()
    for language in languages:
        try:
            extension = extension_mapping[language.key]
        except KeyError:
            print('Warning: no extension found for %s. Setting extension to language key.' % language.key)
            extension = language.key.lower()

        language.extension = extension

    Language.objects.bulk_update(languages, ['extension'])


class Migration(migrations.Migration):

    dependencies = [
        ('judge', '0111_blank_assignees_ticket'),
    ]

    operations = [
        migrations.RunPython(update_language_extensions, reverse_code=migrations.RunPython.noop),
    ]
",Add migration to update Language extensions,"Add migration to update Language extensions
",Python,agpl-3.0,"DMOJ/site,DMOJ/site,DMOJ/site,DMOJ/site",93,"```python
from django.db import migrations


def update_language_extensions(apps, schema_editor):
    Language = apps.get_model('judge', 'Language')

    extension_mapping = {
        'ADA': 'adb',
        'AWK': 'awk',
        'BASH': 'sh',
        'BF': 'c',
        'C': 'c',
        'C11': 'c',
        'CBL': 'cbl',
        'CLANG': 'c',
        'CLANGX': 'cpp',
        'COFFEE': 'coffee',
        'CPP03': 'cpp',
        'CPP11': 'cpp',
        'CPP14': 'cpp',
        'CPP17': 'cpp',
        'D': 'd',
        'DART': 'dart',
        'F95': 'f95',
        'FORTH': 'fs',
        'GAS32': 'asm',
        'GAS64': 'asm',
        'GASARM': 'asm',
        'GO': 'go',
        'GROOVY': 'groovy',
        'HASK': 'hs',
        'ICK': 'i',
        'JAVA10': 'java',
        'JAVA11': 'java',
        'JAVA8': 'java',
        'JAVA9': 'java',
        'KOTLIN': 'kt',
        'LUA': 'lua',
        'MONOCS': 'cs',
        'MONOFS': 'fs',
        'MONOVB': 'vb',
        'NASM': 'asm',
        'NASM64': 'asm',
        'OBJC': 'm',
        'OCAML': 'ml',
        'PAS': 'pas',
        'PERL': 'pl',
        'PHP': 'php',
        'PIKE': 'pike',
        'PRO': 'pl',
        'PY2': 'py',
        'PY3': 'py',
        'PYPY': 'py',
        'PYPY3': 'py',
        'RKT': 'rkt',
        'RUBY18': 'rb',
        'RUBY2': 'rb',
        'RUST': 'rs',
        'SBCL': 'cl',
        'SCALA': 'scala',
        'SCM': 'scm',
        'SED': 'sed',
        'SWIFT': 'swift',
        'TCL': 'tcl',
        'TEXT': 'txt',
        'TUR': 't',
        'V8JS': 'js',
        'ZIG': 'zig',
    }

    languages = Language.objects.all()
    for language in languages:
        try:
            extension = extension_mapping[language.key]
        except KeyError:
            print('Warning: no extension found for %s. Setting extension to language key.' % language.key)
            extension = language.key.lower()

        language.extension = extension

    Language.objects.bulk_update(languages, ['extension'])


class Migration(migrations.Migration):

    dependencies = [
        ('judge', '0111_blank_assignees_ticket'),
    ]

    operations = [
        migrations.RunPython(update_language_extensions, reverse_code=migrations.RunPython.noop),
    ]

```"
8accd764b72299f6ed7be35104f9a2b958b0fce6,tests/commands_test.py,tests/commands_test.py,,"from unittest import TestCase
from mock import Mock, patch

from nyuki.commands import (_update_config, _merge_config, parse_init,
                            exhaustive_config)


class TestUpdateConfig(TestCase):

    def test_001_call(self):
        source = {'a': 1, 'b': {'c': 2}}
        # Update
        _update_config(source, '1', 'a')
        self.assertEqual(source['a'], '1')
        # Nested update
        _update_config(source, 3, 'b.c')
        self.assertEqual(source['b']['c'], 3)
        # Create
        _update_config(source, 4, 'b.d')
        self.assertEqual(source['b']['d'], 4)


class TestMergeConfig(TestCase):

    def test_001_call(self):
        dict1 = {'a': 1, 'b': {'c': 2}}
        dict2 = {'b': {'d': 3}}
        result = _merge_config(dict1, dict2)
        self.assertEqual(result, {'a': 1, 'b': {'c': 2, 'd': 3}})


class TestParseInit(TestCase):

    @patch('nyuki.commands._read_file')
    @patch('nyuki.commands._build_args')
    def test_001_call(self, _build_args, _read_file):
        # Arguments parsed
        args = Mock()
        args.cfg = 'config.json'
        args.jid = 'test@localhost'
        args.pwd = 'test'
        args.srv = '127.0.0.1:5555'
        args.api = 'localhost:8082'
        args.debug = True
        _build_args.return_value = args
        # Config file
        _read_file.return_value = {
            'bus': {
                'jid': 'iamrobert',
                'password': 'mysuperstrongpassword',
            }
        }
        # Result
        configs = parse_init()
        self.assertEqual(configs, {
            'bus': {
                'jid': 'test@localhost',
                'password': 'test',
                'host': '127.0.0.1',
                'port': 5555
            },
            'api': {
                'port': 8082,
                'host': 'localhost'
            },
            'log': {
                'root': {
                    'level': 'DEBUG'}
            }
        })


class TestExhaustiveConfig(TestCase):

    def test_001_call(self):
        parsed_configs = {
            'bus': {
                'jid': 'test@localhost',
                'password': 'test',
                'host': '127.0.0.1',
                'port': 5555
            },
            'api': {
                'port': 8082,
                'host': 'localhost'
            },
            'log': {
                'root': {
                    'level': 'DEBUG'}
            }
        }
        self.assertIsInstance(exhaustive_config(parsed_configs), dict)
        wrong_config = {
            'bus': {
                'jid': 'test@localhost'
            }
        }
        with self.assertRaises(SystemExit) as call:
            exhaustive_config(wrong_config)
        self.assertEqual(call.exception.code, 1)
",Add unit tests on commands.,"Add unit tests on commands.
",Python,apache-2.0,"gdraynz/nyuki,optiflows/nyuki,optiflows/nyuki,gdraynz/nyuki",101,"```python
from unittest import TestCase
from mock import Mock, patch

from nyuki.commands import (_update_config, _merge_config, parse_init,
                            exhaustive_config)


class TestUpdateConfig(TestCase):

    def test_001_call(self):
        source = {'a': 1, 'b': {'c': 2}}
        # Update
        _update_config(source, '1', 'a')
        self.assertEqual(source['a'], '1')
        # Nested update
        _update_config(source, 3, 'b.c')
        self.assertEqual(source['b']['c'], 3)
        # Create
        _update_config(source, 4, 'b.d')
        self.assertEqual(source['b']['d'], 4)


class TestMergeConfig(TestCase):

    def test_001_call(self):
        dict1 = {'a': 1, 'b': {'c': 2}}
        dict2 = {'b': {'d': 3}}
        result = _merge_config(dict1, dict2)
        self.assertEqual(result, {'a': 1, 'b': {'c': 2, 'd': 3}})


class TestParseInit(TestCase):

    @patch('nyuki.commands._read_file')
    @patch('nyuki.commands._build_args')
    def test_001_call(self, _build_args, _read_file):
        # Arguments parsed
        args = Mock()
        args.cfg = 'config.json'
        args.jid = 'test@localhost'
        args.pwd = 'test'
        args.srv = '127.0.0.1:5555'
        args.api = 'localhost:8082'
        args.debug = True
        _build_args.return_value = args
        # Config file
        _read_file.return_value = {
            'bus': {
                'jid': 'iamrobert',
                'password': 'mysuperstrongpassword',
            }
        }
        # Result
        configs = parse_init()
        self.assertEqual(configs, {
            'bus': {
                'jid': 'test@localhost',
                'password': 'test',
                'host': '127.0.0.1',
                'port': 5555
            },
            'api': {
                'port': 8082,
                'host': 'localhost'
            },
            'log': {
                'root': {
                    'level': 'DEBUG'}
            }
        })


class TestExhaustiveConfig(TestCase):

    def test_001_call(self):
        parsed_configs = {
            'bus': {
                'jid': 'test@localhost',
                'password': 'test',
                'host': '127.0.0.1',
                'port': 5555
            },
            'api': {
                'port': 8082,
                'host': 'localhost'
            },
            'log': {
                'root': {
                    'level': 'DEBUG'}
            }
        }
        self.assertIsInstance(exhaustive_config(parsed_configs), dict)
        wrong_config = {
            'bus': {
                'jid': 'test@localhost'
            }
        }
        with self.assertRaises(SystemExit) as call:
            exhaustive_config(wrong_config)
        self.assertEqual(call.exception.code, 1)

```"
93bc13af093186b3a74570882135b81ddeeb6719,drudge/term.py,drudge/term.py,,"""""""Tensor term definition and utility.""""""

from sympy import sympify


class Range:
    """"""A symbolic range that can be summed over.

    This class is for symbolic ranges that is going to be summed over in
    tensors.  Each range should have a label, and optionally lower and upper
    bounds, which should be both given or absent.  The bounds will not be
    directly used for symbolic computation, but rather designed for printers
    and conversion to SymPy summation.  Note that ranges are assumed to be
    atomic and disjoint.  Even in the presence of lower and upper bounds,
    unequal ranges are assumed to be disjoint.

    .. warning::

         Unequal ranges are always assumed to be disjoint.

    """"""

    __slots__ = [
        '_label',
        '_lower',
        '_upper'
    ]

    def __init__(self, label, lower=None, upper=None):
        """"""Initialize the symbolic range.""""""
        self._label = label
        self._lower = sympify(lower) if lower is not None else lower

        if self._lower is None:
            if upper is not None:
                raise ValueError('lower range has not been given.')
            else:
                self._upper = None
        else:
            if upper is None:
                raise ValueError('upper range has not been given.')
            else:
                self._upper = sympify(upper)

    @property
    def label(self):
        """"""Get the label of the range.""""""
        return self._label

    @property
    def lower(self):
        """"""Get the lower bound of the range.""""""
        return self._lower

    @property
    def upper(self):
        """"""Get the upper bound of the range.""""""
        return self._upper

    @property
    def args(self):
        """"""Get the arguments for range creation.

        When the bounds are present, we have a triple, or we have a singleton
        tuple of only the label.
        """"""

        if self._lower is not None:
            return (self._label, self._lower, self._upper)
        else:
            return (self._label,)

    def __hash__(self):
        """"""Hash the symbolic range.""""""
        return hash(self.args)

    def __eq__(self, other):
        """"""Compare equality of two ranges.""""""
        return isinstance(other, type(self)) and (
            self.args == other.args
        )

    def __repr__(self):
        """"""Form the representative string.""""""
        return ''.join([
            'Range(', ', '.join(repr(i) for i in self.args), ')'
        ])

    def __str__(self):
        """"""Form readable string representation.""""""
        return str(self._label)
",Add class for symbolic ranges,"Add class for symbolic ranges

Compared with PySLATA, this definition is a lot more simplified.  All
the ranges are assumed to be atomic and disjoint.  No need to implement
the range arithmetic.
",Python,mit,"tschijnmo/drudge,tschijnmo/drudge,tschijnmo/drudge",92,"```python
""""""Tensor term definition and utility.""""""

from sympy import sympify


class Range:
    """"""A symbolic range that can be summed over.

    This class is for symbolic ranges that is going to be summed over in
    tensors.  Each range should have a label, and optionally lower and upper
    bounds, which should be both given or absent.  The bounds will not be
    directly used for symbolic computation, but rather designed for printers
    and conversion to SymPy summation.  Note that ranges are assumed to be
    atomic and disjoint.  Even in the presence of lower and upper bounds,
    unequal ranges are assumed to be disjoint.

    .. warning::

         Unequal ranges are always assumed to be disjoint.

    """"""

    __slots__ = [
        '_label',
        '_lower',
        '_upper'
    ]

    def __init__(self, label, lower=None, upper=None):
        """"""Initialize the symbolic range.""""""
        self._label = label
        self._lower = sympify(lower) if lower is not None else lower

        if self._lower is None:
            if upper is not None:
                raise ValueError('lower range has not been given.')
            else:
                self._upper = None
        else:
            if upper is None:
                raise ValueError('upper range has not been given.')
            else:
                self._upper = sympify(upper)

    @property
    def label(self):
        """"""Get the label of the range.""""""
        return self._label

    @property
    def lower(self):
        """"""Get the lower bound of the range.""""""
        return self._lower

    @property
    def upper(self):
        """"""Get the upper bound of the range.""""""
        return self._upper

    @property
    def args(self):
        """"""Get the arguments for range creation.

        When the bounds are present, we have a triple, or we have a singleton
        tuple of only the label.
        """"""

        if self._lower is not None:
            return (self._label, self._lower, self._upper)
        else:
            return (self._label,)

    def __hash__(self):
        """"""Hash the symbolic range.""""""
        return hash(self.args)

    def __eq__(self, other):
        """"""Compare equality of two ranges.""""""
        return isinstance(other, type(self)) and (
            self.args == other.args
        )

    def __repr__(self):
        """"""Form the representative string.""""""
        return ''.join([
            'Range(', ', '.join(repr(i) for i in self.args), ')'
        ])

    def __str__(self):
        """"""Form readable string representation.""""""
        return str(self._label)

```"
a86852fe908bb0a44ef267a75b9446ddcaf03f6e,homeassistant/components/light/limitlessled.py,homeassistant/components/light/limitlessled.py,,"""""""
homeassistant.components.light.limitlessled
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Support for LimitlessLED bulbs, also known as...

EasyBulb
AppLight
AppLamp
MiLight
LEDme
dekolight
iLight

""""""
import random
import logging

from homeassistant.helpers.entity import ToggleEntity
from homeassistant.const import STATE_ON, STATE_OFF, DEVICE_DEFAULT_NAME
from homeassistant.components.light import ATTR_BRIGHTNESS

_LOGGER = logging.getLogger(__name__)


def setup_platform(hass, config, add_devices_callback, discovery_info=None):
    try:
        import ledcontroller
    except ImportError:
        _LOGGER.exception(""Error while importing dependency ledcontroller."")
        return

    led = ledcontroller.LedController(config['host'])

    lights = []
    for i in range(1, 5):
        if 'group_%d_name' % (i) in config:
            lights.append(
                LimitlessLED(
                    led,
                    i,
                    config['group_%d_name' % (i)],
                    STATE_OFF
                )
            )

    add_devices_callback(lights)


class LimitlessLED(ToggleEntity):
    def __init__(self, led, group, name, state, brightness=180):
        self.led = led
        self.group = group

        # LimitlessLEDs don't report state, we have track it ourselves.
        self.led.off(self.group)

        self._name = name or DEVICE_DEFAULT_NAME
        self._state = state
        self._brightness = brightness

    @property
    def should_poll(self):
        """""" No polling needed for a demo light. """"""
        return False

    @property
    def name(self):
        """""" Returns the name of the device if any. """"""
        return self._name

    @property
    def state(self):
        """""" Returns the name of the device if any. """"""
        return self._state

    @property
    def state_attributes(self):
        """""" Returns optional state attributes. """"""
        if self.is_on:
            return {
                ATTR_BRIGHTNESS: self._brightness,
            }

    @property
    def is_on(self):
        """""" True if device is on. """"""
        return self._state == STATE_ON

    def turn_on(self, **kwargs):
        """""" Turn the device on. """"""
        self._state = STATE_ON

        if ATTR_BRIGHTNESS in kwargs:
            self._brightness = kwargs[ATTR_BRIGHTNESS]

        self.led.set_brightness(self._brightness, self.group)

    def turn_off(self, **kwargs):
        """""" Turn the device off. """"""
        self._state = STATE_OFF
        self.led.off(self.group)
",Add basic support for LimitlessLED,"Add basic support for LimitlessLED
",Python,mit,"Duoxilian/home-assistant,betrisey/home-assistant,CCOSTAN/home-assistant,open-homeautomation/home-assistant,Smart-Torvy/torvy-home-assistant,pschmitt/home-assistant,hexxter/home-assistant,srcLurker/home-assistant,postlund/home-assistant,alexmogavero/home-assistant,MungoRae/home-assistant,srcLurker/home-assistant,turbokongen/home-assistant,shaftoe/home-assistant,keerts/home-assistant,tchellomello/home-assistant,jamespcole/home-assistant,ewandor/home-assistant,jabesq/home-assistant,qedi-r/home-assistant,w1ll1am23/home-assistant,bdfoster/blumate,luxus/home-assistant,ma314smith/home-assistant,miniconfig/home-assistant,bencmbrook/home-assistant,alexmogavero/home-assistant,Duoxilian/home-assistant,FreekingDean/home-assistant,jamespcole/home-assistant,rohitranjan1991/home-assistant,jaharkes/home-assistant,alexmogavero/home-assistant,ewandor/home-assistant,Julian/home-assistant,JshWright/home-assistant,leppa/home-assistant,stefan-jonasson/home-assistant,tomduijf/home-assistant,justyns/home-assistant,sdague/home-assistant,sanmiguel/home-assistant,tmm1/home-assistant,sffjunkie/home-assistant,nugget/home-assistant,devdelay/home-assistant,molobrakos/home-assistant,sanmiguel/home-assistant,open-homeautomation/home-assistant,betrisey/home-assistant,oandrew/home-assistant,kyvinh/home-assistant,instantchow/home-assistant,mezz64/home-assistant,Duoxilian/home-assistant,Zac-HD/home-assistant,soldag/home-assistant,kyvinh/home-assistant,varunr047/homefile,sfam/home-assistant,open-homeautomation/home-assistant,eagleamon/home-assistant,hmronline/home-assistant,morphis/home-assistant,tboyce1/home-assistant,ct-23/home-assistant,EricRho/home-assistant,Nzaga/home-assistant,g12mcgov/home-assistant,aronsky/home-assistant,Julian/home-assistant,LinuxChristian/home-assistant,partofthething/home-assistant,Theb-1/home-assistant,keerts/home-assistant,aoakeson/home-assistant,happyleavesaoc/home-assistant,nnic/home-assistant,teodoc/home-assistant,emilhetty/home-assistant,mKeRix/home-assistant,PetePriority/home-assistant,Danielhiversen/home-assistant,MartinHjelmare/home-assistant,theolind/home-assistant,hmronline/home-assistant,Theb-1/home-assistant,emilhetty/home-assistant,jawilson/home-assistant,alanbowman/home-assistant,turbokongen/home-assistant,dorant/home-assistant,tmm1/home-assistant,nugget/home-assistant,nnic/home-assistant,pschmitt/home-assistant,jaharkes/home-assistant,varunr047/homefile,xifle/home-assistant,tboyce1/home-assistant,robbiet480/home-assistant,keerts/home-assistant,Danielhiversen/home-assistant,Cinntax/home-assistant,coteyr/home-assistant,qedi-r/home-assistant,alexkolar/home-assistant,ma314smith/home-assistant,varunr047/homefile,partofthething/home-assistant,sander76/home-assistant,leoc/home-assistant,mahendra-r/home-assistant,Zyell/home-assistant,devdelay/home-assistant,pottzer/home-assistant,oandrew/home-assistant,MartinHjelmare/home-assistant,PetePriority/home-assistant,MungoRae/home-assistant,JshWright/home-assistant,miniconfig/home-assistant,molobrakos/home-assistant,SEJeff/home-assistant,LinuxChristian/home-assistant,titilambert/home-assistant,stefan-jonasson/home-assistant,shaftoe/home-assistant,badele/home-assistant,sfam/home-assistant,luxus/home-assistant,aronsky/home-assistant,balloob/home-assistant,ma314smith/home-assistant,bencmbrook/home-assistant,deisi/home-assistant,open-homeautomation/home-assistant,tboyce1/home-assistant,kennedyshead/home-assistant,DavidLP/home-assistant,ErykB2000/home-assistant,mikaelboman/home-assistant,mikaelboman/home-assistant,vitorespindola/home-assistant,rohitranjan1991/home-assistant,caiuspb/home-assistant,ewandor/home-assistant,jawilson/home-assistant,GenericStudent/home-assistant,srcLurker/home-assistant,luxus/home-assistant,HydrelioxGitHub/home-assistant,happyleavesaoc/home-assistant,ct-23/home-assistant,stefan-jonasson/home-assistant,xifle/home-assistant,keerts/home-assistant,miniconfig/home-assistant,badele/home-assistant,Smart-Torvy/torvy-home-assistant,coteyr/home-assistant,deisi/home-assistant,Zyell/home-assistant,Julian/home-assistant,tomduijf/home-assistant,miniconfig/home-assistant,tboyce021/home-assistant,hmronline/home-assistant,adrienbrault/home-assistant,aequitas/home-assistant,hexxter/home-assistant,Nzaga/home-assistant,oandrew/home-assistant,happyleavesaoc/home-assistant,theolind/home-assistant,mikaelboman/home-assistant,titilambert/home-assistant,bencmbrook/home-assistant,DavidLP/home-assistant,EricRho/home-assistant,Zyell/home-assistant,auduny/home-assistant,DavidLP/home-assistant,fbradyirl/home-assistant,aoakeson/home-assistant,adrienbrault/home-assistant,shaftoe/home-assistant,joopert/home-assistant,robjohnson189/home-assistant,xifle/home-assistant,shaftoe/home-assistant,soldag/home-assistant,lukas-hetzenecker/home-assistant,hmronline/home-assistant,HydrelioxGitHub/home-assistant,joopert/home-assistant,eagleamon/home-assistant,betrisey/home-assistant,rohitranjan1991/home-assistant,mahendra-r/home-assistant,deisi/home-assistant,jabesq/home-assistant,hexxter/home-assistant,jnewland/home-assistant,mKeRix/home-assistant,mahendra-r/home-assistant,emilhetty/home-assistant,Julian/home-assistant,instantchow/home-assistant,hmronline/home-assistant,bdfoster/blumate,alanbowman/home-assistant,alanbowman/home-assistant,robjohnson189/home-assistant,dorant/home-assistant,sanmiguel/home-assistant,balloob/home-assistant,sffjunkie/home-assistant,dmeulen/home-assistant,sdague/home-assistant,persandstrom/home-assistant,tinloaf/home-assistant,nkgilley/home-assistant,emilhetty/home-assistant,robbiet480/home-assistant,LinuxChristian/home-assistant,FreekingDean/home-assistant,sander76/home-assistant,xifle/home-assistant,emilhetty/home-assistant,MartinHjelmare/home-assistant,GenericStudent/home-assistant,balloob/home-assistant,leoc/home-assistant,w1ll1am23/home-assistant,caiuspb/home-assistant,maddox/home-assistant,MungoRae/home-assistant,philipbl/home-assistant,persandstrom/home-assistant,LinuxChristian/home-assistant,postlund/home-assistant,CCOSTAN/home-assistant,ErykB2000/home-assistant,ErykB2000/home-assistant,mezz64/home-assistant,dmeulen/home-assistant,kennedyshead/home-assistant,HydrelioxGitHub/home-assistant,nevercast/home-assistant,CCOSTAN/home-assistant,nkgilley/home-assistant,ct-23/home-assistant,alexkolar/home-assistant,justyns/home-assistant,aequitas/home-assistant,ct-23/home-assistant,florianholzapfel/home-assistant,toddeye/home-assistant,LinuxChristian/home-assistant,varunr047/homefile,bdfoster/blumate,alexkolar/home-assistant,pottzer/home-assistant,oandrew/home-assistant,mKeRix/home-assistant,Smart-Torvy/torvy-home-assistant,maddox/home-assistant,ct-23/home-assistant,EricRho/home-assistant,aoakeson/home-assistant,jaharkes/home-assistant,Zac-HD/home-assistant,Teagan42/home-assistant,dmeulen/home-assistant,sffjunkie/home-assistant,aequitas/home-assistant,bdfoster/blumate,betrisey/home-assistant,eagleamon/home-assistant,Nzaga/home-assistant,instantchow/home-assistant,PetePriority/home-assistant,vitorespindola/home-assistant,fbradyirl/home-assistant,fbradyirl/home-assistant,deisi/home-assistant,leoc/home-assistant,nugget/home-assistant,morphis/home-assistant,sffjunkie/home-assistant,bdfoster/blumate,MungoRae/home-assistant,jabesq/home-assistant,JshWright/home-assistant,g12mcgov/home-assistant,michaelarnauts/home-assistant,Zac-HD/home-assistant,stefan-jonasson/home-assistant,morphis/home-assistant,SEJeff/home-assistant,robjohnson189/home-assistant,home-assistant/home-assistant,mikaelboman/home-assistant,SEJeff/home-assistant,badele/home-assistant,tinloaf/home-assistant,caiuspb/home-assistant,auduny/home-assistant,MungoRae/home-assistant,Smart-Torvy/torvy-home-assistant,kyvinh/home-assistant,jnewland/home-assistant,mikaelboman/home-assistant,lukas-hetzenecker/home-assistant,Teagan42/home-assistant,eagleamon/home-assistant,jamespcole/home-assistant,tomduijf/home-assistant,srcLurker/home-assistant,happyleavesaoc/home-assistant,michaelarnauts/home-assistant,devdelay/home-assistant,florianholzapfel/home-assistant,pottzer/home-assistant,leoc/home-assistant,Theb-1/home-assistant,alexmogavero/home-assistant,sffjunkie/home-assistant,leppa/home-assistant,vitorespindola/home-assistant,hexxter/home-assistant,tinloaf/home-assistant,florianholzapfel/home-assistant,tboyce021/home-assistant,Cinntax/home-assistant,deisi/home-assistant,kyvinh/home-assistant,teodoc/home-assistant,sfam/home-assistant,philipbl/home-assistant,tmm1/home-assistant,dorant/home-assistant,philipbl/home-assistant,philipbl/home-assistant,tchellomello/home-assistant,morphis/home-assistant,coteyr/home-assistant,justyns/home-assistant,home-assistant/home-assistant,Duoxilian/home-assistant,nevercast/home-assistant,nevercast/home-assistant,molobrakos/home-assistant,JshWright/home-assistant,g12mcgov/home-assistant,mKeRix/home-assistant,jaharkes/home-assistant,tboyce1/home-assistant,auduny/home-assistant,florianholzapfel/home-assistant,michaelarnauts/home-assistant,dmeulen/home-assistant,persandstrom/home-assistant,teodoc/home-assistant,robjohnson189/home-assistant,Zac-HD/home-assistant,varunr047/homefile,theolind/home-assistant,ma314smith/home-assistant,nnic/home-assistant,maddox/home-assistant,jnewland/home-assistant,toddeye/home-assistant,devdelay/home-assistant",103,"```python
""""""
homeassistant.components.light.limitlessled
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Support for LimitlessLED bulbs, also known as...

EasyBulb
AppLight
AppLamp
MiLight
LEDme
dekolight
iLight

""""""
import random
import logging

from homeassistant.helpers.entity import ToggleEntity
from homeassistant.const import STATE_ON, STATE_OFF, DEVICE_DEFAULT_NAME
from homeassistant.components.light import ATTR_BRIGHTNESS

_LOGGER = logging.getLogger(__name__)


def setup_platform(hass, config, add_devices_callback, discovery_info=None):
    try:
        import ledcontroller
    except ImportError:
        _LOGGER.exception(""Error while importing dependency ledcontroller."")
        return

    led = ledcontroller.LedController(config['host'])

    lights = []
    for i in range(1, 5):
        if 'group_%d_name' % (i) in config:
            lights.append(
                LimitlessLED(
                    led,
                    i,
                    config['group_%d_name' % (i)],
                    STATE_OFF
                )
            )

    add_devices_callback(lights)


class LimitlessLED(ToggleEntity):
    def __init__(self, led, group, name, state, brightness=180):
        self.led = led
        self.group = group

        # LimitlessLEDs don't report state, we have track it ourselves.
        self.led.off(self.group)

        self._name = name or DEVICE_DEFAULT_NAME
        self._state = state
        self._brightness = brightness

    @property
    def should_poll(self):
        """""" No polling needed for a demo light. """"""
        return False

    @property
    def name(self):
        """""" Returns the name of the device if any. """"""
        return self._name

    @property
    def state(self):
        """""" Returns the name of the device if any. """"""
        return self._state

    @property
    def state_attributes(self):
        """""" Returns optional state attributes. """"""
        if self.is_on:
            return {
                ATTR_BRIGHTNESS: self._brightness,
            }

    @property
    def is_on(self):
        """""" True if device is on. """"""
        return self._state == STATE_ON

    def turn_on(self, **kwargs):
        """""" Turn the device on. """"""
        self._state = STATE_ON

        if ATTR_BRIGHTNESS in kwargs:
            self._brightness = kwargs[ATTR_BRIGHTNESS]

        self.led.set_brightness(self._brightness, self.group)

    def turn_off(self, **kwargs):
        """""" Turn the device off. """"""
        self._state = STATE_OFF
        self.led.off(self.group)

```"
e9f88f1c43189fe429730c488f4514bf78edea4e,mistune/__main__.py,mistune/__main__.py,,"import sys
import argparse
from . import (
    create_markdown,
    __version__ as version
)


def _md(args):
    if args.plugin:
        plugins = args.plugin
    else:
        # default plugins
        plugins = ['strikethrough', 'footnotes', 'table', 'speedup']
    return create_markdown(
        escape=args.escape,
        hard_wrap=args.hardwrap,
        renderer=args.renderer,
        plugins=plugins,
    )


def _output(text, args):
    if args.output:
        with open(args.output, 'w') as f:
            f.write(text)
    else:
        print(text)


CMD_HELP = '''Mistune, a sane and fast python markdown parser.

Here are some use cases of the command line tool:

    $ python -m mistune -m ""Hi **Markdown**""
    <p>Hi <strong>Markdown</strong></p>

    $ python -m mistune -f README.md
    <p>...
'''


def cli():
    parser = argparse.ArgumentParser(
        prog='python -m mistune',
        description=CMD_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        '-m', '--message',
        help='the markdown message to conver',
    )
    parser.add_argument(
        '-f', '--file',
        help='the markdown file to convert',
    )
    parser.add_argument(
        '-p', '--plugin',
        metavar='NAME',
        action='extend',
        nargs='+',
        help='specifiy a plugin to use',
    )
    parser.add_argument(
        '--escape',
        action='store_true',
        help='turn on escape option',
    )
    parser.add_argument(
        '--hardwrap',
        action='store_true',
        help='turn on hardwrap option',
    )
    parser.add_argument(
        '-o', '--output',
        help='write the rendered result into file',
    )
    parser.add_argument(
        '-r', '--renderer',
        default='html',
        help='specify the output renderer',
    )
    parser.add_argument('--version', action='version', version='mistune ' + version)
    args = parser.parse_args()

    if not args.message and not args.file:
        print('You MUST specify a message or file')
        return sys.exit(1)

    if args.message:
        md = _md(args)
        text = md(args.message)
        _output(text, args)
    elif args.file:
        md = _md(args)
        text = md.read(args.file)[0]
        _output(text, args)


if __name__ == '__main__':
    cli()
",Add python -m mistune cli,"Add python -m mistune cli
",Python,bsd-3-clause,lepture/mistune,102,"```python
import sys
import argparse
from . import (
    create_markdown,
    __version__ as version
)


def _md(args):
    if args.plugin:
        plugins = args.plugin
    else:
        # default plugins
        plugins = ['strikethrough', 'footnotes', 'table', 'speedup']
    return create_markdown(
        escape=args.escape,
        hard_wrap=args.hardwrap,
        renderer=args.renderer,
        plugins=plugins,
    )


def _output(text, args):
    if args.output:
        with open(args.output, 'w') as f:
            f.write(text)
    else:
        print(text)


CMD_HELP = '''Mistune, a sane and fast python markdown parser.

Here are some use cases of the command line tool:

    $ python -m mistune -m ""Hi **Markdown**""
    <p>Hi <strong>Markdown</strong></p>

    $ python -m mistune -f README.md
    <p>...
'''


def cli():
    parser = argparse.ArgumentParser(
        prog='python -m mistune',
        description=CMD_HELP,
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        '-m', '--message',
        help='the markdown message to conver',
    )
    parser.add_argument(
        '-f', '--file',
        help='the markdown file to convert',
    )
    parser.add_argument(
        '-p', '--plugin',
        metavar='NAME',
        action='extend',
        nargs='+',
        help='specifiy a plugin to use',
    )
    parser.add_argument(
        '--escape',
        action='store_true',
        help='turn on escape option',
    )
    parser.add_argument(
        '--hardwrap',
        action='store_true',
        help='turn on hardwrap option',
    )
    parser.add_argument(
        '-o', '--output',
        help='write the rendered result into file',
    )
    parser.add_argument(
        '-r', '--renderer',
        default='html',
        help='specify the output renderer',
    )
    parser.add_argument('--version', action='version', version='mistune ' + version)
    args = parser.parse_args()

    if not args.message and not args.file:
        print('You MUST specify a message or file')
        return sys.exit(1)

    if args.message:
        md = _md(args)
        text = md(args.message)
        _output(text, args)
    elif args.file:
        md = _md(args)
        text = md.read(args.file)[0]
        _output(text, args)


if __name__ == '__main__':
    cli()

```"
54b94346d2669347cf2a9a2b24df6b657cf80c5b,nisl/mask.py,nisl/mask.py,,"import numpy as np
from scipy import ndimage


###############################################################################
# Operating on connect component
###############################################################################


def largest_cc(mask):
    """""" Return the largest connected component of a 3D mask array.

        Parameters
        -----------
        mask: 3D boolean array
            3D array indicating a mask.

        Returns
        --------
        mask: 3D boolean array
            3D array indicating a mask, with only one connected component.
    """"""
    # We use asarray to be able to work with masked arrays.
    mask = np.asarray(mask)
    labels, label_nb = ndimage.label(mask)
    if not label_nb:
        raise ValueError('No non-zero values: no connected components')
    if label_nb == 1:
        return mask.astype(np.bool)
    label_count = np.bincount(labels.ravel())
    # discard 0 the 0 label
    label_count[0] = 0
    return labels == label_count.argmax()


###############################################################################
# Utilities to calculate masks
###############################################################################


def compute_mask(mean_volume, m=0.2, M=0.9, cc=True,
                    exclude_zeros=False):
    """"""
    Compute a mask file from fMRI data in 3D or 4D ndarrays.

    Compute and write the mask of an image based on the grey level
    This is based on an heuristic proposed by T.Nichols:
    find the least dense point of the histogram, between fractions
    m and M of the total image histogram.

    In case of failure, it is usually advisable to increase m.

    Parameters
    ----------
    mean_volume : 3D ndarray
        mean EPI image, used to compute the threshold for the mask.
    m : float, optional
        lower fraction of the histogram to be discarded.
    M: float, optional
        upper fraction of the histogram to be discarded.
    cc: boolean, optional
        if cc is True, only the largest connect component is kept.
    exclude_zeros: boolean, optional
        Consider zeros as missing values for the computation of the
        threshold. This option is useful if the images have been
        resliced with a large padding of zeros.

    Returns
    -------
    mask : 3D boolean ndarray
        The brain mask
    """"""
    sorted_input = np.sort(mean_volume.reshape(-1))
    if exclude_zeros:
        sorted_input = sorted_input[sorted_input != 0]
    limite_inf = np.floor(m * len(sorted_input))
    limite_sup = np.floor(M * len(sorted_input))

    delta = sorted_input[limite_inf + 1:limite_sup + 1] \
            - sorted_input[limite_inf:limite_sup]
    ia = delta.argmax()
    threshold = 0.5 * (sorted_input[ia + limite_inf]
                        + sorted_input[ia + limite_inf + 1])

    mask = (mean_volume >= threshold)

    if cc:
        mask = largest_cc(mask)

    return mask.astype(bool)
",Mask computation utilities (from nipy).,"Mask computation utilities (from nipy).
",Python,bsd-3-clause,abenicho/isvr,91,"```python
import numpy as np
from scipy import ndimage


###############################################################################
# Operating on connect component
###############################################################################


def largest_cc(mask):
    """""" Return the largest connected component of a 3D mask array.

        Parameters
        -----------
        mask: 3D boolean array
            3D array indicating a mask.

        Returns
        --------
        mask: 3D boolean array
            3D array indicating a mask, with only one connected component.
    """"""
    # We use asarray to be able to work with masked arrays.
    mask = np.asarray(mask)
    labels, label_nb = ndimage.label(mask)
    if not label_nb:
        raise ValueError('No non-zero values: no connected components')
    if label_nb == 1:
        return mask.astype(np.bool)
    label_count = np.bincount(labels.ravel())
    # discard 0 the 0 label
    label_count[0] = 0
    return labels == label_count.argmax()


###############################################################################
# Utilities to calculate masks
###############################################################################


def compute_mask(mean_volume, m=0.2, M=0.9, cc=True,
                    exclude_zeros=False):
    """"""
    Compute a mask file from fMRI data in 3D or 4D ndarrays.

    Compute and write the mask of an image based on the grey level
    This is based on an heuristic proposed by T.Nichols:
    find the least dense point of the histogram, between fractions
    m and M of the total image histogram.

    In case of failure, it is usually advisable to increase m.

    Parameters
    ----------
    mean_volume : 3D ndarray
        mean EPI image, used to compute the threshold for the mask.
    m : float, optional
        lower fraction of the histogram to be discarded.
    M: float, optional
        upper fraction of the histogram to be discarded.
    cc: boolean, optional
        if cc is True, only the largest connect component is kept.
    exclude_zeros: boolean, optional
        Consider zeros as missing values for the computation of the
        threshold. This option is useful if the images have been
        resliced with a large padding of zeros.

    Returns
    -------
    mask : 3D boolean ndarray
        The brain mask
    """"""
    sorted_input = np.sort(mean_volume.reshape(-1))
    if exclude_zeros:
        sorted_input = sorted_input[sorted_input != 0]
    limite_inf = np.floor(m * len(sorted_input))
    limite_sup = np.floor(M * len(sorted_input))

    delta = sorted_input[limite_inf + 1:limite_sup + 1] \
            - sorted_input[limite_inf:limite_sup]
    ia = delta.argmax()
    threshold = 0.5 * (sorted_input[ia + limite_inf]
                        + sorted_input[ia + limite_inf + 1])

    mask = (mean_volume >= threshold)

    if cc:
        mask = largest_cc(mask)

    return mask.astype(bool)

```"
df784323d0da737755def4015840d118e3c8e595,nettests/core/http_body_length.py,nettests/core/http_body_length.py,,"# -*- encoding: utf-8 -*-
#
# :authors: Arturo Filastò
# :licence: see LICENSE

from twisted.internet import defer
from twisted.python import usage
from ooni.templates import httpt

class UsageOptions(usage.Options):
    optParameters = [
                     ['url', 'u', None, 'Specify a single URL to test.'],
                     ['factor', 'f', 0.8, 'What factor should be used for triggering censorship (0.8 == 80%)']
                    ]

class HTTPBodyLength(httpt.HTTPTest):
    """"""
    Performs a two GET requests to the set of sites to be tested for
    censorship, one over a known good control channel (Tor), the other over the
    test network.
    We then look at the response body lengths and see if the control response
    differs from the experiment response by a certain factor.
    """"""
    name = ""HTTP Body length test""
    author = ""Arturo Filastò""
    version = ""0.1""

    usageOptions = UsageOptions

    inputFile = ['file', 'f', None, 
            'List of URLS to perform GET and POST requests to']

    # These values are used for determining censorship based on response body
    # lengths
    control_body_length = None
    experiment_body_length = None

    def setUp(self):
        """"""
        Check for inputs.
        """"""
        if self.input:
            self.url = self.input
        elif self.localOptions['url']:
            self.url = self.localOptions['url']
        else:
            raise Exception(""No input specified"")

        self.factor = self.localOptions['factor']

    def compare_body_lengths(self):
        body_length_a = self.control_body_length
        body_length_b = self.experiment_body_length

        rel = float(body_length_a)/float(body_length_b)
        if rel > 1:
            rel = 1/rel

        self.report['body_proportion'] = rel
        self.report['factor'] = self.factor
        if rel < self.factor:
            self.report['censorship'] = True
        else:
            self.report['censorship'] = False

    def test_get(self):
        def errback(failure):
            log.err(""There was an error while testing %s"" % self.url)
            log.exception(failure)

        def control_body(result):
            self.control_body_length = len(result)
            if self.experiment_body_length:
                self.compare_body_lengths()

        def experiment_body(result):
            self.experiment_body_length = len(result)
            if self.control_body_length:
                self.compare_body_lengths()

        dl = []
        experiment_request = self.doRequest(self.url, method=""GET"",
                body_processor=experiment_body)
        control_request = self.doRequest(self.url, method=""GET"",
                use_tor=True, body_processor=control_body)
        dl.append(experiment_request)
        dl.append(control_request)
        d = defer.DeferredList(dl)
        return d

",Add test that detects censorship in HTTP pages based on HTTP body length,"Add test that detects censorship in HTTP pages based on HTTP body length
",Python,bsd-2-clause,"juga0/ooni-probe,juga0/ooni-probe,lordappsec/ooni-probe,kdmurray91/ooni-probe,0xPoly/ooni-probe,Karthikeyan-kkk/ooni-probe,Karthikeyan-kkk/ooni-probe,Karthikeyan-kkk/ooni-probe,Karthikeyan-kkk/ooni-probe,lordappsec/ooni-probe,juga0/ooni-probe,lordappsec/ooni-probe,0xPoly/ooni-probe,juga0/ooni-probe,lordappsec/ooni-probe,0xPoly/ooni-probe,kdmurray91/ooni-probe,kdmurray91/ooni-probe,0xPoly/ooni-probe,kdmurray91/ooni-probe",91,"```python
# -*- encoding: utf-8 -*-
#
# :authors: Arturo Filastò
# :licence: see LICENSE

from twisted.internet import defer
from twisted.python import usage
from ooni.templates import httpt

class UsageOptions(usage.Options):
    optParameters = [
                     ['url', 'u', None, 'Specify a single URL to test.'],
                     ['factor', 'f', 0.8, 'What factor should be used for triggering censorship (0.8 == 80%)']
                    ]

class HTTPBodyLength(httpt.HTTPTest):
    """"""
    Performs a two GET requests to the set of sites to be tested for
    censorship, one over a known good control channel (Tor), the other over the
    test network.
    We then look at the response body lengths and see if the control response
    differs from the experiment response by a certain factor.
    """"""
    name = ""HTTP Body length test""
    author = ""Arturo Filastò""
    version = ""0.1""

    usageOptions = UsageOptions

    inputFile = ['file', 'f', None, 
            'List of URLS to perform GET and POST requests to']

    # These values are used for determining censorship based on response body
    # lengths
    control_body_length = None
    experiment_body_length = None

    def setUp(self):
        """"""
        Check for inputs.
        """"""
        if self.input:
            self.url = self.input
        elif self.localOptions['url']:
            self.url = self.localOptions['url']
        else:
            raise Exception(""No input specified"")

        self.factor = self.localOptions['factor']

    def compare_body_lengths(self):
        body_length_a = self.control_body_length
        body_length_b = self.experiment_body_length

        rel = float(body_length_a)/float(body_length_b)
        if rel > 1:
            rel = 1/rel

        self.report['body_proportion'] = rel
        self.report['factor'] = self.factor
        if rel < self.factor:
            self.report['censorship'] = True
        else:
            self.report['censorship'] = False

    def test_get(self):
        def errback(failure):
            log.err(""There was an error while testing %s"" % self.url)
            log.exception(failure)

        def control_body(result):
            self.control_body_length = len(result)
            if self.experiment_body_length:
                self.compare_body_lengths()

        def experiment_body(result):
            self.experiment_body_length = len(result)
            if self.control_body_length:
                self.compare_body_lengths()

        dl = []
        experiment_request = self.doRequest(self.url, method=""GET"",
                body_processor=experiment_body)
        control_request = self.doRequest(self.url, method=""GET"",
                use_tor=True, body_processor=control_body)
        dl.append(experiment_request)
        dl.append(control_request)
        d = defer.DeferredList(dl)
        return d


```"
f970198596d8c20c89701fbcce38fd5736096e86,namegen/markov.py,namegen/markov.py,,"#!/usr/bin/env python
""""""
Module which produces readble name from 256-bit of random data
(i.e. sha-256 hash)


""""""
MAXWORDLEN=12
#
# Modules which contain problablity dictionaries
# generated by genmarkov script
#
from surname_hash import surname
from female_hash import female
from male_hash import male
#
import operator

gendernames=[male,female]

class RandError(Exception):
	""""""
		Raised if all bits are exhausted
	""""""
	pass

def getrandom(count):
	""""""
		extracts neccessary amount of randomness (non neccessary
		integer number of bits) from given input
	""""""
	global data
	if data==0:
		raise RandError(""No more random data"")
	data,r=divmod(data,count)
	return r
def morerandom():
	""""""
		Checks if there are some random bits left
	""""""
	global data
	return data!=0

def mkword(x):
	""""""
		Compuites word from given  dictionary.
		Randomly chooses first and second letter and
		with probabilities of their occurence after whicespace
		and then chooses next letter until space is encountered
	""""""
	count=0
	global MAXWORDLEN
	first=x[' ']
	count=reduce(operator.add,map(len,first.itervalues()))
	i=getrandom(count)
	for letter in sorted(first):
		if i<len(first[letter]):
			word=letter+first[letter][i]
			break
		i-=len(first[letter])
	while word[-1]!=' ':
		y=x[word[-2]][word[-1]]
		word+=y[getrandom(len(y))]
		if len(word)>= MAXWORDLEN:
			word+="" ""
			break
	return word.title()

def mkname(hash):
	""""""
		Gets 64 hexadecimal digits and computes name from it
	""""""
	global data
	if (len(hash)<64):
		raise ValueError(""To few random data: ""+hash)
	data=int(hash,16)
	x=gendernames[getrandom(2)]
	name=mkword(surname)
	more=True
	while more:
		try:
			name+=mkword(x)
		except RandError:
			more= False
	return name.strip()

if __name__ == ""__main__"":
	#
	# Test main routine which reads random data from
	#
	import sys
	from base64 import b16encode
	from os import urandom
	if len(sys.argv)>1 and sys.argv[1].isdigit():
		for i in xrange(0,int(sys.argv[1])):
			d=b16encode(urandom(32))
			try:
				print mkname(d)
			except RandError:
				print ""Not enough random: ""+d
	else:
		for i in xrange(0,20):
			d=b16encode(urandom(32))
			print d,mkname(d)
",Set maximal word length limit,"Set maximal word length limit
",Python,agpl-3.0,cheshirenet/cheshirenet,105,"```python
#!/usr/bin/env python
""""""
Module which produces readble name from 256-bit of random data
(i.e. sha-256 hash)


""""""
MAXWORDLEN=12
#
# Modules which contain problablity dictionaries
# generated by genmarkov script
#
from surname_hash import surname
from female_hash import female
from male_hash import male
#
import operator

gendernames=[male,female]

class RandError(Exception):
	""""""
		Raised if all bits are exhausted
	""""""
	pass

def getrandom(count):
	""""""
		extracts neccessary amount of randomness (non neccessary
		integer number of bits) from given input
	""""""
	global data
	if data==0:
		raise RandError(""No more random data"")
	data,r=divmod(data,count)
	return r
def morerandom():
	""""""
		Checks if there are some random bits left
	""""""
	global data
	return data!=0

def mkword(x):
	""""""
		Compuites word from given  dictionary.
		Randomly chooses first and second letter and
		with probabilities of their occurence after whicespace
		and then chooses next letter until space is encountered
	""""""
	count=0
	global MAXWORDLEN
	first=x[' ']
	count=reduce(operator.add,map(len,first.itervalues()))
	i=getrandom(count)
	for letter in sorted(first):
		if i<len(first[letter]):
			word=letter+first[letter][i]
			break
		i-=len(first[letter])
	while word[-1]!=' ':
		y=x[word[-2]][word[-1]]
		word+=y[getrandom(len(y))]
		if len(word)>= MAXWORDLEN:
			word+="" ""
			break
	return word.title()

def mkname(hash):
	""""""
		Gets 64 hexadecimal digits and computes name from it
	""""""
	global data
	if (len(hash)<64):
		raise ValueError(""To few random data: ""+hash)
	data=int(hash,16)
	x=gendernames[getrandom(2)]
	name=mkword(surname)
	more=True
	while more:
		try:
			name+=mkword(x)
		except RandError:
			more= False
	return name.strip()

if __name__ == ""__main__"":
	#
	# Test main routine which reads random data from
	#
	import sys
	from base64 import b16encode
	from os import urandom
	if len(sys.argv)>1 and sys.argv[1].isdigit():
		for i in xrange(0,int(sys.argv[1])):
			d=b16encode(urandom(32))
			try:
				print mkname(d)
			except RandError:
				print ""Not enough random: ""+d
	else:
		for i in xrange(0,20):
			d=b16encode(urandom(32))
			print d,mkname(d)

```"
dbc20f37c7fb1dd00c90ac54d2021fb1ba3b5eda,exam.py,exam.py,,"import time
import sys

from groupy.client import Client


def read_token_from_file(filename):
    with open(filename) as f:
        return f.read().strip()


def test_groups(groups):
    for group in groups:
        print(group)

        print('Members:')
        for member in group.members[:5]:
            print(member)

        print('Recent messages:')
        for message in group.messages.list()[:5]:
            print(message)

        print('Leaderboard (day):')
        for message in group.leaderboard.list_day()[:5]:
            print(message.favorited_by)

        print('Gallery:')
        for message in group.gallery.list()[:5]:
            print(message.attachments)

        print()


def test_messages(messages):
    for message in messages:
        print(message)
        print(message.attachments)
        print('Liking...', message.like())
        time.sleep(1)  # you get rate limited by liking/unliking too fast
        print('Unliking...', message.unlike())


def test_chats(chats):
    for chat in chats:
        print(chat)
        print('Recent messages:')
        for message in chat.messages.list():
            print(message)


def main(*args):
    token_file = args[0]
    token = read_token_from_file(token_file)
    client = Client.from_token(token)

    groups = list(client.groups.list().autopage())
    test_group_ids = ('12268264', '27205597', '27205784', '35799100')
    target_groups = []
    for group in groups:
        if group.id in test_group_ids:
            print('Found {0} (id={0.group_id})'.format(group))
            target_groups.append(group)
    if len(target_groups) < len(test_group_ids):
        raise Exception('could not find group test groups')

    chats = list(client.chats.list())
    test_chat_ids = ('14529712+14612048',)
    target_chats = []
    for chat in chats:
        if chat.last_message['conversation_id'] in test_chat_ids:
            print('Found {}'.format(chat))
            target_chats.append(group)
    if len(target_chats) < len(test_chat_ids):
        raise Exception('could not find group test chats')

    target_messages = []
    for group in target_groups:
        target_messages.append(group.messages.list()[0])
    for chat in target_chats:
        target_messages.append(chat.messages.list()[0])

    print_header('test groups')
    test_groups(target_groups)

    print_header('test chats')
    test_chats(target_chats)

    print_header('test messages')
    test_messages(target_messages)


def print_header(header):
    print('\n')
    print('=' * 50)
    print('|  {}'.format(header))
    print('=' * 50)
    print()


if __name__ == '__main__':
    main(*sys.argv[1:])
",Add some end-to-end functional tests,"Add some end-to-end functional tests

Since running these tests is not part of the automated tests,
and since the test runner picks up any file that has a name
containing ""test"" I have to be clever and call it ""exam.py.""
",Python,apache-2.0,rhgrant10/Groupy,103,"```python
import time
import sys

from groupy.client import Client


def read_token_from_file(filename):
    with open(filename) as f:
        return f.read().strip()


def test_groups(groups):
    for group in groups:
        print(group)

        print('Members:')
        for member in group.members[:5]:
            print(member)

        print('Recent messages:')
        for message in group.messages.list()[:5]:
            print(message)

        print('Leaderboard (day):')
        for message in group.leaderboard.list_day()[:5]:
            print(message.favorited_by)

        print('Gallery:')
        for message in group.gallery.list()[:5]:
            print(message.attachments)

        print()


def test_messages(messages):
    for message in messages:
        print(message)
        print(message.attachments)
        print('Liking...', message.like())
        time.sleep(1)  # you get rate limited by liking/unliking too fast
        print('Unliking...', message.unlike())


def test_chats(chats):
    for chat in chats:
        print(chat)
        print('Recent messages:')
        for message in chat.messages.list():
            print(message)


def main(*args):
    token_file = args[0]
    token = read_token_from_file(token_file)
    client = Client.from_token(token)

    groups = list(client.groups.list().autopage())
    test_group_ids = ('12268264', '27205597', '27205784', '35799100')
    target_groups = []
    for group in groups:
        if group.id in test_group_ids:
            print('Found {0} (id={0.group_id})'.format(group))
            target_groups.append(group)
    if len(target_groups) < len(test_group_ids):
        raise Exception('could not find group test groups')

    chats = list(client.chats.list())
    test_chat_ids = ('14529712+14612048',)
    target_chats = []
    for chat in chats:
        if chat.last_message['conversation_id'] in test_chat_ids:
            print('Found {}'.format(chat))
            target_chats.append(group)
    if len(target_chats) < len(test_chat_ids):
        raise Exception('could not find group test chats')

    target_messages = []
    for group in target_groups:
        target_messages.append(group.messages.list()[0])
    for chat in target_chats:
        target_messages.append(chat.messages.list()[0])

    print_header('test groups')
    test_groups(target_groups)

    print_header('test chats')
    test_chats(target_chats)

    print_header('test messages')
    test_messages(target_messages)


def print_header(header):
    print('\n')
    print('=' * 50)
    print('|  {}'.format(header))
    print('=' * 50)
    print()


if __name__ == '__main__':
    main(*sys.argv[1:])

```"
2d4f09fe8c31aa2b996e71565292d5ef249986c7,tools/gyp-explain.py,tools/gyp-explain.py,,"#!/usr/bin/env python
# Copyright (c) 2011 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Prints paths between gyp targets.
""""""

import json
import os
import sys
import time

from collections import deque

def usage():
  print """"""\
Usage:
  tools/gyp-explain.py chrome_dll gtest#
""""""


def GetPath(graph, fro, to):
  """"""Given a graph in (node -> list of successor nodes) dictionary format,
  yields all paths from |fro| to |to|, starting with the shortest.""""""
  # Storing full paths in the queue is a bit wasteful, but good enough for this.
  q = deque([(fro, [])])
  while q:
    t, path = q.popleft()
    if t == to:
      yield path + [t]
    for d in graph[t]:
      q.append((d, path + [t]))


def MatchNode(graph, substring):
  """"""Given a dictionary, returns the key that matches |substring| best. Exits
  if there's not one single best match.""""""
  candidates = []
  for target in graph:
    if substring in target:
      candidates.append(target)

  if not candidates:
    print 'No targets match ""%s""' % substring
    sys.exit(1)
  if len(candidates) > 1:
    print 'More than one target matches ""%s"": %s' % (
        substring, ' '.join(candidates))
    sys.exit(1)
  return candidates[0]


def Main(argv):
  # Check that dump.json exists and that it's not too old.
  dump_json_dirty = False
  try:
    st = os.stat('dump.json')
    file_age_s = time.time() - st.st_mtime
    if file_age_s > 2 * 60 * 60:
      print 'dump.json is more than 2 hours old.'
      dump_json_dirty = True
  except IOError:
    print 'dump.json not found.'
    dump_json_dirty = True

  if dump_json_dirty:
    print 'Run'
    print '    GYP_GENERATORS=dump_dependency_json build/gyp_chromium'
    print 'first, then try again.'
    sys.exit(1)

  g = json.load(open('dump.json'))

  if len(argv) != 3:
    usage()
    sys.exit(1)

  fro = MatchNode(g, argv[1])
  to = MatchNode(g, argv[2])

  paths = list(GetPath(g, fro, to))
  if len(paths) > 0:
    print 'These paths lead from %s to %s:' % (fro, to)
    for path in paths:
      print ' -> '.join(path)
  else:
    print 'No paths found from %s to %s.' % (fro, to)


if __name__ == '__main__':
  Main(sys.argv)
","Add a small tool to answer questions like ""Why does target A depend on target B"".","Add a small tool to answer questions like ""Why does target A depend on target B"".

BUG=none
TEST=none

Review URL: http://codereview.chromium.org/8672006

git-svn-id: de016e52bd170d2d4f2344f9bf92d50478b649e0@111430 0039d316-1c4b-4281-b951-d872f2087c98
",Python,bsd-3-clause,"yitian134/chromium,yitian134/chromium,ropik/chromium,ropik/chromium,ropik/chromium,ropik/chromium,gavinp/chromium,yitian134/chromium,gavinp/chromium,adobe/chromium,gavinp/chromium,yitian134/chromium,gavinp/chromium,yitian134/chromium,adobe/chromium,adobe/chromium,gavinp/chromium,ropik/chromium,yitian134/chromium,ropik/chromium,ropik/chromium,adobe/chromium,adobe/chromium,gavinp/chromium,adobe/chromium,yitian134/chromium,gavinp/chromium,ropik/chromium,adobe/chromium,yitian134/chromium,gavinp/chromium,ropik/chromium,adobe/chromium,gavinp/chromium,adobe/chromium,gavinp/chromium,yitian134/chromium,adobe/chromium,adobe/chromium,yitian134/chromium",93,"```python
#!/usr/bin/env python
# Copyright (c) 2011 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Prints paths between gyp targets.
""""""

import json
import os
import sys
import time

from collections import deque

def usage():
  print """"""\
Usage:
  tools/gyp-explain.py chrome_dll gtest#
""""""


def GetPath(graph, fro, to):
  """"""Given a graph in (node -> list of successor nodes) dictionary format,
  yields all paths from |fro| to |to|, starting with the shortest.""""""
  # Storing full paths in the queue is a bit wasteful, but good enough for this.
  q = deque([(fro, [])])
  while q:
    t, path = q.popleft()
    if t == to:
      yield path + [t]
    for d in graph[t]:
      q.append((d, path + [t]))


def MatchNode(graph, substring):
  """"""Given a dictionary, returns the key that matches |substring| best. Exits
  if there's not one single best match.""""""
  candidates = []
  for target in graph:
    if substring in target:
      candidates.append(target)

  if not candidates:
    print 'No targets match ""%s""' % substring
    sys.exit(1)
  if len(candidates) > 1:
    print 'More than one target matches ""%s"": %s' % (
        substring, ' '.join(candidates))
    sys.exit(1)
  return candidates[0]


def Main(argv):
  # Check that dump.json exists and that it's not too old.
  dump_json_dirty = False
  try:
    st = os.stat('dump.json')
    file_age_s = time.time() - st.st_mtime
    if file_age_s > 2 * 60 * 60:
      print 'dump.json is more than 2 hours old.'
      dump_json_dirty = True
  except IOError:
    print 'dump.json not found.'
    dump_json_dirty = True

  if dump_json_dirty:
    print 'Run'
    print '    GYP_GENERATORS=dump_dependency_json build/gyp_chromium'
    print 'first, then try again.'
    sys.exit(1)

  g = json.load(open('dump.json'))

  if len(argv) != 3:
    usage()
    sys.exit(1)

  fro = MatchNode(g, argv[1])
  to = MatchNode(g, argv[2])

  paths = list(GetPath(g, fro, to))
  if len(paths) > 0:
    print 'These paths lead from %s to %s:' % (fro, to)
    for path in paths:
      print ' -> '.join(path)
  else:
    print 'No paths found from %s to %s.' % (fro, to)


if __name__ == '__main__':
  Main(sys.argv)

```"
250e0d2d0e2264b83a82548df3b30dbc784a4fe5,docker-registry-show.py,docker-registry-show.py,,"""""""
Copyright 2015 Red Hat, Inc

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
""""""


from __future__ import absolute_import

import argparse
from docker_registry_client import DockerRegistryClient
import logging
import requests


class CLI(object):
    def __init__(self):
        self.parser = argparse.ArgumentParser()
        excl_group = self.parser.add_mutually_exclusive_group()
        excl_group.add_argument(""-q"", ""--quiet"", action=""store_true"")
        excl_group.add_argument(""-v"", ""--verbose"", action=""store_true"")

        self.parser.add_argument('--verify-ssl', dest='verify_ssl',
                                 action='store_true')
        self.parser.add_argument('--no-verify-ssl', dest='verify_ssl',
                                 action='store_false')

        self.parser.add_argument('registry', metavar='REGISTRY', nargs=1,
                                 help='registry URL (including scheme)')
        self.parser.add_argument('repository', metavar='REPOSITORY', nargs='?')

        self.parser.set_defaults(verify_ssl=True)

    def run(self):
        args = self.parser.parse_args()

        basic_config_args = {}
        if args.verbose:
            basic_config_args['level'] = logging.DEBUG
        elif args.quiet:
            basic_config_args['level'] = logging.WARNING

        logging.basicConfig(**basic_config_args)

        client = DockerRegistryClient(args.registry[0],
                                      verify_ssl=args.verify_ssl)

        if args.repository:
            self.show_tags(client, args.repository)
        else:
            self.show_repositories(client)

    def show_repositories(self, client):
        try:
            repositories = client.repositories()
        except requests.HTTPError as e:
            if e.response.status_code == requests.codes.not_found:
                print(""Catalog/Search not supported"")
            else:
                raise
        else:
            print(""Repositories:"")
            for repository in repositories.keys():
                print(""  - {0}"".format(repository))

    def show_tags(self, client, repository):
        try:
            repo = client.repository(repository)
        except requests.HTTPError as e:
            if e.response.status_code == requests.codes.not_found:
                print(""Repository {0} not found"".format(repository))
            else:
                raise
        else:
            print(""Tags in repository {0}:"".format(repository))
            for tag in repo.tags():
                print(""  - {0}"".format(tag))


if __name__ == '__main__':
    try:
        cli = CLI()
        cli.run()
    except KeyboardInterrupt:
        pass
",Add some example client code,"Add some example client code
",Python,apache-2.0,"twaugh/docker-registry-client,yodle/docker-registry-client",96,"```python
""""""
Copyright 2015 Red Hat, Inc

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
""""""


from __future__ import absolute_import

import argparse
from docker_registry_client import DockerRegistryClient
import logging
import requests


class CLI(object):
    def __init__(self):
        self.parser = argparse.ArgumentParser()
        excl_group = self.parser.add_mutually_exclusive_group()
        excl_group.add_argument(""-q"", ""--quiet"", action=""store_true"")
        excl_group.add_argument(""-v"", ""--verbose"", action=""store_true"")

        self.parser.add_argument('--verify-ssl', dest='verify_ssl',
                                 action='store_true')
        self.parser.add_argument('--no-verify-ssl', dest='verify_ssl',
                                 action='store_false')

        self.parser.add_argument('registry', metavar='REGISTRY', nargs=1,
                                 help='registry URL (including scheme)')
        self.parser.add_argument('repository', metavar='REPOSITORY', nargs='?')

        self.parser.set_defaults(verify_ssl=True)

    def run(self):
        args = self.parser.parse_args()

        basic_config_args = {}
        if args.verbose:
            basic_config_args['level'] = logging.DEBUG
        elif args.quiet:
            basic_config_args['level'] = logging.WARNING

        logging.basicConfig(**basic_config_args)

        client = DockerRegistryClient(args.registry[0],
                                      verify_ssl=args.verify_ssl)

        if args.repository:
            self.show_tags(client, args.repository)
        else:
            self.show_repositories(client)

    def show_repositories(self, client):
        try:
            repositories = client.repositories()
        except requests.HTTPError as e:
            if e.response.status_code == requests.codes.not_found:
                print(""Catalog/Search not supported"")
            else:
                raise
        else:
            print(""Repositories:"")
            for repository in repositories.keys():
                print(""  - {0}"".format(repository))

    def show_tags(self, client, repository):
        try:
            repo = client.repository(repository)
        except requests.HTTPError as e:
            if e.response.status_code == requests.codes.not_found:
                print(""Repository {0} not found"".format(repository))
            else:
                raise
        else:
            print(""Tags in repository {0}:"".format(repository))
            for tag in repo.tags():
                print(""  - {0}"".format(tag))


if __name__ == '__main__':
    try:
        cli = CLI()
        cli.run()
    except KeyboardInterrupt:
        pass

```"
fa4b4de37b38f0ff800bbd2ac007ab6521720258,scripts/tests/test_box_migrate_to_external_account.py,scripts/tests/test_box_migrate_to_external_account.py,,"from nose.tools import *

from scripts.box.migrate_to_external_account import do_migration, get_targets

from framework.auth import Auth

from tests.base import OsfTestCase
from tests.factories import ProjectFactory, UserFactory

from website.addons.box.model import BoxUserSettings
from website.addons.box.tests.factories import BoxOAuthSettingsFactory


class TestBoxMigration(OsfTestCase):
    # Note: BoxUserSettings.user_settings has to be changed to foreign_user_settings (model and mongo). See migration instructions

    def test_migration_no_project(self):

        user = UserFactory()

        user.add_addon('box')
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        do_migration([user_addon])
        user_addon.reload()

        assert_is_none(user_addon.oauth_settings)
        assert_equal(len(user.external_accounts), 1)

        account = user.external_accounts[0]
        assert_equal(account.provider, 'box')
        assert_equal(account.oauth_key, 'abcdef1')

    def test_migration_removes_targets(self):
        BoxUserSettings.remove()

        user = UserFactory()
        project = ProjectFactory(creator=user)

        user.add_addon('box', auth=Auth(user))
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        project.add_addon('box', auth=Auth(user))
        node_addon = project.get_addon('box')
        node_addon.foreign_user_settings = user_addon
        node_addon.save()

        assert_equal(get_targets().count(), 1)

        do_migration([user_addon])
        user_addon.reload()

        assert_equal(get_targets().count(), 0)

    def test_migration_multiple_users(self):
        user1 = UserFactory()
        user2 = UserFactory()
        oauth_settings = BoxOAuthSettingsFactory()

        user1.add_addon('box')
        user1_addon = user1.get_addon('box')
        user1_addon.oauth_settings = oauth_settings
        user1_addon.save()

        user2.add_addon('box')
        user2_addon = user2.get_addon('box')
        user2_addon.oauth_settings = oauth_settings
        user2_addon.save()

        do_migration([user1_addon, user2_addon])
        user1_addon.reload()
        user2_addon.reload()

        assert_equal(
            user1.external_accounts[0],
            user2.external_accounts[0],
        )

    def test_get_targets(self):
        BoxUserSettings.remove()
        addons = [
            BoxUserSettings(),
            BoxUserSettings(oauth_settings=BoxOAuthSettingsFactory()),
        ]
        for addon in addons:
            addon.save()
        targets = get_targets()
        assert_equal(targets.count(), 1)
        assert_equal(targets[0]._id, addons[-1]._id)
",Add test for box migration script,"Add test for box migration script
",Python,apache-2.0,"ticklemepierce/osf.io,haoyuchen1992/osf.io,Nesiehr/osf.io,wearpants/osf.io,zachjanicki/osf.io,acshi/osf.io,chennan47/osf.io,ZobairAlijan/osf.io,cwisecarver/osf.io,njantrania/osf.io,TomHeatwole/osf.io,chrisseto/osf.io,felliott/osf.io,erinspace/osf.io,TomBaxter/osf.io,abought/osf.io,jnayak1/osf.io,rdhyee/osf.io,aaxelb/osf.io,billyhunt/osf.io,jnayak1/osf.io,binoculars/osf.io,emetsger/osf.io,wearpants/osf.io,saradbowman/osf.io,alexschiller/osf.io,Johnetordoff/osf.io,HalcyonChimera/osf.io,chrisseto/osf.io,caseyrollins/osf.io,binoculars/osf.io,Nesiehr/osf.io,mfraezz/osf.io,SSJohns/osf.io,hmoco/osf.io,DanielSBrown/osf.io,ticklemepierce/osf.io,KAsante95/osf.io,GageGaskins/osf.io,Johnetordoff/osf.io,kch8qx/osf.io,doublebits/osf.io,cslzchen/osf.io,caseyrygt/osf.io,kch8qx/osf.io,emetsger/osf.io,amyshi188/osf.io,danielneis/osf.io,RomanZWang/osf.io,RomanZWang/osf.io,icereval/osf.io,mattclark/osf.io,DanielSBrown/osf.io,amyshi188/osf.io,jnayak1/osf.io,billyhunt/osf.io,haoyuchen1992/osf.io,GageGaskins/osf.io,amyshi188/osf.io,monikagrabowska/osf.io,Johnetordoff/osf.io,mluke93/osf.io,wearpants/osf.io,alexschiller/osf.io,mluo613/osf.io,kch8qx/osf.io,haoyuchen1992/osf.io,caneruguz/osf.io,icereval/osf.io,brandonPurvis/osf.io,samanehsan/osf.io,chrisseto/osf.io,abought/osf.io,cwisecarver/osf.io,KAsante95/osf.io,chennan47/osf.io,GageGaskins/osf.io,mfraezz/osf.io,KAsante95/osf.io,leb2dg/osf.io,ticklemepierce/osf.io,cosenal/osf.io,adlius/osf.io,chennan47/osf.io,HalcyonChimera/osf.io,caneruguz/osf.io,aaxelb/osf.io,caneruguz/osf.io,haoyuchen1992/osf.io,CenterForOpenScience/osf.io,kwierman/osf.io,acshi/osf.io,Ghalko/osf.io,baylee-d/osf.io,mattclark/osf.io,ZobairAlijan/osf.io,amyshi188/osf.io,Ghalko/osf.io,cosenal/osf.io,billyhunt/osf.io,GageGaskins/osf.io,brianjgeiger/osf.io,felliott/osf.io,acshi/osf.io,cslzchen/osf.io,Nesiehr/osf.io,wearpants/osf.io,zachjanicki/osf.io,mluo613/osf.io,Ghalko/osf.io,sloria/osf.io,cslzchen/osf.io,brianjgeiger/osf.io,doublebits/osf.io,mluke93/osf.io,zamattiac/osf.io,acshi/osf.io,TomBaxter/osf.io,alexschiller/osf.io,caneruguz/osf.io,crcresearch/osf.io,pattisdr/osf.io,billyhunt/osf.io,samanehsan/osf.io,monikagrabowska/osf.io,aaxelb/osf.io,monikagrabowska/osf.io,hmoco/osf.io,asanfilippo7/osf.io,Ghalko/osf.io,acshi/osf.io,HalcyonChimera/osf.io,adlius/osf.io,crcresearch/osf.io,njantrania/osf.io,caseyrollins/osf.io,zamattiac/osf.io,caseyrygt/osf.io,emetsger/osf.io,cwisecarver/osf.io,laurenrevere/osf.io,hmoco/osf.io,kwierman/osf.io,erinspace/osf.io,GageGaskins/osf.io,CenterForOpenScience/osf.io,caseyrygt/osf.io,RomanZWang/osf.io,danielneis/osf.io,sloria/osf.io,billyhunt/osf.io,rdhyee/osf.io,binoculars/osf.io,HalcyonChimera/osf.io,mluo613/osf.io,TomBaxter/osf.io,TomHeatwole/osf.io,ZobairAlijan/osf.io,erinspace/osf.io,doublebits/osf.io,leb2dg/osf.io,TomHeatwole/osf.io,felliott/osf.io,Johnetordoff/osf.io,monikagrabowska/osf.io,alexschiller/osf.io,zachjanicki/osf.io,mluo613/osf.io,samchrisinger/osf.io,brandonPurvis/osf.io,caseyrollins/osf.io,samanehsan/osf.io,kwierman/osf.io,rdhyee/osf.io,danielneis/osf.io,samanehsan/osf.io,alexschiller/osf.io,mattclark/osf.io,saradbowman/osf.io,DanielSBrown/osf.io,laurenrevere/osf.io,samchrisinger/osf.io,crcresearch/osf.io,njantrania/osf.io,zamattiac/osf.io,abought/osf.io,rdhyee/osf.io,jnayak1/osf.io,pattisdr/osf.io,cosenal/osf.io,asanfilippo7/osf.io,samchrisinger/osf.io,brianjgeiger/osf.io,doublebits/osf.io,baylee-d/osf.io,aaxelb/osf.io,monikagrabowska/osf.io,leb2dg/osf.io,SSJohns/osf.io,mluke93/osf.io,ticklemepierce/osf.io,brandonPurvis/osf.io,brianjgeiger/osf.io,mfraezz/osf.io,mluo613/osf.io,caseyrygt/osf.io,doublebits/osf.io,icereval/osf.io,hmoco/osf.io,ZobairAlijan/osf.io,cosenal/osf.io,asanfilippo7/osf.io,adlius/osf.io,SSJohns/osf.io,cslzchen/osf.io,cwisecarver/osf.io,danielneis/osf.io,SSJohns/osf.io,kwierman/osf.io,zamattiac/osf.io,emetsger/osf.io,baylee-d/osf.io,Nesiehr/osf.io,adlius/osf.io,sloria/osf.io,leb2dg/osf.io,mluke93/osf.io,njantrania/osf.io,chrisseto/osf.io,kch8qx/osf.io,laurenrevere/osf.io,TomHeatwole/osf.io,brandonPurvis/osf.io,kch8qx/osf.io,DanielSBrown/osf.io,KAsante95/osf.io,RomanZWang/osf.io,RomanZWang/osf.io,pattisdr/osf.io,CenterForOpenScience/osf.io,mfraezz/osf.io,brandonPurvis/osf.io,samchrisinger/osf.io,abought/osf.io,CenterForOpenScience/osf.io,zachjanicki/osf.io,asanfilippo7/osf.io,felliott/osf.io,KAsante95/osf.io",94,"```python
from nose.tools import *

from scripts.box.migrate_to_external_account import do_migration, get_targets

from framework.auth import Auth

from tests.base import OsfTestCase
from tests.factories import ProjectFactory, UserFactory

from website.addons.box.model import BoxUserSettings
from website.addons.box.tests.factories import BoxOAuthSettingsFactory


class TestBoxMigration(OsfTestCase):
    # Note: BoxUserSettings.user_settings has to be changed to foreign_user_settings (model and mongo). See migration instructions

    def test_migration_no_project(self):

        user = UserFactory()

        user.add_addon('box')
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        do_migration([user_addon])
        user_addon.reload()

        assert_is_none(user_addon.oauth_settings)
        assert_equal(len(user.external_accounts), 1)

        account = user.external_accounts[0]
        assert_equal(account.provider, 'box')
        assert_equal(account.oauth_key, 'abcdef1')

    def test_migration_removes_targets(self):
        BoxUserSettings.remove()

        user = UserFactory()
        project = ProjectFactory(creator=user)

        user.add_addon('box', auth=Auth(user))
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        project.add_addon('box', auth=Auth(user))
        node_addon = project.get_addon('box')
        node_addon.foreign_user_settings = user_addon
        node_addon.save()

        assert_equal(get_targets().count(), 1)

        do_migration([user_addon])
        user_addon.reload()

        assert_equal(get_targets().count(), 0)

    def test_migration_multiple_users(self):
        user1 = UserFactory()
        user2 = UserFactory()
        oauth_settings = BoxOAuthSettingsFactory()

        user1.add_addon('box')
        user1_addon = user1.get_addon('box')
        user1_addon.oauth_settings = oauth_settings
        user1_addon.save()

        user2.add_addon('box')
        user2_addon = user2.get_addon('box')
        user2_addon.oauth_settings = oauth_settings
        user2_addon.save()

        do_migration([user1_addon, user2_addon])
        user1_addon.reload()
        user2_addon.reload()

        assert_equal(
            user1.external_accounts[0],
            user2.external_accounts[0],
        )

    def test_get_targets(self):
        BoxUserSettings.remove()
        addons = [
            BoxUserSettings(),
            BoxUserSettings(oauth_settings=BoxOAuthSettingsFactory()),
        ]
        for addon in addons:
            addon.save()
        targets = get_targets()
        assert_equal(targets.count(), 1)
        assert_equal(targets[0]._id, addons[-1]._id)

```"
71d8ef8a872656df8a2319032855cb2b5ea5ed4b,examples/bench/rlserver.py,examples/bench/rlserver.py,,"import argparse
import asyncio
import gc
import uvloop
import os.path
import socket as socket_module

from socket import *


PRINT = 0


async def echo_client_streams(reader, writer):
    sock = writer.get_extra_info('socket')
    try:
        sock.setsockopt(IPPROTO_TCP, TCP_NODELAY, 1)
    except (OSError, NameError):
        pass
    if PRINT:
        print('Connection from', sock.getpeername())
    while True:
        data = await reader.readline()
        if not data:
            break
        writer.write(data)
    if PRINT:
        print('Connection closed')
    writer.close()


async def print_debug(loop):
    while True:
        print(chr(27) + ""[2J"")  # clear screen
        loop.print_debug_info()
        await asyncio.sleep(0.5, loop=loop)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--uvloop', default=False, action='store_true')
    parser.add_argument('--addr', default='127.0.0.1:25000', type=str)
    parser.add_argument('--print', default=False, action='store_true')
    args = parser.parse_args()

    if args.uvloop:
        loop = uvloop.new_event_loop()
        print('using UVLoop')
    else:
        loop = asyncio.new_event_loop()
        print('using asyncio loop')

    asyncio.set_event_loop(loop)
    loop.set_debug(False)

    if args.print:
        PRINT = 1

    if hasattr(loop, 'print_debug_info'):
        loop.create_task(print_debug(loop))
        PRINT = 0

    unix = False
    if args.addr.startswith('file:'):
        unix = True
        addr = args.addr[5:]
        if os.path.exists(addr):
            os.remove(addr)
    else:
        addr = args.addr.split(':')
        addr[1] = int(addr[1])
        addr = tuple(addr)

    print('readline performance test')
    print('serving on: {}'.format(addr))

    print('using asyncio/streams')
    if unix:
        coro = asyncio.start_unix_server(echo_client_streams,
                                         addr, loop=loop, limit=256000)
    else:
        coro = asyncio.start_server(echo_client_streams,
                                    *addr, loop=loop, limit=256000)
    srv = loop.run_until_complete(coro)

    try:
        loop.run_forever()
    finally:
        if hasattr(loop, 'print_debug_info'):
            gc.collect()
            print(chr(27) + ""[2J"")
            loop.print_debug_info()

        loop.close()
",Add a new benchmark - readline server,"examples/bench: Add a new benchmark - readline server
",Python,apache-2.0,"MagicStack/uvloop,1st1/uvloop,MagicStack/uvloop",95,"```python
import argparse
import asyncio
import gc
import uvloop
import os.path
import socket as socket_module

from socket import *


PRINT = 0


async def echo_client_streams(reader, writer):
    sock = writer.get_extra_info('socket')
    try:
        sock.setsockopt(IPPROTO_TCP, TCP_NODELAY, 1)
    except (OSError, NameError):
        pass
    if PRINT:
        print('Connection from', sock.getpeername())
    while True:
        data = await reader.readline()
        if not data:
            break
        writer.write(data)
    if PRINT:
        print('Connection closed')
    writer.close()


async def print_debug(loop):
    while True:
        print(chr(27) + ""[2J"")  # clear screen
        loop.print_debug_info()
        await asyncio.sleep(0.5, loop=loop)


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--uvloop', default=False, action='store_true')
    parser.add_argument('--addr', default='127.0.0.1:25000', type=str)
    parser.add_argument('--print', default=False, action='store_true')
    args = parser.parse_args()

    if args.uvloop:
        loop = uvloop.new_event_loop()
        print('using UVLoop')
    else:
        loop = asyncio.new_event_loop()
        print('using asyncio loop')

    asyncio.set_event_loop(loop)
    loop.set_debug(False)

    if args.print:
        PRINT = 1

    if hasattr(loop, 'print_debug_info'):
        loop.create_task(print_debug(loop))
        PRINT = 0

    unix = False
    if args.addr.startswith('file:'):
        unix = True
        addr = args.addr[5:]
        if os.path.exists(addr):
            os.remove(addr)
    else:
        addr = args.addr.split(':')
        addr[1] = int(addr[1])
        addr = tuple(addr)

    print('readline performance test')
    print('serving on: {}'.format(addr))

    print('using asyncio/streams')
    if unix:
        coro = asyncio.start_unix_server(echo_client_streams,
                                         addr, loop=loop, limit=256000)
    else:
        coro = asyncio.start_server(echo_client_streams,
                                    *addr, loop=loop, limit=256000)
    srv = loop.run_until_complete(coro)

    try:
        loop.run_forever()
    finally:
        if hasattr(loop, 'print_debug_info'):
            gc.collect()
            print(chr(27) + ""[2J"")
            loop.print_debug_info()

        loop.close()

```"
f09ee3772d6e15a104af284ed6864005cf8450ef,ch11/radix_sort8.py,ch11/radix_sort8.py,,"""""""
Listing 11.4: An eight-element radix sort
""""""

from io import open
import numpy as np
import pyopencl as cl
import utility

NUM_SHORTS = 8

kernel_src = '''
__kernel void radix_sort8(__global ushort8 *global_data) {

   typedef union {
      ushort8 vec;
      ushort array[8];
   } vec_array;

   uint one_count, zero_count;
   uint cmp_value = 1;
   vec_array mask, ones, data;

   data.vec = global_data[0];

   /* Rearrange elements according to bits */
   for(int i=0; i<3; i++) {
      zero_count = 0;
      one_count = 0;

      /* Iterate through each element in the input vector */
      for(int j = 0; j < 8; j++) {
         if(data.array[j] & cmp_value)

            /* Place element in ones vector */
            ones.array[one_count++] = data.array[j];
         else {

            /* Increment number of elements with zero */
            mask.array[zero_count++] = j;
         }
      }

      /* Create sorted vector */
      for(int j = zero_count; j < 8; j++)
         mask.array[j] = 8 - zero_count + j;
      data.vec = shuffle2(data.vec, ones.vec, mask.vec);
      cmp_value <<= 1;
   }
   global_data[0] = data.vec;
}
'''

# Get device and context, create command queue and program
dev = utility.get_default_device()
context = cl.Context(devices=[dev], properties=None, dev_type=None, cache_dir=None)
queue = cl.CommandQueue(context, dev, properties=None)

# Build program in the specified context using the kernel source code
prog = cl.Program(context, kernel_src)
try:
    prog.build(options=['-Werror'], devices=[dev], cache_dir=None)
except:
    print('Build log:')
    print(prog.get_build_info(dev, cl.program_build_info.LOG))
    raise

# Data and device buffers
data = np.arange(start=0, stop=NUM_SHORTS, dtype=np.uint16)
np.random.shuffle(data)
print('Input: ' + str(data))

mf = cl.mem_flags
data_buffer = cl.Buffer(context, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=data)

# Execute kernel
# radix_sort8(__global ushort8 *global_data)
kernel = prog.radix_sort8
kernel.set_arg(0, data_buffer)
cl.enqueue_task(queue, kernel)
cl.enqueue_copy(queue, dest=data, src=data_buffer, is_blocking=True)

print('Output: ' + str(data))






",Add example from listing 11.4,"Add example from listing 11.4
",Python,mit,oysstu/pyopencl-in-action,90,"```python
""""""
Listing 11.4: An eight-element radix sort
""""""

from io import open
import numpy as np
import pyopencl as cl
import utility

NUM_SHORTS = 8

kernel_src = '''
__kernel void radix_sort8(__global ushort8 *global_data) {

   typedef union {
      ushort8 vec;
      ushort array[8];
   } vec_array;

   uint one_count, zero_count;
   uint cmp_value = 1;
   vec_array mask, ones, data;

   data.vec = global_data[0];

   /* Rearrange elements according to bits */
   for(int i=0; i<3; i++) {
      zero_count = 0;
      one_count = 0;

      /* Iterate through each element in the input vector */
      for(int j = 0; j < 8; j++) {
         if(data.array[j] & cmp_value)

            /* Place element in ones vector */
            ones.array[one_count++] = data.array[j];
         else {

            /* Increment number of elements with zero */
            mask.array[zero_count++] = j;
         }
      }

      /* Create sorted vector */
      for(int j = zero_count; j < 8; j++)
         mask.array[j] = 8 - zero_count + j;
      data.vec = shuffle2(data.vec, ones.vec, mask.vec);
      cmp_value <<= 1;
   }
   global_data[0] = data.vec;
}
'''

# Get device and context, create command queue and program
dev = utility.get_default_device()
context = cl.Context(devices=[dev], properties=None, dev_type=None, cache_dir=None)
queue = cl.CommandQueue(context, dev, properties=None)

# Build program in the specified context using the kernel source code
prog = cl.Program(context, kernel_src)
try:
    prog.build(options=['-Werror'], devices=[dev], cache_dir=None)
except:
    print('Build log:')
    print(prog.get_build_info(dev, cl.program_build_info.LOG))
    raise

# Data and device buffers
data = np.arange(start=0, stop=NUM_SHORTS, dtype=np.uint16)
np.random.shuffle(data)
print('Input: ' + str(data))

mf = cl.mem_flags
data_buffer = cl.Buffer(context, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=data)

# Execute kernel
# radix_sort8(__global ushort8 *global_data)
kernel = prog.radix_sort8
kernel.set_arg(0, data_buffer)
cl.enqueue_task(queue, kernel)
cl.enqueue_copy(queue, dest=data, src=data_buffer, is_blocking=True)

print('Output: ' + str(data))







```"
10c83fbc01dee9d95290466338f262abffc12a3e,samples/create_folder_in_datacenter.py,samples/create_folder_in_datacenter.py,,"#!/usr/bin/env python
""""""
Written by Chinmaya Bharadwaj
Github: https://github.com/chinmayb/
Email: acbharadwaj@gmail.com

Create a folder in a datacenter
""""""
from __future__ import print_function

from pyVmomi import vim

from pyVim.connect import SmartConnect, Disconnect

import argparse
import atexit
import getpass


def GetArgs():
    """"""
    Supports the command-line arguments listed below.
    """"""
    parser = argparse.ArgumentParser(
        description='Process args for retrieving all the Virtual Machines')
    parser.add_argument('-s', '--host', required=True, action='store',
                        help='Remote host to connect to')
    parser.add_argument('-o', '--port', type=int, default=443, action='store',
                        help='Port to connect on')
    parser.add_argument('-u', '--user', required=True, action='store',
                        help='User name to use when connecting to host')
    parser.add_argument('-p', '--password', required=False, action='store',
                        help='Password to use when connecting to host')
    parser.add_argument('-d', '--datacenter', required=True,
                        help='name of the datacenter'),
    parser.add_argument('-f', '--folder', required=True,
                        help='name of the folder')
    args = parser.parse_args()
    return args


def get_obj(content, vimtype, name):
    obj = None
    container = content.viewManager.CreateContainerView(
        content.rootFolder, vimtype, True)
    for c in container.view:
        if c.name == name:
            obj = c
            break
    return obj


def create_folder(content, host_folder, folder_name):
    host_folder.CreateFolder(folder_name)


def main():
    """"""
    Simple command-line program for listing the virtual machines on a system.
    """"""
    args = GetArgs()
    if args.password:
        password = args.password
    else:
        password = getpass.getpass(prompt='Enter password for host %s and '
                                   'user %s: ' % (args.host, args.user))

    si = SmartConnect(host=args.host,
                      user=args.user,
                      pwd=password,
                      port=int(args.port))
    if not si:
        print(""Could not connect to the specified host using specified ""
              ""username and password"")
        return -1

    atexit.register(Disconnect, si)

    content = si.RetrieveContent()
    dc = get_obj(content, [vim.Datacenter], args.datacenter)
    if (get_obj(content, [vim.Folder], args.folder)):
        print(""Folder '%s' already exists"" % args.folder)
        return 0
    create_folder(content, dc.hostFolder, args.folder)
    print(""Successfully created the folder '%s'"" % args.folder)
    return 0

# Start program
if __name__ == ""__main__"":
    main()
",Create a folder in a datacenter if not exists,"Example: Create a folder in a datacenter if not exists
",Python,apache-2.0,"vmware/pyvmomi-community-samples,pfitzer/pyvmomi-community-samples,jm66/pyvmomi-community-samples,ddcrjlalumiere/pyvmomi-community-samples,prziborowski/pyvmomi-community-samples,pathcl/pyvmomi-community-samples",91,"```python
#!/usr/bin/env python
""""""
Written by Chinmaya Bharadwaj
Github: https://github.com/chinmayb/
Email: acbharadwaj@gmail.com

Create a folder in a datacenter
""""""
from __future__ import print_function

from pyVmomi import vim

from pyVim.connect import SmartConnect, Disconnect

import argparse
import atexit
import getpass


def GetArgs():
    """"""
    Supports the command-line arguments listed below.
    """"""
    parser = argparse.ArgumentParser(
        description='Process args for retrieving all the Virtual Machines')
    parser.add_argument('-s', '--host', required=True, action='store',
                        help='Remote host to connect to')
    parser.add_argument('-o', '--port', type=int, default=443, action='store',
                        help='Port to connect on')
    parser.add_argument('-u', '--user', required=True, action='store',
                        help='User name to use when connecting to host')
    parser.add_argument('-p', '--password', required=False, action='store',
                        help='Password to use when connecting to host')
    parser.add_argument('-d', '--datacenter', required=True,
                        help='name of the datacenter'),
    parser.add_argument('-f', '--folder', required=True,
                        help='name of the folder')
    args = parser.parse_args()
    return args


def get_obj(content, vimtype, name):
    obj = None
    container = content.viewManager.CreateContainerView(
        content.rootFolder, vimtype, True)
    for c in container.view:
        if c.name == name:
            obj = c
            break
    return obj


def create_folder(content, host_folder, folder_name):
    host_folder.CreateFolder(folder_name)


def main():
    """"""
    Simple command-line program for listing the virtual machines on a system.
    """"""
    args = GetArgs()
    if args.password:
        password = args.password
    else:
        password = getpass.getpass(prompt='Enter password for host %s and '
                                   'user %s: ' % (args.host, args.user))

    si = SmartConnect(host=args.host,
                      user=args.user,
                      pwd=password,
                      port=int(args.port))
    if not si:
        print(""Could not connect to the specified host using specified ""
              ""username and password"")
        return -1

    atexit.register(Disconnect, si)

    content = si.RetrieveContent()
    dc = get_obj(content, [vim.Datacenter], args.datacenter)
    if (get_obj(content, [vim.Folder], args.folder)):
        print(""Folder '%s' already exists"" % args.folder)
        return 0
    create_folder(content, dc.hostFolder, args.folder)
    print(""Successfully created the folder '%s'"" % args.folder)
    return 0

# Start program
if __name__ == ""__main__"":
    main()

```"
f7a1595e39eeb754290c62e9194868d98d9755f4,tests/symbols/test_symbol_selection.py,tests/symbols/test_symbol_selection.py,,"import pytest

from tests.symbols import get_symbols
from thinglang.compiler.errors import NoMatchingOverload
from thinglang.compiler.references import Reference
from thinglang.lexer.values.identifier import Identifier
from thinglang.parser.values.named_access import NamedAccess
from thinglang.symbols.argument_selector import ArgumentSelector

SOURCE_OVERLOADING = '''

thing Container

thing Container1 extends Container
thing Container2 extends Container
    as Container1
thing Container3 extends Container
    as Container1
thing Container4 extends Container
    as Container1
    as Container2
    
thing Container1Child extends Container1
thing Container1Child2 extends Container1
    as Container2
thing Container2Child extends Container2
    

thing A
    does overloaded with Container1 container
    does overloaded with Container2 container
    does overloaded with Container2Child container
    does overloaded with Container1 c1, Container2 c2
    does overloaded with Container1 c1, Container2Child c2

'''


# TODO: verify no cast to base type!


SYMBOLS = get_symbols(SOURCE_OVERLOADING)
BASE = SYMBOLS.resolve(NamedAccess.auto('A.overloaded'))


def get_selection(*target_types):
    selector = BASE.element.selector(SYMBOLS)

    for target_type in target_types:
        selector.constraint(Reference(Identifier(target_type)))

    return selector.disambiguate(None)


def verify_selection(target_type, expected_index, expected_match):
    target = get_selection(*target_type)

    assert target.symbol.index == expected_index
    assert target.match == expected_match


def test_exact_match():
    verify_selection(['Container1'], 1, ArgumentSelector.EXACT)
    verify_selection(['Container2'], 2, ArgumentSelector.EXACT)  # Matches exactly, despite being castable
    verify_selection(['Container2Child'], 3, ArgumentSelector.EXACT)  # Matches exactly, despite being in an inheritance chain

    verify_selection(['Container1', 'Container2'], 4, ArgumentSelector.EXACT)
    verify_selection(['Container1', 'Container2Child'], 5, ArgumentSelector.EXACT)


def test_inheritance_match():
    verify_selection(['Container1Child'], 1, ArgumentSelector.INHERITANCE)
    verify_selection(['Container1Child2'], 1, ArgumentSelector.INHERITANCE)  # Matches in an inheritance chain, despite being castable


def test_casted_match():
    verify_selection(['Container3'], 1, ArgumentSelector.CAST)


def test_inheritance_directionality():  # Verify that a prent is not accepted in place of a child
    with pytest.raises(NoMatchingOverload) as exc:
        get_selection('Container')

    assert not exc.value.exact_matches and not exc.value.inheritance_matches and not exc.value.cast_matches


def test_cast_ambiguity():  # Verify cast ambiguity
    with pytest.raises(NoMatchingOverload) as exc:
        get_selection('Container4')

    assert not exc.value.exact_matches and not exc.value.inheritance_matches and len(exc.value.cast_matches) == 2

",Add test for symbol selection,"Add test for symbol selection
",Python,mit,"ytanay/thinglang,ytanay/thinglang,ytanay/thinglang,ytanay/thinglang",93,"```python
import pytest

from tests.symbols import get_symbols
from thinglang.compiler.errors import NoMatchingOverload
from thinglang.compiler.references import Reference
from thinglang.lexer.values.identifier import Identifier
from thinglang.parser.values.named_access import NamedAccess
from thinglang.symbols.argument_selector import ArgumentSelector

SOURCE_OVERLOADING = '''

thing Container

thing Container1 extends Container
thing Container2 extends Container
    as Container1
thing Container3 extends Container
    as Container1
thing Container4 extends Container
    as Container1
    as Container2
    
thing Container1Child extends Container1
thing Container1Child2 extends Container1
    as Container2
thing Container2Child extends Container2
    

thing A
    does overloaded with Container1 container
    does overloaded with Container2 container
    does overloaded with Container2Child container
    does overloaded with Container1 c1, Container2 c2
    does overloaded with Container1 c1, Container2Child c2

'''


# TODO: verify no cast to base type!


SYMBOLS = get_symbols(SOURCE_OVERLOADING)
BASE = SYMBOLS.resolve(NamedAccess.auto('A.overloaded'))


def get_selection(*target_types):
    selector = BASE.element.selector(SYMBOLS)

    for target_type in target_types:
        selector.constraint(Reference(Identifier(target_type)))

    return selector.disambiguate(None)


def verify_selection(target_type, expected_index, expected_match):
    target = get_selection(*target_type)

    assert target.symbol.index == expected_index
    assert target.match == expected_match


def test_exact_match():
    verify_selection(['Container1'], 1, ArgumentSelector.EXACT)
    verify_selection(['Container2'], 2, ArgumentSelector.EXACT)  # Matches exactly, despite being castable
    verify_selection(['Container2Child'], 3, ArgumentSelector.EXACT)  # Matches exactly, despite being in an inheritance chain

    verify_selection(['Container1', 'Container2'], 4, ArgumentSelector.EXACT)
    verify_selection(['Container1', 'Container2Child'], 5, ArgumentSelector.EXACT)


def test_inheritance_match():
    verify_selection(['Container1Child'], 1, ArgumentSelector.INHERITANCE)
    verify_selection(['Container1Child2'], 1, ArgumentSelector.INHERITANCE)  # Matches in an inheritance chain, despite being castable


def test_casted_match():
    verify_selection(['Container3'], 1, ArgumentSelector.CAST)


def test_inheritance_directionality():  # Verify that a prent is not accepted in place of a child
    with pytest.raises(NoMatchingOverload) as exc:
        get_selection('Container')

    assert not exc.value.exact_matches and not exc.value.inheritance_matches and not exc.value.cast_matches


def test_cast_ambiguity():  # Verify cast ambiguity
    with pytest.raises(NoMatchingOverload) as exc:
        get_selection('Container4')

    assert not exc.value.exact_matches and not exc.value.inheritance_matches and len(exc.value.cast_matches) == 2


```"
41f0533edc9ebe788722711af95e040d4f06abb9,lglass/bird.py,lglass/bird.py,,"# coding: utf-8

import subprocess

import netaddr

import lglass.route

class BirdClient(object):
	def __init__(self, executable=""birdc""):
		self.executable = executable
	
	def send(self, command, raw=False):
		argv = [self.executable]
		if raw:
			argv.append(""-v"")
		if isinstance(command, str):
			argv.extend(command.split())
		else:
			argv.extend(command)
		p = subprocess.Popen(argv,
			stdout=subprocess.PIPE, stdin=subprocess.DEVNULL, stderr=subprocess.PIPE)
		data = b""""
		while True:
			rdata = p.stdout.read()
			if len(rdata) == 0:
				break
			data += rdata
		p.wait()
		return data.split(b""\n"", 1)[1]

	def routes(self, table=None, protocol=None, primary=False, all=True, filtered=False):
		command = [""show"", ""route""]
		if table is not None:
			command.append(""table"")
			command.append(str(table))
		if all:
			command.append(""all"")
		if primary:
			command.append(""primary"")
		if filtered:
			command.append(""filtered"")
		if protocol is not None:
			command.append(str(protocol))
		res = self.send(command)

		return list(parse_routes(res.decode().splitlines()))
	
	def protocols(self):
		command = [""show"", ""protocols""]
		res = self.send(command)
		for line in res.splitlines()[1:]:
			t = line.decode().split()
			while len(t) < 7:
				t.append(None)
			yield tuple(t)

def parse_routes(lines):
	lines_iter = iter(lines)

	cur_prefix = None
	cur_route = None
	
	for line in lines_iter:
		if line[0] == ""\t"":
			# route annotation
			key, value = line.split("":"", 1)
			cur_route[key.strip()] = value.strip()
			continue

		if cur_route is not None:
			yield cur_route

		if line[0] != "" "":
			cur_prefix, *args = line.split()
		else:
			args = line.split()

		cur_route = lglass.route.Route(cur_prefix)

		if args[0] == ""via"":
			cur_route.nexthop = (netaddr.IPAddress(args[1]), args[3])

		if args[-2][0] == ""("" and args[-2][-1] == "")"":
			metric = args[-2][1:-1]
			if ""/"" in metric:
				metric = metric.split(""/"", 1)[0]
			cur_route.metric = int(metric)

	if cur_route is not None:
		yield cur_route
	
",Add simple BIRD client class,"Add simple BIRD client class
",Python,mit,fritz0705/lglass,93,"```python
# coding: utf-8

import subprocess

import netaddr

import lglass.route

class BirdClient(object):
	def __init__(self, executable=""birdc""):
		self.executable = executable
	
	def send(self, command, raw=False):
		argv = [self.executable]
		if raw:
			argv.append(""-v"")
		if isinstance(command, str):
			argv.extend(command.split())
		else:
			argv.extend(command)
		p = subprocess.Popen(argv,
			stdout=subprocess.PIPE, stdin=subprocess.DEVNULL, stderr=subprocess.PIPE)
		data = b""""
		while True:
			rdata = p.stdout.read()
			if len(rdata) == 0:
				break
			data += rdata
		p.wait()
		return data.split(b""\n"", 1)[1]

	def routes(self, table=None, protocol=None, primary=False, all=True, filtered=False):
		command = [""show"", ""route""]
		if table is not None:
			command.append(""table"")
			command.append(str(table))
		if all:
			command.append(""all"")
		if primary:
			command.append(""primary"")
		if filtered:
			command.append(""filtered"")
		if protocol is not None:
			command.append(str(protocol))
		res = self.send(command)

		return list(parse_routes(res.decode().splitlines()))
	
	def protocols(self):
		command = [""show"", ""protocols""]
		res = self.send(command)
		for line in res.splitlines()[1:]:
			t = line.decode().split()
			while len(t) < 7:
				t.append(None)
			yield tuple(t)

def parse_routes(lines):
	lines_iter = iter(lines)

	cur_prefix = None
	cur_route = None
	
	for line in lines_iter:
		if line[0] == ""\t"":
			# route annotation
			key, value = line.split("":"", 1)
			cur_route[key.strip()] = value.strip()
			continue

		if cur_route is not None:
			yield cur_route

		if line[0] != "" "":
			cur_prefix, *args = line.split()
		else:
			args = line.split()

		cur_route = lglass.route.Route(cur_prefix)

		if args[0] == ""via"":
			cur_route.nexthop = (netaddr.IPAddress(args[1]), args[3])

		if args[-2][0] == ""("" and args[-2][-1] == "")"":
			metric = args[-2][1:-1]
			if ""/"" in metric:
				metric = metric.split(""/"", 1)[0]
			cur_route.metric = int(metric)

	if cur_route is not None:
		yield cur_route
	

```"
b95e5cd706a1cf81e41debae30422345cef3a1ee,tests/committerparser.py,tests/committerparser.py,,"#!/usr/bin/python
import sys
import getopt
import re
import email.utils
import datetime

class Usage(Exception):
    def __init__(self, msg):
        self.msg = msg

def parse_date(datestr):
    d = email.utils.parsedate(datestr)

    return datetime.datetime(d[0],d[1],d[2],d[3],d[4],d[5],d[6])

def parse_gitlog(filename=None):

    results = {}
    commits = {}

    if not filename or filename == '-':
        fh = sys.stdin
    else:
        fh = open(filename, 'r+')

    commitcount = 0
    for line in fh.readlines():
        line = line.rstrip()
        if line.startswith('commit '):
            new_commit = True
            commitcount += 1
            continue

        if line.startswith('Author:'):
            author = re.match('Author:\s+(.*)\s+<(.*)>', line)
            if author:
                email = author.group(2)
            continue

        if line.startswith('Date:'):

            isodate = re.match('Date:\s+(.*)', line)
            d = parse_date(isodate.group(1))
            continue

        if len(line) < 2 and new_commit:
            new_commit = False
            key = '{0}-{1}'.format(d.year, str(d.month).zfill(2))

            if key not in results:
                results[key] = []

            if key not in commits:
                commits[key] = 0

            if email not in results[key]:
                results[key].append(email)

            commits[key] += commitcount
            commitcount = 0

    fh.close()

    return (results, commits)


def count_results(results, commits):

    result_str = ''
    print('Date\tContributors\tCommits')
    for k in sorted(results.iterkeys()):
        result_str += '{0}\t{1}\t{2}'.format(k, len(results[k]), commits[k])
        result_str += '\n'

    return result_str


def main(argv=None):
    if argv is None:
        argv = sys.argv
    try:
        try:
            opts, args = getopt.getopt(argv[1:], ""h"", [""help""])
        except getopt.error, msg:
             raise Usage(msg)
    except Usage, err:
        print >>sys.stderr, err.msg
        print >>sys.stderr, ""for help use --help""
        return 2

    if len(opts) > 0:
        if '-h' in opts[0] or '--help' in opts[0]:
            print('committerparser.py [- | logfilename]')
            print('   : Parse commit log from git and print number of commits and unique committers')
            print('   : by month.  Accepts a filename or reads from stdin.')
            return 0

    data, counts = parse_gitlog(filename=args[0])

    print count_results(data, counts)

if __name__ == ""__main__"":
    sys.exit(main())",Add simple parser to return some activity numbers from our git log.,"Add simple parser to return some activity numbers from our git log.
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",104,"```python
#!/usr/bin/python
import sys
import getopt
import re
import email.utils
import datetime

class Usage(Exception):
    def __init__(self, msg):
        self.msg = msg

def parse_date(datestr):
    d = email.utils.parsedate(datestr)

    return datetime.datetime(d[0],d[1],d[2],d[3],d[4],d[5],d[6])

def parse_gitlog(filename=None):

    results = {}
    commits = {}

    if not filename or filename == '-':
        fh = sys.stdin
    else:
        fh = open(filename, 'r+')

    commitcount = 0
    for line in fh.readlines():
        line = line.rstrip()
        if line.startswith('commit '):
            new_commit = True
            commitcount += 1
            continue

        if line.startswith('Author:'):
            author = re.match('Author:\s+(.*)\s+<(.*)>', line)
            if author:
                email = author.group(2)
            continue

        if line.startswith('Date:'):

            isodate = re.match('Date:\s+(.*)', line)
            d = parse_date(isodate.group(1))
            continue

        if len(line) < 2 and new_commit:
            new_commit = False
            key = '{0}-{1}'.format(d.year, str(d.month).zfill(2))

            if key not in results:
                results[key] = []

            if key not in commits:
                commits[key] = 0

            if email not in results[key]:
                results[key].append(email)

            commits[key] += commitcount
            commitcount = 0

    fh.close()

    return (results, commits)


def count_results(results, commits):

    result_str = ''
    print('Date\tContributors\tCommits')
    for k in sorted(results.iterkeys()):
        result_str += '{0}\t{1}\t{2}'.format(k, len(results[k]), commits[k])
        result_str += '\n'

    return result_str


def main(argv=None):
    if argv is None:
        argv = sys.argv
    try:
        try:
            opts, args = getopt.getopt(argv[1:], ""h"", [""help""])
        except getopt.error, msg:
             raise Usage(msg)
    except Usage, err:
        print >>sys.stderr, err.msg
        print >>sys.stderr, ""for help use --help""
        return 2

    if len(opts) > 0:
        if '-h' in opts[0] or '--help' in opts[0]:
            print('committerparser.py [- | logfilename]')
            print('   : Parse commit log from git and print number of commits and unique committers')
            print('   : by month.  Accepts a filename or reads from stdin.')
            return 0

    data, counts = parse_gitlog(filename=args[0])

    print count_results(data, counts)

if __name__ == ""__main__"":
    sys.exit(main())
```"
0781d105e4182bdd8abf1a8c7185311a48273c28,salt/beacons/smartos_imgadm.py,salt/beacons/smartos_imgadm.py,,"# -*- coding: utf-8 -*-
'''
Beacon that fires events on image import/delete.

.. code-block:: yaml

    ## minimal
    # - check for new images every 1 second (salt default)
    # - does not send events at startup
    beacons:
      imgadm: []

    ## standard
    # - check for new images every 60 seconds
    # - send import events at startup for all images
    beacons:
      imgadm:
        - interval: 60
        - startup_import_event: True
'''

# Import Python libs
from __future__ import absolute_import, unicode_literals
import logging

# Import 3rd-party libs
# pylint: disable=import-error
from salt.ext.six.moves import map
# pylint: enable=import-error

__virtualname__ = 'imgadm'
IMGADM_STATE = {
  'first_run': True,
  'images': [],
}

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Provides imgadm beacon on SmartOS
    '''
    if 'imgadm.list' in __salt__:
        return True
    else:
        return (
            False,
            '{0} beacon can only be loaded on SmartOS compute nodes'.format(
                __virtualname__
            )
        )


def validate(config):
    '''
    Validate the beacon configuration
    '''
    vcfg_ret = True
    vcfg_msg = 'Valid beacon configuration'

    if not isinstance(config, list):
        vcfg_ret = False
        vcfg_msg = 'Configuration for imgadm beacon must be a list!'

    return vcfg_ret, vcfg_msg


def beacon(config):
    '''
    Poll imgadm and compare available images
    '''
    ret = []

    # NOTE: lookup current images
    current_images = __salt__['imgadm.list'](verbose=True)

    # NOTE: apply configuration
    if IMGADM_STATE['first_run']:
        log.info('Applying configuration for imgadm beacon')

        _config = {}
        list(map(_config.update, config))

        if 'startup_import_event' not in _config or not _config['startup_import_event']:
            IMGADM_STATE['images'] = current_images

    # NOTE: import events
    for uuid in current_images:
        event = {}
        if uuid not in IMGADM_STATE['images']:
            event['tag'] = ""imported/{}"".format(uuid)
            for label in current_images[uuid]:
                event[label] = current_images[uuid][label]

        if event:
            ret.append(event)

    # NOTE: delete events
    for uuid in IMGADM_STATE['images']:
        event = {}
        if uuid not in current_images:
            event['tag'] = ""deleted/{}"".format(uuid)
            for label in IMGADM_STATE['images'][uuid]:
                event[label] = IMGADM_STATE['images'][uuid][label]

        if event:
            ret.append(event)

    # NOTE: update stored state
    IMGADM_STATE['images'] = current_images

    # NOTE: disable first_run
    if IMGADM_STATE['first_run']:
        IMGADM_STATE['first_run'] = False

    return ret

# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4
",Add imgadm beacons for SmartOS,"Add imgadm beacons for SmartOS
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",120,"```python
# -*- coding: utf-8 -*-
'''
Beacon that fires events on image import/delete.

.. code-block:: yaml

    ## minimal
    # - check for new images every 1 second (salt default)
    # - does not send events at startup
    beacons:
      imgadm: []

    ## standard
    # - check for new images every 60 seconds
    # - send import events at startup for all images
    beacons:
      imgadm:
        - interval: 60
        - startup_import_event: True
'''

# Import Python libs
from __future__ import absolute_import, unicode_literals
import logging

# Import 3rd-party libs
# pylint: disable=import-error
from salt.ext.six.moves import map
# pylint: enable=import-error

__virtualname__ = 'imgadm'
IMGADM_STATE = {
  'first_run': True,
  'images': [],
}

log = logging.getLogger(__name__)


def __virtual__():
    '''
    Provides imgadm beacon on SmartOS
    '''
    if 'imgadm.list' in __salt__:
        return True
    else:
        return (
            False,
            '{0} beacon can only be loaded on SmartOS compute nodes'.format(
                __virtualname__
            )
        )


def validate(config):
    '''
    Validate the beacon configuration
    '''
    vcfg_ret = True
    vcfg_msg = 'Valid beacon configuration'

    if not isinstance(config, list):
        vcfg_ret = False
        vcfg_msg = 'Configuration for imgadm beacon must be a list!'

    return vcfg_ret, vcfg_msg


def beacon(config):
    '''
    Poll imgadm and compare available images
    '''
    ret = []

    # NOTE: lookup current images
    current_images = __salt__['imgadm.list'](verbose=True)

    # NOTE: apply configuration
    if IMGADM_STATE['first_run']:
        log.info('Applying configuration for imgadm beacon')

        _config = {}
        list(map(_config.update, config))

        if 'startup_import_event' not in _config or not _config['startup_import_event']:
            IMGADM_STATE['images'] = current_images

    # NOTE: import events
    for uuid in current_images:
        event = {}
        if uuid not in IMGADM_STATE['images']:
            event['tag'] = ""imported/{}"".format(uuid)
            for label in current_images[uuid]:
                event[label] = current_images[uuid][label]

        if event:
            ret.append(event)

    # NOTE: delete events
    for uuid in IMGADM_STATE['images']:
        event = {}
        if uuid not in current_images:
            event['tag'] = ""deleted/{}"".format(uuid)
            for label in IMGADM_STATE['images'][uuid]:
                event[label] = IMGADM_STATE['images'][uuid][label]

        if event:
            ret.append(event)

    # NOTE: update stored state
    IMGADM_STATE['images'] = current_images

    # NOTE: disable first_run
    if IMGADM_STATE['first_run']:
        IMGADM_STATE['first_run'] = False

    return ret

# vim: tabstop=4 expandtab shiftwidth=4 softtabstop=4

```"
1d021cd7b52ecc4d9684dc607a1ff9f0b8181b37,read_receiver.py,read_receiver.py,,"#! /usr/bin/env python

import numpy
from matplotlib import pyplot as plt
import matplotlib.colors as colors
import os
import sys

import seaborn

def ParseVariableBinaryHeader(header):

    header_detect = '#'

    header = header.strip(""\n"").split()

    assert(header[0] == header_detect)

    name = header[1]
    dtype = header[2]
    nb_components = int(header[3])
    nx = int(header[4])
    ny = int(header[5])
    nz = int(header[6])

    return name, dtype, nb_components, nx, ny, nz

receiver_filename = ""output/receivers.dat""
receiver_file = open(receiver_filename, 'r')

readlines = receiver_file.read().split(""\n"")

receiver_file.close()

temp_filename = ""tmp.binary""
tempfile = open(temp_filename, 'wb')

# Parse header
header = readlines[0]
name, dtype, nb_components, nx, ny, nz = ParseVariableBinaryHeader(header)

# Write data without header
for line in readlines[1:]:
    tempfile.write(line + ""\n"")

tempfile.close()
tempfile = open(temp_filename, 'rb')

data = numpy.fromfile(tempfile, dtype = 'float_')

tempfile.close()

if os.path.exists(temp_filename):
    print ""Removing temporary file "" + str(temp_filename)
    os.remove(temp_filename)

print data.shape, nx, nz

data = data.reshape(nz, nx)

amplitude_max = max(numpy.amax(data), - numpy.amin(data))

print ""amplitude_max="", amplitude_max

rcv_ids = {'rcv1': (nx / 2, 'blue'),
           'rcv2': (nx / 4, 'red'),
           'rcv3': (3 * nx / 5, 'green'),
           # 'rcv4': (1200, 'orange'),
           # 'rcv5': (800, 'purple'),
       }

plt.figure()
with seaborn.axes_style(""dark""):
    cmap = 'gray'
    plt.imshow(data, cmap = cmap, interpolation = 'none', aspect = 'auto', vmin = - 0.1 * amplitude_max, vmax = 0.1 * amplitude_max)

for key, value in rcv_ids.iteritems():
    rcv_id, color = value
    plt.plot([rcv_id, rcv_id], [0.0, nz], color = color, linewidth = 2)
plt.xlim([0,nx])
plt.ylim([nz,0])
plt.figure()

cnt = 1

for key, value in rcv_ids.iteritems():
    rcv_id, color = value
    offset = numpy.power(-1.0, cnt) * (2.0 * amplitude_max) * (cnt / 2)
    print offset
    plt.plot(offset + data[:, rcv_id], color = color, linewidth = 2, label = key)
    cnt += 1
plt.legend()

plt.show()

    


",Add python script to read receiver file,"Add python script to read receiver file
",Python,apache-2.0,RaphaelPoncet/2016-macs2-projet-hpc,99,"```python
#! /usr/bin/env python

import numpy
from matplotlib import pyplot as plt
import matplotlib.colors as colors
import os
import sys

import seaborn

def ParseVariableBinaryHeader(header):

    header_detect = '#'

    header = header.strip(""\n"").split()

    assert(header[0] == header_detect)

    name = header[1]
    dtype = header[2]
    nb_components = int(header[3])
    nx = int(header[4])
    ny = int(header[5])
    nz = int(header[6])

    return name, dtype, nb_components, nx, ny, nz

receiver_filename = ""output/receivers.dat""
receiver_file = open(receiver_filename, 'r')

readlines = receiver_file.read().split(""\n"")

receiver_file.close()

temp_filename = ""tmp.binary""
tempfile = open(temp_filename, 'wb')

# Parse header
header = readlines[0]
name, dtype, nb_components, nx, ny, nz = ParseVariableBinaryHeader(header)

# Write data without header
for line in readlines[1:]:
    tempfile.write(line + ""\n"")

tempfile.close()
tempfile = open(temp_filename, 'rb')

data = numpy.fromfile(tempfile, dtype = 'float_')

tempfile.close()

if os.path.exists(temp_filename):
    print ""Removing temporary file "" + str(temp_filename)
    os.remove(temp_filename)

print data.shape, nx, nz

data = data.reshape(nz, nx)

amplitude_max = max(numpy.amax(data), - numpy.amin(data))

print ""amplitude_max="", amplitude_max

rcv_ids = {'rcv1': (nx / 2, 'blue'),
           'rcv2': (nx / 4, 'red'),
           'rcv3': (3 * nx / 5, 'green'),
           # 'rcv4': (1200, 'orange'),
           # 'rcv5': (800, 'purple'),
       }

plt.figure()
with seaborn.axes_style(""dark""):
    cmap = 'gray'
    plt.imshow(data, cmap = cmap, interpolation = 'none', aspect = 'auto', vmin = - 0.1 * amplitude_max, vmax = 0.1 * amplitude_max)

for key, value in rcv_ids.iteritems():
    rcv_id, color = value
    plt.plot([rcv_id, rcv_id], [0.0, nz], color = color, linewidth = 2)
plt.xlim([0,nx])
plt.ylim([nz,0])
plt.figure()

cnt = 1

for key, value in rcv_ids.iteritems():
    rcv_id, color = value
    offset = numpy.power(-1.0, cnt) * (2.0 * amplitude_max) * (cnt / 2)
    print offset
    plt.plot(offset + data[:, rcv_id], color = color, linewidth = 2, label = key)
    cnt += 1
plt.legend()

plt.show()

    



```"
b2a0247746756cc86074754bc993a757d6702b12,hw02/exercise-02-01.py,hw02/exercise-02-01.py,,"'''
For Homework 02, Exercieses 01-02. EdX Learning From Data course.
Jonathan Miller
'''

import random

# FUNCTIONS ###########################

def runTrial(numCoins, numFlips):

    def flipCoin():
        if random.random() > 0.5:
            return head
        else:
            return tail


    def findv1(vList):
        return vList[0]

    def findvrand(vList):
        return random.choice(vList)

    def findvmin(vList):
        vmin = 1.
        for v in vList:
            if v < vmin:
                vmin = v
        return vmin


    def sequencesToRatios(flipSequences):
        v1 = 0
        vrand = 0
        vmin =  0
        vList = []

        for sequence in flipSequences:
            numHeads = 0
            #print sequence
            for flip in sequence:
                if flip == head:
                    numHeads += 1.
            vList.append( numHeads / numFlips)
        #print vList
        v1 = findv1(vList)
        vrand = findvrand(vList)
        vmin = findvmin(vList)

        return v1, vrand, vmin


    flipSequences = []
    v1 = 0
    vrand = 0
    vmin = 0
    for coin in range(numCoins):
        coinFlipResults = """"
        for flip in range(numFlips):
            coinFlipResults += flipCoin()
        flipSequences.append(coinFlipResults)

    v1, vrand, vmin = sequencesToRatios(flipSequences)

    return v1, vrand, vmin


# MAIN ###########################



numTrials = 100000
#numTrials = 1
numCoins = 1000
numFlips = 10

v1Exp = 0
vrandExp = 0
vminExp = 0

head = ""H""
tail = 't'

for trial in range(numTrials):
    v1Trial, vrandTrial, vminTrial = runTrial(numCoins,numFlips)
    #print v1Trial, vrandTrial, vminTrial
    v1Exp += v1Trial
    vrandExp += vrandTrial
    vminExp += vminTrial

v1Exp /= numTrials
vrandExp /= numTrials
vminExp /= numTrials    

print v1Exp, vrandExp, vminExp
",Add coin flip simulator (hw02),"Add coin flip simulator (hw02)
",Python,apache-2.0,JMill/edX-Learning-From-Data-Programming,97,"```python
'''
For Homework 02, Exercieses 01-02. EdX Learning From Data course.
Jonathan Miller
'''

import random

# FUNCTIONS ###########################

def runTrial(numCoins, numFlips):

    def flipCoin():
        if random.random() > 0.5:
            return head
        else:
            return tail


    def findv1(vList):
        return vList[0]

    def findvrand(vList):
        return random.choice(vList)

    def findvmin(vList):
        vmin = 1.
        for v in vList:
            if v < vmin:
                vmin = v
        return vmin


    def sequencesToRatios(flipSequences):
        v1 = 0
        vrand = 0
        vmin =  0
        vList = []

        for sequence in flipSequences:
            numHeads = 0
            #print sequence
            for flip in sequence:
                if flip == head:
                    numHeads += 1.
            vList.append( numHeads / numFlips)
        #print vList
        v1 = findv1(vList)
        vrand = findvrand(vList)
        vmin = findvmin(vList)

        return v1, vrand, vmin


    flipSequences = []
    v1 = 0
    vrand = 0
    vmin = 0
    for coin in range(numCoins):
        coinFlipResults = """"
        for flip in range(numFlips):
            coinFlipResults += flipCoin()
        flipSequences.append(coinFlipResults)

    v1, vrand, vmin = sequencesToRatios(flipSequences)

    return v1, vrand, vmin


# MAIN ###########################



numTrials = 100000
#numTrials = 1
numCoins = 1000
numFlips = 10

v1Exp = 0
vrandExp = 0
vminExp = 0

head = ""H""
tail = 't'

for trial in range(numTrials):
    v1Trial, vrandTrial, vminTrial = runTrial(numCoins,numFlips)
    #print v1Trial, vrandTrial, vminTrial
    v1Exp += v1Trial
    vrandExp += vrandTrial
    vminExp += vminTrial

v1Exp /= numTrials
vrandExp /= numTrials
vminExp /= numTrials    

print v1Exp, vrandExp, vminExp

```"
4b0221ca503be9450919e4ed4e6a75ce92cd2d63,csdms/dakota/variables/continuous_design.py,csdms/dakota/variables/continuous_design.py,,"""""""Implementation of a Dakota continous design variable.""""""

from .base import VariableBase


classname = 'ContinuousDesign'


class ContinuousDesign(VariableBase):

    """"""Define attributes for Dakota continous design variables.""""""

    def __init__(self,
                 variables=('x1', 'x2'),
                 initial_point=None,
                 lower_bounds=None,
                 upper_bounds=None,
                 scale_types=None,
                 scales=None,
                 **kwargs):
        VariableBase.__init__(self, **kwargs)
        self.variables = variables
        self._initial_point = initial_point
        self._lower_bounds = lower_bounds
        self._upper_bounds = upper_bounds

        if initial_point is None and lower_bounds is None and
            upper_bounds is None: self._initial_point = (0.0, 0.0)

    @property
    def initial_point(self):
        """"""Start points used by study variables.""""""
        return self._initial_point

    @initial_point.setter
    def initial_point(self, value):
        """"""Set start points used by study variables.

        Parameters
        ----------
        value : list or tuple of numbers
          The new initial points.

        """"""
        if not isinstance(value, (tuple, list)):
            raise TypeError(""Initial points must be a tuple or a list"")
        self._initial_point = value

    @property
    def lower_bounds(self):
        """"""Minimum values of study variables.""""""
        return self._lower_bounds

    @lower_bounds.setter
    def lower_bounds(self, value):
        """"""Set minimum values of study variables.

        Parameters
        ----------
        value : list or tuple of numbers
          The minimum values.

        """"""
        if not isinstance(value, (tuple, list)):
            raise TypeError(""Lower bounds must be a tuple or a list"")
        self._lower_bounds = value

    @property
    def upper_bounds(self):
        """"""Maximum values of study variables.""""""
        return self._upper_bounds

    @upper_bounds.setter
    def upper_bounds(self, value):
        """"""Set maximum values of study variables.

        Parameters
        ----------
        value : list or tuple of numbers
          The maximum values.

        """"""
        if not isinstance(value, (tuple, list)):
            raise TypeError(""Upper bounds must be a tuple or a list"")
        self._upper_bounds = value

    def variables_block(self):
        """"""Define the variables block for continous design variables.""""""
        s = 'variables\n'
        s += '  {0} = {1}'.format(self.variable_type,
                                   len(self.variables))
        if self.initial_point is not None:
            s += '\n' \
                 + '    initial_point ='
            for pt in self.initial_point:
                s += ' {}'.format(pt)
        if self.lower_bounds is not None:
            s += '\n' \
                 + '    lower_bounds ='
            for b in self.lower_bounds:
                s += ' {}'.format(b)
        if self.upper_bounds is not None:
            s += '\n' \
                 + '    upper_bounds ='
            for b in self.upper_bounds:
                s += ' {}'.format(b)
        s += '\n' \
             + '    descriptors ='
        for vd in self.variables:
            s += ' {!r}'.format(vd)
        s += '\n\n'
        return(s)
",Create new module for continuous design variables,"Create new module for continuous design variables
",Python,mit,"csdms/dakota,csdms/dakota",113,"```python
""""""Implementation of a Dakota continous design variable.""""""

from .base import VariableBase


classname = 'ContinuousDesign'


class ContinuousDesign(VariableBase):

    """"""Define attributes for Dakota continous design variables.""""""

    def __init__(self,
                 variables=('x1', 'x2'),
                 initial_point=None,
                 lower_bounds=None,
                 upper_bounds=None,
                 scale_types=None,
                 scales=None,
                 **kwargs):
        VariableBase.__init__(self, **kwargs)
        self.variables = variables
        self._initial_point = initial_point
        self._lower_bounds = lower_bounds
        self._upper_bounds = upper_bounds

        if initial_point is None and lower_bounds is None and
            upper_bounds is None: self._initial_point = (0.0, 0.0)

    @property
    def initial_point(self):
        """"""Start points used by study variables.""""""
        return self._initial_point

    @initial_point.setter
    def initial_point(self, value):
        """"""Set start points used by study variables.

        Parameters
        ----------
        value : list or tuple of numbers
          The new initial points.

        """"""
        if not isinstance(value, (tuple, list)):
            raise TypeError(""Initial points must be a tuple or a list"")
        self._initial_point = value

    @property
    def lower_bounds(self):
        """"""Minimum values of study variables.""""""
        return self._lower_bounds

    @lower_bounds.setter
    def lower_bounds(self, value):
        """"""Set minimum values of study variables.

        Parameters
        ----------
        value : list or tuple of numbers
          The minimum values.

        """"""
        if not isinstance(value, (tuple, list)):
            raise TypeError(""Lower bounds must be a tuple or a list"")
        self._lower_bounds = value

    @property
    def upper_bounds(self):
        """"""Maximum values of study variables.""""""
        return self._upper_bounds

    @upper_bounds.setter
    def upper_bounds(self, value):
        """"""Set maximum values of study variables.

        Parameters
        ----------
        value : list or tuple of numbers
          The maximum values.

        """"""
        if not isinstance(value, (tuple, list)):
            raise TypeError(""Upper bounds must be a tuple or a list"")
        self._upper_bounds = value

    def variables_block(self):
        """"""Define the variables block for continous design variables.""""""
        s = 'variables\n'
        s += '  {0} = {1}'.format(self.variable_type,
                                   len(self.variables))
        if self.initial_point is not None:
            s += '\n' \
                 + '    initial_point ='
            for pt in self.initial_point:
                s += ' {}'.format(pt)
        if self.lower_bounds is not None:
            s += '\n' \
                 + '    lower_bounds ='
            for b in self.lower_bounds:
                s += ' {}'.format(b)
        if self.upper_bounds is not None:
            s += '\n' \
                 + '    upper_bounds ='
            for b in self.upper_bounds:
                s += ' {}'.format(b)
        s += '\n' \
             + '    descriptors ='
        for vd in self.variables:
            s += ' {!r}'.format(vd)
        s += '\n\n'
        return(s)

```"
1ef84c24c60cf802aeb4bf6084f9b7fc7696f79a,scripts/album_times.py,scripts/album_times.py,,"#!/usr/bin/env python3

""""""Radio scheduling program.

Usage:
  album_times.py [--host=HOST] PORT

Options:
  --host=HOST  Hostname of MPD [default: localhost]
  -h --help    Show this text

Prints out the last scheduling time of every album.
""""""

from datetime import datetime
from docopt import docopt
from mpd import MPDClient


def album_sticker_get(client, album, sticker):
    """"""Gets a sticker associated with an album.""""""

    # I am pretty sure that MPD only implements stickers for songs, so
    # the sticker gets attached to the first song in the album.
    tracks = client.find(""album"", album)
    if len(tracks) == 0:
        return

    return client.sticker_get(""song"", tracks[0][""file""], ""album_"" + sticker)


def album_sticker_set(client, album, sticker, val):
    """"""Sets a sticker associated with an album.""""""

    # I am pretty sure that MPD only implements stickers for songs, so
    # the sticker gets attached to the first song in the album.
    tracks = client.find(""album"", album)
    if len(tracks) == 0:
        return

    return client.sticker_set(""song"", tracks[0][""file""], ""album_"" + sticker, val)


def list_albums(client):
    """"""Lists albums sorted by last play timestamp.""""""

    # Get all albums
    albums = client.list(""album"")
    all_albums = list(filter(lambda a: a not in ["""", ""Lainchan Radio Transitions""], albums))

    # Group albums by when they were last scheduled
    albums_by_last_scheduled = {}
    last_scheduled_times = []
    for album in all_albums:
        # Get the last scheduled time, defaulting to 0
        try:
            last_scheduled = int(album_sticker_get(client, album, ""last_scheduled""))
        except:
            last_scheduled = 0

        # Put the album into the appropriate bucket
        if last_scheduled in albums_by_last_scheduled:
            albums_by_last_scheduled[last_scheduled].append(album)
        else:
            albums_by_last_scheduled[last_scheduled] = [album]
            last_scheduled_times.append(last_scheduled)

    # Pick the 10 oldest times
    last_scheduled_times.sort()
    for last_scheduled in last_scheduled_times:
        dt = datetime.utcfromtimestamp(last_scheduled)
        albums = albums_by_last_scheduled[last_scheduled]
        print(""{}: {}"".format(dt.strftime('%Y-%m-%d %H:%M:%S'), albums))


if __name__ == ""__main__"":
    args = docopt(__doc__)

    try:
        args[""PORT""] = int(args[""PORT""])
    except:
        print(""PORT must be an integer"")
        exit(1)

    try:
        client = MPDClient()
        client.connect(args[""--host""], args[""PORT""])
    except:
        print(""could not connect to MPD"")
        exit(2)

    list_albums(client)
",Add a script to print the scheduling times of albums,"Add a script to print the scheduling times of albums
",Python,mit,"barrucadu/lainonlife,barrucadu/lainonlife,barrucadu/lainonlife,barrucadu/lainonlife",93,"```python
#!/usr/bin/env python3

""""""Radio scheduling program.

Usage:
  album_times.py [--host=HOST] PORT

Options:
  --host=HOST  Hostname of MPD [default: localhost]
  -h --help    Show this text

Prints out the last scheduling time of every album.
""""""

from datetime import datetime
from docopt import docopt
from mpd import MPDClient


def album_sticker_get(client, album, sticker):
    """"""Gets a sticker associated with an album.""""""

    # I am pretty sure that MPD only implements stickers for songs, so
    # the sticker gets attached to the first song in the album.
    tracks = client.find(""album"", album)
    if len(tracks) == 0:
        return

    return client.sticker_get(""song"", tracks[0][""file""], ""album_"" + sticker)


def album_sticker_set(client, album, sticker, val):
    """"""Sets a sticker associated with an album.""""""

    # I am pretty sure that MPD only implements stickers for songs, so
    # the sticker gets attached to the first song in the album.
    tracks = client.find(""album"", album)
    if len(tracks) == 0:
        return

    return client.sticker_set(""song"", tracks[0][""file""], ""album_"" + sticker, val)


def list_albums(client):
    """"""Lists albums sorted by last play timestamp.""""""

    # Get all albums
    albums = client.list(""album"")
    all_albums = list(filter(lambda a: a not in ["""", ""Lainchan Radio Transitions""], albums))

    # Group albums by when they were last scheduled
    albums_by_last_scheduled = {}
    last_scheduled_times = []
    for album in all_albums:
        # Get the last scheduled time, defaulting to 0
        try:
            last_scheduled = int(album_sticker_get(client, album, ""last_scheduled""))
        except:
            last_scheduled = 0

        # Put the album into the appropriate bucket
        if last_scheduled in albums_by_last_scheduled:
            albums_by_last_scheduled[last_scheduled].append(album)
        else:
            albums_by_last_scheduled[last_scheduled] = [album]
            last_scheduled_times.append(last_scheduled)

    # Pick the 10 oldest times
    last_scheduled_times.sort()
    for last_scheduled in last_scheduled_times:
        dt = datetime.utcfromtimestamp(last_scheduled)
        albums = albums_by_last_scheduled[last_scheduled]
        print(""{}: {}"".format(dt.strftime('%Y-%m-%d %H:%M:%S'), albums))


if __name__ == ""__main__"":
    args = docopt(__doc__)

    try:
        args[""PORT""] = int(args[""PORT""])
    except:
        print(""PORT must be an integer"")
        exit(1)

    try:
        client = MPDClient()
        client.connect(args[""--host""], args[""PORT""])
    except:
        print(""could not connect to MPD"")
        exit(2)

    list_albums(client)

```"
23f626ddaabfa799da48ee35c29db05f95f8a732,polling_stations/apps/data_collection/management/commands/import_rct.py,polling_stations/apps/data_collection/management/commands/import_rct.py,,"""""""
Import Rhondda Cynon Taf

note: this script takes quite a long time to run
""""""
from time import sleep
from django.contrib.gis.geos import Point
from data_collection.management.commands import BaseAddressCsvImporter
from data_finder.helpers import geocode
from data_collection.google_geocoding_api_wrapper import (
    GoogleGeocodingApiWrapper,
    PostcodeNotFoundException
)

class Command(BaseAddressCsvImporter):
    """"""
    Imports the Polling Station data from Rhondda Cynon Taf
    """"""
    council_id      = 'W06000016'
    addresses_name  = 'PROPERTYLISTINGFORDEMOCRACYCLUB.csv'
    stations_name   = 'POLLINGSTATIONS8MARCH2016.csv'

    def station_record_to_dict(self, record):

        # format address
        address = ""\n"".join([
            record.address1,
            record.address2,
            record.address3,
            record.address4,
            record.address5
        ])
        while ""\n\n"" in address:
            address = address.replace(""\n\n"", ""\n"")
        # remove trailing ""\n"" if present
        if address[-1:] == '\n':
            address = address[:-1]


        # attempt to attach postcode if missing
        postcode = record.postcode
        if not postcode:
            gwrapper = GoogleGeocodingApiWrapper(address, self.council_id, 'UTA')
            try:
                postcode = gwrapper.address_to_postcode()
            except PostcodeNotFoundException:
                postcode = ''


        """"""
        No grid references were supplied, so attempt to
        derive a grid ref from postcode if we have that
        """"""
        sleep(1.3) # ensure we don't hit mapit's usage limit
        if postcode:
            try:
                gridref = geocode(postcode)
                location = Point(gridref['wgs84_lon'], gridref['wgs84_lat'], srid=4326)
            except KeyError:
                location = None
        else:
            location = None


        return {
            'internal_council_id': record.polling_district,
            'postcode'           : postcode,
            'address'            : address,
            'location'           : location
        }

    def address_record_to_dict(self, record):

        # format address
        address = "", "".join([
            record.address1,
            record.address2,
            record.address3,
            record.address4,
            record.address5,
            record.address6,
        ])
        while "", , "" in address:
            address = address.replace("", , "", "", "")
        # remove trailing "", "" if present
        if address[-2:] == ', ':
            address = address[:-2]


        return {
            'address'           : address,
            'postcode'          : record.postcode,
            'polling_station_id': record.district
        }
",Add import script for Rhondda Cynon Taff,"Add import script for Rhondda Cynon Taff
",Python,bsd-3-clause,"andylolz/UK-Polling-Stations,DemocracyClub/UK-Polling-Stations,DemocracyClub/UK-Polling-Stations,chris48s/UK-Polling-Stations,DemocracyClub/UK-Polling-Stations,andylolz/UK-Polling-Stations,chris48s/UK-Polling-Stations,andylolz/UK-Polling-Stations,chris48s/UK-Polling-Stations",95,"```python
""""""
Import Rhondda Cynon Taf

note: this script takes quite a long time to run
""""""
from time import sleep
from django.contrib.gis.geos import Point
from data_collection.management.commands import BaseAddressCsvImporter
from data_finder.helpers import geocode
from data_collection.google_geocoding_api_wrapper import (
    GoogleGeocodingApiWrapper,
    PostcodeNotFoundException
)

class Command(BaseAddressCsvImporter):
    """"""
    Imports the Polling Station data from Rhondda Cynon Taf
    """"""
    council_id      = 'W06000016'
    addresses_name  = 'PROPERTYLISTINGFORDEMOCRACYCLUB.csv'
    stations_name   = 'POLLINGSTATIONS8MARCH2016.csv'

    def station_record_to_dict(self, record):

        # format address
        address = ""\n"".join([
            record.address1,
            record.address2,
            record.address3,
            record.address4,
            record.address5
        ])
        while ""\n\n"" in address:
            address = address.replace(""\n\n"", ""\n"")
        # remove trailing ""\n"" if present
        if address[-1:] == '\n':
            address = address[:-1]


        # attempt to attach postcode if missing
        postcode = record.postcode
        if not postcode:
            gwrapper = GoogleGeocodingApiWrapper(address, self.council_id, 'UTA')
            try:
                postcode = gwrapper.address_to_postcode()
            except PostcodeNotFoundException:
                postcode = ''


        """"""
        No grid references were supplied, so attempt to
        derive a grid ref from postcode if we have that
        """"""
        sleep(1.3) # ensure we don't hit mapit's usage limit
        if postcode:
            try:
                gridref = geocode(postcode)
                location = Point(gridref['wgs84_lon'], gridref['wgs84_lat'], srid=4326)
            except KeyError:
                location = None
        else:
            location = None


        return {
            'internal_council_id': record.polling_district,
            'postcode'           : postcode,
            'address'            : address,
            'location'           : location
        }

    def address_record_to_dict(self, record):

        # format address
        address = "", "".join([
            record.address1,
            record.address2,
            record.address3,
            record.address4,
            record.address5,
            record.address6,
        ])
        while "", , "" in address:
            address = address.replace("", , "", "", "")
        # remove trailing "", "" if present
        if address[-2:] == ', ':
            address = address[:-2]


        return {
            'address'           : address,
            'postcode'          : record.postcode,
            'polling_station_id': record.district
        }

```"
d3e90e4dca1cce2f27ccf9c24c1bb944f9d708b9,test/test_message_directory.py,test/test_message_directory.py,,"""""""
Created on 18 May 2018.

@author: Greg Corbett
""""""

import shutil
import tempfile
import unittest

from ssm.message_directory import MessageDirectory


class TestMessageDirectory(unittest.TestCase):
    """"""Class used for testing the MessageDirectory class.""""""

    def setUp(self):
        """"""Create a MessageDirectory class on top of a temporary directory.""""""
        self.tmp_dir = tempfile.mkdtemp(prefix='message_directory')
        self.message_directory = MessageDirectory(self.tmp_dir)
        # Assert no files exist in the underlying file system.
        self.assertEqual(self.message_directory.count(), 0)

    def test_add_and_get(self):
        """"""
        Test the add and get methods of the MessageDirectory class.

        This test adds a file to a MessageDirectory, checks it has been
        written to the underlying directory and then checks the saved file
        for content equality.
        """"""
        test_content = ""FOO""
        # Add the test content to the MessageDirectory.
        file_name = self.message_directory.add(test_content)

        # Assert there is exactly on message in the directory.
        self.assertEqual(self.message_directory.count(), 1)

        # Fetch the saved content using the get method.
        saved_content = self.message_directory.get(file_name)

        # Assert the saved content is equal to the original test content.
        self.assertEqual(saved_content, test_content)

    def test_count(self):
        """"""
        Test the count method of the MessageDirectory class.

        This test adds two files to a MessageDirectory and then checks
        the output of the count() function is as expected.
        """"""
        # Add some files to the MessageDirectory.
        self.message_directory.add(""FOO"")
        self.message_directory.add(""BAR"")

        # Check the count method returns the correct value.
        self.assertEqual(self.message_directory.count(), 2)

    def test_lock(self):
        """"""
        Test the lock method of the MessageDirectory class.

        This test checks the lock method returns true for any file.
        """"""
        self.assertTrue(self.message_directory.lock(""any file""))

    def test_purge(self):
        """"""
        Test the purge method of the MessageDirectory class.

        This test only checks the purge method is callable without error,
        as the purge method only logs that it has been called.
        """"""
        self.message_directory.purge()

    def test_remove(self):
        """"""
        Test the remove method of the MessageDirectory class.

        This test adds a file, removes the file and then checks
        the number of files present.
        """"""
        # Add some files to the MessageDirectory.
        file_name = self.message_directory.add(""FOO"")
        # Use the remove method to delete the recently added file.
        self.message_directory.remove(file_name)
        # Check the count method returns the expected value.
        self.assertEqual(self.message_directory.count(), 0)

    def tearDown(self):
        """"""Remove test directory and all contents.""""""
        try:
            shutil.rmtree(self.tmp_dir)
        except OSError, error:
            print 'Error removing temporary directory %s' % self.tmp_dir
            print error


if __name__ == ""__main__"":
    unittest.main()
",Add a unit test file for MessageDirectory,"Add a unit test file for MessageDirectory
",Python,apache-2.0,"tofu-rocketry/ssm,apel/ssm,tofu-rocketry/ssm,stfc/ssm,apel/ssm,stfc/ssm",101,"```python
""""""
Created on 18 May 2018.

@author: Greg Corbett
""""""

import shutil
import tempfile
import unittest

from ssm.message_directory import MessageDirectory


class TestMessageDirectory(unittest.TestCase):
    """"""Class used for testing the MessageDirectory class.""""""

    def setUp(self):
        """"""Create a MessageDirectory class on top of a temporary directory.""""""
        self.tmp_dir = tempfile.mkdtemp(prefix='message_directory')
        self.message_directory = MessageDirectory(self.tmp_dir)
        # Assert no files exist in the underlying file system.
        self.assertEqual(self.message_directory.count(), 0)

    def test_add_and_get(self):
        """"""
        Test the add and get methods of the MessageDirectory class.

        This test adds a file to a MessageDirectory, checks it has been
        written to the underlying directory and then checks the saved file
        for content equality.
        """"""
        test_content = ""FOO""
        # Add the test content to the MessageDirectory.
        file_name = self.message_directory.add(test_content)

        # Assert there is exactly on message in the directory.
        self.assertEqual(self.message_directory.count(), 1)

        # Fetch the saved content using the get method.
        saved_content = self.message_directory.get(file_name)

        # Assert the saved content is equal to the original test content.
        self.assertEqual(saved_content, test_content)

    def test_count(self):
        """"""
        Test the count method of the MessageDirectory class.

        This test adds two files to a MessageDirectory and then checks
        the output of the count() function is as expected.
        """"""
        # Add some files to the MessageDirectory.
        self.message_directory.add(""FOO"")
        self.message_directory.add(""BAR"")

        # Check the count method returns the correct value.
        self.assertEqual(self.message_directory.count(), 2)

    def test_lock(self):
        """"""
        Test the lock method of the MessageDirectory class.

        This test checks the lock method returns true for any file.
        """"""
        self.assertTrue(self.message_directory.lock(""any file""))

    def test_purge(self):
        """"""
        Test the purge method of the MessageDirectory class.

        This test only checks the purge method is callable without error,
        as the purge method only logs that it has been called.
        """"""
        self.message_directory.purge()

    def test_remove(self):
        """"""
        Test the remove method of the MessageDirectory class.

        This test adds a file, removes the file and then checks
        the number of files present.
        """"""
        # Add some files to the MessageDirectory.
        file_name = self.message_directory.add(""FOO"")
        # Use the remove method to delete the recently added file.
        self.message_directory.remove(file_name)
        # Check the count method returns the expected value.
        self.assertEqual(self.message_directory.count(), 0)

    def tearDown(self):
        """"""Remove test directory and all contents.""""""
        try:
            shutil.rmtree(self.tmp_dir)
        except OSError, error:
            print 'Error removing temporary directory %s' % self.tmp_dir
            print error


if __name__ == ""__main__"":
    unittest.main()

```"
7ab0cc93703abf6716b353f38a009897ab154ce4,nova/tests/test_plugin_api_extensions.py,nova/tests/test_plugin_api_extensions.py,,"# Copyright 2011 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import pkg_resources

import nova
from nova.api.openstack.compute import extensions as computeextensions
from nova.api.openstack import extensions
from nova.openstack.common.plugin import plugin
from nova.openstack.common.plugin import pluginmanager
from nova import test


class StubController(object):

    def i_am_the_stub(self):
        pass


class StubControllerExtension(extensions.ExtensionDescriptor):
    """"""This is a docstring.  We need it.""""""
    name = 'stubextension'
    alias = 'stubby'

    def get_resources(self):
        resources = []
        res = extensions.ResourceExtension('testme',
                                           StubController())
        resources.append(res)
        return resources


service_list = []


class TestPluginClass(plugin.Plugin):

    def __init__(self, service_name):
        super(TestPluginClass, self).__init__(service_name)
        self._add_api_extension_descriptor(StubControllerExtension)
        service_list.append(service_name)


class MockEntrypoint(pkg_resources.EntryPoint):
    def load(self):
        return TestPluginClass


class APITestCase(test.TestCase):
    """"""Test case for the plugin api extension interface""""""
    def test_add_extension(self):
        def mock_load(_s):
            return TestPluginClass()

        def mock_iter_entry_points(_t):
            return [MockEntrypoint(""fake"", ""fake"", [""fake""])]

        self.stubs.Set(pkg_resources, 'iter_entry_points',
                mock_iter_entry_points)
        global service_list
        service_list = []

        # Marking out the default extension paths makes this test MUCH faster.
        self.flags(osapi_compute_extension=[])
        self.flags(osapi_volume_extension=[])

        found = False
        mgr = computeextensions.ExtensionManager()
        for res in mgr.get_resources():
            # We have to use this weird 'dir' check because
            #  the plugin framework muddies up the classname
            #  such that 'isinstance' doesn't work right.
            if 'i_am_the_stub' in dir(res.controller):
                found = True

        self.assertTrue(found)
        self.assertEqual(len(service_list), 1)
        self.assertEqual(service_list[0], 'compute-extensions')
",Add the plugin framework from common; use and test.,"Add the plugin framework from common; use and test.

For blueprint novaplugins.

Change-Id: Id4a5ae3ebb91f941956e2f73ecfd9ea1d290a235
",Python,apache-2.0,n0ano/ganttclient,91,"```python
# Copyright 2011 OpenStack LLC.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import pkg_resources

import nova
from nova.api.openstack.compute import extensions as computeextensions
from nova.api.openstack import extensions
from nova.openstack.common.plugin import plugin
from nova.openstack.common.plugin import pluginmanager
from nova import test


class StubController(object):

    def i_am_the_stub(self):
        pass


class StubControllerExtension(extensions.ExtensionDescriptor):
    """"""This is a docstring.  We need it.""""""
    name = 'stubextension'
    alias = 'stubby'

    def get_resources(self):
        resources = []
        res = extensions.ResourceExtension('testme',
                                           StubController())
        resources.append(res)
        return resources


service_list = []


class TestPluginClass(plugin.Plugin):

    def __init__(self, service_name):
        super(TestPluginClass, self).__init__(service_name)
        self._add_api_extension_descriptor(StubControllerExtension)
        service_list.append(service_name)


class MockEntrypoint(pkg_resources.EntryPoint):
    def load(self):
        return TestPluginClass


class APITestCase(test.TestCase):
    """"""Test case for the plugin api extension interface""""""
    def test_add_extension(self):
        def mock_load(_s):
            return TestPluginClass()

        def mock_iter_entry_points(_t):
            return [MockEntrypoint(""fake"", ""fake"", [""fake""])]

        self.stubs.Set(pkg_resources, 'iter_entry_points',
                mock_iter_entry_points)
        global service_list
        service_list = []

        # Marking out the default extension paths makes this test MUCH faster.
        self.flags(osapi_compute_extension=[])
        self.flags(osapi_volume_extension=[])

        found = False
        mgr = computeextensions.ExtensionManager()
        for res in mgr.get_resources():
            # We have to use this weird 'dir' check because
            #  the plugin framework muddies up the classname
            #  such that 'isinstance' doesn't work right.
            if 'i_am_the_stub' in dir(res.controller):
                found = True

        self.assertTrue(found)
        self.assertEqual(len(service_list), 1)
        self.assertEqual(service_list[0], 'compute-extensions')

```"
c97ab456f22dca6a69e3775cc1353dbf3957389a,homeassistant/components/light/limitlessled.py,homeassistant/components/light/limitlessled.py,,"""""""
homeassistant.components.light.limitlessled
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Support for LimitlessLED bulbs, also known as...

EasyBulb
AppLight
AppLamp
MiLight
LEDme
dekolight
iLight

""""""
import random
import logging

from homeassistant.helpers.entity import ToggleEntity
from homeassistant.const import STATE_ON, STATE_OFF, DEVICE_DEFAULT_NAME
from homeassistant.components.light import ATTR_BRIGHTNESS

_LOGGER = logging.getLogger(__name__)


def setup_platform(hass, config, add_devices_callback, discovery_info=None):
    try:
        import ledcontroller
    except ImportError:
        _LOGGER.exception(""Error while importing dependency ledcontroller."")
        return

    led = ledcontroller.LedController(config['host'])

    lights = []
    for i in range(1, 5):
        if 'group_%d_name' % (i) in config:
            lights.append(
                LimitlessLED(
                    led,
                    i,
                    config['group_%d_name' % (i)],
                    STATE_OFF
                )
            )

    add_devices_callback(lights)


class LimitlessLED(ToggleEntity):
    def __init__(self, led, group, name, state, brightness=180):
        self.led = led
        self.group = group

        # LimitlessLEDs don't report state, we have track it ourselves.
        self.led.off(self.group)

        self._name = name or DEVICE_DEFAULT_NAME
        self._state = state
        self._brightness = brightness

    @property
    def should_poll(self):
        """""" No polling needed for a demo light. """"""
        return False

    @property
    def name(self):
        """""" Returns the name of the device if any. """"""
        return self._name

    @property
    def state(self):
        """""" Returns the name of the device if any. """"""
        return self._state

    @property
    def state_attributes(self):
        """""" Returns optional state attributes. """"""
        if self.is_on:
            return {
                ATTR_BRIGHTNESS: self._brightness,
            }

    @property
    def is_on(self):
        """""" True if device is on. """"""
        return self._state == STATE_ON

    def turn_on(self, **kwargs):
        """""" Turn the device on. """"""
        self._state = STATE_ON

        if ATTR_BRIGHTNESS in kwargs:
            self._brightness = kwargs[ATTR_BRIGHTNESS]

        self.led.set_brightness(self._brightness, self.group)

    def turn_off(self, **kwargs):
        """""" Turn the device off. """"""
        self._state = STATE_OFF
        self.led.off(self.group)
",Add basic support for LimitlessLED,"Add basic support for LimitlessLED
",Python,apache-2.0,"sfam/home-assistant,ErykB2000/home-assistant,open-homeautomation/home-assistant,Danielhiversen/home-assistant,toddeye/home-assistant,nugget/home-assistant,open-homeautomation/home-assistant,deisi/home-assistant,EricRho/home-assistant,tboyce021/home-assistant,theolind/home-assistant,mKeRix/home-assistant,Duoxilian/home-assistant,CCOSTAN/home-assistant,auduny/home-assistant,MungoRae/home-assistant,pottzer/home-assistant,EricRho/home-assistant,instantchow/home-assistant,JshWright/home-assistant,MartinHjelmare/home-assistant,hmronline/home-assistant,tboyce1/home-assistant,lukas-hetzenecker/home-assistant,bdfoster/blumate,eagleamon/home-assistant,Zac-HD/home-assistant,alexmogavero/home-assistant,postlund/home-assistant,HydrelioxGitHub/home-assistant,kennedyshead/home-assistant,oandrew/home-assistant,balloob/home-assistant,alanbowman/home-assistant,eagleamon/home-assistant,maddox/home-assistant,dmeulen/home-assistant,tinloaf/home-assistant,nevercast/home-assistant,bencmbrook/home-assistant,florianholzapfel/home-assistant,leoc/home-assistant,sanmiguel/home-assistant,partofthething/home-assistant,MungoRae/home-assistant,GenericStudent/home-assistant,betrisey/home-assistant,mKeRix/home-assistant,deisi/home-assistant,dorant/home-assistant,betrisey/home-assistant,g12mcgov/home-assistant,leoc/home-assistant,qedi-r/home-assistant,bdfoster/blumate,morphis/home-assistant,aequitas/home-assistant,ma314smith/home-assistant,bencmbrook/home-assistant,mikaelboman/home-assistant,vitorespindola/home-assistant,PetePriority/home-assistant,tchellomello/home-assistant,srcLurker/home-assistant,JshWright/home-assistant,MungoRae/home-assistant,xifle/home-assistant,molobrakos/home-assistant,aequitas/home-assistant,badele/home-assistant,hexxter/home-assistant,mahendra-r/home-assistant,soldag/home-assistant,jaharkes/home-assistant,keerts/home-assistant,partofthething/home-assistant,HydrelioxGitHub/home-assistant,Nzaga/home-assistant,shaftoe/home-assistant,adrienbrault/home-assistant,alexkolar/home-assistant,mezz64/home-assistant,fbradyirl/home-assistant,emilhetty/home-assistant,philipbl/home-assistant,emilhetty/home-assistant,michaelarnauts/home-assistant,eagleamon/home-assistant,ma314smith/home-assistant,aronsky/home-assistant,coteyr/home-assistant,miniconfig/home-assistant,luxus/home-assistant,teodoc/home-assistant,mikaelboman/home-assistant,theolind/home-assistant,happyleavesaoc/home-assistant,sffjunkie/home-assistant,jabesq/home-assistant,pschmitt/home-assistant,sffjunkie/home-assistant,Julian/home-assistant,sffjunkie/home-assistant,caiuspb/home-assistant,coteyr/home-assistant,happyleavesaoc/home-assistant,tboyce1/home-assistant,nkgilley/home-assistant,tomduijf/home-assistant,xifle/home-assistant,soldag/home-assistant,persandstrom/home-assistant,emilhetty/home-assistant,varunr047/homefile,luxus/home-assistant,sfam/home-assistant,mezz64/home-assistant,LinuxChristian/home-assistant,justyns/home-assistant,keerts/home-assistant,aoakeson/home-assistant,qedi-r/home-assistant,varunr047/homefile,w1ll1am23/home-assistant,robbiet480/home-assistant,caiuspb/home-assistant,GenericStudent/home-assistant,jnewland/home-assistant,kyvinh/home-assistant,philipbl/home-assistant,g12mcgov/home-assistant,fbradyirl/home-assistant,tmm1/home-assistant,sffjunkie/home-assistant,bdfoster/blumate,tmm1/home-assistant,titilambert/home-assistant,shaftoe/home-assistant,nugget/home-assistant,jaharkes/home-assistant,vitorespindola/home-assistant,jamespcole/home-assistant,xifle/home-assistant,g12mcgov/home-assistant,florianholzapfel/home-assistant,leppa/home-assistant,DavidLP/home-assistant,ma314smith/home-assistant,SEJeff/home-assistant,nnic/home-assistant,kennedyshead/home-assistant,sfam/home-assistant,dorant/home-assistant,Theb-1/home-assistant,happyleavesaoc/home-assistant,postlund/home-assistant,dorant/home-assistant,nkgilley/home-assistant,Smart-Torvy/torvy-home-assistant,PetePriority/home-assistant,turbokongen/home-assistant,sdague/home-assistant,miniconfig/home-assistant,ct-23/home-assistant,emilhetty/home-assistant,tchellomello/home-assistant,aoakeson/home-assistant,home-assistant/home-assistant,jawilson/home-assistant,jawilson/home-assistant,molobrakos/home-assistant,varunr047/homefile,varunr047/homefile,ct-23/home-assistant,morphis/home-assistant,home-assistant/home-assistant,kyvinh/home-assistant,dmeulen/home-assistant,robbiet480/home-assistant,rohitranjan1991/home-assistant,Cinntax/home-assistant,mikaelboman/home-assistant,justyns/home-assistant,auduny/home-assistant,JshWright/home-assistant,pottzer/home-assistant,devdelay/home-assistant,tboyce021/home-assistant,robjohnson189/home-assistant,nevercast/home-assistant,alanbowman/home-assistant,deisi/home-assistant,Julian/home-assistant,aoakeson/home-assistant,instantchow/home-assistant,michaelarnauts/home-assistant,Julian/home-assistant,hexxter/home-assistant,tomduijf/home-assistant,ewandor/home-assistant,theolind/home-assistant,Zac-HD/home-assistant,shaftoe/home-assistant,Zac-HD/home-assistant,open-homeautomation/home-assistant,Zyell/home-assistant,maddox/home-assistant,jnewland/home-assistant,PetePriority/home-assistant,mikaelboman/home-assistant,rohitranjan1991/home-assistant,Teagan42/home-assistant,betrisey/home-assistant,oandrew/home-assistant,jnewland/home-assistant,maddox/home-assistant,alexmogavero/home-assistant,srcLurker/home-assistant,pottzer/home-assistant,molobrakos/home-assistant,auduny/home-assistant,Zyell/home-assistant,robjohnson189/home-assistant,Duoxilian/home-assistant,persandstrom/home-assistant,EricRho/home-assistant,sdague/home-assistant,MartinHjelmare/home-assistant,tboyce1/home-assistant,Smart-Torvy/torvy-home-assistant,SEJeff/home-assistant,w1ll1am23/home-assistant,MungoRae/home-assistant,coteyr/home-assistant,joopert/home-assistant,teodoc/home-assistant,mahendra-r/home-assistant,DavidLP/home-assistant,LinuxChristian/home-assistant,florianholzapfel/home-assistant,oandrew/home-assistant,kyvinh/home-assistant,stefan-jonasson/home-assistant,Zyell/home-assistant,joopert/home-assistant,Zac-HD/home-assistant,alanbowman/home-assistant,deisi/home-assistant,aronsky/home-assistant,mKeRix/home-assistant,SEJeff/home-assistant,hmronline/home-assistant,varunr047/homefile,tinloaf/home-assistant,ewandor/home-assistant,keerts/home-assistant,nugget/home-assistant,persandstrom/home-assistant,philipbl/home-assistant,jabesq/home-assistant,miniconfig/home-assistant,open-homeautomation/home-assistant,mahendra-r/home-assistant,nnic/home-assistant,dmeulen/home-assistant,tomduijf/home-assistant,tmm1/home-assistant,mKeRix/home-assistant,jaharkes/home-assistant,miniconfig/home-assistant,stefan-jonasson/home-assistant,jamespcole/home-assistant,nnic/home-assistant,srcLurker/home-assistant,bdfoster/blumate,morphis/home-assistant,vitorespindola/home-assistant,Smart-Torvy/torvy-home-assistant,michaelarnauts/home-assistant,jabesq/home-assistant,sanmiguel/home-assistant,LinuxChristian/home-assistant,alexmogavero/home-assistant,nevercast/home-assistant,Duoxilian/home-assistant,balloob/home-assistant,betrisey/home-assistant,morphis/home-assistant,jaharkes/home-assistant,JshWright/home-assistant,emilhetty/home-assistant,jamespcole/home-assistant,leppa/home-assistant,HydrelioxGitHub/home-assistant,shaftoe/home-assistant,Cinntax/home-assistant,caiuspb/home-assistant,adrienbrault/home-assistant,alexkolar/home-assistant,ma314smith/home-assistant,rohitranjan1991/home-assistant,balloob/home-assistant,sanmiguel/home-assistant,titilambert/home-assistant,ct-23/home-assistant,robjohnson189/home-assistant,hexxter/home-assistant,CCOSTAN/home-assistant,lukas-hetzenecker/home-assistant,ErykB2000/home-assistant,bdfoster/blumate,aequitas/home-assistant,FreekingDean/home-assistant,fbradyirl/home-assistant,bencmbrook/home-assistant,mikaelboman/home-assistant,tboyce1/home-assistant,FreekingDean/home-assistant,toddeye/home-assistant,hexxter/home-assistant,stefan-jonasson/home-assistant,sander76/home-assistant,stefan-jonasson/home-assistant,alexkolar/home-assistant,justyns/home-assistant,eagleamon/home-assistant,robjohnson189/home-assistant,hmronline/home-assistant,Theb-1/home-assistant,happyleavesaoc/home-assistant,oandrew/home-assistant,teodoc/home-assistant,MartinHjelmare/home-assistant,luxus/home-assistant,keerts/home-assistant,alexmogavero/home-assistant,ct-23/home-assistant,badele/home-assistant,florianholzapfel/home-assistant,pschmitt/home-assistant,Nzaga/home-assistant,deisi/home-assistant,Duoxilian/home-assistant,srcLurker/home-assistant,ct-23/home-assistant,instantchow/home-assistant,badele/home-assistant,hmronline/home-assistant,LinuxChristian/home-assistant,CCOSTAN/home-assistant,DavidLP/home-assistant,MungoRae/home-assistant,ErykB2000/home-assistant,leoc/home-assistant,devdelay/home-assistant,devdelay/home-assistant,Danielhiversen/home-assistant,Julian/home-assistant,ewandor/home-assistant,LinuxChristian/home-assistant,turbokongen/home-assistant,philipbl/home-assistant,Theb-1/home-assistant,sander76/home-assistant,Teagan42/home-assistant,xifle/home-assistant,Smart-Torvy/torvy-home-assistant,kyvinh/home-assistant,tinloaf/home-assistant,devdelay/home-assistant,dmeulen/home-assistant,leoc/home-assistant,Nzaga/home-assistant,hmronline/home-assistant,sffjunkie/home-assistant",103,"```python
""""""
homeassistant.components.light.limitlessled
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Support for LimitlessLED bulbs, also known as...

EasyBulb
AppLight
AppLamp
MiLight
LEDme
dekolight
iLight

""""""
import random
import logging

from homeassistant.helpers.entity import ToggleEntity
from homeassistant.const import STATE_ON, STATE_OFF, DEVICE_DEFAULT_NAME
from homeassistant.components.light import ATTR_BRIGHTNESS

_LOGGER = logging.getLogger(__name__)


def setup_platform(hass, config, add_devices_callback, discovery_info=None):
    try:
        import ledcontroller
    except ImportError:
        _LOGGER.exception(""Error while importing dependency ledcontroller."")
        return

    led = ledcontroller.LedController(config['host'])

    lights = []
    for i in range(1, 5):
        if 'group_%d_name' % (i) in config:
            lights.append(
                LimitlessLED(
                    led,
                    i,
                    config['group_%d_name' % (i)],
                    STATE_OFF
                )
            )

    add_devices_callback(lights)


class LimitlessLED(ToggleEntity):
    def __init__(self, led, group, name, state, brightness=180):
        self.led = led
        self.group = group

        # LimitlessLEDs don't report state, we have track it ourselves.
        self.led.off(self.group)

        self._name = name or DEVICE_DEFAULT_NAME
        self._state = state
        self._brightness = brightness

    @property
    def should_poll(self):
        """""" No polling needed for a demo light. """"""
        return False

    @property
    def name(self):
        """""" Returns the name of the device if any. """"""
        return self._name

    @property
    def state(self):
        """""" Returns the name of the device if any. """"""
        return self._state

    @property
    def state_attributes(self):
        """""" Returns optional state attributes. """"""
        if self.is_on:
            return {
                ATTR_BRIGHTNESS: self._brightness,
            }

    @property
    def is_on(self):
        """""" True if device is on. """"""
        return self._state == STATE_ON

    def turn_on(self, **kwargs):
        """""" Turn the device on. """"""
        self._state = STATE_ON

        if ATTR_BRIGHTNESS in kwargs:
            self._brightness = kwargs[ATTR_BRIGHTNESS]

        self.led.set_brightness(self._brightness, self.group)

    def turn_off(self, **kwargs):
        """""" Turn the device off. """"""
        self._state = STATE_OFF
        self.led.off(self.group)

```"
33448340d278da7e0653701d78cbab317893279d,AG/datasets/analyze.py,AG/datasets/analyze.py,,"#!/usr/bin/python

import os
import sys
import lxml 
from lxml import etree
import math 

class StatsCounter(object):

    prefixes = {}
    cur_tag = None

    def start( self, tag, attrib ):
        self.cur_tag = tag

    def end( self, tag ):
        pass
        #self.cur_tag = None 

    def data( self, _data ):
        if self.cur_tag != ""File"" and self.cur_tag != ""Dir"":
            return 

        data = _data.rstrip(""/"")
        if data == """":
            return 

        dir_name = os.path.dirname( data )
        if dir_name == """":
            return 

        if not self.prefixes.has_key( dir_name ):
            self.prefixes[ dir_name ] = 0

        self.prefixes[ dir_name ] += 1

    def close( self ):
        return ""closed!""

if __name__ == ""__main__"":

    counter = StatsCounter()
    parser = etree.XMLParser( target=counter )

    fd = open( sys.argv[1], ""r"" )
    
    while True:
        buf = fd.read( 32768 )
        if len(buf) == 0:
            break

        parser.feed( buf )

    result = parser.close()

    order = counter.prefixes.keys()
    order.sort()

    size_bins = {}

    for path in order:
        count = counter.prefixes[path]
        print ""% 15s %s"" % (count, path)

        size_bin = int(math.log(count, 10))
        
        if not size_bins.has_key( size_bin ):
            size_bins[ size_bin ] = 1

        else:
            size_bins[ size_bin ] += 1

    print """"
    print ""sizes""
    max_bin = max( size_bins.keys() )

    bin_fmt = r""1e%0"" + str( int(math.log(max_bin, 10)) + 1 ) + ""s""

    for size in xrange( 0, max_bin + 1 ):
        binsize = 0
        if size_bins.has_key( size ):
            binsize = size_bins[size]

        bin_str = bin_fmt % size
        print ""%s %s"" % (bin_str, binsize)


    
",Add a simple analysis tool to get some structural properties about an AG's specfile.,"Add a simple analysis tool to get some structural properties about an
AG's specfile.
",Python,apache-2.0,"jcnelson/syndicate,iychoi/syndicate,iychoi/syndicate,jcnelson/syndicate,iychoi/syndicate,iychoi/syndicate,iychoi/syndicate,iychoi/syndicate,iychoi/syndicate,jcnelson/syndicate,jcnelson/syndicate,jcnelson/syndicate,jcnelson/syndicate,iychoi/syndicate,jcnelson/syndicate,jcnelson/syndicate",90,"```python
#!/usr/bin/python

import os
import sys
import lxml 
from lxml import etree
import math 

class StatsCounter(object):

    prefixes = {}
    cur_tag = None

    def start( self, tag, attrib ):
        self.cur_tag = tag

    def end( self, tag ):
        pass
        #self.cur_tag = None 

    def data( self, _data ):
        if self.cur_tag != ""File"" and self.cur_tag != ""Dir"":
            return 

        data = _data.rstrip(""/"")
        if data == """":
            return 

        dir_name = os.path.dirname( data )
        if dir_name == """":
            return 

        if not self.prefixes.has_key( dir_name ):
            self.prefixes[ dir_name ] = 0

        self.prefixes[ dir_name ] += 1

    def close( self ):
        return ""closed!""

if __name__ == ""__main__"":

    counter = StatsCounter()
    parser = etree.XMLParser( target=counter )

    fd = open( sys.argv[1], ""r"" )
    
    while True:
        buf = fd.read( 32768 )
        if len(buf) == 0:
            break

        parser.feed( buf )

    result = parser.close()

    order = counter.prefixes.keys()
    order.sort()

    size_bins = {}

    for path in order:
        count = counter.prefixes[path]
        print ""% 15s %s"" % (count, path)

        size_bin = int(math.log(count, 10))
        
        if not size_bins.has_key( size_bin ):
            size_bins[ size_bin ] = 1

        else:
            size_bins[ size_bin ] += 1

    print """"
    print ""sizes""
    max_bin = max( size_bins.keys() )

    bin_fmt = r""1e%0"" + str( int(math.log(max_bin, 10)) + 1 ) + ""s""

    for size in xrange( 0, max_bin + 1 ):
        binsize = 0
        if size_bins.has_key( size ):
            binsize = size_bins[size]

        bin_str = bin_fmt % size
        print ""%s %s"" % (bin_str, binsize)


    

```"
3ae0ea21cc6b1afadb0dd72e29016385d18167ab,DebianDevelChangesBot/utils/fiforeader.py,DebianDevelChangesBot/utils/fiforeader.py,,"# -*- coding: utf-8 -*-
#
#   Debian Changes Bot
#   Copyright (C) 2008 Chris Lamb <chris@chris-lamb.co.uk>
#
#   This program is free software: you can redistribute it and/or modify
#   it under the terms of the GNU Affero General Public License as
#   published by the Free Software Foundation, either version 3 of the
#   License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU Affero General Public License for more details.
#
#   You should have received a copy of the GNU Affero General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

import os
import fcntl
import select
import threading
import traceback

class FifoReader(object):
    __shared_state = {}

    read_lock = threading.Lock()
    stop_lock = threading.Lock()
    running = False
    quitfds = None

    def __init__(self):
        self.__dict__ = self.__shared_state
        print ""lol""

    def start(self, callback, fifo_loc):
        self.callback = callback
        self.fifo_loc = fifo_loc
        threading.Thread(target=self.run).start()

    def run(self):
        self.read_lock.acquire()
        try:
            for fileobj in self.gen_messages():
                try:
                    self.callback(fileobj)
                except Exception, exc:
                    print ""Uncaught exception caught inside fiforeader""
                    traceback.print_exc()
                finally:
                    fileobj.close()
        finally:
            self.read_lock.release()

    def gen_messages(self):
        self.running = True
        self.quitfds = os.pipe()

        while self.running:
            fifo = os.open(self.fifo_loc, os.O_RDONLY | os.O_NONBLOCK)
            flags = fcntl.fcntl(fifo, fcntl.F_GETFL)
            fcntl.fcntl(fifo, fcntl.F_SETFD, flags & ~os.O_NONBLOCK)

            readfds, _, _ = select.select([fifo, self.quitfds[0]], [], [])

            # If our anonymous descriptor was written to, exit loop
            if not self.running or self.quitfds[0] in readfds:
                os.close(fifo)
                os.close(self.quitfds[0])
                os.close(self.quitfds[1])
                break

            if fifo not in readfds:
                continue

            yield os.fdopen(fifo)

    def stop(self):
        self.stop_lock.acquire()
        try:
            if self.running:
                self.running = False
                os.write(self.quitfds[1], '1')

                # Block until we have actually stopped
                self.read_lock.acquire()
                self.read_lock.release()
        finally:
            self.stop_lock.release()
",Add FifoReader class to utils,"Add FifoReader class to utils

Signed-off-by: Chris Lamb <711c73f64afdce07b7e38039a96d2224209e9a6c@chris-lamb.co.uk>
",Python,agpl-3.0,"xtaran/debian-devel-changes-bot,lamby/debian-devel-changes-bot,lamby/debian-devel-changes-bot,xtaran/debian-devel-changes-bot,sebastinas/debian-devel-changes-bot,lamby/debian-devel-changes-bot",91,"```python
# -*- coding: utf-8 -*-
#
#   Debian Changes Bot
#   Copyright (C) 2008 Chris Lamb <chris@chris-lamb.co.uk>
#
#   This program is free software: you can redistribute it and/or modify
#   it under the terms of the GNU Affero General Public License as
#   published by the Free Software Foundation, either version 3 of the
#   License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU Affero General Public License for more details.
#
#   You should have received a copy of the GNU Affero General Public License
#   along with this program.  If not, see <http://www.gnu.org/licenses/>.

import os
import fcntl
import select
import threading
import traceback

class FifoReader(object):
    __shared_state = {}

    read_lock = threading.Lock()
    stop_lock = threading.Lock()
    running = False
    quitfds = None

    def __init__(self):
        self.__dict__ = self.__shared_state
        print ""lol""

    def start(self, callback, fifo_loc):
        self.callback = callback
        self.fifo_loc = fifo_loc
        threading.Thread(target=self.run).start()

    def run(self):
        self.read_lock.acquire()
        try:
            for fileobj in self.gen_messages():
                try:
                    self.callback(fileobj)
                except Exception, exc:
                    print ""Uncaught exception caught inside fiforeader""
                    traceback.print_exc()
                finally:
                    fileobj.close()
        finally:
            self.read_lock.release()

    def gen_messages(self):
        self.running = True
        self.quitfds = os.pipe()

        while self.running:
            fifo = os.open(self.fifo_loc, os.O_RDONLY | os.O_NONBLOCK)
            flags = fcntl.fcntl(fifo, fcntl.F_GETFL)
            fcntl.fcntl(fifo, fcntl.F_SETFD, flags & ~os.O_NONBLOCK)

            readfds, _, _ = select.select([fifo, self.quitfds[0]], [], [])

            # If our anonymous descriptor was written to, exit loop
            if not self.running or self.quitfds[0] in readfds:
                os.close(fifo)
                os.close(self.quitfds[0])
                os.close(self.quitfds[1])
                break

            if fifo not in readfds:
                continue

            yield os.fdopen(fifo)

    def stop(self):
        self.stop_lock.acquire()
        try:
            if self.running:
                self.running = False
                os.write(self.quitfds[1], '1')

                # Block until we have actually stopped
                self.read_lock.acquire()
                self.read_lock.release()
        finally:
            self.stop_lock.release()

```"
a6137714c55ada55571759b851e1e4afa7818f29,app/utils/scripts/delete-docs.py,app/utils/scripts/delete-docs.py,,"#!/usr/bin/python
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

""""""Basic command line script to delete documents.""""""

import argparse
import sys

import models
import utils
import utils.db


COLLECTIONS = [
    models.BOOT_COLLECTION,
    models.DEFCONFIG_COLLECTION,
    models.JOB_COLLECTION,
    models.LAB_COLLECTION
]

ALL_COLLECTIONS = [
    ""all""
]
ALL_COLLECTIONS.extend(COLLECTIONS)


def parse_fields(fields):
    for field in fields:
        if ""="" in field:
            yield field.split(""="", 1)
        else:
            utils.LOG.error(""Field %s is not valid, not considered"", field)


def _delete_with_spec(collection, spec_or_id, database):
    ret_val = None
    if collection == ""all"":
        utils.LOG.info(""Deleting documents in all collections"")
        for coll in COLLECTIONS:
            utils.LOG.info(""Deleting from %s..."", coll)
            ret_val = utils.db.delete(database[coll], spec)
    else:
        ret_val = utils.db.delete(database[collection], spec_or_id)

    if ret_val == 200:
        utils.LOG.info(""Documents identified deleted: %s"", spec_or_id)
    else:
        utils.LOG.error(
            ""Error deleting documents with the provided values: %s"",
            spec_or_id)
        sys.exit(1)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Import boots from disk"",
        version=0.1
    )
    parser.add_argument(
        ""--collection"", ""-c"",
        type=str,
        help=""The name of the job to import"",
        dest=""collection"",
        required=True,
        choices=ALL_COLLECTIONS
    )
    parser.add_argument(
        ""--field"", ""-f"",
        help=(
            ""The necessary fields to identify the elements to delete; ""
            ""they must be defined as key=value pairs""
        ),
        dest=""fields"",
        action=""append"",
        required=True
    )

    args = parser.parse_args()

    collection = args.collection
    fields = args.fields

    spec = {
        k: v for k, v in parse_fields(fields)
    }

    if spec:
        database = utils.db.get_db_connection({})
        _delete_with_spec(collection, spec, database)
    else:
        utils.LOG.error(""Don't know what to look for..."")
        sys.exit(1)
",Add cli tool to delete documents.,"Add cli tool to delete documents.

Change-Id: I16c99d4b625e627c693c6354aaaa191c5076344b
",Python,lgpl-2.1,"kernelci/kernelci-backend,joyxu/kernelci-backend,kernelci/kernelci-backend,joyxu/kernelci-backend,joyxu/kernelci-backend",105,"```python
#!/usr/bin/python
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

""""""Basic command line script to delete documents.""""""

import argparse
import sys

import models
import utils
import utils.db


COLLECTIONS = [
    models.BOOT_COLLECTION,
    models.DEFCONFIG_COLLECTION,
    models.JOB_COLLECTION,
    models.LAB_COLLECTION
]

ALL_COLLECTIONS = [
    ""all""
]
ALL_COLLECTIONS.extend(COLLECTIONS)


def parse_fields(fields):
    for field in fields:
        if ""="" in field:
            yield field.split(""="", 1)
        else:
            utils.LOG.error(""Field %s is not valid, not considered"", field)


def _delete_with_spec(collection, spec_or_id, database):
    ret_val = None
    if collection == ""all"":
        utils.LOG.info(""Deleting documents in all collections"")
        for coll in COLLECTIONS:
            utils.LOG.info(""Deleting from %s..."", coll)
            ret_val = utils.db.delete(database[coll], spec)
    else:
        ret_val = utils.db.delete(database[collection], spec_or_id)

    if ret_val == 200:
        utils.LOG.info(""Documents identified deleted: %s"", spec_or_id)
    else:
        utils.LOG.error(
            ""Error deleting documents with the provided values: %s"",
            spec_or_id)
        sys.exit(1)


if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(
        description=""Import boots from disk"",
        version=0.1
    )
    parser.add_argument(
        ""--collection"", ""-c"",
        type=str,
        help=""The name of the job to import"",
        dest=""collection"",
        required=True,
        choices=ALL_COLLECTIONS
    )
    parser.add_argument(
        ""--field"", ""-f"",
        help=(
            ""The necessary fields to identify the elements to delete; ""
            ""they must be defined as key=value pairs""
        ),
        dest=""fields"",
        action=""append"",
        required=True
    )

    args = parser.parse_args()

    collection = args.collection
    fields = args.fields

    spec = {
        k: v for k, v in parse_fields(fields)
    }

    if spec:
        database = utils.db.get_db_connection({})
        _delete_with_spec(collection, spec, database)
    else:
        utils.LOG.error(""Don't know what to look for..."")
        sys.exit(1)

```"
3d18f6e3ba3519422aa30bd25f3511f62361d5ca,tests/chainer_tests/test_chainer_objects.py,tests/chainer_tests/test_chainer_objects.py,,"import importlib
import inspect
import pkgutil
import types

import six
import unittest

import chainer
from chainer import testing


def walk_modules():
    root = chainer.__path__
    for loader, modname, ispkg in pkgutil.walk_packages(root, 'chainer.'):
        # Skip modules generated by protobuf.
        if '_pb2' in modname:
            continue

        try:
            mod = importlib.import_module(modname)
        except ImportError:
            continue

        yield mod


def get_classes(module):
    # Enumerate classes from a module
    for name, o in module.__dict__.items():
        if (inspect.isclass(o)
                and o.__module__.startswith('chainer.')):
            yield o


def get_functions(module):
    # Enumerate functions from a module

    # Normal functions
    for k, o in module.__dict__.items():
        if (isinstance(o, types.FunctionType)
                and o.__module__.startswith('chainer.')):
            yield o

    # Methods defined in a class
    for cls in get_classes(module):
        if cls.__module__.startswith('chainer.'):
            for k, o in cls.__dict__.items():
                if inspect.isfunction(o):
                    yield o


def get_default_arguments(func):
    # Retrieves the defaults arguments (names and values) of a function.
    if six.PY2:
        # Python 2
        spec = inspect.getargspec(func)
        if spec.defaults is not None:
            n = len(spec.defaults)
            for name, default_value in zip(spec.args[-n:], spec.defaults):
                yield name, default_value
    else:
        # Python 3
        signature = inspect.signature(func)
        for name, param in signature.parameters.items():
            if param.default is not inspect.Parameter.empty:
                yield name, param.default


class TestFunctions(unittest.TestCase):

    def test_no_mutable_default_args(self):
        type_blacklist = (list, dict)
        badlist = []
        # Collect mutable default arguments
        for mod in walk_modules():
            for func in get_functions(mod):
                for arg_name, value in get_default_arguments(func):
                    if isinstance(value, type_blacklist):
                        badlist.append((func, arg_name, type(value)))

        if len(badlist) > 0:
            # Report the error
            s = six.StringIO()
            s.write(
                'Some functions have mutable values as default values:\n\n')
            for func, arg_name, value_type in badlist:
                s.write('{}.{}: arg=\'{}\' type={}\n'.format(
                    func.__module__, func.__name__, arg_name, value_type))
            assert False, s.getvalue()


testing.run_module(__name__, __file__)
",Add test to ensure no mutable default arguments,"Add test to ensure no mutable default arguments
",Python,mit,"wkentaro/chainer,niboshi/chainer,chainer/chainer,niboshi/chainer,wkentaro/chainer,pfnet/chainer,niboshi/chainer,hvy/chainer,wkentaro/chainer,chainer/chainer,okuta/chainer,wkentaro/chainer,okuta/chainer,okuta/chainer,chainer/chainer,chainer/chainer,hvy/chainer,niboshi/chainer,hvy/chainer,okuta/chainer,hvy/chainer",94,"```python
import importlib
import inspect
import pkgutil
import types

import six
import unittest

import chainer
from chainer import testing


def walk_modules():
    root = chainer.__path__
    for loader, modname, ispkg in pkgutil.walk_packages(root, 'chainer.'):
        # Skip modules generated by protobuf.
        if '_pb2' in modname:
            continue

        try:
            mod = importlib.import_module(modname)
        except ImportError:
            continue

        yield mod


def get_classes(module):
    # Enumerate classes from a module
    for name, o in module.__dict__.items():
        if (inspect.isclass(o)
                and o.__module__.startswith('chainer.')):
            yield o


def get_functions(module):
    # Enumerate functions from a module

    # Normal functions
    for k, o in module.__dict__.items():
        if (isinstance(o, types.FunctionType)
                and o.__module__.startswith('chainer.')):
            yield o

    # Methods defined in a class
    for cls in get_classes(module):
        if cls.__module__.startswith('chainer.'):
            for k, o in cls.__dict__.items():
                if inspect.isfunction(o):
                    yield o


def get_default_arguments(func):
    # Retrieves the defaults arguments (names and values) of a function.
    if six.PY2:
        # Python 2
        spec = inspect.getargspec(func)
        if spec.defaults is not None:
            n = len(spec.defaults)
            for name, default_value in zip(spec.args[-n:], spec.defaults):
                yield name, default_value
    else:
        # Python 3
        signature = inspect.signature(func)
        for name, param in signature.parameters.items():
            if param.default is not inspect.Parameter.empty:
                yield name, param.default


class TestFunctions(unittest.TestCase):

    def test_no_mutable_default_args(self):
        type_blacklist = (list, dict)
        badlist = []
        # Collect mutable default arguments
        for mod in walk_modules():
            for func in get_functions(mod):
                for arg_name, value in get_default_arguments(func):
                    if isinstance(value, type_blacklist):
                        badlist.append((func, arg_name, type(value)))

        if len(badlist) > 0:
            # Report the error
            s = six.StringIO()
            s.write(
                'Some functions have mutable values as default values:\n\n')
            for func, arg_name, value_type in badlist:
                s.write('{}.{}: arg=\'{}\' type={}\n'.format(
                    func.__module__, func.__name__, arg_name, value_type))
            assert False, s.getvalue()


testing.run_module(__name__, __file__)

```"
97ae80b08958646e0c937f65a1b396171bf61e72,Lib/test/test_xreload.py,Lib/test/test_xreload.py,,"""""""Doctests for module reloading.

>>> from xreload import xreload
>>> from test.test_xreload import make_mod
>>> make_mod()
>>> import x
>>> C = x.C
>>> Cfoo = C.foo
>>> Cbar = C.bar
>>> Cstomp = C.stomp
>>> b = C()
>>> bfoo = b.foo
>>> b.foo()
42
>>> bfoo()
42
>>> Cfoo(b)
42
>>> Cbar()
42 42
>>> Cstomp()
42 42 42
>>> make_mod(repl=""42"", subst=""24"")
>>> xreload(x)
<module 'x' (built-in)>
>>> b.foo()
24
>>> bfoo()
24
>>> Cfoo(b)
24
>>> Cbar()
24 24
>>> Cstomp()
24 24 24

""""""

SAMPLE_CODE = """"""
class C:
    def foo(self):
        print(42)
    @classmethod
    def bar(cls):
        print(42, 42)
    @staticmethod
    def stomp():
        print (42, 42, 42)
""""""

import os
import sys
import shutil
import doctest
import xreload
import tempfile
from test.test_support import run_unittest

tempdir = None
save_path = None


def setUp(unused=None):
    global tempdir, save_path
    tempdir = tempfile.mkdtemp()
    save_path = list(sys.path)
    sys.path.append(tempdir)


def tearDown(unused=None):
    global tempdir, save_path
    if save_path is not None:
        sys.path = save_path
        save_path = None
    if tempdir is not None:
        shutil.rmtree(tempdir)
        tempdir = None
        

def make_mod(name=""x"", repl=None, subst=None):
    if not tempdir:
        setUp()
        assert tempdir
    fn = os.path.join(tempdir, name + "".py"")
    f = open(fn, ""w"")
    sample = SAMPLE_CODE
    if repl is not None and subst is not None:
        sample = sample.replace(repl, subst)
    try:
        f.write(sample)
    finally:
        f.close()


def test_suite():
    return doctest.DocTestSuite(setUp=setUp, tearDown=tearDown)


def test_main():
    run_unittest(test_suite())

if __name__ == ""__main__"":
    test_main()
",Add a proper unit test for xreload.py.,"Add a proper unit test for xreload.py.
",Python,mit,"sk-/python2.7-type-annotator,sk-/python2.7-type-annotator,sk-/python2.7-type-annotator",104,"```python
""""""Doctests for module reloading.

>>> from xreload import xreload
>>> from test.test_xreload import make_mod
>>> make_mod()
>>> import x
>>> C = x.C
>>> Cfoo = C.foo
>>> Cbar = C.bar
>>> Cstomp = C.stomp
>>> b = C()
>>> bfoo = b.foo
>>> b.foo()
42
>>> bfoo()
42
>>> Cfoo(b)
42
>>> Cbar()
42 42
>>> Cstomp()
42 42 42
>>> make_mod(repl=""42"", subst=""24"")
>>> xreload(x)
<module 'x' (built-in)>
>>> b.foo()
24
>>> bfoo()
24
>>> Cfoo(b)
24
>>> Cbar()
24 24
>>> Cstomp()
24 24 24

""""""

SAMPLE_CODE = """"""
class C:
    def foo(self):
        print(42)
    @classmethod
    def bar(cls):
        print(42, 42)
    @staticmethod
    def stomp():
        print (42, 42, 42)
""""""

import os
import sys
import shutil
import doctest
import xreload
import tempfile
from test.test_support import run_unittest

tempdir = None
save_path = None


def setUp(unused=None):
    global tempdir, save_path
    tempdir = tempfile.mkdtemp()
    save_path = list(sys.path)
    sys.path.append(tempdir)


def tearDown(unused=None):
    global tempdir, save_path
    if save_path is not None:
        sys.path = save_path
        save_path = None
    if tempdir is not None:
        shutil.rmtree(tempdir)
        tempdir = None
        

def make_mod(name=""x"", repl=None, subst=None):
    if not tempdir:
        setUp()
        assert tempdir
    fn = os.path.join(tempdir, name + "".py"")
    f = open(fn, ""w"")
    sample = SAMPLE_CODE
    if repl is not None and subst is not None:
        sample = sample.replace(repl, subst)
    try:
        f.write(sample)
    finally:
        f.close()


def test_suite():
    return doctest.DocTestSuite(setUp=setUp, tearDown=tearDown)


def test_main():
    run_unittest(test_suite())

if __name__ == ""__main__"":
    test_main()

```"
27cb9279670bd513a1559f4865500d84869bb9f0,tests/test_predictor.py,tests/test_predictor.py,,"#! /usr/env/bin python
import numpy as np

from pyboas import predictor, models

# Build random 3-parameter normal posterior.
posterior = np.random.randn(100, 3)


def toy_model(param, time):
    time = np.atleast_1d(time)[:, np.newaxis]

    a = param[:, 0]
    b = param[:, 1]
    c = param[:, 2]

    return a*time**2 + b*time + c


def test_basic_shape():
    """"""Test basic shape conditions on output of predictions.""""""
    time = np.random.rand(4, )

    pred1 = predictor.GaussPredictor(posterior, toy_model)

    pred1.make_prediction(time)

    # Test shape of predictive distributions and x
    assert pred1.x.shape == pred1.predictives.shape
    # Test len of time array and predictives
    assert len(time) == len(pred1.predictives)

    return


def test_time_concatenation():
    """"""
    Test feature to concatenate prediction times over make_prediction calls.
    """"""

    # Built random time array
    time = np.random.rand(4,)

    pred1 = predictor.GaussPredictor(posterior, toy_model)
    pred2 = predictor.GaussPredictor(posterior, toy_model)

    # Run first predictor with full time array
    pred1.make_prediction(time)

    # Run second predictor twice
    pred2.make_prediction(time[:2])
    pred2.make_prediction(time[2:])

    assert np.allclose(pred1.predictives, pred2.predictives)
    assert np.allclose(pred1.x, pred2.x)

    return


def test_sample_draw():
    # Built random time array
    time = np.random.rand(4, )

    pred1 = predictor.GaussPredictor(posterior, toy_model)

    pred1.samplepredictive(time, 100)


def ok():
    print('\033[92mOK\033[0m')


def failed():
    print('\033[91mFAILED\033[0m')


def test_all():
    print('Testing basic functioning....\t'),
    try:
        test_basic_shape()
        ok()
    except AssertionError:
        failed()

    print('Testing time concatenation....\t'),
    try:
        test_time_concatenation()
        ok()
    except AssertionError:
        failed()
    return

if __name__ == '__main__':
    test_all()
",Test module for Predictor class.,"Test module for Predictor class.
",Python,mit,exord/pyboas,95,"```python
#! /usr/env/bin python
import numpy as np

from pyboas import predictor, models

# Build random 3-parameter normal posterior.
posterior = np.random.randn(100, 3)


def toy_model(param, time):
    time = np.atleast_1d(time)[:, np.newaxis]

    a = param[:, 0]
    b = param[:, 1]
    c = param[:, 2]

    return a*time**2 + b*time + c


def test_basic_shape():
    """"""Test basic shape conditions on output of predictions.""""""
    time = np.random.rand(4, )

    pred1 = predictor.GaussPredictor(posterior, toy_model)

    pred1.make_prediction(time)

    # Test shape of predictive distributions and x
    assert pred1.x.shape == pred1.predictives.shape
    # Test len of time array and predictives
    assert len(time) == len(pred1.predictives)

    return


def test_time_concatenation():
    """"""
    Test feature to concatenate prediction times over make_prediction calls.
    """"""

    # Built random time array
    time = np.random.rand(4,)

    pred1 = predictor.GaussPredictor(posterior, toy_model)
    pred2 = predictor.GaussPredictor(posterior, toy_model)

    # Run first predictor with full time array
    pred1.make_prediction(time)

    # Run second predictor twice
    pred2.make_prediction(time[:2])
    pred2.make_prediction(time[2:])

    assert np.allclose(pred1.predictives, pred2.predictives)
    assert np.allclose(pred1.x, pred2.x)

    return


def test_sample_draw():
    # Built random time array
    time = np.random.rand(4, )

    pred1 = predictor.GaussPredictor(posterior, toy_model)

    pred1.samplepredictive(time, 100)


def ok():
    print('\033[92mOK\033[0m')


def failed():
    print('\033[91mFAILED\033[0m')


def test_all():
    print('Testing basic functioning....\t'),
    try:
        test_basic_shape()
        ok()
    except AssertionError:
        failed()

    print('Testing time concatenation....\t'),
    try:
        test_time_concatenation()
        ok()
    except AssertionError:
        failed()
    return

if __name__ == '__main__':
    test_all()

```"
245879ce699b275edc3ee17e4cba1146241f25de,wizbit/xmlrpcdeferred.py,wizbit/xmlrpcdeferred.py,,"import gobject

import xmlrpclib

class XMLRPCDeferred (gobject.GObject):
    """"""Object representing the delayed result of an XML-RPC
    request.

    .is_ready: bool
      True when the result is received; False before then.
    .value : any
      Once is_ready=True, this attribute contains the result of the
      request.  If this value is an instance of the xmlrpclib.Fault
      class, then some exception occurred during the request's
      processing.

    """"""
    __gsignals__ = {
            'ready': (gobject.SIGNAL_RUN_FIRST, gobject.TYPE_NONE, ())
    }
    def __init__ (self, transport, http):
        self.__gobject_init__()
        self.transport = transport
        self.http = http
        self.value = None
        self.is_ready = False

        sock = self.http._conn.sock
        self.src_id = gobject.io_add_watch(sock,
                                           gobject.IO_IN | gobject.IO_HUP,
                                           self.handle_io)

    def handle_io (self, source, condition):
        # Triggered when there's input available on the socket.
        # The assumption is that all the input will be available
        # relatively quickly.
        self.read()

        # Returning false prevents this callback from being triggered
        # again.  We also remove the monitoring of this file
        # descriptor.
        gobject.source_remove(self.src_id)
        return False

    def read (self):
        errcode, errmsg, headers = self.http.getreply()

        if errcode != 200:
            raise ProtocolError(
                host + handler,
                errcode, errmsg,
                headers
                )

        try:
            result = xmlrpclib.Transport._parse_response(self.transport,
                                                         self.http.getfile(), None)
        except xmlrpclib.Fault, exc:
            result = exc

        self.value = result
        self.is_ready = True
        self.emit('ready')

    def __len__ (self):
        # XXX egregious hack!!!
        # The code in xmlrpclib.ServerProxy calls len() on the object
        # returned by the transport, and if it's of length 1 returns
        # the contained object.  Therefore, this __len__ method
        # returns a completely fake length of 2.
        return 2 


class GXMLRPCTransport (xmlrpclib.Transport):
    def request(self, host, handler, request_body, verbose=0):
        # issue XML-RPC request

        h = self.make_connection(host)
        if verbose:
            h.set_debuglevel(1)

        self.send_request(h, handler, request_body)
        self.send_host(h, host)
        self.send_user_agent(h)
        self.send_content(h, request_body)

        self.verbose = verbose

        return XMLRPCDeferred(self, h)

",Add GLib mainllop transport for xmlrpcserver,"Add GLib mainllop transport for xmlrpcserver
",Python,lgpl-2.1,"wizbit-archive/wizbit,wizbit-archive/wizbit",91,"```python
import gobject

import xmlrpclib

class XMLRPCDeferred (gobject.GObject):
    """"""Object representing the delayed result of an XML-RPC
    request.

    .is_ready: bool
      True when the result is received; False before then.
    .value : any
      Once is_ready=True, this attribute contains the result of the
      request.  If this value is an instance of the xmlrpclib.Fault
      class, then some exception occurred during the request's
      processing.

    """"""
    __gsignals__ = {
            'ready': (gobject.SIGNAL_RUN_FIRST, gobject.TYPE_NONE, ())
    }
    def __init__ (self, transport, http):
        self.__gobject_init__()
        self.transport = transport
        self.http = http
        self.value = None
        self.is_ready = False

        sock = self.http._conn.sock
        self.src_id = gobject.io_add_watch(sock,
                                           gobject.IO_IN | gobject.IO_HUP,
                                           self.handle_io)

    def handle_io (self, source, condition):
        # Triggered when there's input available on the socket.
        # The assumption is that all the input will be available
        # relatively quickly.
        self.read()

        # Returning false prevents this callback from being triggered
        # again.  We also remove the monitoring of this file
        # descriptor.
        gobject.source_remove(self.src_id)
        return False

    def read (self):
        errcode, errmsg, headers = self.http.getreply()

        if errcode != 200:
            raise ProtocolError(
                host + handler,
                errcode, errmsg,
                headers
                )

        try:
            result = xmlrpclib.Transport._parse_response(self.transport,
                                                         self.http.getfile(), None)
        except xmlrpclib.Fault, exc:
            result = exc

        self.value = result
        self.is_ready = True
        self.emit('ready')

    def __len__ (self):
        # XXX egregious hack!!!
        # The code in xmlrpclib.ServerProxy calls len() on the object
        # returned by the transport, and if it's of length 1 returns
        # the contained object.  Therefore, this __len__ method
        # returns a completely fake length of 2.
        return 2 


class GXMLRPCTransport (xmlrpclib.Transport):
    def request(self, host, handler, request_body, verbose=0):
        # issue XML-RPC request

        h = self.make_connection(host)
        if verbose:
            h.set_debuglevel(1)

        self.send_request(h, handler, request_body)
        self.send_host(h, host)
        self.send_user_agent(h)
        self.send_content(h, request_body)

        self.verbose = verbose

        return XMLRPCDeferred(self, h)


```"
ea11ae8919139eae8eaa6b9b1dfe256726d3c584,test/test_SBSolarcell.py,test/test_SBSolarcell.py,,"# -*- coding: utf-8 -*-
import numpy as np
import ibei
from astropy import units
import unittest

temp_sun = 5762.
temp_earth = 288.
bandgap = 1.15

input_params = {""temp_sun"": temp_sun,
                ""temp_planet"": temp_earth,
                ""bandgap"": bandgap,
                ""voltage"": 0.5,}


class CalculatorsReturnUnits(unittest.TestCase):
    """"""
    Tests units of the calculator methods returned values.
    """"""
    def setUp(self):
        """"""
        Initialize SBSolarcell object from input_params
        """"""
        self.solarcell = ibei.SQSolarcell(input_params)

    def test_calc_blackbody_radiant_power_density(self):
        """"""
        calc_blackbody_radiant_power_density should return value with unit of W m^-2.
        """"""
        tested_unit = self.solarcell.calc_blackbody_radiant_power_density().unit
        target_unit = units.Unit(""W/m2"")
        self.assertEqual(tested_unit, target_unit)

    def test_calc_power_density(self):
        """"""
        calc_power_density should return value with unit of W m^-2.
        """"""
        tested_unit = self.solarcell.calc_power_density().unit
        target_unit = units.Unit(""W/m2"")
        self.assertEqual(tested_unit, target_unit)

    def test_calc_power_density_zero_bandgap(self):
        """"""
        calc_power_density should return value with unit of W m^-2.
        """"""
        self.solarcell.bandgap = 0
        tested_unit = self.solarcell.calc_power_density().unit
        target_unit = units.Unit(""W/m2"")
        self.assertEqual(tested_unit, target_unit)


class CalculatorsReturnType(unittest.TestCase):
    """"""
    Tests type of the calculator methods returned values.
    """"""
    def setUp(self):
        """"""
        Initialize SBSolarcell object from input_params
        """"""
        self.solarcell = ibei.SQSolarcell(input_params)

    def test_calc_efficiency(self):
        """"""
        calc_power_density should return value with unit of W m^-2.
        """"""
        self.assertIsInstance(self.solarcell.calc_efficiency(), float)


class CalculatorsReturnValue(unittest.TestCase):
    """"""
    Tests special values of the calculator methods.
    """"""
    def setUp(self):
        """"""
        Initialize SBSolarcell object from input_params
        """"""
        self.solarcell = ibei.SQSolarcell(input_params)

    def test_calc_power_density(self):
        """"""
        calc_power_density should return 0 when bandgap = 0.
        """"""
        self.solarcell.bandgap = 0
        self.assertEqual(0, self.solarcell.calc_power_density())


if __name__ == ""__main__"":
    pass
",Copy SBSolarcell tests into individual file,"Copy SBSolarcell tests into individual file
",Python,mit,"jrsmith3/ibei,jrsmith3/tec,jrsmith3/tec",90,"```python
# -*- coding: utf-8 -*-
import numpy as np
import ibei
from astropy import units
import unittest

temp_sun = 5762.
temp_earth = 288.
bandgap = 1.15

input_params = {""temp_sun"": temp_sun,
                ""temp_planet"": temp_earth,
                ""bandgap"": bandgap,
                ""voltage"": 0.5,}


class CalculatorsReturnUnits(unittest.TestCase):
    """"""
    Tests units of the calculator methods returned values.
    """"""
    def setUp(self):
        """"""
        Initialize SBSolarcell object from input_params
        """"""
        self.solarcell = ibei.SQSolarcell(input_params)

    def test_calc_blackbody_radiant_power_density(self):
        """"""
        calc_blackbody_radiant_power_density should return value with unit of W m^-2.
        """"""
        tested_unit = self.solarcell.calc_blackbody_radiant_power_density().unit
        target_unit = units.Unit(""W/m2"")
        self.assertEqual(tested_unit, target_unit)

    def test_calc_power_density(self):
        """"""
        calc_power_density should return value with unit of W m^-2.
        """"""
        tested_unit = self.solarcell.calc_power_density().unit
        target_unit = units.Unit(""W/m2"")
        self.assertEqual(tested_unit, target_unit)

    def test_calc_power_density_zero_bandgap(self):
        """"""
        calc_power_density should return value with unit of W m^-2.
        """"""
        self.solarcell.bandgap = 0
        tested_unit = self.solarcell.calc_power_density().unit
        target_unit = units.Unit(""W/m2"")
        self.assertEqual(tested_unit, target_unit)


class CalculatorsReturnType(unittest.TestCase):
    """"""
    Tests type of the calculator methods returned values.
    """"""
    def setUp(self):
        """"""
        Initialize SBSolarcell object from input_params
        """"""
        self.solarcell = ibei.SQSolarcell(input_params)

    def test_calc_efficiency(self):
        """"""
        calc_power_density should return value with unit of W m^-2.
        """"""
        self.assertIsInstance(self.solarcell.calc_efficiency(), float)


class CalculatorsReturnValue(unittest.TestCase):
    """"""
    Tests special values of the calculator methods.
    """"""
    def setUp(self):
        """"""
        Initialize SBSolarcell object from input_params
        """"""
        self.solarcell = ibei.SQSolarcell(input_params)

    def test_calc_power_density(self):
        """"""
        calc_power_density should return 0 when bandgap = 0.
        """"""
        self.solarcell.bandgap = 0
        self.assertEqual(0, self.solarcell.calc_power_density())


if __name__ == ""__main__"":
    pass

```"
578de6c57f9698c7e273af06d1e815f71269bb18,tests/to_debug.py,tests/to_debug.py,,"import sys
import os
import time
import threading
import ikpdb

TEST_MULTI_THREADING = False
TEST_EXCEPTION_PROPAGATION = False
TEST_POSTMORTEM = True
TEST_SYS_EXIT = 0
TEST_STEPPING = False

# Note that ikpdb.set_trace() will reset/mess breakpoints set using GUI
TEST_SET_TRACE = False  

TCB = TEST_CONDITIONAL_BREAKPOINT = True

class Worker(object):
    def __init__(self):
        self._running = True
    
    def terminate(self):
        self._running = False
        
    def run(self, n):
        work_count = n
        while self._running and n > 0:
            print ""Worker: Doing iteration: %s"" % (work_count - n)
            if n == 3:
                pass  # ikpdb.set_trace()
            n -= 1
            time.sleep(2)

ga = 5
gb =""coucou""
g_dict = {""Genesis"": 1, ""Don't Look Back"": 2, 'array': [1,3,{'coucou': 3.14}]}
a_tuple = (1,'e', 3.14, ['a', 'b'])

class BigBear:
    color = ""white""
    def __init__(self, name='unknown'):
        self._name = name
        
    def grumble(self):
        print ""Roaaarrrrrrr""

def sub_function():
    return True

def the_function(p_nb_seconds):
    a_var = 18.3
    the_function_local_list = [1,2,3,'cyril']
    a_beast = BigBear()
    print ""ga=%s"" % ga
    
    print ""Hello World""
    print ""Ceci est la ligne avec le point d'arret""
    for loop_idx in range(p_nb_seconds):
        print ""hello @ %s seconds"" % loop_idx
        time.sleep(1)
        if loop_idx == 12:
            if TEST_SET_TRACE:
                ikpdb.set_trace()  # will break on next line
            pass # Need this for set_trace()
            a_var = 98.3
            sub_function()                


def sub_raiser():
    raise Exception(""Prends ca dans ta bouille"")


def raiser():
    try:
        sub_raiser()
    except Exception as e:
        raise e


if __name__=='__main__':
    b = 0
    main_bear = BigBear(""Cyril"")
    print ""Type of main_bear=%s"" % type(main_bear)
    print ""sys.argv=%s"" % sys.argv
    
    if TEST_SYS_EXIT:
        sys.exit(TEST_SYS_EXIT)
    
    if TEST_EXCEPTION_PROPAGATION:
        raiser()
    
    if TEST_MULTI_THREADING:
        w = Worker()
        t = threading.Thread(target=w.run, args=(5,))
        t.start()

    duration = 2 if TEST_STEPPING else 15
    the_function(duration)

    if TEST_MULTI_THREADING:
        w.terminate()
        t.join()
    
    print ""finished""
    
    if TEST_POSTMORTEM:
        print 5 / b
    
",Add a sample python file interesting to debug,"Add a sample python file interesting to debug
",Python,mit,"audaxis/ikpdb,audaxis/ikpdb",109,"```python
import sys
import os
import time
import threading
import ikpdb

TEST_MULTI_THREADING = False
TEST_EXCEPTION_PROPAGATION = False
TEST_POSTMORTEM = True
TEST_SYS_EXIT = 0
TEST_STEPPING = False

# Note that ikpdb.set_trace() will reset/mess breakpoints set using GUI
TEST_SET_TRACE = False  

TCB = TEST_CONDITIONAL_BREAKPOINT = True

class Worker(object):
    def __init__(self):
        self._running = True
    
    def terminate(self):
        self._running = False
        
    def run(self, n):
        work_count = n
        while self._running and n > 0:
            print ""Worker: Doing iteration: %s"" % (work_count - n)
            if n == 3:
                pass  # ikpdb.set_trace()
            n -= 1
            time.sleep(2)

ga = 5
gb =""coucou""
g_dict = {""Genesis"": 1, ""Don't Look Back"": 2, 'array': [1,3,{'coucou': 3.14}]}
a_tuple = (1,'e', 3.14, ['a', 'b'])

class BigBear:
    color = ""white""
    def __init__(self, name='unknown'):
        self._name = name
        
    def grumble(self):
        print ""Roaaarrrrrrr""

def sub_function():
    return True

def the_function(p_nb_seconds):
    a_var = 18.3
    the_function_local_list = [1,2,3,'cyril']
    a_beast = BigBear()
    print ""ga=%s"" % ga
    
    print ""Hello World""
    print ""Ceci est la ligne avec le point d'arret""
    for loop_idx in range(p_nb_seconds):
        print ""hello @ %s seconds"" % loop_idx
        time.sleep(1)
        if loop_idx == 12:
            if TEST_SET_TRACE:
                ikpdb.set_trace()  # will break on next line
            pass # Need this for set_trace()
            a_var = 98.3
            sub_function()                


def sub_raiser():
    raise Exception(""Prends ca dans ta bouille"")


def raiser():
    try:
        sub_raiser()
    except Exception as e:
        raise e


if __name__=='__main__':
    b = 0
    main_bear = BigBear(""Cyril"")
    print ""Type of main_bear=%s"" % type(main_bear)
    print ""sys.argv=%s"" % sys.argv
    
    if TEST_SYS_EXIT:
        sys.exit(TEST_SYS_EXIT)
    
    if TEST_EXCEPTION_PROPAGATION:
        raiser()
    
    if TEST_MULTI_THREADING:
        w = Worker()
        t = threading.Thread(target=w.run, args=(5,))
        t.start()

    duration = 2 if TEST_STEPPING else 15
    the_function(duration)

    if TEST_MULTI_THREADING:
        w.terminate()
        t.join()
    
    print ""finished""
    
    if TEST_POSTMORTEM:
        print 5 / b
    

```"
5273a97ab1da4b809573617d3fc01705c322992f,thecut/authorship/tests/test_forms.py,thecut/authorship/tests/test_forms.py,,"# -*- coding: utf-8 -*-
from __future__ import absolute_import, unicode_literals
from django.test import TestCase
from django import forms
from mock import patch
from test_app.models import AuthorshipModel
from thecut.authorship.factories import UserFactory
from thecut.authorship.forms import AuthorshipMixin


class AuthorshipModelForm(AuthorshipMixin, forms.ModelForm):

    class Meta:
        model = AuthorshipModel
        fields = []


class DummyUser(object):

    pass


class TestAuthorshipMixin(TestCase):

    def test_requires_an_extra_argument_on_creating_an_instance(self):
        self.assertRaises(TypeError, AuthorshipModelForm)

    def test_sets_user_attribute(self):

        dummy_user = DummyUser()

        form = AuthorshipModelForm(user=dummy_user)

        self.assertEqual(dummy_user, form.user)


class DummyUnsavedModel(object):

    def __init__(self):
        self.pk = None


class TestAuthorshipMixinSave(TestCase):

    @patch('django.forms.ModelForm.save')
    def test_calls_super_class_save_method(self, superclass_save):

        form = AuthorshipModelForm(user=UserFactory())
        form.instance = DummyUnsavedModel()

        form.save()

        self.assertTrue(superclass_save.called)

    @patch('django.forms.ModelForm.save')
    def test_sets_updated_by_to_given_user(self, superclass_save):
        user = DummyUser()
        form = AuthorshipModelForm(user=user)
        form.instance = DummyUnsavedModel()
        form.cleaned_data = {}

        form.save()

        self.assertEqual(user, form.instance.updated_by)

    @patch('django.forms.ModelForm.save')
    def test_sets_created_by_if_instance_is_not_saved(self, superclass_save):
        user = DummyUser()
        form = AuthorshipModelForm(user=user)
        form.instance = DummyUnsavedModel()
        form.cleaned_data = {}

        form.save()

        self.assertEqual(user, form.instance.created_by)

    @patch('django.forms.ModelForm.save')
    def test_does_not_set_created_by_if_instance_is_saved(self,
                                                          superclass_save):

        class DummySavedModel(object):

            def __init__(self):
                self.pk = 'arbitrary-value'
                self.created_by = 'arbitrary-value'

        user = DummyUser()
        form = AuthorshipModelForm(user=user)
        form.instance = DummySavedModel()
        form.cleaned_data = {}

        form.save()

        self.assertNotEqual(user, form.instance.created_by)
",Add tests for form mixin.,"Add tests for form mixin.
",Python,apache-2.0,thecut/thecut-authorship,95,"```python
# -*- coding: utf-8 -*-
from __future__ import absolute_import, unicode_literals
from django.test import TestCase
from django import forms
from mock import patch
from test_app.models import AuthorshipModel
from thecut.authorship.factories import UserFactory
from thecut.authorship.forms import AuthorshipMixin


class AuthorshipModelForm(AuthorshipMixin, forms.ModelForm):

    class Meta:
        model = AuthorshipModel
        fields = []


class DummyUser(object):

    pass


class TestAuthorshipMixin(TestCase):

    def test_requires_an_extra_argument_on_creating_an_instance(self):
        self.assertRaises(TypeError, AuthorshipModelForm)

    def test_sets_user_attribute(self):

        dummy_user = DummyUser()

        form = AuthorshipModelForm(user=dummy_user)

        self.assertEqual(dummy_user, form.user)


class DummyUnsavedModel(object):

    def __init__(self):
        self.pk = None


class TestAuthorshipMixinSave(TestCase):

    @patch('django.forms.ModelForm.save')
    def test_calls_super_class_save_method(self, superclass_save):

        form = AuthorshipModelForm(user=UserFactory())
        form.instance = DummyUnsavedModel()

        form.save()

        self.assertTrue(superclass_save.called)

    @patch('django.forms.ModelForm.save')
    def test_sets_updated_by_to_given_user(self, superclass_save):
        user = DummyUser()
        form = AuthorshipModelForm(user=user)
        form.instance = DummyUnsavedModel()
        form.cleaned_data = {}

        form.save()

        self.assertEqual(user, form.instance.updated_by)

    @patch('django.forms.ModelForm.save')
    def test_sets_created_by_if_instance_is_not_saved(self, superclass_save):
        user = DummyUser()
        form = AuthorshipModelForm(user=user)
        form.instance = DummyUnsavedModel()
        form.cleaned_data = {}

        form.save()

        self.assertEqual(user, form.instance.created_by)

    @patch('django.forms.ModelForm.save')
    def test_does_not_set_created_by_if_instance_is_saved(self,
                                                          superclass_save):

        class DummySavedModel(object):

            def __init__(self):
                self.pk = 'arbitrary-value'
                self.created_by = 'arbitrary-value'

        user = DummyUser()
        form = AuthorshipModelForm(user=user)
        form.instance = DummySavedModel()
        form.cleaned_data = {}

        form.save()

        self.assertNotEqual(user, form.instance.created_by)

```"
67596d081059a004e5f7ab15f7972773fdf2f15e,tests/syft/grid/messages/setup_msg_test.py,tests/syft/grid/messages/setup_msg_test.py,,"# syft absolute
import syft as sy
from syft.core.io.address import Address
from syft.grid.messages.setup_messages import CreateInitialSetUpMessage
from syft.grid.messages.setup_messages import CreateInitialSetUpResponse
from syft.grid.messages.setup_messages import GetSetUpMessage
from syft.grid.messages.setup_messages import GetSetUpResponse


def test_create_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = CreateInitialSetUpMessage(
        address=target,
        content= request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_create_initial_setup_response_serde() -> None:
    target = Address(name=""Alice"")

    request_content = {""msg"": ""Initial setup registered successfully!""}
    msg = CreateInitialSetUpResponse(
        address=target,
        success=True,
        content= request_content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_get_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = {}
    msg = GetSetUpMessage(
        address=target,
        content=request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_delete_worker_response_serde() -> None:
    target = Address(name=""Alice"")

    content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = GetSetUpResponse(
        success=True,
        address=target,
        content=content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2",ADD PyGrid SetupService message tests,"ADD PyGrid SetupService message tests
",Python,apache-2.0,"OpenMined/PySyft,OpenMined/PySyft,OpenMined/PySyft,OpenMined/PySyft",106,"```python
# syft absolute
import syft as sy
from syft.core.io.address import Address
from syft.grid.messages.setup_messages import CreateInitialSetUpMessage
from syft.grid.messages.setup_messages import CreateInitialSetUpResponse
from syft.grid.messages.setup_messages import GetSetUpMessage
from syft.grid.messages.setup_messages import GetSetUpResponse


def test_create_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = CreateInitialSetUpMessage(
        address=target,
        content= request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_create_initial_setup_response_serde() -> None:
    target = Address(name=""Alice"")

    request_content = {""msg"": ""Initial setup registered successfully!""}
    msg = CreateInitialSetUpResponse(
        address=target,
        success=True,
        content= request_content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_get_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = {}
    msg = GetSetUpMessage(
        address=target,
        content=request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_delete_worker_response_serde() -> None:
    target = Address(name=""Alice"")

    content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = GetSetUpResponse(
        success=True,
        address=target,
        content=content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2
```"
48857638694ceca08c64d7b9c6825e2178c53279,pylearn2/utils/doc.py,pylearn2/utils/doc.py,,"""""""
Documentation-related helper classes/functions
""""""


class soft_wraps:
    """"""
    A Python decorator which concatenates two functions' docstrings: one
    function is defined at initialization and the other one is defined when
    soft_wraps is called.

    This helps reduce the ammount of documentation to write: one can use
    this decorator on child classes' functions when their implementation is
    similar to the one of the parent class. Conversely, if a function defined
    in a child class departs from its parent's implementation, one can simply
    explain the differences in a 'Notes' section without re-writing the whole
    docstring.

    Examples
    --------
    >>> class Parent(object):
    ...     def f(x):
    ...        '''
    ...        Adds 1 to x
    ...        
    ...        Parameters
    ...        ----------
    ...        x : int
    ...            Variable to increment by 1
    ...
    ...        Returns
    ...        -------
    ...        rval : int
    ...            x incremented by 1
    ...        '''
    ...        rval = x + 1
    ...        return rval
    ...
    >>> class Child(Parent):
    ...     @soft_wraps(Parent.f)
    ...     def f(x):
    ...        '''
    ...        Notes
    ...        -----
    ...        Also prints the incremented value
    ...        '''
    ...        rval = x + 1
    ...        print rval
    ...        return rval
    ...
    >>> c = Child()
    >>> print c.f.__doc__

        Adds 1 to x
        
        Parameters
        ----------
        x : int
            Variable to increment by 1
    
        Returns
        -------
        rval : int
           x incremented by 1
    
        Notes
        -----
        Also prints the incremented value
    """"""

    def __init__(self, f, append=False):
        """"""
        Parameters
        ----------
        f : function
            Function whose docstring will be concatenated with the decorated
            function's docstring
        prepend : bool, optional
            If True, appends f's docstring to the decorated function's
            docstring instead of prepending it. Defaults to False.
        """"""
        self.f = f
        self.append = append

    def __call__(self, f):
        """"""
        Prepend self.f's docstring to f's docstring (or append it if
        `self.append == True`).

        Parameters
        ----------
        f : function
            Function to decorate

        Returns
        -------
        f : function
            Function f passed as argument with self.f's docstring
            {pre,ap}pended to it
        """"""
        if self.append:
            f.__doc__ +=  + self.f.__doc__
        else:
            f.__doc__ = self.f.__doc__ + f.__doc__

        return f
",Add function decorator to improve functools.wraps,"Add function decorator to improve functools.wraps
",Python,bsd-3-clause,"goodfeli/pylearn2,JesseLivezey/pylearn2,TNick/pylearn2,fulmicoton/pylearn2,pkainz/pylearn2,Refefer/pylearn2,woozzu/pylearn2,kastnerkyle/pylearn2,CIFASIS/pylearn2,mclaughlin6464/pylearn2,hyqneuron/pylearn2-maxsom,aalmah/pylearn2,bartvm/pylearn2,JesseLivezey/pylearn2,nouiz/pylearn2,lamblin/pylearn2,CIFASIS/pylearn2,junbochen/pylearn2,ddboline/pylearn2,junbochen/pylearn2,ddboline/pylearn2,alexjc/pylearn2,w1kke/pylearn2,abergeron/pylearn2,mkraemer67/pylearn2,jamessergeant/pylearn2,fyffyt/pylearn2,fishcorn/pylearn2,fyffyt/pylearn2,matrogers/pylearn2,matrogers/pylearn2,aalmah/pylearn2,lunyang/pylearn2,skearnes/pylearn2,mkraemer67/pylearn2,theoryno3/pylearn2,kose-y/pylearn2,se4u/pylearn2,aalmah/pylearn2,daemonmaker/pylearn2,jeremyfix/pylearn2,hyqneuron/pylearn2-maxsom,lancezlin/pylearn2,Refefer/pylearn2,ddboline/pylearn2,lancezlin/pylearn2,kose-y/pylearn2,JesseLivezey/plankton,hantek/pylearn2,goodfeli/pylearn2,woozzu/pylearn2,ashhher3/pylearn2,bartvm/pylearn2,shiquanwang/pylearn2,TNick/pylearn2,hantek/pylearn2,lancezlin/pylearn2,TNick/pylearn2,daemonmaker/pylearn2,pkainz/pylearn2,fyffyt/pylearn2,alexjc/pylearn2,lunyang/pylearn2,ddboline/pylearn2,hantek/pylearn2,woozzu/pylearn2,pombredanne/pylearn2,TNick/pylearn2,jeremyfix/pylearn2,hyqneuron/pylearn2-maxsom,fishcorn/pylearn2,lisa-lab/pylearn2,kastnerkyle/pylearn2,mkraemer67/pylearn2,abergeron/pylearn2,alexjc/pylearn2,pombredanne/pylearn2,hyqneuron/pylearn2-maxsom,daemonmaker/pylearn2,KennethPierce/pylearnk,cosmoharrigan/pylearn2,aalmah/pylearn2,shiquanwang/pylearn2,JesseLivezey/plankton,KennethPierce/pylearnk,jamessergeant/pylearn2,caidongyun/pylearn2,fishcorn/pylearn2,mkraemer67/pylearn2,jamessergeant/pylearn2,CIFASIS/pylearn2,chrish42/pylearn,lunyang/pylearn2,fulmicoton/pylearn2,bartvm/pylearn2,mclaughlin6464/pylearn2,lunyang/pylearn2,se4u/pylearn2,fyffyt/pylearn2,Refefer/pylearn2,matrogers/pylearn2,mclaughlin6464/pylearn2,se4u/pylearn2,nouiz/pylearn2,jamessergeant/pylearn2,JesseLivezey/pylearn2,chrish42/pylearn,KennethPierce/pylearnk,cosmoharrigan/pylearn2,sandeepkbhat/pylearn2,theoryno3/pylearn2,w1kke/pylearn2,goodfeli/pylearn2,JesseLivezey/pylearn2,Refefer/pylearn2,caidongyun/pylearn2,msingh172/pylearn2,kastnerkyle/pylearn2,skearnes/pylearn2,KennethPierce/pylearnk,abergeron/pylearn2,kastnerkyle/pylearn2,skearnes/pylearn2,jeremyfix/pylearn2,pombredanne/pylearn2,ashhher3/pylearn2,lisa-lab/pylearn2,lamblin/pylearn2,junbochen/pylearn2,ashhher3/pylearn2,lamblin/pylearn2,CIFASIS/pylearn2,lisa-lab/pylearn2,pkainz/pylearn2,pombredanne/pylearn2,fishcorn/pylearn2,se4u/pylearn2,pkainz/pylearn2,junbochen/pylearn2,cosmoharrigan/pylearn2,goodfeli/pylearn2,theoryno3/pylearn2,ashhher3/pylearn2,alexjc/pylearn2,jeremyfix/pylearn2,JesseLivezey/plankton,chrish42/pylearn,msingh172/pylearn2,mclaughlin6464/pylearn2,sandeepkbhat/pylearn2,shiquanwang/pylearn2,w1kke/pylearn2,caidongyun/pylearn2,skearnes/pylearn2,matrogers/pylearn2,fulmicoton/pylearn2,shiquanwang/pylearn2,chrish42/pylearn,lisa-lab/pylearn2,kose-y/pylearn2,hantek/pylearn2,kose-y/pylearn2,msingh172/pylearn2,sandeepkbhat/pylearn2,w1kke/pylearn2,msingh172/pylearn2,lancezlin/pylearn2,woozzu/pylearn2,daemonmaker/pylearn2,JesseLivezey/plankton,fulmicoton/pylearn2,caidongyun/pylearn2,bartvm/pylearn2,nouiz/pylearn2,cosmoharrigan/pylearn2,sandeepkbhat/pylearn2,nouiz/pylearn2,lamblin/pylearn2,theoryno3/pylearn2,abergeron/pylearn2",107,"```python
""""""
Documentation-related helper classes/functions
""""""


class soft_wraps:
    """"""
    A Python decorator which concatenates two functions' docstrings: one
    function is defined at initialization and the other one is defined when
    soft_wraps is called.

    This helps reduce the ammount of documentation to write: one can use
    this decorator on child classes' functions when their implementation is
    similar to the one of the parent class. Conversely, if a function defined
    in a child class departs from its parent's implementation, one can simply
    explain the differences in a 'Notes' section without re-writing the whole
    docstring.

    Examples
    --------
    >>> class Parent(object):
    ...     def f(x):
    ...        '''
    ...        Adds 1 to x
    ...        
    ...        Parameters
    ...        ----------
    ...        x : int
    ...            Variable to increment by 1
    ...
    ...        Returns
    ...        -------
    ...        rval : int
    ...            x incremented by 1
    ...        '''
    ...        rval = x + 1
    ...        return rval
    ...
    >>> class Child(Parent):
    ...     @soft_wraps(Parent.f)
    ...     def f(x):
    ...        '''
    ...        Notes
    ...        -----
    ...        Also prints the incremented value
    ...        '''
    ...        rval = x + 1
    ...        print rval
    ...        return rval
    ...
    >>> c = Child()
    >>> print c.f.__doc__

        Adds 1 to x
        
        Parameters
        ----------
        x : int
            Variable to increment by 1
    
        Returns
        -------
        rval : int
           x incremented by 1
    
        Notes
        -----
        Also prints the incremented value
    """"""

    def __init__(self, f, append=False):
        """"""
        Parameters
        ----------
        f : function
            Function whose docstring will be concatenated with the decorated
            function's docstring
        prepend : bool, optional
            If True, appends f's docstring to the decorated function's
            docstring instead of prepending it. Defaults to False.
        """"""
        self.f = f
        self.append = append

    def __call__(self, f):
        """"""
        Prepend self.f's docstring to f's docstring (or append it if
        `self.append == True`).

        Parameters
        ----------
        f : function
            Function to decorate

        Returns
        -------
        f : function
            Function f passed as argument with self.f's docstring
            {pre,ap}pended to it
        """"""
        if self.append:
            f.__doc__ +=  + self.f.__doc__
        else:
            f.__doc__ = self.f.__doc__ + f.__doc__

        return f

```"
c0ab9b755b4906129988348b2247452b6dfc157f,plugins/modules/dedicated_server_display_name.py,plugins/modules/dedicated_server_display_name.py,,"#!/usr/bin/python
# -*- coding: utf-8 -*-

from __future__ import (absolute_import, division, print_function)

from ansible.module_utils.basic import AnsibleModule

__metaclass__ = type

DOCUMENTATION = '''
---
module: dedicated_server_display_name
short_description: Modify the server display name in ovh manager
description:
    - Modify the server display name in ovh manager, to help you find your server with your own naming
author: Synthesio SRE Team
requirements:
    - ovh >= 0.5.0
options:
    service_name:
        required: true
        description: The service name
    display_name:
        required: true
        description: The display name to set

'''

EXAMPLES = '''
synthesio.ovh.display_name
  service_name: ""{{ ovhname }}""
  display_name: ""{{ ansible_hostname }}""
delegate_to: localhost
'''

RETURN = ''' # '''

from ansible_collections.synthesio.ovh.plugins.module_utils.ovh import ovh_api_connect, ovh_argument_spec

try:
    from ovh.exceptions import APIError
    HAS_OVH = True
except ImportError:
    HAS_OVH = False


def run_module():
    module_args = ovh_argument_spec()
    module_args.update(dict(
        display_name=dict(required=True),
        service_name=dict(required=True)
    ))

    module = AnsibleModule(
        argument_spec=module_args,
        supports_check_mode=True
    )
    client = ovh_api_connect(module)

    display_name = module.params['display_name']
    service_name = module.params['service_name']

    if module.check_mode:
        module.exit_json(msg=""display_name has been set to {} ! - (dry run mode)"".format(display_name), changed=True)

    try:
        result = client.get('/dedicated/server/%s/serviceInfos' % service_name)
    except APIError as api_error:
        return module.fail_json(msg=""Failed to call OVH API: {0}"".format(api_error))

    service_id = result[""serviceId""]
    resource = {
        ""resource"": {
            'displayName': display_name,
            'name': service_name}}
    try:
        client.put(
            '/service/%s' % service_id,
            **resource
        )
        module.exit_json(
            msg=""displayName succesfully set to {} for {} !"".format(display_name, service_name),
            changed=True)
    except APIError as api_error:
        return module.fail_json(msg=""Failed to call OVH API: {0}"".format(api_error))


def main():
    run_module()


if __name__ == '__main__':
    main()
","Add a module to set the ""display name"" of a dedicated server","INFRA-6896: Add a module to set the ""display name"" of a dedicated server

- Change the ""display name"" in ovh manager, so you can use your internal
naming for example
- No need to set internal names in reverse dns anymore
- Change is only visible in OVH manager (and API, of course)
",Python,mit,synthesio/infra-ovh-ansible-module,94,"```python
#!/usr/bin/python
# -*- coding: utf-8 -*-

from __future__ import (absolute_import, division, print_function)

from ansible.module_utils.basic import AnsibleModule

__metaclass__ = type

DOCUMENTATION = '''
---
module: dedicated_server_display_name
short_description: Modify the server display name in ovh manager
description:
    - Modify the server display name in ovh manager, to help you find your server with your own naming
author: Synthesio SRE Team
requirements:
    - ovh >= 0.5.0
options:
    service_name:
        required: true
        description: The service name
    display_name:
        required: true
        description: The display name to set

'''

EXAMPLES = '''
synthesio.ovh.display_name
  service_name: ""{{ ovhname }}""
  display_name: ""{{ ansible_hostname }}""
delegate_to: localhost
'''

RETURN = ''' # '''

from ansible_collections.synthesio.ovh.plugins.module_utils.ovh import ovh_api_connect, ovh_argument_spec

try:
    from ovh.exceptions import APIError
    HAS_OVH = True
except ImportError:
    HAS_OVH = False


def run_module():
    module_args = ovh_argument_spec()
    module_args.update(dict(
        display_name=dict(required=True),
        service_name=dict(required=True)
    ))

    module = AnsibleModule(
        argument_spec=module_args,
        supports_check_mode=True
    )
    client = ovh_api_connect(module)

    display_name = module.params['display_name']
    service_name = module.params['service_name']

    if module.check_mode:
        module.exit_json(msg=""display_name has been set to {} ! - (dry run mode)"".format(display_name), changed=True)

    try:
        result = client.get('/dedicated/server/%s/serviceInfos' % service_name)
    except APIError as api_error:
        return module.fail_json(msg=""Failed to call OVH API: {0}"".format(api_error))

    service_id = result[""serviceId""]
    resource = {
        ""resource"": {
            'displayName': display_name,
            'name': service_name}}
    try:
        client.put(
            '/service/%s' % service_id,
            **resource
        )
        module.exit_json(
            msg=""displayName succesfully set to {} for {} !"".format(display_name, service_name),
            changed=True)
    except APIError as api_error:
        return module.fail_json(msg=""Failed to call OVH API: {0}"".format(api_error))


def main():
    run_module()


if __name__ == '__main__':
    main()

```"
65b362985d502440b12efc8a6a49ab0603354fd2,liwc_emotional_sentences.py,liwc_emotional_sentences.py,,"""""""Count the numbers of annotated entities and emotional sentences in the
corpus that was manually annotated.

Usage: python annotation_statistics.py <dir containing the folia files with
EmbodiedEmotions annotations>
""""""
from lxml import etree
from bs4 import BeautifulSoup
from emotools.bs4_helpers import sentence, note
import argparse
import os
from collections import Counter
import json
import codecs

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('dir_name', help='the name of the dir containing the '
                        'FoLiA XML files that should be processed.')
    args = parser.parse_args()

    dir_name = args.dir_name

    act_tag = '{http://ilk.uvt.nl/folia}div'

    cur_dir = os.getcwd()
    os.chdir(dir_name)

    folia_counter = 0
    num_sent = 0
    num_emotional = 0
    stats = Counter()
    entity_words = {}
    text_stats = {}
    emotional_cats = ['liwc-Posemo', 'liwc-Negemo']

    print 'Files'
    for file_name in os.listdir(dir_name):
        folia_counter += 1
        print '{}'.format(file_name)

        text_id = file_name[0:13]
        text_stats[text_id] = Counter()

        sents = set()
        # load document
        context = etree.iterparse(file_name,
                                  events=('start', 'end'),
                                  tag=act_tag,
                                  huge_tree=True)
        for event, elem in context:
            if event == 'end' and elem.get('class') == 'act':
                # load act into memory
                act_xml = BeautifulSoup(etree.tostring(elem), 'xml')
                sentences = act_xml.find_all(sentence)
                s = None
                for sent in sentences:
                    if not note(sent.parent):
                        # some t elements appear to be empty (this is not
                        # allowed, but it happens). So, check whether there is
                        # a string to add before adding it.
                        if sent.t:
                            if sent.t.string:
                                s = sent.t.string

                        # calculate stats only for unique sentences in text
                        if s and s not in sents:
                            sents.add(s)
                            num_sent += 1

                            entities = sent.find_all('entity')
                            emotional = False
                            for entity in entities:
                                e = entity.attrs.get('class')
                                if e in emotional_cats:
                                    emotional = True

                            if emotional:
                                num_emotional += 1

        del context
            # clear memory
            # results in segmentation fault (for some reason)
            #if delete:
            #    elem.clear()
            #    while elem.getprevious() is not None:
            #        del elem.getparent()[0]
            #        del context

    # print stats
    print '\nBasic stats'
    print '{} sentences in {} files'.format(num_sent, folia_counter)
    perc = float(num_emotional)/float(num_sent)*100.0
    print '{} emotional sentences ({:.2f}%)'.format(num_emotional, perc)
",Add script to count emotional sentences according to LIWC,"Add script to count emotional sentences according to LIWC

Added a script that counts the number of emotional sentences in titles
in the corpus. A sentence is considered emotional if it contains at
least one Posemo or Negemo term. The statistical results are written to
standard out.
",Python,apache-2.0,"NLeSC/embodied-emotions-scripts,NLeSC/embodied-emotions-scripts",95,"```python
""""""Count the numbers of annotated entities and emotional sentences in the
corpus that was manually annotated.

Usage: python annotation_statistics.py <dir containing the folia files with
EmbodiedEmotions annotations>
""""""
from lxml import etree
from bs4 import BeautifulSoup
from emotools.bs4_helpers import sentence, note
import argparse
import os
from collections import Counter
import json
import codecs

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('dir_name', help='the name of the dir containing the '
                        'FoLiA XML files that should be processed.')
    args = parser.parse_args()

    dir_name = args.dir_name

    act_tag = '{http://ilk.uvt.nl/folia}div'

    cur_dir = os.getcwd()
    os.chdir(dir_name)

    folia_counter = 0
    num_sent = 0
    num_emotional = 0
    stats = Counter()
    entity_words = {}
    text_stats = {}
    emotional_cats = ['liwc-Posemo', 'liwc-Negemo']

    print 'Files'
    for file_name in os.listdir(dir_name):
        folia_counter += 1
        print '{}'.format(file_name)

        text_id = file_name[0:13]
        text_stats[text_id] = Counter()

        sents = set()
        # load document
        context = etree.iterparse(file_name,
                                  events=('start', 'end'),
                                  tag=act_tag,
                                  huge_tree=True)
        for event, elem in context:
            if event == 'end' and elem.get('class') == 'act':
                # load act into memory
                act_xml = BeautifulSoup(etree.tostring(elem), 'xml')
                sentences = act_xml.find_all(sentence)
                s = None
                for sent in sentences:
                    if not note(sent.parent):
                        # some t elements appear to be empty (this is not
                        # allowed, but it happens). So, check whether there is
                        # a string to add before adding it.
                        if sent.t:
                            if sent.t.string:
                                s = sent.t.string

                        # calculate stats only for unique sentences in text
                        if s and s not in sents:
                            sents.add(s)
                            num_sent += 1

                            entities = sent.find_all('entity')
                            emotional = False
                            for entity in entities:
                                e = entity.attrs.get('class')
                                if e in emotional_cats:
                                    emotional = True

                            if emotional:
                                num_emotional += 1

        del context
            # clear memory
            # results in segmentation fault (for some reason)
            #if delete:
            #    elem.clear()
            #    while elem.getprevious() is not None:
            #        del elem.getparent()[0]
            #        del context

    # print stats
    print '\nBasic stats'
    print '{} sentences in {} files'.format(num_sent, folia_counter)
    perc = float(num_emotional)/float(num_sent)*100.0
    print '{} emotional sentences ({:.2f}%)'.format(num_emotional, perc)

```"
f0af14b8fcd420b63a47e18938664e14cf9ea968,subiquity/utils.py,subiquity/utils.py,,"# Copyright 2015 Canonical, Ltd.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import errno
import subprocess
import os
import codecs
import pty
from tornado.process import Subprocess
from subiquity.async import Async
import shlex
import logging

log = logging.getLogger(""subiquity.utils"")
STREAM = Subprocess.STREAM


def run_command_async(cmd, streaming_callback=None):
    return Async.pool.submit(run_command, cmd, streaming_callback)


def run_command(cmd, streaming_callback=None):
    """""" Executes `cmd` sending its output to `streaming_callback`
    """"""
    if isinstance(cmd, str):
        cmd = shlex.split(cmd)
    log.debug(""Running command: {}"".format(cmd))
    stdoutm, stdouts = pty.openpty()
    proc = subprocess.Popen(cmd,
                            stdout=stdouts,
                            stderr=subprocess.PIPE)
    os.close(stdouts)
    decoder = codecs.getincrementaldecoder('utf-8')()

    def last_ten_lines(s):
            chunk = s[-1500:]
            lines = chunk.splitlines(True)
            return ''.join(lines[-10:]).replace('\r', '')

    decoded_output = """"
    try:
        while proc.poll() is None:
            try:
                b = os.read(stdoutm, 512)
            except OSError as e:
                if e.errno != errno.EIO:
                    raise
                break
            else:
                final = False
                if not b:
                    final = True
                decoded_chars = decoder.decode(b, final)
                if decoded_chars is None:
                    continue

                decoded_output += decoded_chars
                if streaming_callback:
                    ls = last_ten_lines(decoded_output)

                    streaming_callback(ls)
                if final:
                    break
    finally:
        os.close(stdoutm)
        if proc.poll() is None:
            proc.kill()
        proc.wait()

    errors = [l.decode('utf-8') for l in proc.stderr.readlines()]
    if streaming_callback:
        streaming_callback(last_ten_lines(decoded_output))

    errors = ''.join(errors)

    if proc.returncode == 0:
        return decoded_output.strip()
    else:
        log.debug(""Error with command: ""
                  ""[Output] '{}' [Error] '{}'"".format(
                      decoded_output.strip(),
                      errors.strip()))
        raise Exception(""Problem running command: [Error] '{}'"".format(
            errors.strip()))
",Add generic asynchronous/synchronous run command,"Add generic asynchronous/synchronous run command

Signed-off-by: Adam Stokes <0a364f4bf549cc82d725fa7fd7ed34404be64079@ubuntu.com>
",Python,agpl-3.0,"CanonicalLtd/subiquity,CanonicalLtd/subiquity",97,"```python
# Copyright 2015 Canonical, Ltd.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

import errno
import subprocess
import os
import codecs
import pty
from tornado.process import Subprocess
from subiquity.async import Async
import shlex
import logging

log = logging.getLogger(""subiquity.utils"")
STREAM = Subprocess.STREAM


def run_command_async(cmd, streaming_callback=None):
    return Async.pool.submit(run_command, cmd, streaming_callback)


def run_command(cmd, streaming_callback=None):
    """""" Executes `cmd` sending its output to `streaming_callback`
    """"""
    if isinstance(cmd, str):
        cmd = shlex.split(cmd)
    log.debug(""Running command: {}"".format(cmd))
    stdoutm, stdouts = pty.openpty()
    proc = subprocess.Popen(cmd,
                            stdout=stdouts,
                            stderr=subprocess.PIPE)
    os.close(stdouts)
    decoder = codecs.getincrementaldecoder('utf-8')()

    def last_ten_lines(s):
            chunk = s[-1500:]
            lines = chunk.splitlines(True)
            return ''.join(lines[-10:]).replace('\r', '')

    decoded_output = """"
    try:
        while proc.poll() is None:
            try:
                b = os.read(stdoutm, 512)
            except OSError as e:
                if e.errno != errno.EIO:
                    raise
                break
            else:
                final = False
                if not b:
                    final = True
                decoded_chars = decoder.decode(b, final)
                if decoded_chars is None:
                    continue

                decoded_output += decoded_chars
                if streaming_callback:
                    ls = last_ten_lines(decoded_output)

                    streaming_callback(ls)
                if final:
                    break
    finally:
        os.close(stdoutm)
        if proc.poll() is None:
            proc.kill()
        proc.wait()

    errors = [l.decode('utf-8') for l in proc.stderr.readlines()]
    if streaming_callback:
        streaming_callback(last_ten_lines(decoded_output))

    errors = ''.join(errors)

    if proc.returncode == 0:
        return decoded_output.strip()
    else:
        log.debug(""Error with command: ""
                  ""[Output] '{}' [Error] '{}'"".format(
                      decoded_output.strip(),
                      errors.strip()))
        raise Exception(""Problem running command: [Error] '{}'"".format(
            errors.strip()))

```"
6ad72a0c624abdda0df8d5c49366bfc597a12340,cptm/tests/test_utils_experiment.py,cptm/tests/test_utils_experiment.py,,"from nose.tools import assert_equal, assert_false

from os import remove
from os.path import join
from json import dump

from cptm.utils.experiment import load_config, add_parameter, thetaFileName, \
    topicFileName, opinionFileName, tarFileName, experimentName


def setup():
    global jsonFile
    global config
    global nTopics

    jsonFile = 'config.json'
    # create cofig.json
    params = {}
    with open(jsonFile, 'wb') as f:
        dump(params, f, sort_keys=True, indent=4)
    config = load_config(jsonFile)

    nTopics = 100


def teardown():
    remove(jsonFile)


def test_load_config_default_values():
    params = {}
    params['inputData'] = None
    params['outDir'] = '/{}'
    params['testSplit'] = 20
    params['minFreq'] = None
    params['removeTopTF'] = None
    params['removeTopDF'] = None
    params['nIter'] = 200
    params['beta'] = 0.02
    params['beta_o'] = 0.02
    params['expNumTopics'] = range(20, 201, 20)
    params['nTopics'] = None
    params['nProcesses'] = None
    params['topicLines'] = [0]
    params['opinionLines'] = [1]
    params['sampleEstimateStart'] = None
    params['sampleEstimateEnd'] = None

    for p, v in params.iteritems():
        yield assert_equal, v, params[p]


def test_add_parameter():
    pName = 'nTopics'

    yield assert_false, hasattr(config, pName)

    add_parameter(pName, nTopics, jsonFile)
    config2 = load_config(jsonFile)

    yield assert_equal, config2[pName], nTopics


def test_thetaFileName():
    config['nTopics'] = nTopics
    fName = thetaFileName(config)
    assert_equal(fName, '/theta_{}.csv'.format(nTopics))


def test_topicFileName():
    config['nTopics'] = nTopics
    fName = topicFileName(config)
    assert_equal(fName, '/topics_{}.csv'.format(nTopics))


def test_opinionFileName():
    config['nTopics'] = nTopics
    return join(params.get('outDir').format(''),
                'opinions_{}_{}.csv'.format(name, nTopics))


#def experimentName(params):
#    fName = params.get('outDir')
#    fName = fName.replace('/{}', '')
#    _p, name = os.path.split(fName)
#    return name


#def tarFileName(params):
#    nTopics = params.get('nTopics')
#    name = experimentName(params)
#    return os.path.join(params.get('outDir').format(''),
#                        '{}_{}.tgz'.format(name, nTopics))
",Add tests for utils experiment module,"Add tests for utils experiment module
",Python,apache-2.0,"NLeSC/cptm,NLeSC/cptm",94,"```python
from nose.tools import assert_equal, assert_false

from os import remove
from os.path import join
from json import dump

from cptm.utils.experiment import load_config, add_parameter, thetaFileName, \
    topicFileName, opinionFileName, tarFileName, experimentName


def setup():
    global jsonFile
    global config
    global nTopics

    jsonFile = 'config.json'
    # create cofig.json
    params = {}
    with open(jsonFile, 'wb') as f:
        dump(params, f, sort_keys=True, indent=4)
    config = load_config(jsonFile)

    nTopics = 100


def teardown():
    remove(jsonFile)


def test_load_config_default_values():
    params = {}
    params['inputData'] = None
    params['outDir'] = '/{}'
    params['testSplit'] = 20
    params['minFreq'] = None
    params['removeTopTF'] = None
    params['removeTopDF'] = None
    params['nIter'] = 200
    params['beta'] = 0.02
    params['beta_o'] = 0.02
    params['expNumTopics'] = range(20, 201, 20)
    params['nTopics'] = None
    params['nProcesses'] = None
    params['topicLines'] = [0]
    params['opinionLines'] = [1]
    params['sampleEstimateStart'] = None
    params['sampleEstimateEnd'] = None

    for p, v in params.iteritems():
        yield assert_equal, v, params[p]


def test_add_parameter():
    pName = 'nTopics'

    yield assert_false, hasattr(config, pName)

    add_parameter(pName, nTopics, jsonFile)
    config2 = load_config(jsonFile)

    yield assert_equal, config2[pName], nTopics


def test_thetaFileName():
    config['nTopics'] = nTopics
    fName = thetaFileName(config)
    assert_equal(fName, '/theta_{}.csv'.format(nTopics))


def test_topicFileName():
    config['nTopics'] = nTopics
    fName = topicFileName(config)
    assert_equal(fName, '/topics_{}.csv'.format(nTopics))


def test_opinionFileName():
    config['nTopics'] = nTopics
    return join(params.get('outDir').format(''),
                'opinions_{}_{}.csv'.format(name, nTopics))


#def experimentName(params):
#    fName = params.get('outDir')
#    fName = fName.replace('/{}', '')
#    _p, name = os.path.split(fName)
#    return name


#def tarFileName(params):
#    nTopics = params.get('nTopics')
#    name = experimentName(params)
#    return os.path.join(params.get('outDir').format(''),
#                        '{}_{}.tgz'.format(name, nTopics))

```"
5c602a98098bdedeffc2b7359a4b3d8407cb1449,scripts/migrate_inconsistent_file_keys.py,scripts/migrate_inconsistent_file_keys.py,,"#!/usr/bin/env python
# encoding: utf-8
""""""Find all nodes with different sets of keys for `files_current` and
`files_versions`, and ensure that all keys present in the former are also
present in the latter.
""""""

from website.models import Node
from website.app import init_app


def find_file_mismatch_nodes():
    """"""Find nodes with inconsistent `files_current` and `files_versions` field
    keys.
    """"""
    return [
        node for node in Node.find()
        if set(node.files_versions.keys()) != set(node.files_current.keys())
    ]


def migrate_node(node):
    """"""Ensure that all keys present in `files_current` are also present in
    `files_versions`.
    """"""
    for key, file_id in node.files_current.iteritems():
        if key not in node.files_versions:
            node.files_versions[key] = [file_id]
        else:
            if file_id not in node.files_versions[key]:
                node.files_versions[key].append(file_id)
    node.save()


def main(dry_run=True):
    init_app()
    nodes = find_file_mismatch_nodes()
    print('Migrating {0} nodes'.format(len(nodes)))
    if dry_run:
        return
    for node in nodes:
        migrate_node(node)


if __name__ == '__main__':
    import sys
    dry_run = 'dry' in sys.argv
    main(dry_run=dry_run)


from nose.tools import *  # noqa

from tests.base import OsfTestCase
from tests.factories import ProjectFactory

from framework.auth import Auth


class TestMigrateFiles(OsfTestCase):

    def clear(self):
        Node.remove()

    def setUp(self):
        super(TestMigrateFiles, self).setUp()
        self.clear()
        self.nodes = []
        for idx in range(3):
            node = ProjectFactory()
            node.add_file(
                Auth(user=node.creator),
                'name',
                'contents',
                len('contents'),
                'text/plain',
            )
            self.nodes.append(node)
        self.nodes[-1].files_versions = {}
        self.nodes[-1].save()
        # Sanity check
        assert_in('name', self.nodes[-1].files_current)
        assert_not_in('name', self.nodes[-1].files_versions)

    def tearDown(self):
        super(TestMigrateFiles, self).tearDown()
        self.clear()

    def test_get_targets(self):
        targets = find_file_mismatch_nodes()
        assert_equal(len(targets), 1)
        assert_equal(targets[0], self.nodes[-1])

    def test_migrate(self):
        main(dry_run=False)
        assert_equal(len(find_file_mismatch_nodes()), 0)
        assert_in('name', self.nodes[-1].files_versions)
        assert_equal(
            self.nodes[-1].files_current['name'],
            self.nodes[-1].files_versions['name'][0],
        )
",Add migration to ensure consistency on file keys.,"Add migration to ensure consistency on file keys.

Resolves https://github.com/CenterForOpenScience/openscienceframework.org/issues/1119
",Python,apache-2.0,"rdhyee/osf.io,amyshi188/osf.io,sloria/osf.io,zachjanicki/osf.io,mluo613/osf.io,asanfilippo7/osf.io,brandonPurvis/osf.io,doublebits/osf.io,mfraezz/osf.io,laurenrevere/osf.io,pattisdr/osf.io,erinspace/osf.io,bdyetton/prettychart,revanthkolli/osf.io,MerlinZhang/osf.io,caseyrygt/osf.io,TomHeatwole/osf.io,cldershem/osf.io,HalcyonChimera/osf.io,amyshi188/osf.io,HarryRybacki/osf.io,icereval/osf.io,ticklemepierce/osf.io,RomanZWang/osf.io,emetsger/osf.io,revanthkolli/osf.io,fabianvf/osf.io,himanshuo/osf.io,kch8qx/osf.io,jolene-esposito/osf.io,cldershem/osf.io,HalcyonChimera/osf.io,kch8qx/osf.io,saradbowman/osf.io,njantrania/osf.io,lamdnhan/osf.io,MerlinZhang/osf.io,baylee-d/osf.io,cosenal/osf.io,DanielSBrown/osf.io,alexschiller/osf.io,HarryRybacki/osf.io,cslzchen/osf.io,brandonPurvis/osf.io,jnayak1/osf.io,jinluyuan/osf.io,sloria/osf.io,danielneis/osf.io,jnayak1/osf.io,himanshuo/osf.io,binoculars/osf.io,fabianvf/osf.io,samanehsan/osf.io,felliott/osf.io,billyhunt/osf.io,reinaH/osf.io,TomHeatwole/osf.io,haoyuchen1992/osf.io,aaxelb/osf.io,samanehsan/osf.io,Ghalko/osf.io,haoyuchen1992/osf.io,danielneis/osf.io,asanfilippo7/osf.io,mattclark/osf.io,jmcarp/osf.io,monikagrabowska/osf.io,adlius/osf.io,caseyrygt/osf.io,DanielSBrown/osf.io,lamdnhan/osf.io,Ghalko/osf.io,aaxelb/osf.io,jnayak1/osf.io,cosenal/osf.io,cwisecarver/osf.io,RomanZWang/osf.io,jeffreyliu3230/osf.io,GageGaskins/osf.io,sloria/osf.io,doublebits/osf.io,jinluyuan/osf.io,jmcarp/osf.io,brianjgeiger/osf.io,kwierman/osf.io,mfraezz/osf.io,GaryKriebel/osf.io,HalcyonChimera/osf.io,wearpants/osf.io,arpitar/osf.io,icereval/osf.io,hmoco/osf.io,acshi/osf.io,KAsante95/osf.io,jnayak1/osf.io,HarryRybacki/osf.io,leb2dg/osf.io,billyhunt/osf.io,GaryKriebel/osf.io,bdyetton/prettychart,alexschiller/osf.io,rdhyee/osf.io,hmoco/osf.io,RomanZWang/osf.io,TomHeatwole/osf.io,TomHeatwole/osf.io,mluo613/osf.io,arpitar/osf.io,acshi/osf.io,DanielSBrown/osf.io,GageGaskins/osf.io,HalcyonChimera/osf.io,pattisdr/osf.io,ticklemepierce/osf.io,hmoco/osf.io,KAsante95/osf.io,kch8qx/osf.io,jolene-esposito/osf.io,amyshi188/osf.io,reinaH/osf.io,bdyetton/prettychart,pattisdr/osf.io,abought/osf.io,chennan47/osf.io,ticklemepierce/osf.io,cldershem/osf.io,caneruguz/osf.io,MerlinZhang/osf.io,jeffreyliu3230/osf.io,chennan47/osf.io,jinluyuan/osf.io,ZobairAlijan/osf.io,petermalcolm/osf.io,hmoco/osf.io,sbt9uc/osf.io,dplorimer/osf,ZobairAlijan/osf.io,kwierman/osf.io,njantrania/osf.io,leb2dg/osf.io,zamattiac/osf.io,acshi/osf.io,fabianvf/osf.io,SSJohns/osf.io,ckc6cz/osf.io,crcresearch/osf.io,petermalcolm/osf.io,billyhunt/osf.io,amyshi188/osf.io,acshi/osf.io,HarryRybacki/osf.io,SSJohns/osf.io,kushG/osf.io,chennan47/osf.io,felliott/osf.io,AndrewSallans/osf.io,Johnetordoff/osf.io,GaryKriebel/osf.io,abought/osf.io,leb2dg/osf.io,doublebits/osf.io,chrisseto/osf.io,dplorimer/osf,dplorimer/osf,jolene-esposito/osf.io,cslzchen/osf.io,baylee-d/osf.io,mluke93/osf.io,njantrania/osf.io,alexschiller/osf.io,samchrisinger/osf.io,CenterForOpenScience/osf.io,leb2dg/osf.io,petermalcolm/osf.io,cldershem/osf.io,AndrewSallans/osf.io,CenterForOpenScience/osf.io,cwisecarver/osf.io,alexschiller/osf.io,reinaH/osf.io,zamattiac/osf.io,GageGaskins/osf.io,jolene-esposito/osf.io,adlius/osf.io,cslzchen/osf.io,mattclark/osf.io,zkraime/osf.io,jeffreyliu3230/osf.io,cwisecarver/osf.io,reinaH/osf.io,Nesiehr/osf.io,icereval/osf.io,acshi/osf.io,GageGaskins/osf.io,kushG/osf.io,saradbowman/osf.io,lamdnhan/osf.io,bdyetton/prettychart,kwierman/osf.io,cwisecarver/osf.io,felliott/osf.io,chrisseto/osf.io,caneruguz/osf.io,samanehsan/osf.io,mfraezz/osf.io,kwierman/osf.io,Nesiehr/osf.io,chrisseto/osf.io,ckc6cz/osf.io,haoyuchen1992/osf.io,caneruguz/osf.io,TomBaxter/osf.io,SSJohns/osf.io,jmcarp/osf.io,alexschiller/osf.io,arpitar/osf.io,sbt9uc/osf.io,caseyrollins/osf.io,Nesiehr/osf.io,zachjanicki/osf.io,brandonPurvis/osf.io,cosenal/osf.io,rdhyee/osf.io,TomBaxter/osf.io,asanfilippo7/osf.io,ZobairAlijan/osf.io,lyndsysimon/osf.io,njantrania/osf.io,KAsante95/osf.io,asanfilippo7/osf.io,jeffreyliu3230/osf.io,CenterForOpenScience/osf.io,barbour-em/osf.io,kch8qx/osf.io,mluo613/osf.io,dplorimer/osf,haoyuchen1992/osf.io,barbour-em/osf.io,wearpants/osf.io,zachjanicki/osf.io,zkraime/osf.io,monikagrabowska/osf.io,zkraime/osf.io,brandonPurvis/osf.io,billyhunt/osf.io,revanthkolli/osf.io,lamdnhan/osf.io,binoculars/osf.io,Ghalko/osf.io,ckc6cz/osf.io,caneruguz/osf.io,crcresearch/osf.io,felliott/osf.io,lyndsysimon/osf.io,sbt9uc/osf.io,kch8qx/osf.io,abought/osf.io,SSJohns/osf.io,danielneis/osf.io,mluo613/osf.io,zamattiac/osf.io,emetsger/osf.io,mfraezz/osf.io,lyndsysimon/osf.io,himanshuo/osf.io,emetsger/osf.io,lyndsysimon/osf.io,caseyrygt/osf.io,KAsante95/osf.io,GageGaskins/osf.io,mluke93/osf.io,mattclark/osf.io,KAsante95/osf.io,caseyrygt/osf.io,doublebits/osf.io,mluke93/osf.io,caseyrollins/osf.io,petermalcolm/osf.io,Johnetordoff/osf.io,emetsger/osf.io,sbt9uc/osf.io,billyhunt/osf.io,RomanZWang/osf.io,monikagrabowska/osf.io,wearpants/osf.io,caseyrollins/osf.io,adlius/osf.io,chrisseto/osf.io,brianjgeiger/osf.io,Johnetordoff/osf.io,arpitar/osf.io,monikagrabowska/osf.io,barbour-em/osf.io,samanehsan/osf.io,GaryKriebel/osf.io,cosenal/osf.io,aaxelb/osf.io,baylee-d/osf.io,cslzchen/osf.io,rdhyee/osf.io,crcresearch/osf.io,samchrisinger/osf.io,ZobairAlijan/osf.io,Ghalko/osf.io,adlius/osf.io,brianjgeiger/osf.io,samchrisinger/osf.io,jmcarp/osf.io,erinspace/osf.io,binoculars/osf.io,zachjanicki/osf.io,doublebits/osf.io,zkraime/osf.io,MerlinZhang/osf.io,laurenrevere/osf.io,brandonPurvis/osf.io,ticklemepierce/osf.io,kushG/osf.io,barbour-em/osf.io,mluo613/osf.io,CenterForOpenScience/osf.io,revanthkolli/osf.io,TomBaxter/osf.io,kushG/osf.io,laurenrevere/osf.io,DanielSBrown/osf.io,brianjgeiger/osf.io,himanshuo/osf.io,erinspace/osf.io,zamattiac/osf.io,jinluyuan/osf.io,samchrisinger/osf.io,wearpants/osf.io,Johnetordoff/osf.io,aaxelb/osf.io,fabianvf/osf.io,RomanZWang/osf.io,ckc6cz/osf.io,abought/osf.io,danielneis/osf.io,mluke93/osf.io,Nesiehr/osf.io,monikagrabowska/osf.io",101,"```python
#!/usr/bin/env python
# encoding: utf-8
""""""Find all nodes with different sets of keys for `files_current` and
`files_versions`, and ensure that all keys present in the former are also
present in the latter.
""""""

from website.models import Node
from website.app import init_app


def find_file_mismatch_nodes():
    """"""Find nodes with inconsistent `files_current` and `files_versions` field
    keys.
    """"""
    return [
        node for node in Node.find()
        if set(node.files_versions.keys()) != set(node.files_current.keys())
    ]


def migrate_node(node):
    """"""Ensure that all keys present in `files_current` are also present in
    `files_versions`.
    """"""
    for key, file_id in node.files_current.iteritems():
        if key not in node.files_versions:
            node.files_versions[key] = [file_id]
        else:
            if file_id not in node.files_versions[key]:
                node.files_versions[key].append(file_id)
    node.save()


def main(dry_run=True):
    init_app()
    nodes = find_file_mismatch_nodes()
    print('Migrating {0} nodes'.format(len(nodes)))
    if dry_run:
        return
    for node in nodes:
        migrate_node(node)


if __name__ == '__main__':
    import sys
    dry_run = 'dry' in sys.argv
    main(dry_run=dry_run)


from nose.tools import *  # noqa

from tests.base import OsfTestCase
from tests.factories import ProjectFactory

from framework.auth import Auth


class TestMigrateFiles(OsfTestCase):

    def clear(self):
        Node.remove()

    def setUp(self):
        super(TestMigrateFiles, self).setUp()
        self.clear()
        self.nodes = []
        for idx in range(3):
            node = ProjectFactory()
            node.add_file(
                Auth(user=node.creator),
                'name',
                'contents',
                len('contents'),
                'text/plain',
            )
            self.nodes.append(node)
        self.nodes[-1].files_versions = {}
        self.nodes[-1].save()
        # Sanity check
        assert_in('name', self.nodes[-1].files_current)
        assert_not_in('name', self.nodes[-1].files_versions)

    def tearDown(self):
        super(TestMigrateFiles, self).tearDown()
        self.clear()

    def test_get_targets(self):
        targets = find_file_mismatch_nodes()
        assert_equal(len(targets), 1)
        assert_equal(targets[0], self.nodes[-1])

    def test_migrate(self):
        main(dry_run=False)
        assert_equal(len(find_file_mismatch_nodes()), 0)
        assert_in('name', self.nodes[-1].files_versions)
        assert_equal(
            self.nodes[-1].files_current['name'],
            self.nodes[-1].files_versions['name'][0],
        )

```"
3ef1e39d476a8b3e41ff0b06dcd6f700c083682d,data_controller/abc.py,data_controller/abc.py,,"from typing import Dict, Optional

from data_controller.enums import Medium, Site
from utils.helpers import await_func


class DataController:
    """"""
    An ABC for all classes that deals with database read write.
    """"""
    __slots__ = ()

    def get_identifier(self, query: str,
                       medium: Medium) -> Optional[Dict[Site, str]]:
        """"""
        Get the identifier of a given search query.

        :param query: the search query.

        :param medium: the medium type.

        :return: A dict of all identifiers for this search query for all sites,
                 None if nothing is found.
        """"""
        raise NotImplementedError

    def set_identifier(self, name: str, medium: Medium,
                       site: Site, identifier: str):
        """"""
        Set the identifier for a given name.

        :param name: the name.

        :param medium: the medium type.

        :param site: the site.

        :param identifier: the identifier.
        """"""
        raise NotImplementedError

    def get_mal_title(self, id_: str, medium: Medium) -> Optional[str]:
        """"""
        Get a MAL title by its id.
        :param id_: th MAL id.
        :param medium: the medium type.
        :return: The MAL title if it's found.
        """"""
        raise NotImplementedError

    def set_mal_title(self, id_: str, medium: Medium, title: str):
        """"""
        Set the MAL title for a given id.
        :param id_: the MAL id.

        :param medium: The medium type.

        :param title: The MAL title for the given id.
        """"""
        raise NotImplementedError

    def medium_data_by_id(self, id_: str, medium: Medium,
                          site: Site) -> Optional[dict]:
        """"""
        Get data by id.
        :param id_: the id.
        :param medium: the medium type.
        :param site: the site.
        :return: the data for that id if found.
        """"""
        raise NotImplementedError

    def set_medium_data(self, id_: str, medium: Medium, site: Site, data: dict):
        """"""
        Set the data for a given id.

        :param id_: the id.
        
        :param medium: the medium type.

        :param site: the site.

        :param data: the data for the id.
        """"""
        raise NotImplementedError

    async def get_medium_data(self, query: str,
                              medium: Medium, loop=None) -> Optional[dict]:
        """"""
        Get the cached data for the given search query.

        :param query: the search query.

        :param medium: the medium type.

        :param loop: the asyncio event loop, optional. If None is provided,
        will use the default event loop.

        :return: the cached data, for all sites that has the data.
        """"""
        id_dict = await await_func(
            self.get_identifier, loop, query, medium
        )
        if not id_dict:
            return
        return {site: data for site, data in {
            site: await await_func(self.medium_data_by_id, loop,
                                   id_, medium, site)
            for site, id_ in id_dict.items()}.items() if data}
",Add an ABC for all sub classes of `DataController`,"Add an ABC for all sub classes of `DataController`

",Python,mit,MaT1g3R/Roboragi,110,"```python
from typing import Dict, Optional

from data_controller.enums import Medium, Site
from utils.helpers import await_func


class DataController:
    """"""
    An ABC for all classes that deals with database read write.
    """"""
    __slots__ = ()

    def get_identifier(self, query: str,
                       medium: Medium) -> Optional[Dict[Site, str]]:
        """"""
        Get the identifier of a given search query.

        :param query: the search query.

        :param medium: the medium type.

        :return: A dict of all identifiers for this search query for all sites,
                 None if nothing is found.
        """"""
        raise NotImplementedError

    def set_identifier(self, name: str, medium: Medium,
                       site: Site, identifier: str):
        """"""
        Set the identifier for a given name.

        :param name: the name.

        :param medium: the medium type.

        :param site: the site.

        :param identifier: the identifier.
        """"""
        raise NotImplementedError

    def get_mal_title(self, id_: str, medium: Medium) -> Optional[str]:
        """"""
        Get a MAL title by its id.
        :param id_: th MAL id.
        :param medium: the medium type.
        :return: The MAL title if it's found.
        """"""
        raise NotImplementedError

    def set_mal_title(self, id_: str, medium: Medium, title: str):
        """"""
        Set the MAL title for a given id.
        :param id_: the MAL id.

        :param medium: The medium type.

        :param title: The MAL title for the given id.
        """"""
        raise NotImplementedError

    def medium_data_by_id(self, id_: str, medium: Medium,
                          site: Site) -> Optional[dict]:
        """"""
        Get data by id.
        :param id_: the id.
        :param medium: the medium type.
        :param site: the site.
        :return: the data for that id if found.
        """"""
        raise NotImplementedError

    def set_medium_data(self, id_: str, medium: Medium, site: Site, data: dict):
        """"""
        Set the data for a given id.

        :param id_: the id.
        
        :param medium: the medium type.

        :param site: the site.

        :param data: the data for the id.
        """"""
        raise NotImplementedError

    async def get_medium_data(self, query: str,
                              medium: Medium, loop=None) -> Optional[dict]:
        """"""
        Get the cached data for the given search query.

        :param query: the search query.

        :param medium: the medium type.

        :param loop: the asyncio event loop, optional. If None is provided,
        will use the default event loop.

        :return: the cached data, for all sites that has the data.
        """"""
        id_dict = await await_func(
            self.get_identifier, loop, query, medium
        )
        if not id_dict:
            return
        return {site: data for site, data in {
            site: await await_func(self.medium_data_by_id, loop,
                                   id_, medium, site)
            for site, id_ in id_dict.items()}.items() if data}

```"
74354263acb3399295e7fde18d6aeed4b7bb7397,what_transcode/tests.py,what_transcode/tests.py,,"""""""
This file demonstrates writing tests using the unittest module. These will pass
when you run ""manage.py test"".

Replace this with more appropriate tests for your application.
""""""

from django.test import TestCase

from what_transcode.utils import get_mp3_ids


class UtilsTests(TestCase):
    def test_get_mp3_ids(self):
        what_group = {
            'torrents': [
                {
                    'id': 0,
                    'format': 'FLAC',
                    'encoding': 'Lossless',
                    'media': 'CD',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 1,
                    'format': 'MP3',
                    'encoding': '320',
                    'media': 'CD',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 2,
                    'format': 'FLAC',
                    'encoding': 'Lossless',
                    'media': 'CD',
                    'remastered': True,
                    'remasterCatalogueNumber': 'catno',
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 3,
                    'format': 'FLAC',
                    'encoding': 'Lossless',
                    'media': 'WEB',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 4,
                    'format': 'MP3',
                    'encoding': 'V0 (VBR)',
                    'media': 'WEB',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 5,
                    'format': 'MP3',
                    'encoding': 'V2 (VBR)',
                    'media': 'WEB',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
            ]
        }
        self.assertEqual(get_mp3_ids(what_group, {
            'torrent': what_group['torrents'][0]
        }), {'320': 1})
        self.assertEqual(get_mp3_ids(what_group, {
            'torrent': what_group['torrents'][2]
        }), {})
        self.assertEqual(get_mp3_ids(what_group, {
            'torrent': what_group['torrents'][3]
        }), {'V0': 4, 'V2': 5})
",Fix maybe all flake8 errors. Add first test.,"Fix maybe all flake8 errors. Add first test.
",Python,mit,"grandmasterchef/WhatManager2,MADindustries/WhatManager2,grandmasterchef/WhatManager2,karamanolev/WhatManager2,davols/WhatManager2,MADindustries/WhatManager2,karamanolev/WhatManager2,MADindustries/WhatManager2,grandmasterchef/WhatManager2,davols/WhatManager2,MADindustries/WhatManager2,karamanolev/WhatManager2,grandmasterchef/WhatManager2,karamanolev/WhatManager2,davols/WhatManager2",94,"```python
""""""
This file demonstrates writing tests using the unittest module. These will pass
when you run ""manage.py test"".

Replace this with more appropriate tests for your application.
""""""

from django.test import TestCase

from what_transcode.utils import get_mp3_ids


class UtilsTests(TestCase):
    def test_get_mp3_ids(self):
        what_group = {
            'torrents': [
                {
                    'id': 0,
                    'format': 'FLAC',
                    'encoding': 'Lossless',
                    'media': 'CD',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 1,
                    'format': 'MP3',
                    'encoding': '320',
                    'media': 'CD',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 2,
                    'format': 'FLAC',
                    'encoding': 'Lossless',
                    'media': 'CD',
                    'remastered': True,
                    'remasterCatalogueNumber': 'catno',
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 3,
                    'format': 'FLAC',
                    'encoding': 'Lossless',
                    'media': 'WEB',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 4,
                    'format': 'MP3',
                    'encoding': 'V0 (VBR)',
                    'media': 'WEB',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
                {
                    'id': 5,
                    'format': 'MP3',
                    'encoding': 'V2 (VBR)',
                    'media': 'WEB',
                    'remastered': False,
                    'remasterCatalogueNumber': None,
                    'remasterRecordLabel': None,
                    'remasterTitle': None,
                    'remasterYear': None,
                },
            ]
        }
        self.assertEqual(get_mp3_ids(what_group, {
            'torrent': what_group['torrents'][0]
        }), {'320': 1})
        self.assertEqual(get_mp3_ids(what_group, {
            'torrent': what_group['torrents'][2]
        }), {})
        self.assertEqual(get_mp3_ids(what_group, {
            'torrent': what_group['torrents'][3]
        }), {'V0': 4, 'V2': 5})

```"
a0a2017e05af986cd0a7207c429e7dc5e8b3fcd2,tests/test_solver_variable.py,tests/test_solver_variable.py,,"from gaphas.solver import Variable


def test_equality():
    v = Variable(3)
    w = Variable(3)
    o = Variable(2)

    assert v == 3
    assert 3 == v
    assert v == w
    assert not v == o

    assert v != 2
    assert 2 != v
    assert not 3 != v
    assert v != o


def test_add_to_variable():
    v = Variable(3)

    assert v + 1 == 4
    assert v - 1 == 2
    assert 1 + v == 4
    assert 4 - v == 1


def test_add_to_variable_with_variable():
    v = Variable(3)
    o = Variable(1)

    assert v + o == 4
    assert v - o == 2


def test_mutiplication():
    v = Variable(3)

    assert v * 2 == 6
    assert v / 2 == 1.5
    assert v // 2 == 1

    assert 2 * v == 6
    assert 4.5 / v == 1.5
    assert 4 // v == 1


def test_mutiplication_with_variable():
    v = Variable(3)
    o = Variable(2)

    assert v * o == 6
    assert v / o == 1.5
    assert v // o == 1


def test_comparison():
    v = Variable(3)

    assert v > 2
    assert v < 4
    assert v >= 2
    assert v >= 3
    assert v <= 4
    assert v <= 3

    assert not v > 3
    assert not v < 3
    assert not v <= 2
    assert not v >= 4


def test_inverse_comparison():
    v = Variable(3)

    assert 4 > v
    assert 2 < v
    assert 4 >= v
    assert 3 >= v
    assert 2 <= v
    assert 3 <= v

    assert not 3 > v
    assert not 3 < v
    assert not 4 <= v
    assert not 2 >= v


def test_power():
    v = Variable(3)
    o = Variable(2)

    assert v ** 2 == 9
    assert 2 ** v == 8
    assert v ** o == 9


def test_modulo():
    v = Variable(3)
    o = Variable(2)

    assert v % 2 == 1
    assert 4 % v == 1
    assert v % o == 1
    assert divmod(v, 2) == (1, 1)
    assert divmod(4, v) == (1, 1)
    assert divmod(v, o) == (1, 1)
",Add missing tests for Variable,"Add missing tests for Variable
",Python,lgpl-2.1,amolenaar/gaphas,109,"```python
from gaphas.solver import Variable


def test_equality():
    v = Variable(3)
    w = Variable(3)
    o = Variable(2)

    assert v == 3
    assert 3 == v
    assert v == w
    assert not v == o

    assert v != 2
    assert 2 != v
    assert not 3 != v
    assert v != o


def test_add_to_variable():
    v = Variable(3)

    assert v + 1 == 4
    assert v - 1 == 2
    assert 1 + v == 4
    assert 4 - v == 1


def test_add_to_variable_with_variable():
    v = Variable(3)
    o = Variable(1)

    assert v + o == 4
    assert v - o == 2


def test_mutiplication():
    v = Variable(3)

    assert v * 2 == 6
    assert v / 2 == 1.5
    assert v // 2 == 1

    assert 2 * v == 6
    assert 4.5 / v == 1.5
    assert 4 // v == 1


def test_mutiplication_with_variable():
    v = Variable(3)
    o = Variable(2)

    assert v * o == 6
    assert v / o == 1.5
    assert v // o == 1


def test_comparison():
    v = Variable(3)

    assert v > 2
    assert v < 4
    assert v >= 2
    assert v >= 3
    assert v <= 4
    assert v <= 3

    assert not v > 3
    assert not v < 3
    assert not v <= 2
    assert not v >= 4


def test_inverse_comparison():
    v = Variable(3)

    assert 4 > v
    assert 2 < v
    assert 4 >= v
    assert 3 >= v
    assert 2 <= v
    assert 3 <= v

    assert not 3 > v
    assert not 3 < v
    assert not 4 <= v
    assert not 2 >= v


def test_power():
    v = Variable(3)
    o = Variable(2)

    assert v ** 2 == 9
    assert 2 ** v == 8
    assert v ** o == 9


def test_modulo():
    v = Variable(3)
    o = Variable(2)

    assert v % 2 == 1
    assert 4 % v == 1
    assert v % o == 1
    assert divmod(v, 2) == (1, 1)
    assert divmod(4, v) == (1, 1)
    assert divmod(v, o) == (1, 1)

```"
b72c421696b5714d256b7ac461833bc692ca5354,robot/robot/src/autonomous/hot_aim_shoot.py,robot/robot/src/autonomous/hot_aim_shoot.py,,"
try:
    import wpilib
except ImportError:
    from pyfrc import wpilib

import timed_shoot

class HotShootAutonomous(timed_shoot.TimedShootAutonomous):
    '''
        Based on the TimedShootAutonomous mode. Modified to allow
        shooting based on whether the hot goal is enabled or not.
    '''
    
    DEFAULT = False
    MODE_NAME = ""Hot Aim shoot""

    def __init__(self, components):
        super().__init__(components)
        
        wpilib.SmartDashboard.PutNumber('DriveStrafeSpeed', 0.5)
        wpilib.SmartDashboard.PutBoolean('IsHotLeft', False)
        wpilib.SmartDashboard.PutBoolean('IsHotRight', False)

    def on_enable(self):
        '''these are called when autonomous starts'''
        
        super().on_enable()
        
        self.drive_strafe_speed = wpilib.SmartDashboard.GetNumber('DriveStrafeSpeed')
        
        print(""-> Drive strafe:"", self.drive_strafe_speed)
        
        self.decided = False
        self.start_time = None
    
    def on_disable(self):
         '''This function is called when autonomous mode is disabled'''
         pass

    def update(self, time_elapsed):   
        '''The actual autonomous program'''     
       
       
        # decide if it's hot or not
        if not self.decided:
            self.hotLeft = wpilib.SmartDashboard.GetBoolean(""IsHotLeft"")
            self.hotRight = wpilib.SmartDashboard.GetBoolean(""IsHotRight"")
            
            if (self.hotLeft or self.hotRight) and not (self.hotLeft and self.hotRight):
                self.decided = True
                
                if self.hotLeft:
                    self.drive_strafe_speed *= -1
                
            elif time_elapsed > 6:
                # at 6 seconds, give up and shoot anyways
                self.decided = True
       
       
        # always keep the arm down
        self.intake.armDown()
        
        # wait a split second for the arm to come down, then
        # keep bringing the catapult down so we're ready to go
        if time_elapsed > 0.3:
            self.catapult.pulldown()
        
                
        # wait some period before we start driving
        if time_elapsed < self.drive_wait:
            pass
        
        else:
                         
            if self.decided:
                
                # only set this once, so we can calculate time from this
                # point on 
                if self.start_time is None:
                    self.start_time = time_elapsed
                
                
                time_elapsed = time_elapsed - self.start_time
                
                if time_elapsed < self.drive_time:
                    # Drive slowly forward for N seconds
                    self.drive.move(self.drive_strafe_speed, self.drive_speed, 0)
                    
                    
                elif time_elapsed < self.drive_time + 1.0:
                    # Finally, fire and keep firing for 1 seconds
                    self.catapult.launchNoSensor()
",Add an autonomous mode to strafe and shoot. Doesn't work,"Add an autonomous mode to strafe and shoot. Doesn't work
",Python,bsd-3-clause,frc1418/2014,94,"```python

try:
    import wpilib
except ImportError:
    from pyfrc import wpilib

import timed_shoot

class HotShootAutonomous(timed_shoot.TimedShootAutonomous):
    '''
        Based on the TimedShootAutonomous mode. Modified to allow
        shooting based on whether the hot goal is enabled or not.
    '''
    
    DEFAULT = False
    MODE_NAME = ""Hot Aim shoot""

    def __init__(self, components):
        super().__init__(components)
        
        wpilib.SmartDashboard.PutNumber('DriveStrafeSpeed', 0.5)
        wpilib.SmartDashboard.PutBoolean('IsHotLeft', False)
        wpilib.SmartDashboard.PutBoolean('IsHotRight', False)

    def on_enable(self):
        '''these are called when autonomous starts'''
        
        super().on_enable()
        
        self.drive_strafe_speed = wpilib.SmartDashboard.GetNumber('DriveStrafeSpeed')
        
        print(""-> Drive strafe:"", self.drive_strafe_speed)
        
        self.decided = False
        self.start_time = None
    
    def on_disable(self):
         '''This function is called when autonomous mode is disabled'''
         pass

    def update(self, time_elapsed):   
        '''The actual autonomous program'''     
       
       
        # decide if it's hot or not
        if not self.decided:
            self.hotLeft = wpilib.SmartDashboard.GetBoolean(""IsHotLeft"")
            self.hotRight = wpilib.SmartDashboard.GetBoolean(""IsHotRight"")
            
            if (self.hotLeft or self.hotRight) and not (self.hotLeft and self.hotRight):
                self.decided = True
                
                if self.hotLeft:
                    self.drive_strafe_speed *= -1
                
            elif time_elapsed > 6:
                # at 6 seconds, give up and shoot anyways
                self.decided = True
       
       
        # always keep the arm down
        self.intake.armDown()
        
        # wait a split second for the arm to come down, then
        # keep bringing the catapult down so we're ready to go
        if time_elapsed > 0.3:
            self.catapult.pulldown()
        
                
        # wait some period before we start driving
        if time_elapsed < self.drive_wait:
            pass
        
        else:
                         
            if self.decided:
                
                # only set this once, so we can calculate time from this
                # point on 
                if self.start_time is None:
                    self.start_time = time_elapsed
                
                
                time_elapsed = time_elapsed - self.start_time
                
                if time_elapsed < self.drive_time:
                    # Drive slowly forward for N seconds
                    self.drive.move(self.drive_strafe_speed, self.drive_speed, 0)
                    
                    
                elif time_elapsed < self.drive_time + 1.0:
                    # Finally, fire and keep firing for 1 seconds
                    self.catapult.launchNoSensor()

```"
01f21a16e4bcecccf51a565b51222ab18b79adb4,st2common/tests/unit/test_util_shell.py,st2common/tests/unit/test_util_shell.py,,"# Licensed to the StackStorm, Inc ('StackStorm') under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import unittest2

from st2common.util.shell import quote_unix
from st2common.util.shell import quote_windows


class ShellUtilsTestCase(unittest2.TestCase):
    def test_quote_unix(self):
        arguments = [
            'foo',
            'foo bar',
            'foo1 bar1',
            '""foo""',
            '""foo"" ""bar""',
            ""'foo bar'""
        ]
        expected_values = [
            """"""
            foo
            """""",

            """"""
            'foo bar'
            """""",

            """"""
            'foo1 bar1'
            """""",

            """"""
            '""foo""'
            """""",

            """"""
            '""foo"" ""bar""'
            """""",

            """"""
            ''""'""'foo bar'""'""''
            """"""
        ]

        for argument, expected_value in zip(arguments, expected_values):
            actual_value = quote_unix(value=argument)
            expected_value = expected_value.lstrip()
            self.assertEqual(actual_value, expected_value.strip())

    def test_quote_windows(self):
        arguments = [
            'foo',
            'foo bar',
            'foo1 bar1',
            '""foo""',
            '""foo"" ""bar""',
            ""'foo bar'""
        ]
        expected_values = [
            """"""
            foo
            """""",

            """"""
            ""foo bar""
            """""",

            """"""
            ""foo1 bar1""
            """""",

            """"""
            \\""foo\\""
            """""",

            """"""
            ""\\""foo\\"" \\""bar\\""""
            """""",

            """"""
            ""'foo bar'""
            """"""
        ]

        for argument, expected_value in zip(arguments, expected_values):
            actual_value = quote_windows(value=argument)
            expected_value = expected_value.lstrip()
            self.assertEqual(actual_value, expected_value.strip())
",Add tests for shell utils.,"Add tests for shell utils.
",Python,apache-2.0,"tonybaloney/st2,pixelrebel/st2,Plexxi/st2,emedvedev/st2,pixelrebel/st2,grengojbo/st2,armab/st2,StackStorm/st2,StackStorm/st2,lakshmi-kannan/st2,Itxaka/st2,emedvedev/st2,punalpatel/st2,alfasin/st2,punalpatel/st2,dennybaa/st2,pinterb/st2,dennybaa/st2,StackStorm/st2,peak6/st2,armab/st2,tonybaloney/st2,Plexxi/st2,Plexxi/st2,dennybaa/st2,armab/st2,jtopjian/st2,emedvedev/st2,jtopjian/st2,Itxaka/st2,nzlosh/st2,Plexxi/st2,peak6/st2,StackStorm/st2,lakshmi-kannan/st2,Itxaka/st2,pixelrebel/st2,pinterb/st2,grengojbo/st2,alfasin/st2,jtopjian/st2,alfasin/st2,peak6/st2,punalpatel/st2,nzlosh/st2,nzlosh/st2,nzlosh/st2,grengojbo/st2,tonybaloney/st2,lakshmi-kannan/st2,pinterb/st2",102,"```python
# Licensed to the StackStorm, Inc ('StackStorm') under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import unittest2

from st2common.util.shell import quote_unix
from st2common.util.shell import quote_windows


class ShellUtilsTestCase(unittest2.TestCase):
    def test_quote_unix(self):
        arguments = [
            'foo',
            'foo bar',
            'foo1 bar1',
            '""foo""',
            '""foo"" ""bar""',
            ""'foo bar'""
        ]
        expected_values = [
            """"""
            foo
            """""",

            """"""
            'foo bar'
            """""",

            """"""
            'foo1 bar1'
            """""",

            """"""
            '""foo""'
            """""",

            """"""
            '""foo"" ""bar""'
            """""",

            """"""
            ''""'""'foo bar'""'""''
            """"""
        ]

        for argument, expected_value in zip(arguments, expected_values):
            actual_value = quote_unix(value=argument)
            expected_value = expected_value.lstrip()
            self.assertEqual(actual_value, expected_value.strip())

    def test_quote_windows(self):
        arguments = [
            'foo',
            'foo bar',
            'foo1 bar1',
            '""foo""',
            '""foo"" ""bar""',
            ""'foo bar'""
        ]
        expected_values = [
            """"""
            foo
            """""",

            """"""
            ""foo bar""
            """""",

            """"""
            ""foo1 bar1""
            """""",

            """"""
            \\""foo\\""
            """""",

            """"""
            ""\\""foo\\"" \\""bar\\""""
            """""",

            """"""
            ""'foo bar'""
            """"""
        ]

        for argument, expected_value in zip(arguments, expected_values):
            actual_value = quote_windows(value=argument)
            expected_value = expected_value.lstrip()
            self.assertEqual(actual_value, expected_value.strip())

```"
0388e7323452f7b2a94ac9593ffce07e35012e09,tests/aws_disk_integration_test.py,tests/aws_disk_integration_test.py,,"# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Integration tests for AWS scratch disks.""""""

import os
import unittest

from perfkitbenchmarker import pkb
from perfkitbenchmarker import test_util

MOUNT_POINT = '/scratch'


@unittest.skipUnless('PERFKIT_INTEGRATION' in os.environ,
                     'PERFKIT_INTEGRATION not in environment')
class AwsScratchDiskIntegrationTest(unittest.TestCase):
  def setUp(self):
    pkb.SetUpPKB()

  def testEBSStandard(self):
    test_util.testDiskMounts({
        'vm_groups': {
            'vm_group_1': {
                'cloud': 'AWS',
                'vm_spec': {
                    'AWS': {
                        'machine_type': 'm4.large',
                        'zone': 'us-east-1a'
                    }
                },
                'disk_spec': {
                    'AWS': {
                        'disk_type': 'standard',
                        'disk_size': 2,
                        'mount_point': MOUNT_POINT
                    }
                }
            }
        }
    }, MOUNT_POINT)

  def testEBSGP(self):
    test_util.testDiskMounts({
        'vm_groups': {
            'vm_group_1': {
                'cloud': 'AWS',
                'vm_spec': {
                    'AWS': {
                        'machine_type': 'm4.large',
                        'zone': 'us-east-1a'
                    }
                },
                'disk_spec': {
                    'AWS': {
                        'disk_type': 'remote_ssd',
                        'disk_size': 2,
                        'mount_point': MOUNT_POINT
                    }
                }
            }
        }
    }, MOUNT_POINT)

  def testLocalSSD(self):
    test_util.testDiskMounts({
        'vm_groups': {
            'vm_group_1': {
                'cloud': 'AWS',
                'vm_spec': {
                    'AWS': {
                        'machine_type': 'm3.medium',
                        'zone': 'us-east-1a'
                    }
                },
                'disk_spec': {
                    'AWS': {
                        'disk_type': 'local',
                        'mount_point': MOUNT_POINT
                    }
                }
            }
        }
    }, MOUNT_POINT)
",Add AWS disk integration tests,"Add AWS disk integration tests
",Python,apache-2.0,"GoogleCloudPlatform/PerfKitBenchmarker,meteorfox/PerfKitBenchmarker,AdamIsrael/PerfKitBenchmarker,mateusz-blaszkowski/PerfKitBenchmarker,GoogleCloudPlatform/PerfKitBenchmarker,kivio/PerfKitBenchmarker,syed/PerfKitBenchmarker,GoogleCloudPlatform/PerfKitBenchmarker,kivio/PerfKitBenchmarker,AdamIsrael/PerfKitBenchmarker,syed/PerfKitBenchmarker,meteorfox/PerfKitBenchmarker,mateusz-blaszkowski/PerfKitBenchmarker,GoogleCloudPlatform/PerfKitBenchmarker",96,"```python
# Copyright 2015 Google Inc. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""Integration tests for AWS scratch disks.""""""

import os
import unittest

from perfkitbenchmarker import pkb
from perfkitbenchmarker import test_util

MOUNT_POINT = '/scratch'


@unittest.skipUnless('PERFKIT_INTEGRATION' in os.environ,
                     'PERFKIT_INTEGRATION not in environment')
class AwsScratchDiskIntegrationTest(unittest.TestCase):
  def setUp(self):
    pkb.SetUpPKB()

  def testEBSStandard(self):
    test_util.testDiskMounts({
        'vm_groups': {
            'vm_group_1': {
                'cloud': 'AWS',
                'vm_spec': {
                    'AWS': {
                        'machine_type': 'm4.large',
                        'zone': 'us-east-1a'
                    }
                },
                'disk_spec': {
                    'AWS': {
                        'disk_type': 'standard',
                        'disk_size': 2,
                        'mount_point': MOUNT_POINT
                    }
                }
            }
        }
    }, MOUNT_POINT)

  def testEBSGP(self):
    test_util.testDiskMounts({
        'vm_groups': {
            'vm_group_1': {
                'cloud': 'AWS',
                'vm_spec': {
                    'AWS': {
                        'machine_type': 'm4.large',
                        'zone': 'us-east-1a'
                    }
                },
                'disk_spec': {
                    'AWS': {
                        'disk_type': 'remote_ssd',
                        'disk_size': 2,
                        'mount_point': MOUNT_POINT
                    }
                }
            }
        }
    }, MOUNT_POINT)

  def testLocalSSD(self):
    test_util.testDiskMounts({
        'vm_groups': {
            'vm_group_1': {
                'cloud': 'AWS',
                'vm_spec': {
                    'AWS': {
                        'machine_type': 'm3.medium',
                        'zone': 'us-east-1a'
                    }
                },
                'disk_spec': {
                    'AWS': {
                        'disk_type': 'local',
                        'mount_point': MOUNT_POINT
                    }
                }
            }
        }
    }, MOUNT_POINT)

```"
979101349fc35efaba5f23ecf2e8a09af0f52a7b,runtests.py,runtests.py,,"# runtests for django reusable apps
# see http://stackoverflow.com/questions/3841725/how-to-launch-tests-for-django-reusable-app

import glob
import os
import sys

import django
from django.conf import settings
from django.core.management import execute_from_command_line

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
sys.path.append(os.path.abspath(os.path.join(BASE_DIR, '..')))

# Unfortunately, apps can not be installed via ``modify_settings``
# decorator, because it would miss the database setup.
CUSTOM_INSTALLED_APPS = (
    'dynamodb2_sessions',
)

ALWAYS_INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
)

ALWAYS_MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)


settings.configure(
    SECRET_KEY=""django_tests_secret_key"",
    DEBUG=False,
    TEMPLATE_DEBUG=False,
    ALLOWED_HOSTS=[],
    INSTALLED_APPS=ALWAYS_INSTALLED_APPS + CUSTOM_INSTALLED_APPS,
    MIDDLEWARE_CLASSES=ALWAYS_MIDDLEWARE_CLASSES,
    ROOT_URLCONF='tests.urls',
    DATABASES={
        'default': {
            'ENGINE': 'django.db.backends.sqlite3',
        }
    },
    LANGUAGE_CODE='en-us',
    TIME_ZONE='UTC',
    USE_I18N=True,
    USE_L10N=True,
    USE_TZ=True,
    STATIC_URL='/static/',
    # Use a fast hasher to speed up tests.
    PASSWORD_HASHERS=(
        'django.contrib.auth.hashers.MD5PasswordHasher',
    ),
    FIXTURE_DIRS=glob.glob(BASE_DIR + '/' + '*/fixtures/'),

    # secret settings
    AWS_REGION_NAME='',
    AWS_ACCESS_KEY_ID='',
    AWS_SECRET_ACCESS_KEY='',
    DYNAMODB_SESSIONS_TABLE_NAME = '',
)


django.setup()
args = [sys.argv[0], 'test']
# Current module (``tests``) and its submodules.
test_cases = '.'

# Allow accessing test options from the command line.
offset = 1
try:
    sys.argv[1]
except IndexError:
    pass
else:
    option = sys.argv[1].startswith('-')
    if not option:
        test_cases = sys.argv[1]
        offset = 2

args.append(test_cases)
# ``verbosity`` can be overwritten from command line.
args.append('--verbosity=2')
args.extend(sys.argv[offset:])

execute_from_command_line(args)
",Add settings file for launching tests,"Add settings file for launching tests
",Python,bsd-3-clause,amrael/django-dynamodb2-sessions,95,"```python
# runtests for django reusable apps
# see http://stackoverflow.com/questions/3841725/how-to-launch-tests-for-django-reusable-app

import glob
import os
import sys

import django
from django.conf import settings
from django.core.management import execute_from_command_line

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
sys.path.append(os.path.abspath(os.path.join(BASE_DIR, '..')))

# Unfortunately, apps can not be installed via ``modify_settings``
# decorator, because it would miss the database setup.
CUSTOM_INSTALLED_APPS = (
    'dynamodb2_sessions',
)

ALWAYS_INSTALLED_APPS = (
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.sites',
    'django.contrib.messages',
    'django.contrib.staticfiles',
)

ALWAYS_MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
)


settings.configure(
    SECRET_KEY=""django_tests_secret_key"",
    DEBUG=False,
    TEMPLATE_DEBUG=False,
    ALLOWED_HOSTS=[],
    INSTALLED_APPS=ALWAYS_INSTALLED_APPS + CUSTOM_INSTALLED_APPS,
    MIDDLEWARE_CLASSES=ALWAYS_MIDDLEWARE_CLASSES,
    ROOT_URLCONF='tests.urls',
    DATABASES={
        'default': {
            'ENGINE': 'django.db.backends.sqlite3',
        }
    },
    LANGUAGE_CODE='en-us',
    TIME_ZONE='UTC',
    USE_I18N=True,
    USE_L10N=True,
    USE_TZ=True,
    STATIC_URL='/static/',
    # Use a fast hasher to speed up tests.
    PASSWORD_HASHERS=(
        'django.contrib.auth.hashers.MD5PasswordHasher',
    ),
    FIXTURE_DIRS=glob.glob(BASE_DIR + '/' + '*/fixtures/'),

    # secret settings
    AWS_REGION_NAME='',
    AWS_ACCESS_KEY_ID='',
    AWS_SECRET_ACCESS_KEY='',
    DYNAMODB_SESSIONS_TABLE_NAME = '',
)


django.setup()
args = [sys.argv[0], 'test']
# Current module (``tests``) and its submodules.
test_cases = '.'

# Allow accessing test options from the command line.
offset = 1
try:
    sys.argv[1]
except IndexError:
    pass
else:
    option = sys.argv[1].startswith('-')
    if not option:
        test_cases = sys.argv[1]
        offset = 2

args.append(test_cases)
# ``verbosity`` can be overwritten from command line.
args.append('--verbosity=2')
args.extend(sys.argv[offset:])

execute_from_command_line(args)

```"
3220af50dd6c4642ac668b2c6dc89e9d76d0e732,apps/submission/tests/test_flows.py,apps/submission/tests/test_flows.py,,"from unittest import mock

from django.test import TestCase
from viewflow import lock
from viewflow.activation import STATUS
from viewflow.base import this

from ..flows import AsyncActivationHandler, AsyncHandler


class ProcessStub(object):
    _default_manager = mock.Mock()

    def __init__(self, flow_class=None):
        self.flow_class = flow_class

    def active_tasks(self):
        return []

    def save(self):
        self.pk = 1
        return


class TaskStub(object):
    _default_manager = mock.Mock()

    def __init__(self, flow_task=None):
        self.flow_task = flow_task
        self.process_id = 1
        self.pk = 1
        self.status = STATUS.NEW
        self.started = None

    @property
    def leading(self):
        from viewflow.models import Task
        return Task.objects.none()

    def save(self):
        self.pk = 1
        return


class FlowStub(object):
    process_class = ProcessStub
    task_class = TaskStub
    lock_impl = lock.no_lock
    instance = None


class AsyncFlow(FlowStub):
    handler_task = AsyncHandler(this.task_handler)
    method_called = False

    def task_handler(self, activation):
        AsyncFlow.method_called = True


class AsyncActivationHandlerTestCase(TestCase):

    def init_node(self, node, flow_class=None, name='test_node'):
        node.flow_class = flow_class or FlowStub
        node.name = name
        node.ready()
        return node

    def setUp(self):
        ProcessStub._default_manager.get.return_value = ProcessStub()
        TaskStub._default_manager.get.return_value = TaskStub()
        FlowStub.instance = FlowStub()

    def test_perform(self):

        AsyncFlow.instance = AsyncFlow()
        flow_task = self.init_node(
            AsyncFlow.handler_task,
            flow_class=AsyncFlow,
            name='task'
        )

        act = AsyncActivationHandler()
        act.initialize(flow_task, TaskStub())

        # execute
        act.perform()
        self.assertEqual(act.task.status, STATUS.NEW)
        self.assertTrue(AsyncFlow.method_called)

    def test_callback(self):
        assert False
",Add tests for the AsyncActivationHandler,"[draft] Add tests for the AsyncActivationHandler
",Python,bsd-3-clause,"Candihub/pixel,Candihub/pixel,Candihub/pixel,Candihub/pixel,Candihub/pixel",92,"```python
from unittest import mock

from django.test import TestCase
from viewflow import lock
from viewflow.activation import STATUS
from viewflow.base import this

from ..flows import AsyncActivationHandler, AsyncHandler


class ProcessStub(object):
    _default_manager = mock.Mock()

    def __init__(self, flow_class=None):
        self.flow_class = flow_class

    def active_tasks(self):
        return []

    def save(self):
        self.pk = 1
        return


class TaskStub(object):
    _default_manager = mock.Mock()

    def __init__(self, flow_task=None):
        self.flow_task = flow_task
        self.process_id = 1
        self.pk = 1
        self.status = STATUS.NEW
        self.started = None

    @property
    def leading(self):
        from viewflow.models import Task
        return Task.objects.none()

    def save(self):
        self.pk = 1
        return


class FlowStub(object):
    process_class = ProcessStub
    task_class = TaskStub
    lock_impl = lock.no_lock
    instance = None


class AsyncFlow(FlowStub):
    handler_task = AsyncHandler(this.task_handler)
    method_called = False

    def task_handler(self, activation):
        AsyncFlow.method_called = True


class AsyncActivationHandlerTestCase(TestCase):

    def init_node(self, node, flow_class=None, name='test_node'):
        node.flow_class = flow_class or FlowStub
        node.name = name
        node.ready()
        return node

    def setUp(self):
        ProcessStub._default_manager.get.return_value = ProcessStub()
        TaskStub._default_manager.get.return_value = TaskStub()
        FlowStub.instance = FlowStub()

    def test_perform(self):

        AsyncFlow.instance = AsyncFlow()
        flow_task = self.init_node(
            AsyncFlow.handler_task,
            flow_class=AsyncFlow,
            name='task'
        )

        act = AsyncActivationHandler()
        act.initialize(flow_task, TaskStub())

        # execute
        act.perform()
        self.assertEqual(act.task.status, STATUS.NEW)
        self.assertTrue(AsyncFlow.method_called)

    def test_callback(self):
        assert False

```"
815a9c802440375cc283179c15d3b1a371863418,tests/test_class_based.py,tests/test_class_based.py,,"""""""tests/test_decorators.py.

Tests that class based hug routes interact as expected

Copyright (C) 2015 Timothy Edmund Crosley

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import sys
import hug

api = sys.modules[__name__]


def test_simple_class_based_view():
    '''Test creating class based routers'''
    @hug.classy.urls('/endpoint', requires=())
    class MyClass(object):

        @hug.classy.get()
        def my_method(self):
            return 'hi there!'

        @hug.classy.post()
        def my_method_two(self):
            return 'bye'

    assert hug.test.get(api, 'endpoint').data == 'hi there!'
    assert hug.test.post(api, 'endpoint').data == 'bye'


def test_simple_class_based_method_view():
    '''Test creating class based routers using method mappings'''
    @hug.classy.auto_http_methods()
    class EndPoint(object):

        def get(self):
            return 'hi there!'

        def post(self):
            return 'bye'

    assert hug.test.get(api, 'endpoint').data == 'hi there!'
    assert hug.test.post(api, 'endpoint').data == 'bye'


def test_routing_class_based_method_view_with_sub_routing():
    '''Test creating class based routers using method mappings, then overriding url on sub method'''
    @hug.classy.auto_http_methods()
    class EndPoint(object):

        def get(self):
            return 'hi there!'

        @hug.classy.urls('/home/')
        def post(self):
            return 'bye'

    assert hug.test.get(api, 'endpoint').data == 'hi there!'
    assert hug.test.post(api, 'home').data == 'bye'


def test_routing_instance():
    '''Test to ensure its possible to route a class after it is instanciated'''
    class EndPoint(object):

        @hug.classy
        def one(self):
            return 'one'

        @hug.classy
        def two(self):
            return 2

    hug.classy.get()(EndPoint())
    assert hug.test.get(api, 'one').data == 'one'
    assert hug.test.get(api, 'two').data == 2
",Add test for desired support of class based routers,"Add test for desired support of class based routers
",Python,mit,"timothycrosley/hug,timothycrosley/hug,MuhammadAlkarouri/hug,MuhammadAlkarouri/hug,MuhammadAlkarouri/hug,timothycrosley/hug",91,"```python
""""""tests/test_decorators.py.

Tests that class based hug routes interact as expected

Copyright (C) 2015 Timothy Edmund Crosley

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
documentation files (the ""Software""), to deal in the Software without restriction, including without limitation
the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or
substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED
TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF
CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR
OTHER DEALINGS IN THE SOFTWARE.

""""""
import sys
import hug

api = sys.modules[__name__]


def test_simple_class_based_view():
    '''Test creating class based routers'''
    @hug.classy.urls('/endpoint', requires=())
    class MyClass(object):

        @hug.classy.get()
        def my_method(self):
            return 'hi there!'

        @hug.classy.post()
        def my_method_two(self):
            return 'bye'

    assert hug.test.get(api, 'endpoint').data == 'hi there!'
    assert hug.test.post(api, 'endpoint').data == 'bye'


def test_simple_class_based_method_view():
    '''Test creating class based routers using method mappings'''
    @hug.classy.auto_http_methods()
    class EndPoint(object):

        def get(self):
            return 'hi there!'

        def post(self):
            return 'bye'

    assert hug.test.get(api, 'endpoint').data == 'hi there!'
    assert hug.test.post(api, 'endpoint').data == 'bye'


def test_routing_class_based_method_view_with_sub_routing():
    '''Test creating class based routers using method mappings, then overriding url on sub method'''
    @hug.classy.auto_http_methods()
    class EndPoint(object):

        def get(self):
            return 'hi there!'

        @hug.classy.urls('/home/')
        def post(self):
            return 'bye'

    assert hug.test.get(api, 'endpoint').data == 'hi there!'
    assert hug.test.post(api, 'home').data == 'bye'


def test_routing_instance():
    '''Test to ensure its possible to route a class after it is instanciated'''
    class EndPoint(object):

        @hug.classy
        def one(self):
            return 'one'

        @hug.classy
        def two(self):
            return 2

    hug.classy.get()(EndPoint())
    assert hug.test.get(api, 'one').data == 'one'
    assert hug.test.get(api, 'two').data == 2

```"
ff81d21d5e68e916282b61b3c65bf0af41a1bad8,app/scripts/po_stats.py,app/scripts/po_stats.py,,"#! /usr/bin/env python

import argparse
import glob
import json
import os
import subprocess
import sys

# Import local libraries
library_path = os.path.abspath(os.path.join(
    os.path.dirname(__file__), os.pardir, 'libraries'))

# Polib library (https://bitbucket.org/izi/polib)
polib_path = os.path.join(library_path, 'polib')
if not os.path.isdir(polib_path):
    try:
        print 'Cloning polib...'
        cmd_status = subprocess.check_output(
            'hg clone https://bitbucket.org/izi/polib/ %s -u 1.0.7' % polib_path,
            stderr=subprocess.STDOUT,
            shell=True)
        print cmd_status
    except Exception as e:
        print e
sys.path.append(os.path.join(polib_path))
try:
    import polib
except ImportError:
    print 'Error importing polib library'
    sys.exit(1)


def create_file_list(repo_folder, locale, source_pattern):
    ''' Search for files to analyze '''

    # Get a list of all reference files, since source_pattern can use wildcards
    locale_files = glob.glob(
        os.path.join(repo_folder, locale, source_pattern)
    )
    locale_files.sort()

    return locale_files


def analyze_files(repo_folder, locale, source_pattern):
    ''' Analyze files, returning an array with stats and errors '''

    global_stats = {}

    # Get a list of all files for the reference locale
    locale_files = create_file_list(repo_folder, locale, source_pattern)
    for locale_file in locale_files:
        fuzzy = 0
        total = 0
        translated = 0
        untranslated = 0
        try:
            po = polib.pofile(locale_file)
            fuzzy = len(po.fuzzy_entries())
            translated = len(po.translated_entries())
            untranslated = len(po.untranslated_entries())
        except Exception as e:
            print e
            sys.exit(1)

        total = translated + untranslated + fuzzy
        source_index = os.path.basename(locale_file)
        global_stats[source_index] = {
            'fuzzy': fuzzy,
            'total': total,
            'translated': translated,
            'untranslated': untranslated
        }

    return global_stats


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('repo_folder', help='Path to repository')
    parser.add_argument(
        'source_pattern', help='Source file pattern (wildcards are supported)')
    parser.add_argument('locale', help='Locale code to analyze')
    parser.add_argument('--pretty', action='store_true',
                        help='export indented and more readable JSON')
    args = parser.parse_args()

    global_stats = analyze_files(
        args.repo_folder, args.locale, args.source_pattern)

    if args.pretty:
        print json.dumps(global_stats, sort_keys=True, indent=2)
    else:
        print json.dumps(global_stats)


if __name__ == '__main__':
    main()
",Create script using polib to generate stats for gettext file,"Create script using polib to generate stats for gettext file
",Python,mpl-2.0,"flodolo/webstatus,mozilla-l10n/webstatus,mozilla-l10n/webstatus,flodolo/webstatus,flodolo/webstatus,mozilla-l10n/webstatus,mozilla-l10n/webstatus,flodolo/webstatus,mozilla-l10n/webstatus,flodolo/webstatus",100,"```python
#! /usr/bin/env python

import argparse
import glob
import json
import os
import subprocess
import sys

# Import local libraries
library_path = os.path.abspath(os.path.join(
    os.path.dirname(__file__), os.pardir, 'libraries'))

# Polib library (https://bitbucket.org/izi/polib)
polib_path = os.path.join(library_path, 'polib')
if not os.path.isdir(polib_path):
    try:
        print 'Cloning polib...'
        cmd_status = subprocess.check_output(
            'hg clone https://bitbucket.org/izi/polib/ %s -u 1.0.7' % polib_path,
            stderr=subprocess.STDOUT,
            shell=True)
        print cmd_status
    except Exception as e:
        print e
sys.path.append(os.path.join(polib_path))
try:
    import polib
except ImportError:
    print 'Error importing polib library'
    sys.exit(1)


def create_file_list(repo_folder, locale, source_pattern):
    ''' Search for files to analyze '''

    # Get a list of all reference files, since source_pattern can use wildcards
    locale_files = glob.glob(
        os.path.join(repo_folder, locale, source_pattern)
    )
    locale_files.sort()

    return locale_files


def analyze_files(repo_folder, locale, source_pattern):
    ''' Analyze files, returning an array with stats and errors '''

    global_stats = {}

    # Get a list of all files for the reference locale
    locale_files = create_file_list(repo_folder, locale, source_pattern)
    for locale_file in locale_files:
        fuzzy = 0
        total = 0
        translated = 0
        untranslated = 0
        try:
            po = polib.pofile(locale_file)
            fuzzy = len(po.fuzzy_entries())
            translated = len(po.translated_entries())
            untranslated = len(po.untranslated_entries())
        except Exception as e:
            print e
            sys.exit(1)

        total = translated + untranslated + fuzzy
        source_index = os.path.basename(locale_file)
        global_stats[source_index] = {
            'fuzzy': fuzzy,
            'total': total,
            'translated': translated,
            'untranslated': untranslated
        }

    return global_stats


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('repo_folder', help='Path to repository')
    parser.add_argument(
        'source_pattern', help='Source file pattern (wildcards are supported)')
    parser.add_argument('locale', help='Locale code to analyze')
    parser.add_argument('--pretty', action='store_true',
                        help='export indented and more readable JSON')
    args = parser.parse_args()

    global_stats = analyze_files(
        args.repo_folder, args.locale, args.source_pattern)

    if args.pretty:
        print json.dumps(global_stats, sort_keys=True, indent=2)
    else:
        print json.dumps(global_stats)


if __name__ == '__main__':
    main()

```"
810eeddaff32f9b608b0b61cfcb48826ec1b15bf,various/Crop_Big_ROIs.py,various/Crop_Big_ROIs.py,,"# @DatasetService datasetservice
# @ImageDisplayService displayservice
# @ImageJ ij
# @AbstractLogService log
# @DefaultLegacyService legacyservice

from ij import IJ
from ij import Macro
from ij.plugin.frame import RoiManager

from io.scif.img import ImgSaver
from net.imagej import DefaultDataset

from loci.plugins import BF
from loci.plugins import LociExporter
from loci.plugins.out import Exporter
from loci.plugins.in import ImporterOptions
from loci.common import Region

import os
import sys
import glob

sys.path.append(os.path.join(IJ.getDirectory('plugins'), ""Scripts"", ""Plugins""))
from libtools import crop
from libtools.utils import get_dt

def main():

    # Get image path
    fname = ""/home/hadim/local/data/microscopy_data/zurick/movies/metC.ome.tif""

    basename = os.path.basename(fname)
    dir_path = os.path.dirname(fname)

    if not fname:
        IJ.showMessage('Source image needs to match a file on the system.')
        return

    # Open ROIs
    roi_path = os.path.join(dir_path, ""RoiSet.zip"")
    if not os.path.isfile(roi_path):
        try:
            roi_path = glob.glob(os.path.join(dir_path, ""*.roi""))[0]
        except:
            roi_path = None
    
    if not roi_path:
        IJ.showMessage('No ROIs. Please use Analyze > Tools > ROI Manager...')
        return
    
    rois = RoiManager(True)
    rois.reset()
    rois.runCommand(""Open"", roi_path)

    IJ.log('Image filename is %s' % fname)

    rois_array = rois.getRoisAsArray()
    for i, roi in enumerate(rois_array):

        crop_id = i +1
        IJ.log(""Open cropped region %i / %i"" % (crop_id, len(rois_array)))

        # Get filename and basename of the current cropped image
        crop_basename = ""crop%i_%s"" % (crop_id, basename)
        crop_fname = os.path.join(os.path.dirname(fname), crop_basename)

        # Get bounds and crop
        bounds = roi.getBounds()

        x = bounds.x
        y = bounds.y
        w = bounds.width
        h = bounds.height
    
        # Import only cropped region of the image
        options = ImporterOptions()
        options.setCrop(True)
        options.setCropRegion(0, Region(x, y, w, h))
        options.setId(fname)
        #options.setVirtual(True)
        imps = BF.openImagePlus(options)
    
        imp = imps[0]

        # Save cropped image
        bfExporter = LociExporter()
        macroOpts = ""save=["" + crop_fname + ""]""
        bfExporter.setup(None, imp)
        Macro.setOptions(macroOpts)
        bfExporter.run(None)
        
        imp.close()

    IJ.log('Done')

main()",Add crop multi roi alternative script,"Add crop multi roi alternative script
",Python,bsd-3-clause,"hadim/fiji_tools,hadim/fiji_scripts,hadim/fiji_scripts,hadim/fiji_scripts,hadim/fiji_tools",97,"```python
# @DatasetService datasetservice
# @ImageDisplayService displayservice
# @ImageJ ij
# @AbstractLogService log
# @DefaultLegacyService legacyservice

from ij import IJ
from ij import Macro
from ij.plugin.frame import RoiManager

from io.scif.img import ImgSaver
from net.imagej import DefaultDataset

from loci.plugins import BF
from loci.plugins import LociExporter
from loci.plugins.out import Exporter
from loci.plugins.in import ImporterOptions
from loci.common import Region

import os
import sys
import glob

sys.path.append(os.path.join(IJ.getDirectory('plugins'), ""Scripts"", ""Plugins""))
from libtools import crop
from libtools.utils import get_dt

def main():

    # Get image path
    fname = ""/home/hadim/local/data/microscopy_data/zurick/movies/metC.ome.tif""

    basename = os.path.basename(fname)
    dir_path = os.path.dirname(fname)

    if not fname:
        IJ.showMessage('Source image needs to match a file on the system.')
        return

    # Open ROIs
    roi_path = os.path.join(dir_path, ""RoiSet.zip"")
    if not os.path.isfile(roi_path):
        try:
            roi_path = glob.glob(os.path.join(dir_path, ""*.roi""))[0]
        except:
            roi_path = None
    
    if not roi_path:
        IJ.showMessage('No ROIs. Please use Analyze > Tools > ROI Manager...')
        return
    
    rois = RoiManager(True)
    rois.reset()
    rois.runCommand(""Open"", roi_path)

    IJ.log('Image filename is %s' % fname)

    rois_array = rois.getRoisAsArray()
    for i, roi in enumerate(rois_array):

        crop_id = i +1
        IJ.log(""Open cropped region %i / %i"" % (crop_id, len(rois_array)))

        # Get filename and basename of the current cropped image
        crop_basename = ""crop%i_%s"" % (crop_id, basename)
        crop_fname = os.path.join(os.path.dirname(fname), crop_basename)

        # Get bounds and crop
        bounds = roi.getBounds()

        x = bounds.x
        y = bounds.y
        w = bounds.width
        h = bounds.height
    
        # Import only cropped region of the image
        options = ImporterOptions()
        options.setCrop(True)
        options.setCropRegion(0, Region(x, y, w, h))
        options.setId(fname)
        #options.setVirtual(True)
        imps = BF.openImagePlus(options)
    
        imp = imps[0]

        # Save cropped image
        bfExporter = LociExporter()
        macroOpts = ""save=["" + crop_fname + ""]""
        bfExporter.setup(None, imp)
        Macro.setOptions(macroOpts)
        bfExporter.run(None)
        
        imp.close()

    IJ.log('Done')

main()
```"
dfd7ee9d60c1ea0b2ecd5b09eb9a5468f6045852,gen_training_data_1.py,gen_training_data_1.py,,"import sys
import os
import json
import numpy as np

MAX_ACTIONS = 20
PLAYER_RANGE = (4, 7)

def init_vec(num_players):
    input_vec = [0]*8
    input_vec[num_players - PLAYER_RANGE[0]] = 1
    return input_vec

def gen_training_data(hand):
    num_players = hand['num_players']
    if PLAYER_RANGE[0] <= num_players <= PLAYER_RANGE[1]:
        players = []
        for action in hand['actions'][:num_players]:
            players.append(action[0])

        i = 0
        j = 0

        inputs = [init_vec(num_players)]
        outputs = []
        while i < len(hand['actions']):
            action = hand['actions'][i]
            if action == 'NEXT':
                break

            input_vec = init_vec(num_players)
            output_vec = [0] * 4

            if action[0] == players[j]:
                if action[1] == 0:
                    input_vec[4] = 1
                    output_vec[0] = 1
                elif action[1] in [1, 2]:
                    input_vec[5] = 1
                    output_vec[1] = 1
                else:
                    input_vec[6] = 1
                    output_vec[2] = 1
                i += 1
            else:
                input_vec[7] = 1
                output_vec[3] = 1

            inputs.append(input_vec)
            outputs.append(output_vec)

            j = (j + 1) % num_players

        inputs = inputs[:-1]
        if len(inputs) < MAX_ACTIONS:
            for _ in range(MAX_ACTIONS - len(inputs)):
                inputs.append([0]*8)
                outputs.append([0, 0, 0, 1])
        elif len(inputs) > MAX_ACTIONS:
            inputs = inputs[:MAX_ACTIONS]
            outputs = outputs[:MAX_ACTIONS]

        return inputs, outputs
    return False

if __name__ == '__main__':
    input_dir = sys.argv[1]
    output_file = sys.argv[2]

    inputs = []
    outputs = []
    for filename in os.listdir(input_dir)[:20]:
        filename = os.path.join(input_dir, filename)
        print ""Filename:"", filename 

        f = open(filename, 'r')

        counter = 0
        for hand in json.loads(f.read()):
            res = gen_training_data(hand)
            if res:
                inp, out = res
                inputs.append(inp)
                outputs.append(out)
                counter += 1

        print ""Num Hands: "", counter

        f.close()

    input_arr = np.asarray(inputs)
    output_arr = np.asarray(outputs)

    np.savez_compressed(output_file, input=input_arr, output=output_arr)",Add first training data generator.,"Add first training data generator.
",Python,apache-2.0,session-id/poker-predictor,94,"```python
import sys
import os
import json
import numpy as np

MAX_ACTIONS = 20
PLAYER_RANGE = (4, 7)

def init_vec(num_players):
    input_vec = [0]*8
    input_vec[num_players - PLAYER_RANGE[0]] = 1
    return input_vec

def gen_training_data(hand):
    num_players = hand['num_players']
    if PLAYER_RANGE[0] <= num_players <= PLAYER_RANGE[1]:
        players = []
        for action in hand['actions'][:num_players]:
            players.append(action[0])

        i = 0
        j = 0

        inputs = [init_vec(num_players)]
        outputs = []
        while i < len(hand['actions']):
            action = hand['actions'][i]
            if action == 'NEXT':
                break

            input_vec = init_vec(num_players)
            output_vec = [0] * 4

            if action[0] == players[j]:
                if action[1] == 0:
                    input_vec[4] = 1
                    output_vec[0] = 1
                elif action[1] in [1, 2]:
                    input_vec[5] = 1
                    output_vec[1] = 1
                else:
                    input_vec[6] = 1
                    output_vec[2] = 1
                i += 1
            else:
                input_vec[7] = 1
                output_vec[3] = 1

            inputs.append(input_vec)
            outputs.append(output_vec)

            j = (j + 1) % num_players

        inputs = inputs[:-1]
        if len(inputs) < MAX_ACTIONS:
            for _ in range(MAX_ACTIONS - len(inputs)):
                inputs.append([0]*8)
                outputs.append([0, 0, 0, 1])
        elif len(inputs) > MAX_ACTIONS:
            inputs = inputs[:MAX_ACTIONS]
            outputs = outputs[:MAX_ACTIONS]

        return inputs, outputs
    return False

if __name__ == '__main__':
    input_dir = sys.argv[1]
    output_file = sys.argv[2]

    inputs = []
    outputs = []
    for filename in os.listdir(input_dir)[:20]:
        filename = os.path.join(input_dir, filename)
        print ""Filename:"", filename 

        f = open(filename, 'r')

        counter = 0
        for hand in json.loads(f.read()):
            res = gen_training_data(hand)
            if res:
                inp, out = res
                inputs.append(inp)
                outputs.append(out)
                counter += 1

        print ""Num Hands: "", counter

        f.close()

    input_arr = np.asarray(inputs)
    output_arr = np.asarray(outputs)

    np.savez_compressed(output_file, input=input_arr, output=output_arr)
```"
f67a6748b7282cbc256423b3c98c3321d4c028c0,FrontGateManager.py,FrontGateManager.py,,"#!/usr/bin/python

import urllib2
import RPi.GPIO as GPIO
from time import sleep

class Sonos:
        apiPort = 5005
        apiHost = '127.0.0.1'

        def say(self, room, str, lang = 'en-us'):
                path = 'http://{host}:{port}/{room}/say/{str}/{lang}'.format(
									host = self.apiHost,
									port = self.apiPort,
									room = urllib2.quote(room),
									str = urllib2.quote(str),
									lang = lang
								)
		print path
                self.__sonosRequest(path)

        def sayAll(self, str, lang = 'en-us'):
                path = 'http://{host}:{port}/sayall/{str}/{lang}'.format(
									host = self.apiHost,
									port = self.apiPort,
									str = urllib2.quote(str),
									lang = lang
								)
		print path
                self.__sonosRequest(path)

        def __sonosRequest(self, url):
                req = urllib2.Request(url)
                try:
                        urllib2.urlopen(req)
                except urllib2.URLError as e:
                        print e.reason


class FrontGateState:
	open = False
	lastState = False

	def __init__(self, pin):
		self.pin = pin
		GPIO.setmode(GPIO.BCM)
		GPIO.setup(self.pin, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)

	def run(self):
		if GPIO.input(self.pin):
			self.open = False 
		else:
			self.open = True 

		if self.open != self.lastState:
			if self.open:
				FrontGateManager.sonos.sayAll('The gate is now open')
			else:
				FrontGateManager.sonos.sayAll('The gate is now closed')
			self.lastState = self.open

class SkyBell:
	pressed = False
	lastState = False

	def __init__(self, pin):
		self.pin = pin
		GPIO.setmode(GPIO.BCM)
		GPIO.setup(self.pin, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)

	def run(self):
		if GPIO.input(self.pin):
			self.pressed = False 
		else:
			self.pressed = True 
		
		if self.pressed != self.lastState:
			if self.pressed:
				FrontGateManager.sonos.sayAll('There is someone at the door')
		self.lastState = self.pressed

class FrontGateManager:
	sonos = Sonos()

	def init(self):
		self.frontGateState = FrontGateState(22)
		self.skyBell = SkyBell(17)

		try:
			while True:
				self.frontGateState.run()
				self.skyBell.run()
				sleep(0.5)
		finally:
			GPIO.cleanup()


if __name__ == ""__main__"":
	frontGateManager = FrontGateManager()
	frontGateManager.init()	
",Add Front Gate Manager python file,"Add Front Gate Manager python file
",Python,mit,tdlm/front_gate_pi,101,"```python
#!/usr/bin/python

import urllib2
import RPi.GPIO as GPIO
from time import sleep

class Sonos:
        apiPort = 5005
        apiHost = '127.0.0.1'

        def say(self, room, str, lang = 'en-us'):
                path = 'http://{host}:{port}/{room}/say/{str}/{lang}'.format(
									host = self.apiHost,
									port = self.apiPort,
									room = urllib2.quote(room),
									str = urllib2.quote(str),
									lang = lang
								)
		print path
                self.__sonosRequest(path)

        def sayAll(self, str, lang = 'en-us'):
                path = 'http://{host}:{port}/sayall/{str}/{lang}'.format(
									host = self.apiHost,
									port = self.apiPort,
									str = urllib2.quote(str),
									lang = lang
								)
		print path
                self.__sonosRequest(path)

        def __sonosRequest(self, url):
                req = urllib2.Request(url)
                try:
                        urllib2.urlopen(req)
                except urllib2.URLError as e:
                        print e.reason


class FrontGateState:
	open = False
	lastState = False

	def __init__(self, pin):
		self.pin = pin
		GPIO.setmode(GPIO.BCM)
		GPIO.setup(self.pin, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)

	def run(self):
		if GPIO.input(self.pin):
			self.open = False 
		else:
			self.open = True 

		if self.open != self.lastState:
			if self.open:
				FrontGateManager.sonos.sayAll('The gate is now open')
			else:
				FrontGateManager.sonos.sayAll('The gate is now closed')
			self.lastState = self.open

class SkyBell:
	pressed = False
	lastState = False

	def __init__(self, pin):
		self.pin = pin
		GPIO.setmode(GPIO.BCM)
		GPIO.setup(self.pin, GPIO.IN, pull_up_down=GPIO.PUD_DOWN)

	def run(self):
		if GPIO.input(self.pin):
			self.pressed = False 
		else:
			self.pressed = True 
		
		if self.pressed != self.lastState:
			if self.pressed:
				FrontGateManager.sonos.sayAll('There is someone at the door')
		self.lastState = self.pressed

class FrontGateManager:
	sonos = Sonos()

	def init(self):
		self.frontGateState = FrontGateState(22)
		self.skyBell = SkyBell(17)

		try:
			while True:
				self.frontGateState.run()
				self.skyBell.run()
				sleep(0.5)
		finally:
			GPIO.cleanup()


if __name__ == ""__main__"":
	frontGateManager = FrontGateManager()
	frontGateManager.init()	

```"
dbff077245130f175fdabaa347e33de7a2b71450,twitch_channels.py,twitch_channels.py,,"#!/usr/bin/env python
"""""" twitch_channels.py - Queries Twitch for channels of interest.
    Used in conjunction with livestreamer-completion.

    Usage examples:

      ./twitch_channels.py --follows myname
      ./twitch_channels.py --team eg --team teamliquid

""""""

from __future__ import print_function

from argparse import ArgumentParser
from functools import partial
from itertools import chain

import requests


API_BASE_URL = ""api.twitch.tv""

def api(path, *args, **params):
    insecure = params.pop(""insecure"", False)
    proto = insecure and ""http://"" or ""https://""
    url = proto + API_BASE_URL + path.format(*args)
    res = requests.get(url, params=params)

    return res.json()


def page_result(res):
    for key, values in res.items():
        if not key.startswith(""_""):
            return values

    return []


def iterate_pages(func, limit=25):
    offset, total = 0, limit

    while offset < total:
        res = func(limit=limit, offset=offset)
        values = page_result(res)

        offset += len(values)
        total = res.get(""_total"")

        yield values


def iterate_pages_result(*args, **kwargs):
    return chain.from_iterable(iterate_pages(*args, **kwargs))


# Twitch APIs
team_channels = partial(api, ""/api/team/{0}/all_channels.json"", insecure=True)
user_follows = partial(api, ""/kraken/users/{0}/follows/channels"")


parser = ArgumentParser()
parser.add_argument(""-f"", ""--follows"", action=""append"", default=[],
                    metavar=""user"", help=""channels a user is following"",)
parser.add_argument(""-t"", ""--team"", action=""append"", default=[],
                    metavar=""team"", help=""channels that are part of a team"")


def main():
    args = parser.parse_args()
    if not (args.follows or args.team):
        return parser.print_help()

    for user in args.follows:
        myuser_follows = partial(user_follows, user)

        for channel in iterate_pages_result(myuser_follows, limit=100):
            channel_name = channel.get(""channel"").get(""name"")
            print(""twitch.tv/{0}"".format(channel_name))

    for team in args.team:
        channels = team_channels(team).get(""channels"", [])
        for channel in channels:
            channel_name = channel.get(""channel"").get(""name"")
            print(""twitch.tv/{0}"".format(channel_name))


if __name__ == ""__main__"":
    main()

",Add extra utility to generate a URL list.,"Add extra utility to generate a URL list.
",Python,mit,"chrippa/livestreamer-completion,chrippa/livestreamer-completion",91,"```python
#!/usr/bin/env python
"""""" twitch_channels.py - Queries Twitch for channels of interest.
    Used in conjunction with livestreamer-completion.

    Usage examples:

      ./twitch_channels.py --follows myname
      ./twitch_channels.py --team eg --team teamliquid

""""""

from __future__ import print_function

from argparse import ArgumentParser
from functools import partial
from itertools import chain

import requests


API_BASE_URL = ""api.twitch.tv""

def api(path, *args, **params):
    insecure = params.pop(""insecure"", False)
    proto = insecure and ""http://"" or ""https://""
    url = proto + API_BASE_URL + path.format(*args)
    res = requests.get(url, params=params)

    return res.json()


def page_result(res):
    for key, values in res.items():
        if not key.startswith(""_""):
            return values

    return []


def iterate_pages(func, limit=25):
    offset, total = 0, limit

    while offset < total:
        res = func(limit=limit, offset=offset)
        values = page_result(res)

        offset += len(values)
        total = res.get(""_total"")

        yield values


def iterate_pages_result(*args, **kwargs):
    return chain.from_iterable(iterate_pages(*args, **kwargs))


# Twitch APIs
team_channels = partial(api, ""/api/team/{0}/all_channels.json"", insecure=True)
user_follows = partial(api, ""/kraken/users/{0}/follows/channels"")


parser = ArgumentParser()
parser.add_argument(""-f"", ""--follows"", action=""append"", default=[],
                    metavar=""user"", help=""channels a user is following"",)
parser.add_argument(""-t"", ""--team"", action=""append"", default=[],
                    metavar=""team"", help=""channels that are part of a team"")


def main():
    args = parser.parse_args()
    if not (args.follows or args.team):
        return parser.print_help()

    for user in args.follows:
        myuser_follows = partial(user_follows, user)

        for channel in iterate_pages_result(myuser_follows, limit=100):
            channel_name = channel.get(""channel"").get(""name"")
            print(""twitch.tv/{0}"".format(channel_name))

    for team in args.team:
        channels = team_channels(team).get(""channels"", [])
        for channel in channels:
            channel_name = channel.get(""channel"").get(""name"")
            print(""twitch.tv/{0}"".format(channel_name))


if __name__ == ""__main__"":
    main()


```"
e9917a9f4855ebb3b464977a26c7549baeb6a610,contrib/satgpio.py,contrib/satgpio.py,,"""""""
Author: Juan Luis Cano Rodríguez

Code to read GP data from Celestrak using the HTTP API and python-sgp4.

Requires some extra dependencies:

  $ pip install httpx sgp4

""""""

import io
import json
import xml.etree.ElementTree as ET

import httpx
from sgp4 import exporter, omm
from sgp4.api import Satrec


def _generate_url(catalog_number, international_designator, name):
    params = {""CATNR"": catalog_number, ""INTDES"": international_designator, ""NAME"": name}
    param_names = [
        param_name
        for param_name, param_value in params.items()
        if param_value is not None
    ]
    if len(param_names) != 1:
        raise ValueError(
            ""Specify exactly one of catalog_number, international_designator, or name""
        )
    param_name = param_names[0]
    param_value = params[param_name]
    url = (
        ""https://celestrak.com/NORAD/elements/gp.php?""
        f""{param_name}={param_value}""
        ""&FORMAT=XML""
    )
    return url


def _make_query(url):
    response = httpx.get(url)
    response.raise_for_status()

    if response.text == ""No GP data found"":
        raise ValueError(
            f""Query '{url}' did not return any results, try a different one""
        )
    tree = ET.parse(io.StringIO(response.text))
    root = tree.getroot()

    if len(root) != 1:
        raise ValueError(
            f""Query '{url}' returned {len(root)} results, try a different one""
        )
    fields = next(omm.parse_xml(io.StringIO(response.text)))
    return fields


def load_gp_from_celestrak(
    *, catalog_number=None, international_designator=None, name=None
):
    """"""Load general perturbations orbital data from Celestrak.

    Returns
    -------
    Satrec
        Orbital data from specified object.

    Notes
    -----
    This uses the OMM XML format from Celestrak as described in [1]_.

    References
    ----------
    .. [1] Kelso, T.S. ""A New Way to Obtain GP Data (aka TLEs)""
       https://celestrak.com/NORAD/documentation/gp-data-formats.php

    """"""
    # Assemble query, raise an error if malformed
    url = _generate_url(catalog_number, international_designator, name)

    # Make API call, raise an error if data is malformed
    fields = _make_query(url)

    # Initialize and return Satrec object
    sat = Satrec()
    omm.initialize(sat, fields)

    return sat


def print_sat(sat, name):
    """"""Prints Satrec object in convenient form.""""""
    print(json.dumps(exporter.export_omm(sat, name), indent=2))
",Add contrib code to load GP data from Celestrak,"Add contrib code to load GP data from Celestrak
",Python,mit,poliastro/poliastro,97,"```python
""""""
Author: Juan Luis Cano Rodríguez

Code to read GP data from Celestrak using the HTTP API and python-sgp4.

Requires some extra dependencies:

  $ pip install httpx sgp4

""""""

import io
import json
import xml.etree.ElementTree as ET

import httpx
from sgp4 import exporter, omm
from sgp4.api import Satrec


def _generate_url(catalog_number, international_designator, name):
    params = {""CATNR"": catalog_number, ""INTDES"": international_designator, ""NAME"": name}
    param_names = [
        param_name
        for param_name, param_value in params.items()
        if param_value is not None
    ]
    if len(param_names) != 1:
        raise ValueError(
            ""Specify exactly one of catalog_number, international_designator, or name""
        )
    param_name = param_names[0]
    param_value = params[param_name]
    url = (
        ""https://celestrak.com/NORAD/elements/gp.php?""
        f""{param_name}={param_value}""
        ""&FORMAT=XML""
    )
    return url


def _make_query(url):
    response = httpx.get(url)
    response.raise_for_status()

    if response.text == ""No GP data found"":
        raise ValueError(
            f""Query '{url}' did not return any results, try a different one""
        )
    tree = ET.parse(io.StringIO(response.text))
    root = tree.getroot()

    if len(root) != 1:
        raise ValueError(
            f""Query '{url}' returned {len(root)} results, try a different one""
        )
    fields = next(omm.parse_xml(io.StringIO(response.text)))
    return fields


def load_gp_from_celestrak(
    *, catalog_number=None, international_designator=None, name=None
):
    """"""Load general perturbations orbital data from Celestrak.

    Returns
    -------
    Satrec
        Orbital data from specified object.

    Notes
    -----
    This uses the OMM XML format from Celestrak as described in [1]_.

    References
    ----------
    .. [1] Kelso, T.S. ""A New Way to Obtain GP Data (aka TLEs)""
       https://celestrak.com/NORAD/documentation/gp-data-formats.php

    """"""
    # Assemble query, raise an error if malformed
    url = _generate_url(catalog_number, international_designator, name)

    # Make API call, raise an error if data is malformed
    fields = _make_query(url)

    # Initialize and return Satrec object
    sat = Satrec()
    omm.initialize(sat, fields)

    return sat


def print_sat(sat, name):
    """"""Prints Satrec object in convenient form.""""""
    print(json.dumps(exporter.export_omm(sat, name), indent=2))

```"
221ce25961d1344acf04146c1006c5dc239649ac,anaconda-mode/0.1.1/anaconda_mode.py,anaconda-mode/0.1.1/anaconda_mode.py,,"""""""
    anaconda_mode
    ~~~~~~~~~~~~~

    This is anaconda_mode autocompletion server.

    :copyright: (c) 2013-2015 by Artem Malyshev.
    :license: GPL3, see LICENSE for more details.
""""""

from __future__ import (
    absolute_import, unicode_literals, division, print_function)

import sys
from functools import wraps

from jedi import Script, NotFoundError
from service_factory import service_factory


def script_method(f):
    """"""Create jedi.Script instance and apply f to it.""""""

    @wraps(f)
    def wrapper(source, line, column, path):
        try:
            return f(Script(source, line, column, path))
        except NotFoundError:
            return []

    return wrapper


def process_definitions(f):
    """"""Call f and convert it result into json dumpable format.""""""

    @wraps(f)
    def wrapper(script):

        return [{'name': definition.name,
                 'type': definition.type,
                 'module-name': definition.module_name,
                 'module-path': definition.module_path,
                 'line': definition.line,
                 'column': definition.column,
                 'docstring': definition.docstring(),
                 'description': definition.description,
                 'full-name': definition.full_name}
                for definition in f(script)]

    return wrapper


@script_method
@process_definitions
def complete(script):
    """"""Select auto-complete candidates for source position.""""""

    return script.completions()


@script_method
@process_definitions
def goto_definitions(script):
    """"""Get definitions for thing under cursor.""""""

    return script.goto_definitions()


@script_method
@process_definitions
def goto_assignments(script):
    """"""Get assignments for thing under cursor.""""""

    return script.goto_assignments()


@script_method
@process_definitions
def usages(script):
    """"""Get usage information for thing under cursor.""""""

    return script.usages()


@script_method
def eldoc(script):
    """"""Return eldoc format documentation string or ''.""""""

    signatures = script.call_signatures()
    if len(signatures) == 1:
        signature = signatures[0]
        return {
            'name': signature.name,
            'index': signature.index,
            'params': [param.description for param in signature.params]
        }


app = [complete, goto_definitions, goto_assignments, usages, eldoc]


if __name__ == '__main__':
    host = sys.argv[1] if len(sys.argv) == 2 else '127.0.0.1'
    service_factory(app, host, 'auto', 'anaconda_mode port {port}')
",Check in anaconda-mode.py so pip doesn't need to install it.,"Check in anaconda-mode.py so pip doesn't need to install it.
",Python,mit,"Wilfred/.emacs.d,Wilfred/.emacs.d,Wilfred/.emacs.d,Wilfred/.emacs.d,Wilfred/.emacs.d,Wilfred/.emacs.d,Wilfred/.emacs.d",106,"```python
""""""
    anaconda_mode
    ~~~~~~~~~~~~~

    This is anaconda_mode autocompletion server.

    :copyright: (c) 2013-2015 by Artem Malyshev.
    :license: GPL3, see LICENSE for more details.
""""""

from __future__ import (
    absolute_import, unicode_literals, division, print_function)

import sys
from functools import wraps

from jedi import Script, NotFoundError
from service_factory import service_factory


def script_method(f):
    """"""Create jedi.Script instance and apply f to it.""""""

    @wraps(f)
    def wrapper(source, line, column, path):
        try:
            return f(Script(source, line, column, path))
        except NotFoundError:
            return []

    return wrapper


def process_definitions(f):
    """"""Call f and convert it result into json dumpable format.""""""

    @wraps(f)
    def wrapper(script):

        return [{'name': definition.name,
                 'type': definition.type,
                 'module-name': definition.module_name,
                 'module-path': definition.module_path,
                 'line': definition.line,
                 'column': definition.column,
                 'docstring': definition.docstring(),
                 'description': definition.description,
                 'full-name': definition.full_name}
                for definition in f(script)]

    return wrapper


@script_method
@process_definitions
def complete(script):
    """"""Select auto-complete candidates for source position.""""""

    return script.completions()


@script_method
@process_definitions
def goto_definitions(script):
    """"""Get definitions for thing under cursor.""""""

    return script.goto_definitions()


@script_method
@process_definitions
def goto_assignments(script):
    """"""Get assignments for thing under cursor.""""""

    return script.goto_assignments()


@script_method
@process_definitions
def usages(script):
    """"""Get usage information for thing under cursor.""""""

    return script.usages()


@script_method
def eldoc(script):
    """"""Return eldoc format documentation string or ''.""""""

    signatures = script.call_signatures()
    if len(signatures) == 1:
        signature = signatures[0]
        return {
            'name': signature.name,
            'index': signature.index,
            'params': [param.description for param in signature.params]
        }


app = [complete, goto_definitions, goto_assignments, usages, eldoc]


if __name__ == '__main__':
    host = sys.argv[1] if len(sys.argv) == 2 else '127.0.0.1'
    service_factory(app, host, 'auto', 'anaconda_mode port {port}')

```"
10169d66c91360f562367a00002bac23ec036719,src/sidecar/connection.py,src/sidecar/connection.py,,"# -*- coding: utf-8 -*-

import json
import logging
import os
from sockjs.tornado import SockJSRouter, SockJSConnection
from tornado.web import RequestHandler, StaticFileHandler
from tornado.web import Application
from tornado.ioloop import IOLoop

from sidecar.utils import log


class WebHandler(RequestHandler):
    def initialize(self, page, **kwargs):
        self.page = page
        self.kwargs = kwargs

    def get(self):
        self.render(self.page, **self.kwargs)


class TextHandler(RequestHandler):
    def initialize(self, content):
        self.content = content

    def get(self):
        self.finish(self.content)


class FileHandler(StaticFileHandler):
    def initialize(self, path):
        if path is None:
            self.absolute_path = None
        else:
            path = os.path.join(os.path.dirname(__file__), path)
            self.absolute_path = os.path.abspath(os.path.expanduser(path))
            self.root, self.filename = os.path.split(self.absolute_path)

    def get(self, path=None, include_body=True):
        if self.absolute_path is not None:
            return super(FileHandler, self).get(self.filename, include_body)
        self.finish('')


class Connection(SockJSConnection):
    def send_json(self, kind, data=None):
        log.debug()
        self.send(json.dumps({'kind': kind, 'data': data or {}}))

    def on_open(self, info):
        log.debug()
        self.send_json('ready')

    def on_message(self, msg):
        msg = json.loads(msg)
        log.debug(msg)

    def on_close(self):
        log.debug()

    @classmethod
    def tornado_app(cls, ui, title, debug=False):
        root = os.path.dirname(__file__)
        router = SockJSRouter(cls, '/api')

        settings = {
            'static_path': os.path.join(root, 'static'),
            'template_path': os.path.join(root, 'static'),
            'debug': debug
        }

        handlers = [
            ('/', WebHandler, {'page': 'index.html', 'title': title}),
            ('/ui.json', TextHandler, {'content': ui})
        ]
        handlers.extend(router.urls)

        return Application(handlers, **settings)

    @classmethod
    def start(cls, ui, title, debug=False, port=9999):
        if debug:
            logging.basicConfig(level=logging.DEBUG)
        log.debug()

        app = cls.tornado_app(ui, title, debug=debug)
        app.listen(port)

        IOLoop.instance().start()
",Add basic SockJS server implementation,"Add basic SockJS server implementation
",Python,apache-2.0,"aldanor/sidecar,aldanor/sidecar,aldanor/sidecar",91,"```python
# -*- coding: utf-8 -*-

import json
import logging
import os
from sockjs.tornado import SockJSRouter, SockJSConnection
from tornado.web import RequestHandler, StaticFileHandler
from tornado.web import Application
from tornado.ioloop import IOLoop

from sidecar.utils import log


class WebHandler(RequestHandler):
    def initialize(self, page, **kwargs):
        self.page = page
        self.kwargs = kwargs

    def get(self):
        self.render(self.page, **self.kwargs)


class TextHandler(RequestHandler):
    def initialize(self, content):
        self.content = content

    def get(self):
        self.finish(self.content)


class FileHandler(StaticFileHandler):
    def initialize(self, path):
        if path is None:
            self.absolute_path = None
        else:
            path = os.path.join(os.path.dirname(__file__), path)
            self.absolute_path = os.path.abspath(os.path.expanduser(path))
            self.root, self.filename = os.path.split(self.absolute_path)

    def get(self, path=None, include_body=True):
        if self.absolute_path is not None:
            return super(FileHandler, self).get(self.filename, include_body)
        self.finish('')


class Connection(SockJSConnection):
    def send_json(self, kind, data=None):
        log.debug()
        self.send(json.dumps({'kind': kind, 'data': data or {}}))

    def on_open(self, info):
        log.debug()
        self.send_json('ready')

    def on_message(self, msg):
        msg = json.loads(msg)
        log.debug(msg)

    def on_close(self):
        log.debug()

    @classmethod
    def tornado_app(cls, ui, title, debug=False):
        root = os.path.dirname(__file__)
        router = SockJSRouter(cls, '/api')

        settings = {
            'static_path': os.path.join(root, 'static'),
            'template_path': os.path.join(root, 'static'),
            'debug': debug
        }

        handlers = [
            ('/', WebHandler, {'page': 'index.html', 'title': title}),
            ('/ui.json', TextHandler, {'content': ui})
        ]
        handlers.extend(router.urls)

        return Application(handlers, **settings)

    @classmethod
    def start(cls, ui, title, debug=False, port=9999):
        if debug:
            logging.basicConfig(level=logging.DEBUG)
        log.debug()

        app = cls.tornado_app(ui, title, debug=debug)
        app.listen(port)

        IOLoop.instance().start()

```"
3fd7cda9be34dd0bbf884aae8012096d3962fad3,tests/test_web_urldispatcher.py,tests/test_web_urldispatcher.py,,"import pytest
import tempfile
import aiohttp
from aiohttp import web
import os
import shutil
import asyncio

SERVER_HOST = '127.0.0.1'
SERVER_PORT = 8080

# Timeout in seconds for an asynchronous test:
ASYNC_TEST_TIMEOUT = 1

class ExceptAsyncTestTimeout(Exception): pass

def run_timeout(cor,loop,timeout=ASYNC_TEST_TIMEOUT):
    """"""
    Run a given coroutine with timeout.
    """"""
    task_with_timeout = asyncio.wait_for(cor,timeout,loop=loop)
    try:
        return loop.run_until_complete(task_with_timeout)
    except asyncio.futures.TimeoutError:
        # Timeout:
        raise ExceptAsyncTestTimeout()


@pytest.fixture(scope='function')
def tloop(request):
    """"""
    Obtain a test loop. We want each test case to have its own loop.
    """"""
    # Create a new test loop:
    tloop = asyncio.new_event_loop()
    asyncio.set_event_loop(None)

    def teardown():
        # Close the test loop:
        tloop.close()

    request.addfinalizer(teardown)
    return tloop


@pytest.fixture(scope='function')
def tmp_dir_path(request):
    """"""
    Give a path for a temporary directory
    The directory is destroyed at the end of the test.
    """"""
    # Temporary directory.
    tmp_dir = tempfile.mkdtemp()

    def teardown():
        # Delete the whole directory:
        shutil.rmtree(tmp_dir)

    request.addfinalizer(teardown)
    return tmp_dir


def test_access_root_of_static_handler(tloop,tmp_dir_path):
    """"""
    Tests the operation of static file server.
    Try to access the root of static file server, and make
    sure that a proper not found error is returned.
    """"""
    # Put a file inside tmp_dir_path:
    my_file_path = os.path.join(tmp_dir_path,'my_file')
    with open(my_file_path,'w') as fw:
        fw.write('hello')

    asyncio.set_event_loop(None)
    app = web.Application(loop=tloop)
    # Register global static route:
    app.router.add_static('/', tmp_dir_path)

    @asyncio.coroutine
    def inner_cor():
        handler = app.make_handler()
        srv = yield from tloop.create_server(handler,\
                SERVER_HOST,SERVER_PORT ,reuse_address=True) 

        # Request the root of the static directory.
        # Expect an 404 error page.
        url = 'http://{}:{}/'.format(\
                SERVER_HOST,SERVER_PORT) 

        r = ( yield from aiohttp.get(url,loop=tloop) )
        assert r.status == 404
        # data = (yield from r.read())
        yield from r.release()

        srv.close()
        yield from srv.wait_closed()

        yield from app.shutdown()
        yield from handler.finish_connections(10.0)
        yield from app.cleanup()


    run_timeout(inner_cor(),tloop,timeout=5)
",Test for accessing the root of a statically served dir.,"Test for accessing the root of a statically served dir.
",Python,apache-2.0,"z2v/aiohttp,esaezgil/aiohttp,AraHaanOrg/aiohttp,elastic-coders/aiohttp,moden-py/aiohttp,vaskalas/aiohttp,panda73111/aiohttp,jashandeep-sohi/aiohttp,elastic-coders/aiohttp,hellysmile/aiohttp,juliatem/aiohttp,panda73111/aiohttp,moden-py/aiohttp,esaezgil/aiohttp,jashandeep-sohi/aiohttp,KeepSafe/aiohttp,singulared/aiohttp,z2v/aiohttp,vaskalas/aiohttp,z2v/aiohttp,vaskalas/aiohttp,elastic-coders/aiohttp,playpauseandstop/aiohttp,singulared/aiohttp,mind1master/aiohttp,panda73111/aiohttp,moden-py/aiohttp,AraHaanOrg/aiohttp,mind1master/aiohttp,jashandeep-sohi/aiohttp,rutsky/aiohttp,arthurdarcet/aiohttp,KeepSafe/aiohttp,KeepSafe/aiohttp,arthurdarcet/aiohttp,arthurdarcet/aiohttp,esaezgil/aiohttp,pfreixes/aiohttp,jettify/aiohttp,pfreixes/aiohttp,singulared/aiohttp,Eyepea/aiohttp,rutsky/aiohttp,alex-eri/aiohttp-1,juliatem/aiohttp,rutsky/aiohttp,alex-eri/aiohttp-1,hellysmile/aiohttp,mind1master/aiohttp,alex-eri/aiohttp-1,jettify/aiohttp,jettify/aiohttp",104,"```python
import pytest
import tempfile
import aiohttp
from aiohttp import web
import os
import shutil
import asyncio

SERVER_HOST = '127.0.0.1'
SERVER_PORT = 8080

# Timeout in seconds for an asynchronous test:
ASYNC_TEST_TIMEOUT = 1

class ExceptAsyncTestTimeout(Exception): pass

def run_timeout(cor,loop,timeout=ASYNC_TEST_TIMEOUT):
    """"""
    Run a given coroutine with timeout.
    """"""
    task_with_timeout = asyncio.wait_for(cor,timeout,loop=loop)
    try:
        return loop.run_until_complete(task_with_timeout)
    except asyncio.futures.TimeoutError:
        # Timeout:
        raise ExceptAsyncTestTimeout()


@pytest.fixture(scope='function')
def tloop(request):
    """"""
    Obtain a test loop. We want each test case to have its own loop.
    """"""
    # Create a new test loop:
    tloop = asyncio.new_event_loop()
    asyncio.set_event_loop(None)

    def teardown():
        # Close the test loop:
        tloop.close()

    request.addfinalizer(teardown)
    return tloop


@pytest.fixture(scope='function')
def tmp_dir_path(request):
    """"""
    Give a path for a temporary directory
    The directory is destroyed at the end of the test.
    """"""
    # Temporary directory.
    tmp_dir = tempfile.mkdtemp()

    def teardown():
        # Delete the whole directory:
        shutil.rmtree(tmp_dir)

    request.addfinalizer(teardown)
    return tmp_dir


def test_access_root_of_static_handler(tloop,tmp_dir_path):
    """"""
    Tests the operation of static file server.
    Try to access the root of static file server, and make
    sure that a proper not found error is returned.
    """"""
    # Put a file inside tmp_dir_path:
    my_file_path = os.path.join(tmp_dir_path,'my_file')
    with open(my_file_path,'w') as fw:
        fw.write('hello')

    asyncio.set_event_loop(None)
    app = web.Application(loop=tloop)
    # Register global static route:
    app.router.add_static('/', tmp_dir_path)

    @asyncio.coroutine
    def inner_cor():
        handler = app.make_handler()
        srv = yield from tloop.create_server(handler,\
                SERVER_HOST,SERVER_PORT ,reuse_address=True) 

        # Request the root of the static directory.
        # Expect an 404 error page.
        url = 'http://{}:{}/'.format(\
                SERVER_HOST,SERVER_PORT) 

        r = ( yield from aiohttp.get(url,loop=tloop) )
        assert r.status == 404
        # data = (yield from r.read())
        yield from r.release()

        srv.close()
        yield from srv.wait_closed()

        yield from app.shutdown()
        yield from handler.finish_connections(10.0)
        yield from app.cleanup()


    run_timeout(inner_cor(),tloop,timeout=5)

```"
4812103b4d9be418aecdc64341fb32be7865f113,core/backends/IUnikernelBackend.py,core/backends/IUnikernelBackend.py,,"from abc import ABCMeta, abstractmethod


class IUnikernelBackend(object):
    """"""
    Interface that must be implemented by every Unikernel Backend. It contains method stubs used by the REST API
    provider and other components.

    Redefinition of functions decorated with @asbstractmethod is compulsory.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def register(self, _id):
        """"""
        Initialize directory structure for the unikernel, and register it to the database and scheduler.
        :param _id: ID of the unikernel
        :return: None
        """"""
        pass

    @abstractmethod
    def configure(self, _id):
        """"""
        Configure the unikernel to be built for the specific backend
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def compile(self, _id):
        """"""
        Build the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def optimize(self, _id):
        """"""
        Optimize the unikernel binary/VM by stripping off debug symbols / applying data compression, etc.
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def start(self, _id):
        """"""
        Launch/boot the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def get_status(self, _id):
        """"""
        Get status of the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def get_log(self, _id):
        """"""
        Get runtime log of the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def stop(self, _id):
        """"""
        Kill execution of the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def destroy(self, _id):
        """"""
        Destroy the unikernel, remove all assets, and unregister from database and scheduler.
        :param _id: ID of the unikernel
        :return:
        """"""
        pass
",Add interface for unikernel backends to implement,"Add interface for unikernel backends to implement
",Python,apache-2.0,"onyb/dune,adyasha/dune,adyasha/dune,adyasha/dune",93,"```python
from abc import ABCMeta, abstractmethod


class IUnikernelBackend(object):
    """"""
    Interface that must be implemented by every Unikernel Backend. It contains method stubs used by the REST API
    provider and other components.

    Redefinition of functions decorated with @asbstractmethod is compulsory.
    """"""
    __metaclass__ = ABCMeta

    @abstractmethod
    def register(self, _id):
        """"""
        Initialize directory structure for the unikernel, and register it to the database and scheduler.
        :param _id: ID of the unikernel
        :return: None
        """"""
        pass

    @abstractmethod
    def configure(self, _id):
        """"""
        Configure the unikernel to be built for the specific backend
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def compile(self, _id):
        """"""
        Build the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def optimize(self, _id):
        """"""
        Optimize the unikernel binary/VM by stripping off debug symbols / applying data compression, etc.
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def start(self, _id):
        """"""
        Launch/boot the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def get_status(self, _id):
        """"""
        Get status of the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def get_log(self, _id):
        """"""
        Get runtime log of the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def stop(self, _id):
        """"""
        Kill execution of the unikernel
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

    @abstractmethod
    def destroy(self, _id):
        """"""
        Destroy the unikernel, remove all assets, and unregister from database and scheduler.
        :param _id: ID of the unikernel
        :return:
        """"""
        pass

```"
46c64c4612417a16fb8027c39e252f5d4a44378b,mindbender/maya/tests/test_workflow.py,mindbender/maya/tests/test_workflow.py,,"""""""Integration tests

These tests include external libraries in order to test
the integration between them.

""""""

import os
import sys
import shutil
import tempfile

from maya import cmds

import pyblish_maya
import pyblish.api
import pyblish.util

from mindbender import api, maya

from nose.tools import (
    assert_equals,
    with_setup
)

self = sys.modules[__name__]
self.tempdir = None


def setup():
    pyblish_maya.setup()
    api.install(maya)

    self.tempdir = tempfile.mkdtemp()

    assetdir = os.path.join(
        self.tempdir,
        ""assets"",
        ""Test""
    )

    os.makedirs(assetdir)

    api.register_root(self.tempdir)
    assert api.registered_root() == self.tempdir
    api.register_silo(""assets"")

    # Setup environment
    os.environ[""ASSETDIR""] = assetdir
    os.environ[""MINDBENDER_SILO""] = ""assets""


def teardown():
    pyblish_maya.teardown()
    api.uninstall()

    shutil.rmtree(self.tempdir)


def clear():
    pass


def test_setup():
    """"""Fixture is setup ok""""""
    assert_equals(next(api.ls())[""name""], ""Test"")


@with_setup(clear)
def test_modeling():
    """"""Modeling workflow is functional""""""

    cmds.file(new=True, force=True)

    transform, generator = cmds.polyCube(name=""body_PLY"")
    group = cmds.group(transform, name=""ROOT"")

    cmds.select(group, replace=True)
    maya.create(""modelDefault"",
                family=""mindbender.model"",
                options={""useSelection"": True})

    # Comply with save validator
    cmds.file(rename=""temp.ma"")
    cmds.file(save=True)

    # Comply with ID validator
    cmds.addAttr(transform, longName=""mbID"", dataType=""string"")

    pyblish.util.publish()

    asset = next(api.ls())
    assert_equals(asset[""name""], ""Test"")

    subset = asset[""subsets""][0]
    assert_equals(subset[""name""], ""modelDefault"")

    version = subset[""versions""][0]
    assert_equals(version[""version""], 1)

    representation = version[""representations""][0]
    assert_equals(representation[""format""], "".ma"")
","Add initial workflow test. This is an example of how workflow can be automatically tested. In this case, the modeling workflow is tested. If something is done here that isn't valid, and is caught during validation, this test would fail.","Add initial workflow test.
This is an example of how workflow can be automatically tested. In this case, the modeling workflow is tested. If something is done here that isn't valid, and is caught during validation, this test would fail.

Update this test whenever the modeling workflow changes, and keep an eye out for when it *hasn't* changed, but the test still fails. It may mean there has been an accidental breakage of a valid workflow.
",Python,mit,"getavalon/core,MoonShineVFX/core,mindbender-studio/core,MoonShineVFX/core,mindbender-studio/core,getavalon/core",103,"```python
""""""Integration tests

These tests include external libraries in order to test
the integration between them.

""""""

import os
import sys
import shutil
import tempfile

from maya import cmds

import pyblish_maya
import pyblish.api
import pyblish.util

from mindbender import api, maya

from nose.tools import (
    assert_equals,
    with_setup
)

self = sys.modules[__name__]
self.tempdir = None


def setup():
    pyblish_maya.setup()
    api.install(maya)

    self.tempdir = tempfile.mkdtemp()

    assetdir = os.path.join(
        self.tempdir,
        ""assets"",
        ""Test""
    )

    os.makedirs(assetdir)

    api.register_root(self.tempdir)
    assert api.registered_root() == self.tempdir
    api.register_silo(""assets"")

    # Setup environment
    os.environ[""ASSETDIR""] = assetdir
    os.environ[""MINDBENDER_SILO""] = ""assets""


def teardown():
    pyblish_maya.teardown()
    api.uninstall()

    shutil.rmtree(self.tempdir)


def clear():
    pass


def test_setup():
    """"""Fixture is setup ok""""""
    assert_equals(next(api.ls())[""name""], ""Test"")


@with_setup(clear)
def test_modeling():
    """"""Modeling workflow is functional""""""

    cmds.file(new=True, force=True)

    transform, generator = cmds.polyCube(name=""body_PLY"")
    group = cmds.group(transform, name=""ROOT"")

    cmds.select(group, replace=True)
    maya.create(""modelDefault"",
                family=""mindbender.model"",
                options={""useSelection"": True})

    # Comply with save validator
    cmds.file(rename=""temp.ma"")
    cmds.file(save=True)

    # Comply with ID validator
    cmds.addAttr(transform, longName=""mbID"", dataType=""string"")

    pyblish.util.publish()

    asset = next(api.ls())
    assert_equals(asset[""name""], ""Test"")

    subset = asset[""subsets""][0]
    assert_equals(subset[""name""], ""modelDefault"")

    version = subset[""versions""][0]
    assert_equals(version[""version""], 1)

    representation = version[""representations""][0]
    assert_equals(representation[""format""], "".ma"")

```"
801c8c7463811af88f232e23d8496180d7b413ad,python/extract_duplicate_sets.py,python/extract_duplicate_sets.py,,"""""""
Copyright 2016 Ronald J. Nowling

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
""""""

import argparse
from collections import defaultdict
import json
import sys

def extract_duplicates(query_results):
    # Every record entry has a field ""tm_field_dupe_of"" which points
    # to the ""entity_id"" field of another article. We need to group
    # the articles by their parent

    duplicate_sets = defaultdict(set)
    
    for doc in query_results[""response""][""docs""]:
        id = doc[""entity_id""]
        
        
        duplicate_sets[id].add(id)
        for duplicate_of in doc[""tm_field_dupe_of""]:
            duplicate_sets[id].add(duplicate_of)

    return duplicate_sets.values()

def merge_duplicates(duplicate_sets):
    merged_sets = []

    for duplicate_set in duplicate_sets:
        was_merged = False
        for other_duplicate_set in merged_sets:
            if len(duplicate_set.intersection(other_duplicate_set)) > 0:
                other_duplicate_set.update(duplicate_set)
                was_merged = True
        if not was_merged:
            merged_sets.append(duplicate_set)

    return merged_sets

def run(args):
    input_fl = open(args[""input""])
    query_results = json.load(input_fl)
    input_fl.close()
    
    duplicate_sets = extract_duplicates(query_results)

    print len(duplicate_sets), ""duplicate sets""

    while True:
        merged_duplicate_sets = merge_duplicates(duplicate_sets)
        if len(duplicate_sets) == len(merged_duplicate_sets):
            break
        print len(merged_duplicate_sets), ""duplicate sets after merge""
        duplicate_sets = merged_duplicate_sets

    duplicate_lists = []
    for duplicate_set in duplicate_sets:
        duplicate_lists.append(list(duplicate_set))

    output_fl = open(args[""output""], ""w"")
    json.dump(duplicate_lists, output_fl)
    output_fl.close()
        

def parseargs():
    parser = argparse.ArgumentParser(description=""Extract sets of duplicate articles from SOLR query dump"")

    parser.add_argument(""--input"", type=str, required=True, help=""SOLR query JSON dump"")
    parser.add_argument(""--output"", type=str, required=True, help=""Duplicate set JSON output"")

    return vars(parser.parse_args())

if __name__ == ""__main__"":
    args = parseargs()

    run(args)

",Add script for extracting duplicate sets from SOLR dump,"Add script for extracting duplicate sets from SOLR dump
",Python,apache-2.0,"rnowling/article-deduplication,rnowling/article-deduplication",91,"```python
""""""
Copyright 2016 Ronald J. Nowling

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
""""""

import argparse
from collections import defaultdict
import json
import sys

def extract_duplicates(query_results):
    # Every record entry has a field ""tm_field_dupe_of"" which points
    # to the ""entity_id"" field of another article. We need to group
    # the articles by their parent

    duplicate_sets = defaultdict(set)
    
    for doc in query_results[""response""][""docs""]:
        id = doc[""entity_id""]
        
        
        duplicate_sets[id].add(id)
        for duplicate_of in doc[""tm_field_dupe_of""]:
            duplicate_sets[id].add(duplicate_of)

    return duplicate_sets.values()

def merge_duplicates(duplicate_sets):
    merged_sets = []

    for duplicate_set in duplicate_sets:
        was_merged = False
        for other_duplicate_set in merged_sets:
            if len(duplicate_set.intersection(other_duplicate_set)) > 0:
                other_duplicate_set.update(duplicate_set)
                was_merged = True
        if not was_merged:
            merged_sets.append(duplicate_set)

    return merged_sets

def run(args):
    input_fl = open(args[""input""])
    query_results = json.load(input_fl)
    input_fl.close()
    
    duplicate_sets = extract_duplicates(query_results)

    print len(duplicate_sets), ""duplicate sets""

    while True:
        merged_duplicate_sets = merge_duplicates(duplicate_sets)
        if len(duplicate_sets) == len(merged_duplicate_sets):
            break
        print len(merged_duplicate_sets), ""duplicate sets after merge""
        duplicate_sets = merged_duplicate_sets

    duplicate_lists = []
    for duplicate_set in duplicate_sets:
        duplicate_lists.append(list(duplicate_set))

    output_fl = open(args[""output""], ""w"")
    json.dump(duplicate_lists, output_fl)
    output_fl.close()
        

def parseargs():
    parser = argparse.ArgumentParser(description=""Extract sets of duplicate articles from SOLR query dump"")

    parser.add_argument(""--input"", type=str, required=True, help=""SOLR query JSON dump"")
    parser.add_argument(""--output"", type=str, required=True, help=""Duplicate set JSON output"")

    return vars(parser.parse_args())

if __name__ == ""__main__"":
    args = parseargs()

    run(args)


```"
c9ce5f7eafcfc3d15c9ff3d7c72b44d0192fd452,tests/test_handlers.py,tests/test_handlers.py,,"from __future__ import print_function
import unittest

import teres
import teres.handlers
import logging
import os.path
import shutil
import StringIO
import tempfile


class LoggingHandlerSetUp(unittest.TestCase):
    def setUp(self):
        self.reporter = teres.Reporter.get_reporter()

        self.logger = logging.getLogger(""test.logger"")
        self.loghan_path = ""/tmp/logging_handler_test.log""
        self.loghan = logging.FileHandler(self.loghan_path,
                                          mode='w')
        self.loghan.setLevel(logging.DEBUG)
        self.logger.addHandler(self.loghan)

        self.handler = teres.handlers.LoggingHandler(""logginghandler.test"",
                                                     self.logger,
                                                     dest=""/tmp/"")

        self.reporter.add_handler(self.handler)

    def tearDown(self):
        teres.Reporter.drop_reporter()
#        shutil.rmtree(self.handler.logdir)


class LoggingHandlerTest(LoggingHandlerSetUp):
    def test_log_ordinary_file_simple(self):
        test = ""test_log_ordinary_file""
        text = ""This is my log file.""

        src_file = ""/tmp/test log file""
        fd = open(src_file, ""w"")
        fd.write(text)
        fd.close()

        self.reporter.send_file(src_file)

        # Check the result.
        self.assertTrue(os.path.isdir(self.handler.logdir))

        tgt = open(""{}/{}"".format(self.handler.logdir, ""test_log_file""))
        content = tgt.read()
        tgt.close()

        self.assertEqual(content, text)
        os.remove(src_file)

    def test_log_stringio_file(self):
        test = ""test_log_stringio_file""
        text = ""This is my stringio file.""

        src_file = StringIO.StringIO(text)

        self.reporter.send_file(src_file, logname=test)

        # Check the result.
        self.assertTrue(os.path.isdir(self.handler.logdir))

        tgt = open(""{}/{}"".format(self.handler.logdir, test))
        content = tgt.read()
        tgt.close()

        self.assertEqual(content, text)

    def test_log_temp_file(self):
        test = ""test_log_temp_file""
        text = ""This is my temporary file.""

        src_file = tempfile.TemporaryFile()
        src_file.write(text)

        self.reporter.send_file(src_file, logname=test)
        src_file.close()

        # Check the result.
        self.assertTrue(os.path.isdir(self.handler.logdir))

        tgt = open(""{}/{}"".format(self.handler.logdir, test))
        content = tgt.read()
        tgt.close()

        self.assertEqual(content, text)
",Add basic tests for LoggingHandler.,"Add basic tests for LoggingHandler.
",Python,lgpl-2.1,"tlamer/teres,tlamer/teres",92,"```python
from __future__ import print_function
import unittest

import teres
import teres.handlers
import logging
import os.path
import shutil
import StringIO
import tempfile


class LoggingHandlerSetUp(unittest.TestCase):
    def setUp(self):
        self.reporter = teres.Reporter.get_reporter()

        self.logger = logging.getLogger(""test.logger"")
        self.loghan_path = ""/tmp/logging_handler_test.log""
        self.loghan = logging.FileHandler(self.loghan_path,
                                          mode='w')
        self.loghan.setLevel(logging.DEBUG)
        self.logger.addHandler(self.loghan)

        self.handler = teres.handlers.LoggingHandler(""logginghandler.test"",
                                                     self.logger,
                                                     dest=""/tmp/"")

        self.reporter.add_handler(self.handler)

    def tearDown(self):
        teres.Reporter.drop_reporter()
#        shutil.rmtree(self.handler.logdir)


class LoggingHandlerTest(LoggingHandlerSetUp):
    def test_log_ordinary_file_simple(self):
        test = ""test_log_ordinary_file""
        text = ""This is my log file.""

        src_file = ""/tmp/test log file""
        fd = open(src_file, ""w"")
        fd.write(text)
        fd.close()

        self.reporter.send_file(src_file)

        # Check the result.
        self.assertTrue(os.path.isdir(self.handler.logdir))

        tgt = open(""{}/{}"".format(self.handler.logdir, ""test_log_file""))
        content = tgt.read()
        tgt.close()

        self.assertEqual(content, text)
        os.remove(src_file)

    def test_log_stringio_file(self):
        test = ""test_log_stringio_file""
        text = ""This is my stringio file.""

        src_file = StringIO.StringIO(text)

        self.reporter.send_file(src_file, logname=test)

        # Check the result.
        self.assertTrue(os.path.isdir(self.handler.logdir))

        tgt = open(""{}/{}"".format(self.handler.logdir, test))
        content = tgt.read()
        tgt.close()

        self.assertEqual(content, text)

    def test_log_temp_file(self):
        test = ""test_log_temp_file""
        text = ""This is my temporary file.""

        src_file = tempfile.TemporaryFile()
        src_file.write(text)

        self.reporter.send_file(src_file, logname=test)
        src_file.close()

        # Check the result.
        self.assertTrue(os.path.isdir(self.handler.logdir))

        tgt = open(""{}/{}"".format(self.handler.logdir, test))
        content = tgt.read()
        tgt.close()

        self.assertEqual(content, text)

```"
8ac380905f969eab9be64cc97d8e5ec4d7c53e26,sorting/merge_sort.py,sorting/merge_sort.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''
Merge Sort.

Best, Average, Worst: O(nlogn).
'''


def merge_sort(array, result, left, right, order):

    if right - left < 2:
        return

    if (right - left) == 2:
        if order == 'asc':
            if result[left] > result[right-1]:
                result[left], result[right-1] = result[right-1], result[left]
        else:
            if result[left] < result[right-1]:
                result[left], result[right-1] = result[right-1], result[left]
        return

    mid = (right + left + 1) / 2
    merge_sort(result, array, left, mid, order)
    merge_sort(result, array, mid, right, order)

    i = left
    j = mid
    if order == 'asc':
        for x in xrange(left, right):
            if i >= mid or (j < right and array[i] > array[j]):
                result[x] = array[j]
                j += 1
            else:
                result[x] = array[i]
                i += 1
    else:
        for x in xrange(left, right):
            if i >= mid or (j < right and array[i] < array[j]):
                result[x] = array[j]
                j += 1
            else:
                result[x] = array[i]
                i += 1


def sort(array, order='asc'):
    '''
    In-place sort array use merge sort algorithm in ascending or descending
    order.

    Return the sorted array.
    '''

    if not array:
        raise Exception('No element to sort.')

    copy = list(array)

    merge_sort(copy, array, 0, len(array), order)

    return array


if __name__ == '__main__':

    import random
    from argparse import ArgumentParser

    parser = ArgumentParser(description='Sort array use Merge Sort algorithm.')
    parser.add_argument('random', type=int, help='max random number count')
    parser.add_argument('--order', type=str, default='asc', choices=['asc', 'desc'],
            help='sort in ascending or descending.')

    args = parser.parse_args()

    # to avoid 'ValueError(""sample larger than population"")' when call
    # random.sample().
    r_end = 1000
    if args.random >= r_end:
        r_end = args.random + 10

    randoms = random.sample(xrange(1, r_end), args.random)

    print 'before sort:\t', randoms
    sort(randoms, args.order)
    print 'after sort:\t', randoms
",Implement the merge sort algorithm.,"Implement the merge sort algorithm.
",Python,mit,"weichen2046/algorithm-study,weichen2046/algorithm-study",90,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''
Merge Sort.

Best, Average, Worst: O(nlogn).
'''


def merge_sort(array, result, left, right, order):

    if right - left < 2:
        return

    if (right - left) == 2:
        if order == 'asc':
            if result[left] > result[right-1]:
                result[left], result[right-1] = result[right-1], result[left]
        else:
            if result[left] < result[right-1]:
                result[left], result[right-1] = result[right-1], result[left]
        return

    mid = (right + left + 1) / 2
    merge_sort(result, array, left, mid, order)
    merge_sort(result, array, mid, right, order)

    i = left
    j = mid
    if order == 'asc':
        for x in xrange(left, right):
            if i >= mid or (j < right and array[i] > array[j]):
                result[x] = array[j]
                j += 1
            else:
                result[x] = array[i]
                i += 1
    else:
        for x in xrange(left, right):
            if i >= mid or (j < right and array[i] < array[j]):
                result[x] = array[j]
                j += 1
            else:
                result[x] = array[i]
                i += 1


def sort(array, order='asc'):
    '''
    In-place sort array use merge sort algorithm in ascending or descending
    order.

    Return the sorted array.
    '''

    if not array:
        raise Exception('No element to sort.')

    copy = list(array)

    merge_sort(copy, array, 0, len(array), order)

    return array


if __name__ == '__main__':

    import random
    from argparse import ArgumentParser

    parser = ArgumentParser(description='Sort array use Merge Sort algorithm.')
    parser.add_argument('random', type=int, help='max random number count')
    parser.add_argument('--order', type=str, default='asc', choices=['asc', 'desc'],
            help='sort in ascending or descending.')

    args = parser.parse_args()

    # to avoid 'ValueError(""sample larger than population"")' when call
    # random.sample().
    r_end = 1000
    if args.random >= r_end:
        r_end = args.random + 10

    randoms = random.sample(xrange(1, r_end), args.random)

    print 'before sort:\t', randoms
    sort(randoms, args.order)
    print 'after sort:\t', randoms

```"
068a273d859a082c153acee4192a34df0b1ce4ea,scripts/update-readme-with-pack-list.py,scripts/update-readme-with-pack-list.py,,"#!/usr/bin/env python

""""""
Script which updates README.md with a list of all the available packs.
""""""

import os
import copy
import argparse

import yaml

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PACKS_DIR = os.path.join(CURRENT_DIR, '../packs')
README_PATH = os.path.join(CURRENT_DIR, '../README.md')

BASE_URL = 'https://github.com/StackStorm/st2contrib/tree/master/packs'


def get_pack_list():
    packs = os.listdir(PACKS_DIR)
    packs = sorted(packs)
    return packs


def get_pack_metadata(pack):
    metadata_path = os.path.join(PACKS_DIR, pack, 'pack.yaml')
    with open(metadata_path, 'r') as fp:
        content = fp.read()

    metadata = yaml.safe_load(content)
    return metadata


def generate_pack_list_table(packs):
    lines = []

    lines.append('Name | Description | Author | Latest Version')
    lines.append('---- | ----------- | ------ | -------------- ')

    for pack_name, metadata in packs:
        values = copy.deepcopy(metadata)
        values['base_url'] = BASE_URL
        line = '| [%(name)s](%(base_url)s/%(name)s) | %(description)s | %(author)s | %(version)s' % (values)
        lines.append(line)

    result = '\n'.join(lines)
    return result


def get_updated_readme(table):
    with open(README_PATH, 'r') as fp:
        current_readme = fp.read()

    head = current_readme.split('## Available Packs\n\n')[0]
    tail = current_readme.split('## License, and Contributors Agreement')[1]

    replacement = '## Available Packs\n\n'
    replacement += table + '\n\n'
    replacement += '## License, and Contributors Agreement'
    updated_readme = head + replacement + tail
    return updated_readme


def main(dry_run):
    packs = get_pack_list()

    packs_with_metadata = []
    for pack in packs:
        try:
            metadata = get_pack_metadata(pack=pack)
        except IOError:
            continue

        packs_with_metadata.append((pack, metadata))
    table = generate_pack_list_table(packs=packs_with_metadata)
    updated_readme = get_updated_readme(table=table)

    if dry_run:
        print(updated_readme)
    else:
        with open(README_PATH, 'w') as fp:
            fp.write(updated_readme)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='')
    parser.add_argument('--dry-run', help='Print the new readme to stdout',
                        action='store_true', default=False)

    args = parser.parse_args()
    main(dry_run=args.dry_run)
",Add script for upating readme with auto-generated pack list.,"Add script for upating readme with auto-generated pack list.
",Python,apache-2.0,"pidah/st2contrib,tonybaloney/st2contrib,lmEshoo/st2contrib,pearsontechnology/st2contrib,meirwah/st2contrib,jtopjian/st2contrib,lmEshoo/st2contrib,pearsontechnology/st2contrib,Aamir-raza-1/st2contrib,jtopjian/st2contrib,pearsontechnology/st2contrib,armab/st2contrib,digideskio/st2contrib,pinterb/st2contrib,tonybaloney/st2contrib,StackStorm/st2contrib,tonybaloney/st2contrib,armab/st2contrib,Aamir-raza-1/st2contrib,lakshmi-kannan/st2contrib,pearsontechnology/st2contrib,psychopenguin/st2contrib,pidah/st2contrib,digideskio/st2contrib,meirwah/st2contrib,psychopenguin/st2contrib,pidah/st2contrib,lakshmi-kannan/st2contrib,dennybaa/st2contrib,dennybaa/st2contrib,StackStorm/st2contrib,pinterb/st2contrib,armab/st2contrib,StackStorm/st2contrib",93,"```python
#!/usr/bin/env python

""""""
Script which updates README.md with a list of all the available packs.
""""""

import os
import copy
import argparse

import yaml

CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PACKS_DIR = os.path.join(CURRENT_DIR, '../packs')
README_PATH = os.path.join(CURRENT_DIR, '../README.md')

BASE_URL = 'https://github.com/StackStorm/st2contrib/tree/master/packs'


def get_pack_list():
    packs = os.listdir(PACKS_DIR)
    packs = sorted(packs)
    return packs


def get_pack_metadata(pack):
    metadata_path = os.path.join(PACKS_DIR, pack, 'pack.yaml')
    with open(metadata_path, 'r') as fp:
        content = fp.read()

    metadata = yaml.safe_load(content)
    return metadata


def generate_pack_list_table(packs):
    lines = []

    lines.append('Name | Description | Author | Latest Version')
    lines.append('---- | ----------- | ------ | -------------- ')

    for pack_name, metadata in packs:
        values = copy.deepcopy(metadata)
        values['base_url'] = BASE_URL
        line = '| [%(name)s](%(base_url)s/%(name)s) | %(description)s | %(author)s | %(version)s' % (values)
        lines.append(line)

    result = '\n'.join(lines)
    return result


def get_updated_readme(table):
    with open(README_PATH, 'r') as fp:
        current_readme = fp.read()

    head = current_readme.split('## Available Packs\n\n')[0]
    tail = current_readme.split('## License, and Contributors Agreement')[1]

    replacement = '## Available Packs\n\n'
    replacement += table + '\n\n'
    replacement += '## License, and Contributors Agreement'
    updated_readme = head + replacement + tail
    return updated_readme


def main(dry_run):
    packs = get_pack_list()

    packs_with_metadata = []
    for pack in packs:
        try:
            metadata = get_pack_metadata(pack=pack)
        except IOError:
            continue

        packs_with_metadata.append((pack, metadata))
    table = generate_pack_list_table(packs=packs_with_metadata)
    updated_readme = get_updated_readme(table=table)

    if dry_run:
        print(updated_readme)
    else:
        with open(README_PATH, 'w') as fp:
            fp.write(updated_readme)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='')
    parser.add_argument('--dry-run', help='Print the new readme to stdout',
                        action='store_true', default=False)

    args = parser.parse_args()
    main(dry_run=args.dry_run)

```"
6863a9526af42f2c7615abe22f3a6eb40b759da2,sorting/quick_sort.py,sorting/quick_sort.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''
Quick Sort. It is an application of divide and conquer strategy.

Best, Average: O(nlogn), Worst: O(n^2).
'''


def partion(array, left, right, pivot, order):

    p = left
    pv = array[pivot]

    array[right], array[pivot] = array[pivot], array[right]

    if order == 'asc':
        for i in xrange(left, right):
            if array[i] <= pv:
                if i != p:
                    array[i], array[p] = array[p], array[i]
                p += 1
    else:
        for i in xrange(left, right):
            if array[i] >= pv:
                if i != p:
                    array[i], array[p] = array[p], array[i]
                p += 1

    array[p], array[right] = array[right], array[p]

    return p


def quick_sort(array, left, right, order):

    if left >= right:
        return

    # here we always use the middle index between left and right as the pivot,
    # it is not always the best one of course.
    pivot = left + (right - left) / 2

    p = partion(array, left, right, pivot, order)

    quick_sort(array, left, p - 1, order)
    quick_sort(array, p + 1, right, order)

    return array


def sort(array, order='asc'):
    '''
    In-place sort array use quick sort algorithm in ascending or descending
    order.

    Return the sorted array.
    '''

    if not array:
        raise Exception('No element to sort.')

    n = len(array)

    return quick_sort(array, 0, n - 1, order)


if __name__ == '__main__':

    import random
    from argparse import ArgumentParser

    parser = ArgumentParser(description='Sort array use Quick Sort algorithm.')
    parser.add_argument('random', type=int, help='max random number count')
    parser.add_argument('--order', type=str, default='asc', choices=['asc', 'desc'],
            help='sort in ascending or descending.')

    args = parser.parse_args()

    # to avoid 'ValueError(""sample larger than population"")' when call
    # random.sample().
    r_end = 1000
    if args.random >= r_end:
        r_end = args.random + 10

    randoms = random.sample(xrange(1, r_end), args.random)

    print 'before sort:\t', randoms
    sort(randoms, args.order)
    print 'after sort:\t', randoms
",Implement the quick sort algorithm.,"Implement the quick sort algorithm.
",Python,mit,"weichen2046/algorithm-study,weichen2046/algorithm-study",92,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''
Quick Sort. It is an application of divide and conquer strategy.

Best, Average: O(nlogn), Worst: O(n^2).
'''


def partion(array, left, right, pivot, order):

    p = left
    pv = array[pivot]

    array[right], array[pivot] = array[pivot], array[right]

    if order == 'asc':
        for i in xrange(left, right):
            if array[i] <= pv:
                if i != p:
                    array[i], array[p] = array[p], array[i]
                p += 1
    else:
        for i in xrange(left, right):
            if array[i] >= pv:
                if i != p:
                    array[i], array[p] = array[p], array[i]
                p += 1

    array[p], array[right] = array[right], array[p]

    return p


def quick_sort(array, left, right, order):

    if left >= right:
        return

    # here we always use the middle index between left and right as the pivot,
    # it is not always the best one of course.
    pivot = left + (right - left) / 2

    p = partion(array, left, right, pivot, order)

    quick_sort(array, left, p - 1, order)
    quick_sort(array, p + 1, right, order)

    return array


def sort(array, order='asc'):
    '''
    In-place sort array use quick sort algorithm in ascending or descending
    order.

    Return the sorted array.
    '''

    if not array:
        raise Exception('No element to sort.')

    n = len(array)

    return quick_sort(array, 0, n - 1, order)


if __name__ == '__main__':

    import random
    from argparse import ArgumentParser

    parser = ArgumentParser(description='Sort array use Quick Sort algorithm.')
    parser.add_argument('random', type=int, help='max random number count')
    parser.add_argument('--order', type=str, default='asc', choices=['asc', 'desc'],
            help='sort in ascending or descending.')

    args = parser.parse_args()

    # to avoid 'ValueError(""sample larger than population"")' when call
    # random.sample().
    r_end = 1000
    if args.random >= r_end:
        r_end = args.random + 10

    randoms = random.sample(xrange(1, r_end), args.random)

    print 'before sort:\t', randoms
    sort(randoms, args.order)
    print 'after sort:\t', randoms

```"
1984fbe9a59eb5de3c8e9af477ac277a978cf21d,evaluation/packages/displacementKernels.py,evaluation/packages/displacementKernels.py,,"""""""@package DisplacementKernels
This module defines an interface to read and use InputGen projects

See C++ InputGen project for more details on Projects
""""""
from decimal import Decimal


class DisplacementKernel(object):
    """"""Python representation of the C++ class AbstractDisplacementKernel
    """"""
    def __init__(self, name, typeId, enabled):
        self.name    = name
        self.typeId  = typeId
        self.enabled = enabled

    def getName(self):
        return self.name

    def getType(self):
        return self.typeId

    def isEnabled(self):
        return self.enabled

    def __str__(self):
        return ""%s displacement kernel (enabled=%s)"" % (self.name, self.enabled)

class UniformRandomDisplacementKernel(DisplacementKernel):
    """"""Python representation of the C++ class UniformRandomDisplacementKernel
    """"""
    def __init__(self, rangeMin, rangeMax, enabled):
        super(UniformRandomDisplacementKernel, self).__init__(""Random (Uniform)"", 0, enabled)

        self.rangeMin = rangeMin
        self.rangeMax = rangeMax

    def __init__(self, paramList, enabled):
        super(UniformRandomDisplacementKernel, self).__init__(""Random (Uniform)"", 0, enabled)

        self.rangeMin = paramList[0]
        self.rangeMax = paramList[1]

class NormalRandomDisplacementKernel(DisplacementKernel):
    """"""Python representation of the C++ class NormalRandomDisplacementKernel
    """"""
    def __init__(self, mean, stdev, enabled):
        super(NormalRandomDisplacementKernel, self).__init__(""Random (Normal)"", 1, enabled)         

        self.mean  = mean
        self.stdev = stdev

    def __init__(self, paramList, enabled):
        super(NormalRandomDisplacementKernel, self).__init__(""Random (Normal)"", 1, enabled)         

        self.mean  = paramList[0]
        self.stdev = paramList[1]

class BiasDisplacementKernel(DisplacementKernel):
    """"""Python representation of the C++ class BiasDisplacementKernel
    """"""
    def __init__(self, bias, enabled):
        super(BiasDisplacementKernel, self).__init__(""Bias"", 2, enabled)         

        self.bias  = bias

""""""Factory to generate displacement kernels from its typeId
    \param paramArray Array generated when parsing the xml file
""""""
def generateDisplacementKernel(paramArray):
    def UniformRandomDisplacementParam(paramArray):
        return float(paramArray['distributionMin']), float(paramArray['distributionMax'])
    def NormalRandomDisplacementParam(paramArray):
        return float(paramArray['distributionStdDev']), float(paramArray['distributionMean'])
    def BiasDisplacementParam(paramArray):
        return float(paramArray['bias'])

    def isKernelEnabled(paramArray):
        return paramArray['enabled'] != '0'

    factory = { '0': UniformRandomDisplacementKernel,
        '1': NormalRandomDisplacementKernel,
        '2': BiasDisplacementKernel }
    paramfactory = { '0': UniformRandomDisplacementParam,
        '1': NormalRandomDisplacementParam ,
        '2': BiasDisplacementParam }

    typeId = paramArray['typeId']

    return factory[typeId](paramfactory[typeId](paramArray), isKernelEnabled(paramArray))

",Define kernel classes and a factory to generate them from xml,"Define kernel classes and a factory to generate them from xml
",Python,apache-2.0,"amonszpart/globOpt,NUAAXXY/globOpt,NUAAXXY/globOpt,amonszpart/globOpt,NUAAXXY/globOpt,amonszpart/globOpt,amonszpart/globOpt,NUAAXXY/globOpt,amonszpart/globOpt,NUAAXXY/globOpt,amonszpart/globOpt,NUAAXXY/globOpt",92,"```python
""""""@package DisplacementKernels
This module defines an interface to read and use InputGen projects

See C++ InputGen project for more details on Projects
""""""
from decimal import Decimal


class DisplacementKernel(object):
    """"""Python representation of the C++ class AbstractDisplacementKernel
    """"""
    def __init__(self, name, typeId, enabled):
        self.name    = name
        self.typeId  = typeId
        self.enabled = enabled

    def getName(self):
        return self.name

    def getType(self):
        return self.typeId

    def isEnabled(self):
        return self.enabled

    def __str__(self):
        return ""%s displacement kernel (enabled=%s)"" % (self.name, self.enabled)

class UniformRandomDisplacementKernel(DisplacementKernel):
    """"""Python representation of the C++ class UniformRandomDisplacementKernel
    """"""
    def __init__(self, rangeMin, rangeMax, enabled):
        super(UniformRandomDisplacementKernel, self).__init__(""Random (Uniform)"", 0, enabled)

        self.rangeMin = rangeMin
        self.rangeMax = rangeMax

    def __init__(self, paramList, enabled):
        super(UniformRandomDisplacementKernel, self).__init__(""Random (Uniform)"", 0, enabled)

        self.rangeMin = paramList[0]
        self.rangeMax = paramList[1]

class NormalRandomDisplacementKernel(DisplacementKernel):
    """"""Python representation of the C++ class NormalRandomDisplacementKernel
    """"""
    def __init__(self, mean, stdev, enabled):
        super(NormalRandomDisplacementKernel, self).__init__(""Random (Normal)"", 1, enabled)         

        self.mean  = mean
        self.stdev = stdev

    def __init__(self, paramList, enabled):
        super(NormalRandomDisplacementKernel, self).__init__(""Random (Normal)"", 1, enabled)         

        self.mean  = paramList[0]
        self.stdev = paramList[1]

class BiasDisplacementKernel(DisplacementKernel):
    """"""Python representation of the C++ class BiasDisplacementKernel
    """"""
    def __init__(self, bias, enabled):
        super(BiasDisplacementKernel, self).__init__(""Bias"", 2, enabled)         

        self.bias  = bias

""""""Factory to generate displacement kernels from its typeId
    \param paramArray Array generated when parsing the xml file
""""""
def generateDisplacementKernel(paramArray):
    def UniformRandomDisplacementParam(paramArray):
        return float(paramArray['distributionMin']), float(paramArray['distributionMax'])
    def NormalRandomDisplacementParam(paramArray):
        return float(paramArray['distributionStdDev']), float(paramArray['distributionMean'])
    def BiasDisplacementParam(paramArray):
        return float(paramArray['bias'])

    def isKernelEnabled(paramArray):
        return paramArray['enabled'] != '0'

    factory = { '0': UniformRandomDisplacementKernel,
        '1': NormalRandomDisplacementKernel,
        '2': BiasDisplacementKernel }
    paramfactory = { '0': UniformRandomDisplacementParam,
        '1': NormalRandomDisplacementParam ,
        '2': BiasDisplacementParam }

    typeId = paramArray['typeId']

    return factory[typeId](paramfactory[typeId](paramArray), isKernelEnabled(paramArray))


```"
cd5a846e82ec023a1d69ff832924493e8dfc3068,tests/api_tests/base/test_utils.py,tests/api_tests/base/test_utils.py,,"from nose.tools import *  # noqa

from api.base import utils as api_utils

from tests.base import ApiTestCase

class DummyAttrAttr(object):

    def __init__(self, key):
        self.key = key


class DummyAttr(object):

    def __init__(self, key):
        self.key = key
        self.attr_attr = DummyAttrAttr(key.upper())


class Dummy(object):

    def __init__(self, key):
        self.attr = DummyAttr(key)
        self.hash = {
            'bang': DummyAttr(key)
        }


class APIUtilsTestCase(ApiTestCase):

    def setUp(self):

        self.dummy = Dummy('foo')
        self.data = {
            'foo': {
                'bar': 'baz'
            }
        }

    def test_deep_get_object(self):

        attr = api_utils.deep_get(self.dummy, 'attr')
        assert_true(isinstance(attr, DummyAttr))
        assert_equal(attr.key, 'foo')

    def test_deep_get_object_multiple_depth(self):

        attr_attr = api_utils.deep_get(self.dummy, 'attr.attr_attr')
        assert_true(isinstance(attr_attr, DummyAttrAttr))
        assert_equal(attr_attr.key, 'FOO')

    def test_deep_get_dict(self):

        foo = api_utils.deep_get(self.data, 'foo')
        assert_true(isinstance(foo, dict))
        assert_equal(foo, {
            'bar': 'baz'
        })

    def test_deep_get_dict_multiple_depth(self):

        bar = api_utils.deep_get(self.data, 'foo.bar')
        assert_true(isinstance(bar, str))
        assert_equal(bar, 'baz')

    def test_deep_get_object_and_dict(self):

        hash_bang_attr = api_utils.deep_get(self.dummy, 'hash.bang.attr_attr')
        assert_true(isinstance(hash_bang_attr, DummyAttrAttr))
        assert_equal(hash_bang_attr.key, 'FOO')

    def test_deep_get_key_not_found(self):

        hash_bang_attr = api_utils.deep_get(self.dummy, 'hash.bang.baz')
        assert_equal(hash_bang_attr, None)

    def test_soft_get_object(self):

        attr = api_utils.soft_get(self.dummy, 'attr')
        assert_equal(attr.key, 'foo')

    def test_soft_get_object_not_found(self):

        bat = api_utils.soft_get(self.dummy, 'bat')
        assert_equal(bat, None)

    def test_soft_get_dict(self):

        foo = api_utils.soft_get(self.data, 'foo')
        assert_equal(foo, {
            'bar': 'baz'
        })

    def test_soft_get_dict_not_found(self):

        bat = api_utils.soft_get(self.data, 'bat')
        assert_equal(bat, None)
",Add unit tests for deep_get and soft_get api utils,"Add unit tests for deep_get and soft_get api utils
",Python,apache-2.0,"mluo613/osf.io,kwierman/osf.io,leb2dg/osf.io,samchrisinger/osf.io,Nesiehr/osf.io,Nesiehr/osf.io,rdhyee/osf.io,laurenrevere/osf.io,kch8qx/osf.io,zamattiac/osf.io,hmoco/osf.io,asanfilippo7/osf.io,zachjanicki/osf.io,hmoco/osf.io,brandonPurvis/osf.io,leb2dg/osf.io,GageGaskins/osf.io,kch8qx/osf.io,abought/osf.io,pattisdr/osf.io,abought/osf.io,brandonPurvis/osf.io,billyhunt/osf.io,kwierman/osf.io,Ghalko/osf.io,TomHeatwole/osf.io,zachjanicki/osf.io,hmoco/osf.io,amyshi188/osf.io,baylee-d/osf.io,danielneis/osf.io,GageGaskins/osf.io,billyhunt/osf.io,aaxelb/osf.io,icereval/osf.io,chrisseto/osf.io,samanehsan/osf.io,zachjanicki/osf.io,danielneis/osf.io,monikagrabowska/osf.io,mluke93/osf.io,mfraezz/osf.io,CenterForOpenScience/osf.io,wearpants/osf.io,asanfilippo7/osf.io,laurenrevere/osf.io,cslzchen/osf.io,felliott/osf.io,erinspace/osf.io,Johnetordoff/osf.io,DanielSBrown/osf.io,alexschiller/osf.io,emetsger/osf.io,icereval/osf.io,ticklemepierce/osf.io,leb2dg/osf.io,brianjgeiger/osf.io,caneruguz/osf.io,caseyrollins/osf.io,RomanZWang/osf.io,crcresearch/osf.io,samchrisinger/osf.io,brianjgeiger/osf.io,pattisdr/osf.io,mluke93/osf.io,mattclark/osf.io,zamattiac/osf.io,binoculars/osf.io,RomanZWang/osf.io,laurenrevere/osf.io,brianjgeiger/osf.io,caseyrollins/osf.io,rdhyee/osf.io,crcresearch/osf.io,sloria/osf.io,billyhunt/osf.io,aaxelb/osf.io,acshi/osf.io,KAsante95/osf.io,asanfilippo7/osf.io,binoculars/osf.io,KAsante95/osf.io,GageGaskins/osf.io,mfraezz/osf.io,mluo613/osf.io,RomanZWang/osf.io,wearpants/osf.io,acshi/osf.io,HalcyonChimera/osf.io,cslzchen/osf.io,amyshi188/osf.io,erinspace/osf.io,ticklemepierce/osf.io,saradbowman/osf.io,HalcyonChimera/osf.io,baylee-d/osf.io,crcresearch/osf.io,kch8qx/osf.io,mluo613/osf.io,jnayak1/osf.io,leb2dg/osf.io,acshi/osf.io,doublebits/osf.io,samanehsan/osf.io,brianjgeiger/osf.io,Nesiehr/osf.io,TomBaxter/osf.io,chrisseto/osf.io,HalcyonChimera/osf.io,cwisecarver/osf.io,baylee-d/osf.io,emetsger/osf.io,aaxelb/osf.io,zamattiac/osf.io,mattclark/osf.io,chennan47/osf.io,adlius/osf.io,danielneis/osf.io,jnayak1/osf.io,mluo613/osf.io,kch8qx/osf.io,alexschiller/osf.io,erinspace/osf.io,cslzchen/osf.io,mluke93/osf.io,CenterForOpenScience/osf.io,amyshi188/osf.io,brandonPurvis/osf.io,SSJohns/osf.io,SSJohns/osf.io,felliott/osf.io,jnayak1/osf.io,monikagrabowska/osf.io,abought/osf.io,sloria/osf.io,felliott/osf.io,monikagrabowska/osf.io,acshi/osf.io,rdhyee/osf.io,Ghalko/osf.io,zachjanicki/osf.io,alexschiller/osf.io,chrisseto/osf.io,TomBaxter/osf.io,KAsante95/osf.io,wearpants/osf.io,adlius/osf.io,brandonPurvis/osf.io,samanehsan/osf.io,kwierman/osf.io,zamattiac/osf.io,emetsger/osf.io,caneruguz/osf.io,monikagrabowska/osf.io,abought/osf.io,ticklemepierce/osf.io,TomBaxter/osf.io,cwisecarver/osf.io,mluke93/osf.io,mattclark/osf.io,samchrisinger/osf.io,cwisecarver/osf.io,KAsante95/osf.io,Johnetordoff/osf.io,alexschiller/osf.io,samchrisinger/osf.io,GageGaskins/osf.io,HalcyonChimera/osf.io,acshi/osf.io,Ghalko/osf.io,amyshi188/osf.io,cwisecarver/osf.io,jnayak1/osf.io,DanielSBrown/osf.io,hmoco/osf.io,emetsger/osf.io,cslzchen/osf.io,mfraezz/osf.io,felliott/osf.io,Ghalko/osf.io,monikagrabowska/osf.io,mluo613/osf.io,GageGaskins/osf.io,doublebits/osf.io,chrisseto/osf.io,RomanZWang/osf.io,doublebits/osf.io,sloria/osf.io,pattisdr/osf.io,caseyrollins/osf.io,alexschiller/osf.io,danielneis/osf.io,kch8qx/osf.io,Nesiehr/osf.io,asanfilippo7/osf.io,kwierman/osf.io,billyhunt/osf.io,SSJohns/osf.io,SSJohns/osf.io,CenterForOpenScience/osf.io,samanehsan/osf.io,TomHeatwole/osf.io,brandonPurvis/osf.io,RomanZWang/osf.io,doublebits/osf.io,TomHeatwole/osf.io,chennan47/osf.io,chennan47/osf.io,wearpants/osf.io,saradbowman/osf.io,adlius/osf.io,Johnetordoff/osf.io,rdhyee/osf.io,ticklemepierce/osf.io,DanielSBrown/osf.io,CenterForOpenScience/osf.io,billyhunt/osf.io,icereval/osf.io,aaxelb/osf.io,Johnetordoff/osf.io,adlius/osf.io,doublebits/osf.io,binoculars/osf.io,KAsante95/osf.io,caneruguz/osf.io,caneruguz/osf.io,TomHeatwole/osf.io,mfraezz/osf.io,DanielSBrown/osf.io",98,"```python
from nose.tools import *  # noqa

from api.base import utils as api_utils

from tests.base import ApiTestCase

class DummyAttrAttr(object):

    def __init__(self, key):
        self.key = key


class DummyAttr(object):

    def __init__(self, key):
        self.key = key
        self.attr_attr = DummyAttrAttr(key.upper())


class Dummy(object):

    def __init__(self, key):
        self.attr = DummyAttr(key)
        self.hash = {
            'bang': DummyAttr(key)
        }


class APIUtilsTestCase(ApiTestCase):

    def setUp(self):

        self.dummy = Dummy('foo')
        self.data = {
            'foo': {
                'bar': 'baz'
            }
        }

    def test_deep_get_object(self):

        attr = api_utils.deep_get(self.dummy, 'attr')
        assert_true(isinstance(attr, DummyAttr))
        assert_equal(attr.key, 'foo')

    def test_deep_get_object_multiple_depth(self):

        attr_attr = api_utils.deep_get(self.dummy, 'attr.attr_attr')
        assert_true(isinstance(attr_attr, DummyAttrAttr))
        assert_equal(attr_attr.key, 'FOO')

    def test_deep_get_dict(self):

        foo = api_utils.deep_get(self.data, 'foo')
        assert_true(isinstance(foo, dict))
        assert_equal(foo, {
            'bar': 'baz'
        })

    def test_deep_get_dict_multiple_depth(self):

        bar = api_utils.deep_get(self.data, 'foo.bar')
        assert_true(isinstance(bar, str))
        assert_equal(bar, 'baz')

    def test_deep_get_object_and_dict(self):

        hash_bang_attr = api_utils.deep_get(self.dummy, 'hash.bang.attr_attr')
        assert_true(isinstance(hash_bang_attr, DummyAttrAttr))
        assert_equal(hash_bang_attr.key, 'FOO')

    def test_deep_get_key_not_found(self):

        hash_bang_attr = api_utils.deep_get(self.dummy, 'hash.bang.baz')
        assert_equal(hash_bang_attr, None)

    def test_soft_get_object(self):

        attr = api_utils.soft_get(self.dummy, 'attr')
        assert_equal(attr.key, 'foo')

    def test_soft_get_object_not_found(self):

        bat = api_utils.soft_get(self.dummy, 'bat')
        assert_equal(bat, None)

    def test_soft_get_dict(self):

        foo = api_utils.soft_get(self.data, 'foo')
        assert_equal(foo, {
            'bar': 'baz'
        })

    def test_soft_get_dict_not_found(self):

        bat = api_utils.soft_get(self.data, 'bat')
        assert_equal(bat, None)

```"
c27f7651a933fa831f936ca90fd25ede3f79aa50,pythran/analyses/ast_matcher.py,pythran/analyses/ast_matcher.py,,""""""" Module to looks for a specified pattern in a given AST. """"""

from ast import AST, iter_fields
import ast


class AST_no_cond(object):

    """""" Class to specify we don't care about a field value in ast. """"""


def match(node, pattern):
    """"""
    Check matching between an ast.Node and a pattern.

    AST_no_cond permit to specify we don't care about a field value
    """"""
    if node.__class__.__name__ != pattern.__class__.__name__:
        return False
    zipped_nodes = zip(iter_fields(node), iter_fields(pattern))
    for (_, value_n), (_, value_p) in zipped_nodes:
        if isinstance(value_p, AST_no_cond):
            # We don't check this field
            continue
        elif type(value_n) != type(value_p):
            # Type mismatch, so node differ
            return False

        bad_list_matching = (isinstance(value_n, list) and
                             not list_matching(value_n, value_p))
        bad_ast_matching = (isinstance(value_n, AST) and
                            not match(value_n, value_p))
        bad_matching = value_n != value_p

        # If values are not matching list, matching ast node or equal value
        # pattern and node differ
        if bad_list_matching and bad_ast_matching and bad_matching:
            return False
    return True


def list_matching(node_list, pattern_list):
    """""" Check matching between nodes and patterns of ast.Node. """"""
    if len(node_list) != len(pattern_list):
        return False
    for item_n, item_p in zip(node_list, pattern_list):
        if isinstance(item_p, AST_no_cond):
            continue
        elif isinstance(item_n, AST) and not match(item_n, item_p):
            return False
        elif item_n != item_p:
            return False
    return True


class ASTMatcher(ast.NodeVisitor):

    """"""
    Visitor to gather node matching with a given pattern.

    Examples
    --------
    >>> import ast
    >>> code = ""[(i, j) for i in xrange(a) for j in xrange(b)]""
    >>> pattern = ast.Call(func=ast.Name(id='xrange', ctx=ast.Load()),
    ...                    args=AST_no_cond(), keywords=[],
    ...                    starargs=None, kwargs=None)
    >>> len(ASTMatcher(pattern).get(ast.parse(code)))
    2
    """"""

    def __init__(self, pattern):
        """""" Basic initialiser saving pattern and initialising result set. """"""
        self.pattern = pattern
        self.result = set()

    def visit(self, node):
        """"""
        Visitor looking for matching between current node and pattern.

        If it match, save it but whatever happen, keep going.
        """"""
        if match(node, self.pattern):
            self.result.add(node)
        self.generic_visit(node)

    def get(self, node):
        """""" Facility to get values of the matcher for a given node. """"""
        self.visit(node)
        return self.result
",Add ast matcher for pattern recognition,"Add ast matcher for pattern recognition
",Python,bsd-3-clause,"artas360/pythran,pbrunet/pythran,pbrunet/pythran,hainm/pythran,artas360/pythran,artas360/pythran,hainm/pythran,pombredanne/pythran,pombredanne/pythran,serge-sans-paille/pythran,pombredanne/pythran,hainm/pythran,serge-sans-paille/pythran,pbrunet/pythran",91,"```python
"""""" Module to looks for a specified pattern in a given AST. """"""

from ast import AST, iter_fields
import ast


class AST_no_cond(object):

    """""" Class to specify we don't care about a field value in ast. """"""


def match(node, pattern):
    """"""
    Check matching between an ast.Node and a pattern.

    AST_no_cond permit to specify we don't care about a field value
    """"""
    if node.__class__.__name__ != pattern.__class__.__name__:
        return False
    zipped_nodes = zip(iter_fields(node), iter_fields(pattern))
    for (_, value_n), (_, value_p) in zipped_nodes:
        if isinstance(value_p, AST_no_cond):
            # We don't check this field
            continue
        elif type(value_n) != type(value_p):
            # Type mismatch, so node differ
            return False

        bad_list_matching = (isinstance(value_n, list) and
                             not list_matching(value_n, value_p))
        bad_ast_matching = (isinstance(value_n, AST) and
                            not match(value_n, value_p))
        bad_matching = value_n != value_p

        # If values are not matching list, matching ast node or equal value
        # pattern and node differ
        if bad_list_matching and bad_ast_matching and bad_matching:
            return False
    return True


def list_matching(node_list, pattern_list):
    """""" Check matching between nodes and patterns of ast.Node. """"""
    if len(node_list) != len(pattern_list):
        return False
    for item_n, item_p in zip(node_list, pattern_list):
        if isinstance(item_p, AST_no_cond):
            continue
        elif isinstance(item_n, AST) and not match(item_n, item_p):
            return False
        elif item_n != item_p:
            return False
    return True


class ASTMatcher(ast.NodeVisitor):

    """"""
    Visitor to gather node matching with a given pattern.

    Examples
    --------
    >>> import ast
    >>> code = ""[(i, j) for i in xrange(a) for j in xrange(b)]""
    >>> pattern = ast.Call(func=ast.Name(id='xrange', ctx=ast.Load()),
    ...                    args=AST_no_cond(), keywords=[],
    ...                    starargs=None, kwargs=None)
    >>> len(ASTMatcher(pattern).get(ast.parse(code)))
    2
    """"""

    def __init__(self, pattern):
        """""" Basic initialiser saving pattern and initialising result set. """"""
        self.pattern = pattern
        self.result = set()

    def visit(self, node):
        """"""
        Visitor looking for matching between current node and pattern.

        If it match, save it but whatever happen, keep going.
        """"""
        if match(node, self.pattern):
            self.result.add(node)
        self.generic_visit(node)

    def get(self, node):
        """""" Facility to get values of the matcher for a given node. """"""
        self.visit(node)
        return self.result

```"
4233919f94296e1e4986345d876b3f2ebf14e0e4,unix_rpc.py,unix_rpc.py,,"import socket
import struct
import json
import os

RPC_VERSION = 1

def msg_send(sock, msg):
    j = json.dumps(msg)
    header = struct.pack('!LL', RPC_VERSION, len(j))
    sock.sendall(b'%b%b' % (header, j.encode('utf-8')))

def msg_recv(sock):
    buff = b''
    while len(buff) < 8:
        buff += sock.recv(1024)

    (version, length) = struct.unpack('!LL', buff[0:8])
    assert RPC_VERSION == version

    buff = buff[8:]

    while len(buff) < length:
        buff += sock.recv(1024)

    assert len(buff) == length

    return json.loads(buff.decode('utf-8'))

class UnknownRPCError(Exception):
    pass

class RPCError(Exception):
    pass

class RPC(object):
    def __init__(self, sock, method):
        self.sock = sock
        self.method = method

    def __call__(self, *args, **kwargs):
        msg_send(self.sock, ['rpc', self.method, args, kwargs])
        [msg_type, msg] = msg_recv(self.sock)
        assert msg_type in ['return', 'error']
        if msg_type == 'error':
            if msg == 'UnknownRPCError':
                raise UnknownRPCError
            raise RPCError

        return msg


class Server(object):
    def __init__(self, path):
        self.path = path
        self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.socket.bind(self.path)
        self.socket.listen(1)
        self.funs = {}

    def register(self, name, function):
        self.funs[name] = function

    def start(self):
        try:
            while True:
                conn, _ = self.socket.accept()
                [msg_type, method, args, kwargs] = msg_recv(conn)
                assert msg_type == 'rpc'
                if method not in self.funs:
                    msg_send(conn, ['error', 'UnknownRPCError'])
                else:
                    try:
                        ret = self.funs[method](*args, **kwargs)
                        msg_send(conn, ['return', ret])
                    except Exception as e:
                        try:
                            msg_send(conn, ['error', str(e)])
                        except:
                            pass
        finally:
            try:
                if os.path.exists(self.path):
                    os.remove(self.path)
            except: # pylint: disable=bare-except
                pass

class Client(object):
    def __init__(self, path):
        self.__path = path
        self.__socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.__socket.connect(self.__path)

    def __getattr__(self, method):
        return RPC(self.__socket, method)

    def fileno(self):
        return self.__socket.fileno()

    def read(self):
        return self.__socket.recv(1024)
",Add module for executing RPCs over UNIX sockets,"Add module for executing RPCs over UNIX sockets
",Python,apache-2.0,sagelywizard/dlex,102,"```python
import socket
import struct
import json
import os

RPC_VERSION = 1

def msg_send(sock, msg):
    j = json.dumps(msg)
    header = struct.pack('!LL', RPC_VERSION, len(j))
    sock.sendall(b'%b%b' % (header, j.encode('utf-8')))

def msg_recv(sock):
    buff = b''
    while len(buff) < 8:
        buff += sock.recv(1024)

    (version, length) = struct.unpack('!LL', buff[0:8])
    assert RPC_VERSION == version

    buff = buff[8:]

    while len(buff) < length:
        buff += sock.recv(1024)

    assert len(buff) == length

    return json.loads(buff.decode('utf-8'))

class UnknownRPCError(Exception):
    pass

class RPCError(Exception):
    pass

class RPC(object):
    def __init__(self, sock, method):
        self.sock = sock
        self.method = method

    def __call__(self, *args, **kwargs):
        msg_send(self.sock, ['rpc', self.method, args, kwargs])
        [msg_type, msg] = msg_recv(self.sock)
        assert msg_type in ['return', 'error']
        if msg_type == 'error':
            if msg == 'UnknownRPCError':
                raise UnknownRPCError
            raise RPCError

        return msg


class Server(object):
    def __init__(self, path):
        self.path = path
        self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.socket.bind(self.path)
        self.socket.listen(1)
        self.funs = {}

    def register(self, name, function):
        self.funs[name] = function

    def start(self):
        try:
            while True:
                conn, _ = self.socket.accept()
                [msg_type, method, args, kwargs] = msg_recv(conn)
                assert msg_type == 'rpc'
                if method not in self.funs:
                    msg_send(conn, ['error', 'UnknownRPCError'])
                else:
                    try:
                        ret = self.funs[method](*args, **kwargs)
                        msg_send(conn, ['return', ret])
                    except Exception as e:
                        try:
                            msg_send(conn, ['error', str(e)])
                        except:
                            pass
        finally:
            try:
                if os.path.exists(self.path):
                    os.remove(self.path)
            except: # pylint: disable=bare-except
                pass

class Client(object):
    def __init__(self, path):
        self.__path = path
        self.__socket = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        self.__socket.connect(self.__path)

    def __getattr__(self, method):
        return RPC(self.__socket, method)

    def fileno(self):
        return self.__socket.fileno()

    def read(self):
        return self.__socket.recv(1024)

```"
0ea6f45792a3e1ce321f774a8582171fe4d39ad0,demo/python-overview/intro.py,demo/python-overview/intro.py,,"# Hello world
print('Hello, world!')


# Indentation is important in Python!
x = 1
if x == 1:
    print('x is 1')


# Numbers and Strings
my_int = 8
print(my_int)

my_float = 1.4
print(my_float)

a, b, c = 1, 2, 3
print(a, b, c)


my_string = 'hello'
print(my_string)

my_string = ""hello""
print(my_string)


# Operators
one = 1
two = 2
three = one + two

print(one < two)
print(one > two)
print(two > one)
print(three == 3)

helloworld = 'hello' + ' ' + 'world'
print(helloworld)

print(one + helloworld)


# Lists
mylist = []
mylist.append(1)
mylist.append(2)
mylist.append(3)

print(mylist)
print(mylist[0])
print(mylist[1])
print(mylist[2])

print(mylist[3]) # IndexError

print(len(mylist))

print(0 in mylist)
print(4 not in mylist)

x, y, z = mylist
print(x)
print(y)
print(z)


# Looping over lists
for item in mylist:
    print(item)

mylist = [1, ""Alice"", True, ['Bill']]


# More Operators
print(7 + 3.0)      # addition
print(2 ** 3)       # exponents
print(3 - 4)        # subtraction
print(3 / 2)        # division
print(3 // 2)       # integer division
print(14 % 12)      # remainder. like clock arithmetic

# Operators on Strings
print('hello' + ' ' + 'world')   # concatenation
print('hello'*5)                 # repeat

# Operators on Lists
print([1, 2, 3] + [7, 8, 9])
print([1, 2, 3] * 3)

# Formatting Strings
answer = 42
print(f'the answer is {number}')

# Dictionary
ages = {'alice': 23, 'bob': 12, 'frank': 66}


# Functions
def square(x):
    return x ** 2

print(square(2))

def say_hello():
    print('hello')

x = say_hello()
print(x)

def square_multiple(a, b, c):
    return a ** 2, b ** 2, c ** 2

result = square_multiple(1, 2, 3)
print(result)
x, y, z = result

# Looping
x = 10
while x > 0:
    print(x)
    x -= 1
",Add basic python for notes,"Add basic python for notes
",Python,mit,"HackBinghamton/club,HackBinghamton/club,HackBinghamton/club",124,"```python
# Hello world
print('Hello, world!')


# Indentation is important in Python!
x = 1
if x == 1:
    print('x is 1')


# Numbers and Strings
my_int = 8
print(my_int)

my_float = 1.4
print(my_float)

a, b, c = 1, 2, 3
print(a, b, c)


my_string = 'hello'
print(my_string)

my_string = ""hello""
print(my_string)


# Operators
one = 1
two = 2
three = one + two

print(one < two)
print(one > two)
print(two > one)
print(three == 3)

helloworld = 'hello' + ' ' + 'world'
print(helloworld)

print(one + helloworld)


# Lists
mylist = []
mylist.append(1)
mylist.append(2)
mylist.append(3)

print(mylist)
print(mylist[0])
print(mylist[1])
print(mylist[2])

print(mylist[3]) # IndexError

print(len(mylist))

print(0 in mylist)
print(4 not in mylist)

x, y, z = mylist
print(x)
print(y)
print(z)


# Looping over lists
for item in mylist:
    print(item)

mylist = [1, ""Alice"", True, ['Bill']]


# More Operators
print(7 + 3.0)      # addition
print(2 ** 3)       # exponents
print(3 - 4)        # subtraction
print(3 / 2)        # division
print(3 // 2)       # integer division
print(14 % 12)      # remainder. like clock arithmetic

# Operators on Strings
print('hello' + ' ' + 'world')   # concatenation
print('hello'*5)                 # repeat

# Operators on Lists
print([1, 2, 3] + [7, 8, 9])
print([1, 2, 3] * 3)

# Formatting Strings
answer = 42
print(f'the answer is {number}')

# Dictionary
ages = {'alice': 23, 'bob': 12, 'frank': 66}


# Functions
def square(x):
    return x ** 2

print(square(2))

def say_hello():
    print('hello')

x = say_hello()
print(x)

def square_multiple(a, b, c):
    return a ** 2, b ** 2, c ** 2

result = square_multiple(1, 2, 3)
print(result)
x, y, z = result

# Looping
x = 10
while x > 0:
    print(x)
    x -= 1

```"
aa2b62a19c8a988f2582a5e13475322e6efb4880,src/collectors/hbase/hbase.py,src/collectors/hbase/hbase.py,,"# coding=utf-8

""""""
Diamond collector for HBase metrics, see:
""""""

from diamond.metric import Metric
import diamond.collector
import glob
import re
import os


class HBaseCollector(diamond.collector.Collector):

    re_log = re.compile(r'^(?P<timestamp>\d+) (?P<name>\S+): (?P<metrics>.*)$')

    def get_default_config_help(self):
        config_help = super(HBaseCollector, self).get_default_config_help()
        config_help.update({
            'metrics': ""List of paths to process metrics from"",
        })
        return config_help

    def get_default_config(self):
        """"""
        Returns the default collector settings
        """"""
        config = super(HBaseCollector, self).get_default_config()
        config.update({
            'path':     'hbase',
            'method':   'Threaded',
            'metrics':  ['/var/log/hbase/*.metrics'],
        })
        return config

    def collect(self):
        for pattern in self.config['metrics']:
            for filename in glob.glob(pattern):
                self.collect_from(filename)

    def collect_from(self, filename):
        if not os.access(filename, os.R_OK):
            self.log.error('HBaseCollector unable to read ""%s""', filename)
            return False

        fd = open(filename, 'r+')
        for line in fd:
            match = self.re_log.match(line)
            if not match:
                continue

            metrics = {}

            data = match.groupdict()
            for metric in data['metrics'].split(','):
                metric = metric.strip()
                if '=' in metric:
                    key, value = metric.split('=', 1)
                    metrics[key] = value

            for metric in metrics.keys():
                try:

                    if data['name'] == 'jvm.metrics':
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['processName'].replace(' ', '_'),
                            metric, ]))

                    elif data['name'] == 'mapred.job':
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['group'].replace(' ', '_'),
                            metrics['counter'].replace(' ', '_'),
                            metric, ]))

                    elif data['name'] == 'rpc.metrics':

                        if metric == 'port':
                            continue

                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['port'],
                            metric, ]))

                    else:
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metric, ]))

                    value = float(metrics[metric])

                    self.publish_metric(Metric(path,
                                        value,
                                        timestamp=int(data['timestamp'])/1000))

                except ValueError:
                    pass
        fd.seek(0)
        fd.truncate()
        fd.close()
",Add a HBase metrics collector.,"Add a HBase metrics collector.

This change adds a basic hbase metrics collector that works in the same
way that the hadoop collector does.
",Python,mit,"thardie/Diamond,python-diamond/Diamond,CYBERBUGJR/Diamond,Ssawa/Diamond,acquia/Diamond,Ormod/Diamond,acquia/Diamond,szibis/Diamond,Ssawa/Diamond,hvnsweeting/Diamond,actmd/Diamond,jumping/Diamond,saucelabs/Diamond,works-mobile/Diamond,signalfx/Diamond,signalfx/Diamond,stuartbfox/Diamond,russss/Diamond,CYBERBUGJR/Diamond,jriguera/Diamond,anandbhoraskar/Diamond,skbkontur/Diamond,Slach/Diamond,CYBERBUGJR/Diamond,stuartbfox/Diamond,TAKEALOT/Diamond,sebbrandt87/Diamond,signalfx/Diamond,socialwareinc/Diamond,EzyInsights/Diamond,rtoma/Diamond,acquia/Diamond,cannium/Diamond,python-diamond/Diamond,tusharmakkar08/Diamond,TAKEALOT/Diamond,Ensighten/Diamond,jumping/Diamond,dcsquared13/Diamond,janisz/Diamond-1,h00dy/Diamond,tuenti/Diamond,bmhatfield/Diamond,codepython/Diamond,saucelabs/Diamond,MichaelDoyle/Diamond,works-mobile/Diamond,saucelabs/Diamond,jriguera/Diamond,Clever/Diamond,Netuitive/Diamond,Ensighten/Diamond,hamelg/Diamond,Basis/Diamond,Ssawa/Diamond,mfriedenhagen/Diamond,jumping/Diamond,MichaelDoyle/Diamond,rtoma/Diamond,skbkontur/Diamond,timchenxiaoyu/Diamond,mfriedenhagen/Diamond,tellapart/Diamond,EzyInsights/Diamond,mzupan/Diamond,eMerzh/Diamond-1,Netuitive/netuitive-diamond,Ormod/Diamond,tellapart/Diamond,gg7/diamond,jumping/Diamond,python-diamond/Diamond,Precis/Diamond,Precis/Diamond,tusharmakkar08/Diamond,Basis/Diamond,joel-airspring/Diamond,actmd/Diamond,joel-airspring/Diamond,mfriedenhagen/Diamond,tusharmakkar08/Diamond,Netuitive/netuitive-diamond,cannium/Diamond,Ormod/Diamond,zoidbergwill/Diamond,sebbrandt87/Diamond,thardie/Diamond,Basis/Diamond,Ssawa/Diamond,rtoma/Diamond,rtoma/Diamond,stuartbfox/Diamond,krbaker/Diamond,jaingaurav/Diamond,tuenti/Diamond,Precis/Diamond,TAKEALOT/Diamond,Netuitive/netuitive-diamond,ramjothikumar/Diamond,timchenxiaoyu/Diamond,timchenxiaoyu/Diamond,anandbhoraskar/Diamond,h00dy/Diamond,jaingaurav/Diamond,codepython/Diamond,hamelg/Diamond,zoidbergwill/Diamond,sebbrandt87/Diamond,gg7/diamond,joel-airspring/Diamond,ramjothikumar/Diamond,Netuitive/Diamond,Nihn/Diamond-1,Slach/Diamond,sebbrandt87/Diamond,ramjothikumar/Diamond,socialwareinc/Diamond,works-mobile/Diamond,Ensighten/Diamond,jriguera/Diamond,mfriedenhagen/Diamond,socialwareinc/Diamond,bmhatfield/Diamond,disqus/Diamond,EzyInsights/Diamond,eMerzh/Diamond-1,Ensighten/Diamond,mzupan/Diamond,Clever/Diamond,tellapart/Diamond,h00dy/Diamond,TinLe/Diamond,Ormod/Diamond,TAKEALOT/Diamond,Netuitive/Diamond,anandbhoraskar/Diamond,jriguera/Diamond,tuenti/Diamond,Netuitive/netuitive-diamond,disqus/Diamond,acquia/Diamond,zoidbergwill/Diamond,eMerzh/Diamond-1,anandbhoraskar/Diamond,Nihn/Diamond-1,russss/Diamond,stuartbfox/Diamond,thardie/Diamond,Nihn/Diamond-1,janisz/Diamond-1,mzupan/Diamond,jaingaurav/Diamond,bmhatfield/Diamond,CYBERBUGJR/Diamond,russss/Diamond,cannium/Diamond,tusharmakkar08/Diamond,hvnsweeting/Diamond,saucelabs/Diamond,actmd/Diamond,tuenti/Diamond,codepython/Diamond,skbkontur/Diamond,hamelg/Diamond,hamelg/Diamond,bmhatfield/Diamond,hvnsweeting/Diamond,Clever/Diamond,hvnsweeting/Diamond,joel-airspring/Diamond,MichaelDoyle/Diamond,dcsquared13/Diamond,szibis/Diamond,disqus/Diamond,timchenxiaoyu/Diamond,signalfx/Diamond,jaingaurav/Diamond,works-mobile/Diamond,gg7/diamond,tellapart/Diamond,skbkontur/Diamond,krbaker/Diamond,TinLe/Diamond,janisz/Diamond-1,mzupan/Diamond,Clever/Diamond,actmd/Diamond,h00dy/Diamond,TinLe/Diamond,krbaker/Diamond,zoidbergwill/Diamond,eMerzh/Diamond-1,EzyInsights/Diamond,russss/Diamond,codepython/Diamond,Slach/Diamond,thardie/Diamond,gg7/diamond,szibis/Diamond,cannium/Diamond,dcsquared13/Diamond,MichaelDoyle/Diamond,szibis/Diamond,Precis/Diamond,Nihn/Diamond-1,ramjothikumar/Diamond,janisz/Diamond-1,dcsquared13/Diamond,krbaker/Diamond,Slach/Diamond,Basis/Diamond,TinLe/Diamond,Netuitive/Diamond,socialwareinc/Diamond",107,"```python
# coding=utf-8

""""""
Diamond collector for HBase metrics, see:
""""""

from diamond.metric import Metric
import diamond.collector
import glob
import re
import os


class HBaseCollector(diamond.collector.Collector):

    re_log = re.compile(r'^(?P<timestamp>\d+) (?P<name>\S+): (?P<metrics>.*)$')

    def get_default_config_help(self):
        config_help = super(HBaseCollector, self).get_default_config_help()
        config_help.update({
            'metrics': ""List of paths to process metrics from"",
        })
        return config_help

    def get_default_config(self):
        """"""
        Returns the default collector settings
        """"""
        config = super(HBaseCollector, self).get_default_config()
        config.update({
            'path':     'hbase',
            'method':   'Threaded',
            'metrics':  ['/var/log/hbase/*.metrics'],
        })
        return config

    def collect(self):
        for pattern in self.config['metrics']:
            for filename in glob.glob(pattern):
                self.collect_from(filename)

    def collect_from(self, filename):
        if not os.access(filename, os.R_OK):
            self.log.error('HBaseCollector unable to read ""%s""', filename)
            return False

        fd = open(filename, 'r+')
        for line in fd:
            match = self.re_log.match(line)
            if not match:
                continue

            metrics = {}

            data = match.groupdict()
            for metric in data['metrics'].split(','):
                metric = metric.strip()
                if '=' in metric:
                    key, value = metric.split('=', 1)
                    metrics[key] = value

            for metric in metrics.keys():
                try:

                    if data['name'] == 'jvm.metrics':
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['processName'].replace(' ', '_'),
                            metric, ]))

                    elif data['name'] == 'mapred.job':
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['group'].replace(' ', '_'),
                            metrics['counter'].replace(' ', '_'),
                            metric, ]))

                    elif data['name'] == 'rpc.metrics':

                        if metric == 'port':
                            continue

                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metrics['hostName'].replace('.', '_'),
                            metrics['port'],
                            metric, ]))

                    else:
                        path = self.get_metric_path('.'.join([
                            data['name'],
                            metric, ]))

                    value = float(metrics[metric])

                    self.publish_metric(Metric(path,
                                        value,
                                        timestamp=int(data['timestamp'])/1000))

                except ValueError:
                    pass
        fd.seek(0)
        fd.truncate()
        fd.close()

```"
16e27029f32ae0329a7da022efd06625987d1622,itunes.py,itunes.py,,"#!/usr/bin/env python3

from collections import defaultdict
from dateutil.parser import parse
from pathlib import Path
import xml.etree.ElementTree as ET

LIBRARY = Path.home() / ""Music/iTunes/iTunes Library.xml""


def plist_iter(iterable, all_dicts=False):
    a = iter(iterable)
    for k, v in zip(a, a):
        assert k.tag == ""key""
        if all_dicts:
            if v.tag != ""dict"":
                print(f""For key {k.text}, not dict but {v.tag}"")
            assert v.tag == ""dict""
        yield k.text, v


def extract_songs(tree):
    root = tree.getroot()[0]
    tracks = None
    for key, node in plist_iter(root):
        if key == ""Tracks"":
            tracks = node
    songs = []
    for key, node in plist_iter(tracks, all_dicts=True):
        is_music = False
        song = {}
        for k, n in plist_iter(node):
            if k == ""Kind"":
                if ""audio"" in n.text:
                    is_music = True
                else:
                    break
            elif k in (""Podcast"", ""Movie"", ""Audiobooks""):
                is_music = False
                break
            elif k == ""Play Count"":
                song[""play_count""] = int(n.text)
            elif k == ""Date Added"":
                song[""date_added""] = parse(n.text)
            elif k == ""Name"":
                if ""wcpe"" in n.text.lower():
                    is_music = False
                    break
                song[""name""] = n.text
            elif k == ""Album"":
                song[""album""] = n.text
            elif k == ""Artist"":
                song[""artist""] = n.text
        if is_music:
            songs.append(song)
    return songs


def make_playlists(songs):
    by_year = defaultdict(list)
    for song in songs:
        if ""date_added"" in song and ""play_count"" in song:
            by_year[song[""date_added""].year].append(song)
    for _, song_list in by_year.items():
        song_list.sort(key=lambda s: s[""play_count""], reverse=True)
    return by_year


def print_top_songs(playlists):
    for year in range(2010, 2020):
        print(f""{year}\n===="")
        for i, song in enumerate(playlists[year][:25]):
            play_count = song[""play_count""]
            name = song.get(""name"", ""Unknown name"")
            artist = song.get(""artist"", ""Unknown artist"")
            album = song.get(""album"", ""Unknown album"")
            print(f""{i+1}. [{play_count}] {name}  |  {artist}  |  {album}"")
        print(""\n\n"")


def main():
    tree = ET.parse(LIBRARY)
    songs = extract_songs(tree)
    playlists = make_playlists(songs)
    print_top_songs(playlists)


if __name__ == ""__main__"":
    main()
",Add iTunes script for making year playlists,"Add iTunes script for making year playlists
",Python,mit,"mk12/scripts,mk12/scripts,mk12/scripts,mk12/scripts",90,"```python
#!/usr/bin/env python3

from collections import defaultdict
from dateutil.parser import parse
from pathlib import Path
import xml.etree.ElementTree as ET

LIBRARY = Path.home() / ""Music/iTunes/iTunes Library.xml""


def plist_iter(iterable, all_dicts=False):
    a = iter(iterable)
    for k, v in zip(a, a):
        assert k.tag == ""key""
        if all_dicts:
            if v.tag != ""dict"":
                print(f""For key {k.text}, not dict but {v.tag}"")
            assert v.tag == ""dict""
        yield k.text, v


def extract_songs(tree):
    root = tree.getroot()[0]
    tracks = None
    for key, node in plist_iter(root):
        if key == ""Tracks"":
            tracks = node
    songs = []
    for key, node in plist_iter(tracks, all_dicts=True):
        is_music = False
        song = {}
        for k, n in plist_iter(node):
            if k == ""Kind"":
                if ""audio"" in n.text:
                    is_music = True
                else:
                    break
            elif k in (""Podcast"", ""Movie"", ""Audiobooks""):
                is_music = False
                break
            elif k == ""Play Count"":
                song[""play_count""] = int(n.text)
            elif k == ""Date Added"":
                song[""date_added""] = parse(n.text)
            elif k == ""Name"":
                if ""wcpe"" in n.text.lower():
                    is_music = False
                    break
                song[""name""] = n.text
            elif k == ""Album"":
                song[""album""] = n.text
            elif k == ""Artist"":
                song[""artist""] = n.text
        if is_music:
            songs.append(song)
    return songs


def make_playlists(songs):
    by_year = defaultdict(list)
    for song in songs:
        if ""date_added"" in song and ""play_count"" in song:
            by_year[song[""date_added""].year].append(song)
    for _, song_list in by_year.items():
        song_list.sort(key=lambda s: s[""play_count""], reverse=True)
    return by_year


def print_top_songs(playlists):
    for year in range(2010, 2020):
        print(f""{year}\n===="")
        for i, song in enumerate(playlists[year][:25]):
            play_count = song[""play_count""]
            name = song.get(""name"", ""Unknown name"")
            artist = song.get(""artist"", ""Unknown artist"")
            album = song.get(""album"", ""Unknown album"")
            print(f""{i+1}. [{play_count}] {name}  |  {artist}  |  {album}"")
        print(""\n\n"")


def main():
    tree = ET.parse(LIBRARY)
    songs = extract_songs(tree)
    playlists = make_playlists(songs)
    print_top_songs(playlists)


if __name__ == ""__main__"":
    main()

```"
7cea3bbcb844bd29ee14bb490217d990213620d0,scripts/make_changelog.py,scripts/make_changelog.py,,"#! /usr/bin/env python
from __future__ import print_function

import os
import sys
import subprocess
from collections import defaultdict

import jinja2


CHANGELOG = """"""
# Change Log
All notable changes to PyMT will be documented in this file.

The format is based on [Keep a Changelog](http://keepachangelog.com/)
and this project adheres to [Semantic Versioning](http://semver.org/).

This file was auto-generated using `scripts/make_changelog.py`.

{% for tag, sections in releases.iteritems() %}
## [{{ tag }}] {{ release_date[tag] }}
{% for section, changes in sections.iteritems() %}
### {{section}}
{% for change in changes -%}
* {{ change }}
{% endfor -%}
{% endfor -%}
{% endfor -%}
"""""".strip()

SECTIONS = ['Added', 'Changed', 'Deprecated', 'Removed', 'Fixed', 'Security']


def git_log(start=None, stop='HEAD'):
    cmd = ['git', 'log', '--first-parent', '--merges', '--topo-order',
           '--oneline']
    if start:
        cmd.append('{start}..{stop}'.format(start=start, stop=stop))

    return subprocess.check_output(cmd).strip()


def git_tag():
    return subprocess.check_output(['git', 'tag']).strip()


def git_tag_date(tag):
    return subprocess.check_output(['git', 'show', tag,
                                    '--pretty=%ci']).strip().split()[0]


def releases():
    return git_tag().split(os.linesep)


def brief(start=None, stop='HEAD'):
    changes = []
    for change in git_log(start=start, stop=stop).split(os.linesep):
        try:
            changes.append(change[change.index(' ') + 1:-1])
        except ValueError:
            pass

    return changes


def group_changes(changes):
    groups = defaultdict(list)
    for change in changes:
        if change.startswith('Merge'):
            continue

        if change.startswith('Add'):
            group = 'Added'
        elif change.startswith('Deprecate'):
            group = 'Deprecated'
        elif change.startswith('Remove'):
            group = 'Removed'
        elif change.startswith('Fix'):
            group = 'Fixed'
        elif change.startswith('Security'):
            group = 'Security'
        else:
            group = 'Changed'
        groups[group].append(change)
    return groups


def main():
    tags = releases()
    tags.append('HEAD')
    changelog = defaultdict(dict)
    release_date = dict()
    for start, stop in zip(tags[:-1], tags[1:]):
        changelog[stop] = group_changes(brief(start=start, stop=stop))
        release_date[stop] = git_tag_date(stop)

    env = jinja2.Environment(loader=jinja2.DictLoader({'changelog': CHANGELOG}))
    print(env.get_template('changelog').render(releases=changelog,
                                               release_date=release_date))


if __name__ == '__main__':
    sys.exit(main())
",Add script to auto-generate a Change Log.,"Add script to auto-generate a Change Log.
",Python,mit,"csdms/pymt,csdms/coupling,csdms/coupling",106,"```python
#! /usr/bin/env python
from __future__ import print_function

import os
import sys
import subprocess
from collections import defaultdict

import jinja2


CHANGELOG = """"""
# Change Log
All notable changes to PyMT will be documented in this file.

The format is based on [Keep a Changelog](http://keepachangelog.com/)
and this project adheres to [Semantic Versioning](http://semver.org/).

This file was auto-generated using `scripts/make_changelog.py`.

{% for tag, sections in releases.iteritems() %}
## [{{ tag }}] {{ release_date[tag] }}
{% for section, changes in sections.iteritems() %}
### {{section}}
{% for change in changes -%}
* {{ change }}
{% endfor -%}
{% endfor -%}
{% endfor -%}
"""""".strip()

SECTIONS = ['Added', 'Changed', 'Deprecated', 'Removed', 'Fixed', 'Security']


def git_log(start=None, stop='HEAD'):
    cmd = ['git', 'log', '--first-parent', '--merges', '--topo-order',
           '--oneline']
    if start:
        cmd.append('{start}..{stop}'.format(start=start, stop=stop))

    return subprocess.check_output(cmd).strip()


def git_tag():
    return subprocess.check_output(['git', 'tag']).strip()


def git_tag_date(tag):
    return subprocess.check_output(['git', 'show', tag,
                                    '--pretty=%ci']).strip().split()[0]


def releases():
    return git_tag().split(os.linesep)


def brief(start=None, stop='HEAD'):
    changes = []
    for change in git_log(start=start, stop=stop).split(os.linesep):
        try:
            changes.append(change[change.index(' ') + 1:-1])
        except ValueError:
            pass

    return changes


def group_changes(changes):
    groups = defaultdict(list)
    for change in changes:
        if change.startswith('Merge'):
            continue

        if change.startswith('Add'):
            group = 'Added'
        elif change.startswith('Deprecate'):
            group = 'Deprecated'
        elif change.startswith('Remove'):
            group = 'Removed'
        elif change.startswith('Fix'):
            group = 'Fixed'
        elif change.startswith('Security'):
            group = 'Security'
        else:
            group = 'Changed'
        groups[group].append(change)
    return groups


def main():
    tags = releases()
    tags.append('HEAD')
    changelog = defaultdict(dict)
    release_date = dict()
    for start, stop in zip(tags[:-1], tags[1:]):
        changelog[stop] = group_changes(brief(start=start, stop=stop))
        release_date[stop] = git_tag_date(stop)

    env = jinja2.Environment(loader=jinja2.DictLoader({'changelog': CHANGELOG}))
    print(env.get_template('changelog').render(releases=changelog,
                                               release_date=release_date))


if __name__ == '__main__':
    sys.exit(main())

```"
7ed869d5713e936ff1b71dab62ee3599ca23c884,egopowerflow/tools/pypsa_io.py,egopowerflow/tools/pypsa_io.py,,"import pypsa
import pandas as pd

from sqlalchemy.orm import sessionmaker, load_only
from sqlalchemy import create_engine
from pypsa import io
from oemof import db

from egoio.db_tables.calc_ego_mv_powerflow import Bus, Line, Generator, Load, \
    Transformer, TempResolution, BusVMagSet, GeneratorPqSet, LoadPqSet

def oedb_session():
    """"""Get SQLAlchemy session object with valid connection to OEDB""""""

    # get session object by oemof.db tools (requires .oemof/config.ini
    try:
        conn = db.connection(section='oedb')

    except:
        print('Please provide connection parameters to database:')

        host = input('host (default 127.0.0.1): ') or '127.0.0.1'
        port = input('port (default 5432): ') or '5432'
        user = input('user (default postgres): ') or 'postgres'
        database = input('database name: ')
        password = input('password: ')

        conn = create_engine(
            'postgresql://' + '%s:%s@%s:%s/%s' % (user,
                                                  password,
                                                  host,
                                                  port,
                                                  database))

    Session = sessionmaker(bind=conn)
    session = Session()
    return session


def init_pypsa_network(time_range_lim):
    """"""
    Instantiate PyPSA network

    Parameters
    ----------
    time_range_lim:
    Returns
    -------
    network: PyPSA network object
        Contains powerflow problem
    snapshots: iterable
        Contains snapshots to be analyzed by powerplow calculation
    """"""
    network = pypsa.Network()
    network.set_snapshots(time_range_lim)
    snapshots = network.snapshots

    return network, snapshots


def get_pq_sets(session, table, columns=None, index_col=None, slicer=None):
    """"""

    Parameters
    ----------
    session: SQLAlchemy sessino object
    table: SQLAlchemy orm table object
        Specified pq-sets table
    columns: list of strings
        Columns to be selected from pq-sets table (default None)
    index_col: string
        Column to set index on (default None)
    slicer: list of int's
        Slices array of time-dependent p/q-values to apply in PF (default None)

    Returns
    -------
    pq_set: pandas DataFrame
        Table with pq-Values to be applied in PF analysis
    """"""

    # retrieve table
    if columns is not None:
        pq_query = session.query(table).options(load_only(*columns))
    else:
        pq_query = session.query(table)
    pq_set = pd.read_sql_query(pq_query.statement,
                               session.bind,
                               index_col=index_col)

    # slice relevant part by given slicer
    #TODO: implement slicing of p,q-array   

    return pq_set



if __name__ == '__main__':
    session = oedb_session()

    gen_cols = ['temp_id', 'p_set', 'q_set']

    gen_pq_set = get_pq_sets(session, GeneratorPqSet, index_col='generator_id',
                             columns=gen_cols)
    print(gen_pq_set)
",Add pypsa-db-io file with first functions,"Add pypsa-db-io file with first functions
",Python,agpl-3.0,openego/ego.powerflow,106,"```python
import pypsa
import pandas as pd

from sqlalchemy.orm import sessionmaker, load_only
from sqlalchemy import create_engine
from pypsa import io
from oemof import db

from egoio.db_tables.calc_ego_mv_powerflow import Bus, Line, Generator, Load, \
    Transformer, TempResolution, BusVMagSet, GeneratorPqSet, LoadPqSet

def oedb_session():
    """"""Get SQLAlchemy session object with valid connection to OEDB""""""

    # get session object by oemof.db tools (requires .oemof/config.ini
    try:
        conn = db.connection(section='oedb')

    except:
        print('Please provide connection parameters to database:')

        host = input('host (default 127.0.0.1): ') or '127.0.0.1'
        port = input('port (default 5432): ') or '5432'
        user = input('user (default postgres): ') or 'postgres'
        database = input('database name: ')
        password = input('password: ')

        conn = create_engine(
            'postgresql://' + '%s:%s@%s:%s/%s' % (user,
                                                  password,
                                                  host,
                                                  port,
                                                  database))

    Session = sessionmaker(bind=conn)
    session = Session()
    return session


def init_pypsa_network(time_range_lim):
    """"""
    Instantiate PyPSA network

    Parameters
    ----------
    time_range_lim:
    Returns
    -------
    network: PyPSA network object
        Contains powerflow problem
    snapshots: iterable
        Contains snapshots to be analyzed by powerplow calculation
    """"""
    network = pypsa.Network()
    network.set_snapshots(time_range_lim)
    snapshots = network.snapshots

    return network, snapshots


def get_pq_sets(session, table, columns=None, index_col=None, slicer=None):
    """"""

    Parameters
    ----------
    session: SQLAlchemy sessino object
    table: SQLAlchemy orm table object
        Specified pq-sets table
    columns: list of strings
        Columns to be selected from pq-sets table (default None)
    index_col: string
        Column to set index on (default None)
    slicer: list of int's
        Slices array of time-dependent p/q-values to apply in PF (default None)

    Returns
    -------
    pq_set: pandas DataFrame
        Table with pq-Values to be applied in PF analysis
    """"""

    # retrieve table
    if columns is not None:
        pq_query = session.query(table).options(load_only(*columns))
    else:
        pq_query = session.query(table)
    pq_set = pd.read_sql_query(pq_query.statement,
                               session.bind,
                               index_col=index_col)

    # slice relevant part by given slicer
    #TODO: implement slicing of p,q-array   

    return pq_set



if __name__ == '__main__':
    session = oedb_session()

    gen_cols = ['temp_id', 'p_set', 'q_set']

    gen_pq_set = get_pq_sets(session, GeneratorPqSet, index_col='generator_id',
                             columns=gen_cols)
    print(gen_pq_set)

```"
0c669b20b021b07fde251b7b005f2877da1b7202,test/integration/ggrc/models/test_cad.py,test/integration/ggrc/models/test_cad.py,,"# Copyright (C) 2016 Google Inc.
# Licensed under http://www.apache.org/licenses/LICENSE-2.0 <see LICENSE file>

""""""Integration tests for custom attribute definitions model.""""""

from sqlalchemy.exc import IntegrityError

from ggrc import db
from ggrc import models
from integration.ggrc import TestCase


class TestCAD(TestCase):
  """"""Tests for basic functionality of cad model.""""""

  def test_setting_reserved_words(self):
    """"""Test setting any of the existing attribute names.""""""

    with self.assertRaises(ValueError):
      cad = models.CustomAttributeDefinition()
      cad.definition_type = ""Section""
      cad.title = ""title""

    with self.assertRaises(ValueError):
      cad = models.CustomAttributeDefinition()
      cad.title = ""title""
      cad.definition_type = ""Section""

    with self.assertRaises(ValueError):
      models.CustomAttributeDefinition(
          title=""title"",
          definition_type=""Assessment"",
      )

    with self.assertRaises(ValueError):
      models.CustomAttributeDefinition(
          title=""TITLE"",
          definition_type=""Program"",
      )

    with self.assertRaises(ValueError):
      models.CustomAttributeDefinition(
          title=""Secondary CONTACT"",
          definition_type=""Program"",
      )

    cad = models.CustomAttributeDefinition(
        title=""non existing title"",
        definition_type=""Program"",
    )
    self.assertEqual(cad.title, ""non existing title"")

  def test_setting_global_cad_names(self):
    """"""Test duplicates with global attribute names.""""""

    db.session.add(models.CustomAttributeDefinition(
        title=""global cad"",
        definition_type=""Section"",
        attribute_type=""Text"",
    ))
    db.session.add(models.CustomAttributeDefinition(
        title=""non existing title"",
        definition_type=""Section"",
        definition_id=1,
        attribute_type=""Text"",
    ))
    db.session.add(models.CustomAttributeDefinition(
        title=""non existing title"",
        definition_type=""Section"",
        definition_id=2,
        attribute_type=""Text"",
    ))
    db.session.commit()

    with self.assertRaises(IntegrityError):
      db.session.add(models.CustomAttributeDefinition(
          title=""non existing title"",
          definition_type=""Section"",
          definition_id=2,
          attribute_type=""Text"",
      ))
      db.session.commit()
    db.session.rollback()

    with self.assertRaises(ValueError):
      db.session.add(models.CustomAttributeDefinition(
          title=""global cad"",
          definition_type=""Section"",
          definition_id=2,
          attribute_type=""Text"",
      ))
      db.session.commit()

  def test_different_models(self):
    """"""Test unique names over on different models.""""""
    db.session.add(models.CustomAttributeDefinition(
        title=""my custom attribute title"",
        definition_type=""Section"",
        attribute_type=""Text"",
    ))
    db.session.commit()
    cad = models.CustomAttributeDefinition(
        title=""my custom attribute title"",
        definition_type=""Program"",
        attribute_type=""Text"",
    )
    self.assertEqual(cad.title, ""my custom attribute title"")
",Add unique cad name tests,"Add unique cad name tests
",Python,apache-2.0,"AleksNeStu/ggrc-core,josthkko/ggrc-core,selahssea/ggrc-core,kr41/ggrc-core,VinnieJohns/ggrc-core,edofic/ggrc-core,plamut/ggrc-core,j0gurt/ggrc-core,j0gurt/ggrc-core,edofic/ggrc-core,plamut/ggrc-core,josthkko/ggrc-core,VinnieJohns/ggrc-core,AleksNeStu/ggrc-core,andrei-karalionak/ggrc-core,selahssea/ggrc-core,AleksNeStu/ggrc-core,andrei-karalionak/ggrc-core,edofic/ggrc-core,josthkko/ggrc-core,kr41/ggrc-core,edofic/ggrc-core,josthkko/ggrc-core,kr41/ggrc-core,j0gurt/ggrc-core,andrei-karalionak/ggrc-core,VinnieJohns/ggrc-core,plamut/ggrc-core,andrei-karalionak/ggrc-core,plamut/ggrc-core,selahssea/ggrc-core,VinnieJohns/ggrc-core,selahssea/ggrc-core,j0gurt/ggrc-core,kr41/ggrc-core,AleksNeStu/ggrc-core",108,"```python
# Copyright (C) 2016 Google Inc.
# Licensed under http://www.apache.org/licenses/LICENSE-2.0 <see LICENSE file>

""""""Integration tests for custom attribute definitions model.""""""

from sqlalchemy.exc import IntegrityError

from ggrc import db
from ggrc import models
from integration.ggrc import TestCase


class TestCAD(TestCase):
  """"""Tests for basic functionality of cad model.""""""

  def test_setting_reserved_words(self):
    """"""Test setting any of the existing attribute names.""""""

    with self.assertRaises(ValueError):
      cad = models.CustomAttributeDefinition()
      cad.definition_type = ""Section""
      cad.title = ""title""

    with self.assertRaises(ValueError):
      cad = models.CustomAttributeDefinition()
      cad.title = ""title""
      cad.definition_type = ""Section""

    with self.assertRaises(ValueError):
      models.CustomAttributeDefinition(
          title=""title"",
          definition_type=""Assessment"",
      )

    with self.assertRaises(ValueError):
      models.CustomAttributeDefinition(
          title=""TITLE"",
          definition_type=""Program"",
      )

    with self.assertRaises(ValueError):
      models.CustomAttributeDefinition(
          title=""Secondary CONTACT"",
          definition_type=""Program"",
      )

    cad = models.CustomAttributeDefinition(
        title=""non existing title"",
        definition_type=""Program"",
    )
    self.assertEqual(cad.title, ""non existing title"")

  def test_setting_global_cad_names(self):
    """"""Test duplicates with global attribute names.""""""

    db.session.add(models.CustomAttributeDefinition(
        title=""global cad"",
        definition_type=""Section"",
        attribute_type=""Text"",
    ))
    db.session.add(models.CustomAttributeDefinition(
        title=""non existing title"",
        definition_type=""Section"",
        definition_id=1,
        attribute_type=""Text"",
    ))
    db.session.add(models.CustomAttributeDefinition(
        title=""non existing title"",
        definition_type=""Section"",
        definition_id=2,
        attribute_type=""Text"",
    ))
    db.session.commit()

    with self.assertRaises(IntegrityError):
      db.session.add(models.CustomAttributeDefinition(
          title=""non existing title"",
          definition_type=""Section"",
          definition_id=2,
          attribute_type=""Text"",
      ))
      db.session.commit()
    db.session.rollback()

    with self.assertRaises(ValueError):
      db.session.add(models.CustomAttributeDefinition(
          title=""global cad"",
          definition_type=""Section"",
          definition_id=2,
          attribute_type=""Text"",
      ))
      db.session.commit()

  def test_different_models(self):
    """"""Test unique names over on different models.""""""
    db.session.add(models.CustomAttributeDefinition(
        title=""my custom attribute title"",
        definition_type=""Section"",
        attribute_type=""Text"",
    ))
    db.session.commit()
    cad = models.CustomAttributeDefinition(
        title=""my custom attribute title"",
        definition_type=""Program"",
        attribute_type=""Text"",
    )
    self.assertEqual(cad.title, ""my custom attribute title"")

```"
97b1e034b8028aff4566d0dcf24d3e1d41c803e9,migrations/versions/0259_remove_service_postage.py,migrations/versions/0259_remove_service_postage.py,,"""""""

Revision ID: 0259_remove_service_postage
Revises: 0258_service_postage_nullable
Create Date: 2019-02-11 17:12:22.341599

""""""
from alembic import op
import sqlalchemy as sa


revision = '0259_remove_service_postage'
down_revision = '0258_service_postage_nullable'


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('services', 'postage')
    op.drop_column('services_history', 'postage')
    op.execute(""DELETE FROM service_permissions WHERE permission = 'choose_postage'"")
    op.execute(""DELETE FROM service_permission_types WHERE name = 'choose_postage'"")
    op.execute(
        """"""UPDATE templates_history SET postage = templates.postage
        FROM templates WHERE templates_history.id = templates.id AND templates_history.template_type = 'letter'
        AND templates_history.postage is null""""""
    )
    op.execute(""""""
        ALTER TABLE templates ADD CONSTRAINT ""chk_templates_postage""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage is not null and postage in ('first', 'second')
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates_history ADD CONSTRAINT ""chk_templates_history_postage""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage is not null and postage in ('first', 'second')
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates DROP CONSTRAINT ""chk_templates_postage_null""
    """""")
    op.execute(""""""
        ALTER TABLE templates_history DROP CONSTRAINT ""chk_templates_history_postage_null""
    """""")
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('services_history', sa.Column('postage', sa.VARCHAR(length=255), autoincrement=False, nullable=True))
    op.add_column('services', sa.Column('postage', sa.VARCHAR(length=255), autoincrement=False, nullable=False))
    op.execute(""INSERT INTO service_permission_types VALUES ('choose_postage')"")
    op.execute(""""""
        ALTER TABLE templates ADD CONSTRAINT ""chk_templates_postage_null""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage in ('first', 'second') OR
                postage is null
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates_history ADD CONSTRAINT ""chk_templates_history_postage_null""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage in ('first', 'second') OR
                postage is null
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates DROP CONSTRAINT ""chk_templates_postage""
    """""")
    op.execute(""""""
        ALTER TABLE templates_history DROP CONSTRAINT ""chk_templates_history_postage""
    """""")
    # ### end Alembic commands ###
",Remove service.postage and choose_postage permission from database,"Remove service.postage and choose_postage permission from database

Also change constraint on template postage so it cannot be null for
letters

Also add postage to all letters in template_history to respect new constraint
",Python,mit,"alphagov/notifications-api,alphagov/notifications-api",90,"```python
""""""

Revision ID: 0259_remove_service_postage
Revises: 0258_service_postage_nullable
Create Date: 2019-02-11 17:12:22.341599

""""""
from alembic import op
import sqlalchemy as sa


revision = '0259_remove_service_postage'
down_revision = '0258_service_postage_nullable'


def upgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column('services', 'postage')
    op.drop_column('services_history', 'postage')
    op.execute(""DELETE FROM service_permissions WHERE permission = 'choose_postage'"")
    op.execute(""DELETE FROM service_permission_types WHERE name = 'choose_postage'"")
    op.execute(
        """"""UPDATE templates_history SET postage = templates.postage
        FROM templates WHERE templates_history.id = templates.id AND templates_history.template_type = 'letter'
        AND templates_history.postage is null""""""
    )
    op.execute(""""""
        ALTER TABLE templates ADD CONSTRAINT ""chk_templates_postage""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage is not null and postage in ('first', 'second')
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates_history ADD CONSTRAINT ""chk_templates_history_postage""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage is not null and postage in ('first', 'second')
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates DROP CONSTRAINT ""chk_templates_postage_null""
    """""")
    op.execute(""""""
        ALTER TABLE templates_history DROP CONSTRAINT ""chk_templates_history_postage_null""
    """""")
    # ### end Alembic commands ###


def downgrade():
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column('services_history', sa.Column('postage', sa.VARCHAR(length=255), autoincrement=False, nullable=True))
    op.add_column('services', sa.Column('postage', sa.VARCHAR(length=255), autoincrement=False, nullable=False))
    op.execute(""INSERT INTO service_permission_types VALUES ('choose_postage')"")
    op.execute(""""""
        ALTER TABLE templates ADD CONSTRAINT ""chk_templates_postage_null""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage in ('first', 'second') OR
                postage is null
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates_history ADD CONSTRAINT ""chk_templates_history_postage_null""
        CHECK (
            CASE WHEN template_type = 'letter' THEN
                postage in ('first', 'second') OR
                postage is null
            ELSE
                postage is null
            END
        )
    """""")
    op.execute(""""""
        ALTER TABLE templates DROP CONSTRAINT ""chk_templates_postage""
    """""")
    op.execute(""""""
        ALTER TABLE templates_history DROP CONSTRAINT ""chk_templates_history_postage""
    """""")
    # ### end Alembic commands ###

```"
7b6081ac2918ca8a6c3400a3284129e1329c1cac,letsencrypt/client/apache_obj.py,letsencrypt/client/apache_obj.py,,"""""""Module contains classes used by the Apache Configurator.""""""

class Addr(object):
    """"""Represents an Apache VirtualHost address.""""""
    def __init__(self, addr):
        """""":param tuple addr: tuple of strings (ip, port)""""""
        self.tup = addr
        
    @classmethod
    def fromstring(cls, str_addr):
        """"""Initialize Addr from string.""""""
        tup = str_addr.partition(':')
        return cls((tup[0], tup[2]))

    def __str__(self):
        return ':'.join(self.tup)

    def __eq__(self, other):
        if isinstance(other, self.__class__):
            return self.tup == other.tup
        return False

    def set_port(self, port):
        """"""Set the port of the address.

        :param str port: new port
        """"""
        self.tup = (self.tup[0], port)

    def get_addr(self):
        """"""Return addr part of Addr object.""""""
        return self.tup[0]

    def get_port(self):
        """"""Return port.""""""
        return self.tup[1]

    def get_ssl_addr_obj(self):
        return cls((self.tup[0], ""443""))

    def get_80_addr_obj(self):
        return cls((self.tup[0], ""80""))

    def get_addr_obj(self, port):
        return cls((self.tup[0], port))

class VH(object):
    """"""Represents an Apache Virtualhost.

    :ivar str filep: file path of VH
    :ivar str path: Augeas path to virtual host
    :ivar set addrs: Virtual Host addresses (:class:`set` of :class:`Addr`)
    :ivar set names: Server names/aliases of vhost
        (:class:`list` of :class:`str`)

    :ivar bool ssl: SSLEngine on in vhost
    :ivar bool enabled: Virtual host is enabled

    """"""

    def __init__(self, filep, path, addrs, ssl, enabled, names=None):
        """"""Initialize a VH.""""""
        self.filep = filep
        self.path = path
        self.addrs = addrs
        self.names = set() if names is None else names
        self.ssl = ssl
        self.enabled = enabled

    def add_name(self, name):
        """"""Add name to vhost.""""""
        self.names.add(name)

    def __str__(self):
        return (""file: %s\n""
                ""vh_path: %s\n""
                ""addrs: %s\n""
                ""names: %s\n""
                ""ssl: %s\n""
                ""enabled: %s"" % (self.filep, self.path, self.addrs,
                                 self.names, self.ssl, self.enabled))

    def __eq__(self, other):
        if isinstance(other, self.__class__):
            return (self.filep == other.filep and self.path == other.path and
                    self.addrs == other.addrs and
                    self.names == other.names and
                    self.ssl == other.ssl and self.enabled == other.enabled)

        return False
",Move out Apache specific Objects,"Move out Apache specific Objects
",Python,apache-2.0,"mrb/letsencrypt,dietsche/letsencrypt,tdfischer/lets-encrypt-preview,hlieberman/letsencrypt,lbeltrame/letsencrypt,ruo91/letsencrypt,rutsky/letsencrypt,skynet/letsencrypt,Sveder/letsencrypt,stweil/letsencrypt,g1franc/lets-encrypt-preview,rutsky/letsencrypt,BillKeenan/lets-encrypt-preview,jtl999/certbot,Jadaw1n/letsencrypt,hsduk/lets-encrypt-preview,beermix/letsencrypt,ruo91/letsencrypt,brentdax/letsencrypt,stweil/letsencrypt,mitnk/letsencrypt,Sveder/letsencrypt,jmhodges/letsencrypt,goofwear/letsencrypt,goofwear/letsencrypt,bsmr-misc-forks/letsencrypt,VladimirTyrin/letsencrypt,modulexcite/letsencrypt,riseofthetigers/letsencrypt,fmarier/letsencrypt,Jonadabe/letsencrypt,vcavallo/letsencrypt,rlustin/letsencrypt,TheBoegl/letsencrypt,deserted/letsencrypt,jsha/letsencrypt,bsmr-misc-forks/letsencrypt,martindale/letsencrypt,PeterMosmans/letsencrypt,ahojjati/letsencrypt,modulexcite/letsencrypt,Hasimir/letsencrypt,jmaurice/letsencrypt,TheBoegl/letsencrypt,jtl999/certbot,kuba/letsencrypt,deserted/letsencrypt,BillKeenan/lets-encrypt-preview,BKreisel/letsencrypt,thanatos/lets-encrypt-preview,diracdeltas/lets-encrypt-preview,stewnorriss/letsencrypt,mitnk/letsencrypt,beermix/letsencrypt,martindale/letsencrypt,rugk/letsencrypt,mrb/letsencrypt,stewnorriss/letsencrypt,ghyde/letsencrypt,ghyde/letsencrypt,twstrike/le_for_patching,letsencrypt/letsencrypt,diracdeltas/lets-encrypt-preview,solidgoldbomb/letsencrypt,rlustin/letsencrypt,Jadaw1n/letsencrypt,sjerdo/letsencrypt,luorenjin/letsencrypt,thanatos/lets-encrypt-preview,sapics/letsencrypt,kevinlondon/letsencrypt,kevinlondon/letsencrypt,armersong/letsencrypt,digideskio/lets-encrypt-preview,luorenjin/letsencrypt,sjerdo/letsencrypt,tdfischer/lets-encrypt-preview,DavidGarciaCat/letsencrypt,piru/letsencrypt,bestwpw/letsencrypt,jmaurice/letsencrypt,xgin/letsencrypt,VladimirTyrin/letsencrypt,fmarier/letsencrypt,lmcro/letsencrypt,rugk/letsencrypt,BKreisel/letsencrypt,dietsche/letsencrypt,wteiken/letsencrypt,jmhodges/letsencrypt,armersong/letsencrypt,lmcro/letsencrypt,twstrike/le_for_patching,sapics/letsencrypt,wteiken/letsencrypt,PeterMosmans/letsencrypt,hlieberman/letsencrypt,lbeltrame/letsencrypt,vcavallo/letsencrypt,tyagi-prashant/letsencrypt,kuba/letsencrypt,DavidGarciaCat/letsencrypt,piru/letsencrypt,Bachmann1234/letsencrypt,ahojjati/letsencrypt,Bachmann1234/letsencrypt,jsha/letsencrypt,Jonadabe/letsencrypt,Hasimir/letsencrypt,digideskio/lets-encrypt-preview,xgin/letsencrypt,hsduk/lets-encrypt-preview,g1franc/lets-encrypt-preview,skynet/letsencrypt,riseofthetigers/letsencrypt,bestwpw/letsencrypt,solidgoldbomb/letsencrypt,letsencrypt/letsencrypt,tyagi-prashant/letsencrypt,brentdax/letsencrypt",91,"```python
""""""Module contains classes used by the Apache Configurator.""""""

class Addr(object):
    """"""Represents an Apache VirtualHost address.""""""
    def __init__(self, addr):
        """""":param tuple addr: tuple of strings (ip, port)""""""
        self.tup = addr
        
    @classmethod
    def fromstring(cls, str_addr):
        """"""Initialize Addr from string.""""""
        tup = str_addr.partition(':')
        return cls((tup[0], tup[2]))

    def __str__(self):
        return ':'.join(self.tup)

    def __eq__(self, other):
        if isinstance(other, self.__class__):
            return self.tup == other.tup
        return False

    def set_port(self, port):
        """"""Set the port of the address.

        :param str port: new port
        """"""
        self.tup = (self.tup[0], port)

    def get_addr(self):
        """"""Return addr part of Addr object.""""""
        return self.tup[0]

    def get_port(self):
        """"""Return port.""""""
        return self.tup[1]

    def get_ssl_addr_obj(self):
        return cls((self.tup[0], ""443""))

    def get_80_addr_obj(self):
        return cls((self.tup[0], ""80""))

    def get_addr_obj(self, port):
        return cls((self.tup[0], port))

class VH(object):
    """"""Represents an Apache Virtualhost.

    :ivar str filep: file path of VH
    :ivar str path: Augeas path to virtual host
    :ivar set addrs: Virtual Host addresses (:class:`set` of :class:`Addr`)
    :ivar set names: Server names/aliases of vhost
        (:class:`list` of :class:`str`)

    :ivar bool ssl: SSLEngine on in vhost
    :ivar bool enabled: Virtual host is enabled

    """"""

    def __init__(self, filep, path, addrs, ssl, enabled, names=None):
        """"""Initialize a VH.""""""
        self.filep = filep
        self.path = path
        self.addrs = addrs
        self.names = set() if names is None else names
        self.ssl = ssl
        self.enabled = enabled

    def add_name(self, name):
        """"""Add name to vhost.""""""
        self.names.add(name)

    def __str__(self):
        return (""file: %s\n""
                ""vh_path: %s\n""
                ""addrs: %s\n""
                ""names: %s\n""
                ""ssl: %s\n""
                ""enabled: %s"" % (self.filep, self.path, self.addrs,
                                 self.names, self.ssl, self.enabled))

    def __eq__(self, other):
        if isinstance(other, self.__class__):
            return (self.filep == other.filep and self.path == other.path and
                    self.addrs == other.addrs and
                    self.names == other.names and
                    self.ssl == other.ssl and self.enabled == other.enabled)

        return False

```"
736412dc51711b41ea45f993d80d5ae3895306e9,tests/sentry/digests/test_notifications.py,tests/sentry/digests/test_notifications.py,,"from __future__ import absolute_import

from collections import OrderedDict

from exam import fixture

from sentry.digests import Record
from sentry.digests.notifications import (
    Notification,
    event_to_record,
    rewrite_record,
    group_records,
    sort_groups,
)
from sentry.testutils import TestCase


class RewriteRecordTestCase(TestCase):
    @fixture
    def rule(self):
        return self.event.project.rule_set.all()[0]

    @fixture
    def record(self):
        return event_to_record(self.event, (self.rule,))

    def test_success(self):
        assert rewrite_record(
            self.record,
            project=self.event.project,
            groups={
                self.event.group.id: self.event.group,
            },
            rules={
                self.rule.id: self.rule,
            },
        ) == Record(
            self.record.key,
            Notification(
                self.event,
                [self.rule],
            ),
            self.record.timestamp,
        )

    def test_without_group(self):
        # If the record can't be associated with a group, it should be returned as None.
        assert rewrite_record(
            self.record,
            project=self.event.project,
            groups={},
            rules={
                self.rule.id: self.rule,
            },
        ) is None

    def test_filters_invalid_rules(self):
        # If the record can't be associated with a group, it should be returned as None.
        assert rewrite_record(
            self.record,
            project=self.event.project,
            groups={
                self.event.group.id: self.event.group,
            },
            rules={},
        ) == Record(
            self.record.key,
            Notification(self.event, []),
            self.record.timestamp,
        )


class GroupRecordsTestCase(TestCase):
    @fixture
    def rule(self):
        return self.project.rule_set.all()[0]

    def test_success(self):
        events = [self.create_event(group=self.group) for _ in xrange(3)]
        records = [Record(event.id, Notification(event, [self.rule]), event.datetime) for event in events]
        assert group_records(records) == {
            self.rule: {
                self.group: records,
            },
        }


class SortRecordsTestCase(TestCase):
    def test_success(self):
        rules = list(self.project.rule_set.all())
        groups = [self.create_group() for _ in xrange(3)]

        groups[0].event_count = 10
        groups[0].user_count = 4

        groups[1].event_count = 5
        groups[1].user_count = 2

        groups[2].event_count = 5
        groups[2].user_count = 1

        grouped = {
            rules[0]: {
                groups[0]: [],
            },
            rules[1]: {
                groups[1]: [],
                groups[2]: [],
            },
        }

        assert sort_groups(grouped) == OrderedDict((
            (rules[1], OrderedDict((
                (groups[1], []),
                (groups[2], []),
            ))),
            (rules[0], OrderedDict((
                (groups[0], []),
            ))),
        ))
",Test some of the digest building functions.,"Test some of the digest building functions.
",Python,bsd-3-clause,"zenefits/sentry,daevaorn/sentry,BuildingLink/sentry,beeftornado/sentry,jean/sentry,BuildingLink/sentry,zenefits/sentry,nicholasserra/sentry,JackDanger/sentry,JamesMura/sentry,fotinakis/sentry,zenefits/sentry,daevaorn/sentry,gencer/sentry,alexm92/sentry,ifduyue/sentry,gencer/sentry,ifduyue/sentry,BuildingLink/sentry,mvaled/sentry,imankulov/sentry,nicholasserra/sentry,mvaled/sentry,JamesMura/sentry,mvaled/sentry,mvaled/sentry,JamesMura/sentry,daevaorn/sentry,looker/sentry,jean/sentry,alexm92/sentry,zenefits/sentry,BuildingLink/sentry,zenefits/sentry,JackDanger/sentry,BuildingLink/sentry,fotinakis/sentry,JamesMura/sentry,fotinakis/sentry,gencer/sentry,beeftornado/sentry,imankulov/sentry,mitsuhiko/sentry,ifduyue/sentry,JackDanger/sentry,daevaorn/sentry,beeftornado/sentry,fotinakis/sentry,gencer/sentry,alexm92/sentry,gencer/sentry,mitsuhiko/sentry,mvaled/sentry,JamesMura/sentry,looker/sentry,jean/sentry,ifduyue/sentry,looker/sentry,ifduyue/sentry,jean/sentry,looker/sentry,jean/sentry,nicholasserra/sentry,mvaled/sentry,looker/sentry,imankulov/sentry",121,"```python
from __future__ import absolute_import

from collections import OrderedDict

from exam import fixture

from sentry.digests import Record
from sentry.digests.notifications import (
    Notification,
    event_to_record,
    rewrite_record,
    group_records,
    sort_groups,
)
from sentry.testutils import TestCase


class RewriteRecordTestCase(TestCase):
    @fixture
    def rule(self):
        return self.event.project.rule_set.all()[0]

    @fixture
    def record(self):
        return event_to_record(self.event, (self.rule,))

    def test_success(self):
        assert rewrite_record(
            self.record,
            project=self.event.project,
            groups={
                self.event.group.id: self.event.group,
            },
            rules={
                self.rule.id: self.rule,
            },
        ) == Record(
            self.record.key,
            Notification(
                self.event,
                [self.rule],
            ),
            self.record.timestamp,
        )

    def test_without_group(self):
        # If the record can't be associated with a group, it should be returned as None.
        assert rewrite_record(
            self.record,
            project=self.event.project,
            groups={},
            rules={
                self.rule.id: self.rule,
            },
        ) is None

    def test_filters_invalid_rules(self):
        # If the record can't be associated with a group, it should be returned as None.
        assert rewrite_record(
            self.record,
            project=self.event.project,
            groups={
                self.event.group.id: self.event.group,
            },
            rules={},
        ) == Record(
            self.record.key,
            Notification(self.event, []),
            self.record.timestamp,
        )


class GroupRecordsTestCase(TestCase):
    @fixture
    def rule(self):
        return self.project.rule_set.all()[0]

    def test_success(self):
        events = [self.create_event(group=self.group) for _ in xrange(3)]
        records = [Record(event.id, Notification(event, [self.rule]), event.datetime) for event in events]
        assert group_records(records) == {
            self.rule: {
                self.group: records,
            },
        }


class SortRecordsTestCase(TestCase):
    def test_success(self):
        rules = list(self.project.rule_set.all())
        groups = [self.create_group() for _ in xrange(3)]

        groups[0].event_count = 10
        groups[0].user_count = 4

        groups[1].event_count = 5
        groups[1].user_count = 2

        groups[2].event_count = 5
        groups[2].user_count = 1

        grouped = {
            rules[0]: {
                groups[0]: [],
            },
            rules[1]: {
                groups[1]: [],
                groups[2]: [],
            },
        }

        assert sort_groups(grouped) == OrderedDict((
            (rules[1], OrderedDict((
                (groups[1], []),
                (groups[2], []),
            ))),
            (rules[0], OrderedDict((
                (groups[0], []),
            ))),
        ))

```"
210695ab755a9c1d1d863eec0fedb4ac63931fda,utest/resources/robotdata/datagenerator.py,utest/resources/robotdata/datagenerator.py,,"#!/usr/bin/env python

from getopt import getopt, GetoptError
from random import randint
import os

SUITE=\
""""""*** Settings ***
Resource    resource.txt

*** Test Cases ***
%TESTCASES%

*** Keywords ***
%KEYWORDS%
""""""

RESOURCE=\
""""""*** Variables ***
@{Resource Var}  MOI

*** Keywords ***
%KEYWORDS%
""""""

KEYWORD_TEMPLATE=\
""""""My Keyword %KW_ID%
    No Operation""""""

TEST_CASE_TEMPLATE=\
""""""My Test %TEST_ID%
    My Keyword %KW_ID%
    Log  moi""""""


def generate_tests(number_of_tests, number_of_keywords):
    return '\n'.join(TEST_CASE_TEMPLATE.replace('%TEST_ID%', str(test_id))\
                      .replace('%KW_ID%', str(randint(0,number_of_keywords-1)))\
                      for test_id in xrange(number_of_tests))

def generate_keywords(number_of_keywords):
    return '\n'.join(KEYWORD_TEMPLATE.replace('%KW_ID%', str(i)) for i in xrange(number_of_keywords))

def generate_suite(number_of_tests, number_of_keywords):
    return SUITE.replace('%TESTCASES%', generate_tests(number_of_tests, number_of_keywords))\
                .replace('%KEYWORDS%', generate_keywords(number_of_keywords))

def generate_resource(number_of_keywords):
    return RESOURCE.replace('%KEYWORDS%', generate_keywords(number_of_keywords))

def generate(directory, suites, tests, keywords):
    os.mkdir(directory)
    for suite_index in xrange(suites):
        f = open(os.path.join('.', directory, 'suite%s.txt' % suite_index), 'w')
        f.write(generate_suite(tests, keywords))
        f.close()
    r = open(os.path.join('.', directory, 'resource.txt'), 'w')
    r.write(generate_resource(keywords))
    r.close()

def usage():
    print 'datagenerator.py -d [directory] -s [NUMBER OF SUITES] -t [NUMBER OF TESTS IN SUITE] -k [NUMBER OF KEYWORDS]'

def main(args):
    try:
        opts, args = getopt(args, 'd:s:t:k:', [])
    except GetoptError, e:
        print e
        usage()
        sys.exit(2)
    if len(opts) != 4:
        print opts
        usage()
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-d':
            directory = arg
        if opt == '-s':
            suites = int(arg)
        if opt == '-t':
            tests = int(arg)
        if opt == '-k':
            keywords = int(arg)
    generate(directory, suites, tests, keywords)

if __name__ == '__main__':
    import sys
    main(sys.argv[1:])

",Add test data generator tool,"Add test data generator tool
",Python,apache-2.0,"HelioGuilherme66/RIDE,caio2k/RIDE,HelioGuilherme66/RIDE,fingeronthebutton/RIDE,HelioGuilherme66/RIDE,robotframework/RIDE,robotframework/RIDE,caio2k/RIDE,fingeronthebutton/RIDE,robotframework/RIDE,caio2k/RIDE,HelioGuilherme66/RIDE,fingeronthebutton/RIDE,robotframework/RIDE",90,"```python
#!/usr/bin/env python

from getopt import getopt, GetoptError
from random import randint
import os

SUITE=\
""""""*** Settings ***
Resource    resource.txt

*** Test Cases ***
%TESTCASES%

*** Keywords ***
%KEYWORDS%
""""""

RESOURCE=\
""""""*** Variables ***
@{Resource Var}  MOI

*** Keywords ***
%KEYWORDS%
""""""

KEYWORD_TEMPLATE=\
""""""My Keyword %KW_ID%
    No Operation""""""

TEST_CASE_TEMPLATE=\
""""""My Test %TEST_ID%
    My Keyword %KW_ID%
    Log  moi""""""


def generate_tests(number_of_tests, number_of_keywords):
    return '\n'.join(TEST_CASE_TEMPLATE.replace('%TEST_ID%', str(test_id))\
                      .replace('%KW_ID%', str(randint(0,number_of_keywords-1)))\
                      for test_id in xrange(number_of_tests))

def generate_keywords(number_of_keywords):
    return '\n'.join(KEYWORD_TEMPLATE.replace('%KW_ID%', str(i)) for i in xrange(number_of_keywords))

def generate_suite(number_of_tests, number_of_keywords):
    return SUITE.replace('%TESTCASES%', generate_tests(number_of_tests, number_of_keywords))\
                .replace('%KEYWORDS%', generate_keywords(number_of_keywords))

def generate_resource(number_of_keywords):
    return RESOURCE.replace('%KEYWORDS%', generate_keywords(number_of_keywords))

def generate(directory, suites, tests, keywords):
    os.mkdir(directory)
    for suite_index in xrange(suites):
        f = open(os.path.join('.', directory, 'suite%s.txt' % suite_index), 'w')
        f.write(generate_suite(tests, keywords))
        f.close()
    r = open(os.path.join('.', directory, 'resource.txt'), 'w')
    r.write(generate_resource(keywords))
    r.close()

def usage():
    print 'datagenerator.py -d [directory] -s [NUMBER OF SUITES] -t [NUMBER OF TESTS IN SUITE] -k [NUMBER OF KEYWORDS]'

def main(args):
    try:
        opts, args = getopt(args, 'd:s:t:k:', [])
    except GetoptError, e:
        print e
        usage()
        sys.exit(2)
    if len(opts) != 4:
        print opts
        usage()
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-d':
            directory = arg
        if opt == '-s':
            suites = int(arg)
        if opt == '-t':
            tests = int(arg)
        if opt == '-k':
            keywords = int(arg)
    generate(directory, suites, tests, keywords)

if __name__ == '__main__':
    import sys
    main(sys.argv[1:])


```"
2a34dd198110401bd485552ad857c3d4f26c7b8c,csunplugged/tests/resources/views/test_generate_resource.py,csunplugged/tests/resources/views/test_generate_resource.py,,"import os
from django.test import tag
from django.urls import reverse
from tests.BaseTestWithDB import BaseTestWithDB
from tests.resources.ResourcesTestDataGenerator import ResourcesTestDataGenerator
from utils.create_query_string import query_string


@tag(""resource"")
class GenerateResourceTest(BaseTestWithDB):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_data = ResourcesTestDataGenerator()
        self.language = ""en""

    def test_generate_view_valid_slug(self):
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        get_parameters = {
            ""paper_size"": ""a4""
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        url += query_string(get_parameters)
        response = self.client.get(url)
        self.assertEqual(200, response.status_code)
        self.assertEqual(
            response.get(""Content-Disposition""),
            'attachment; filename=""Resource Grid (a4).pdf""'
        )

    def test_generate_view_valid_slug_production_cache(self):
        os.environ[""DJANGO_PRODUCTION""] = ""TRUE""
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        get_parameters = {
            ""paper_size"": ""a4""
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        url += query_string(get_parameters)
        response = self.client.get(url)
        self.assertEqual(302, response.status_code)
        self.assertEqual(
            response.url,
            ""/staticfiles/resources/Resource%20Grid%20(a4).pdf""
        )

    def test_generate_view_valid_slug_missing_parameter(self):
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        response = self.client.get(url)
        self.assertEqual(404, response.status_code)

    def test_generate_view_valid_slug_invalid_parameter(self):
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        get_parameters = {
            ""paper_size"": ""b7""
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        url += query_string(get_parameters)
        response = self.client.get(url)
        self.assertEqual(404, response.status_code)
",Add tests for generate resource view,"Add tests for generate resource view
",Python,mit,"uccser/cs-unplugged,uccser/cs-unplugged,uccser/cs-unplugged,uccser/cs-unplugged",93,"```python
import os
from django.test import tag
from django.urls import reverse
from tests.BaseTestWithDB import BaseTestWithDB
from tests.resources.ResourcesTestDataGenerator import ResourcesTestDataGenerator
from utils.create_query_string import query_string


@tag(""resource"")
class GenerateResourceTest(BaseTestWithDB):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.test_data = ResourcesTestDataGenerator()
        self.language = ""en""

    def test_generate_view_valid_slug(self):
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        get_parameters = {
            ""paper_size"": ""a4""
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        url += query_string(get_parameters)
        response = self.client.get(url)
        self.assertEqual(200, response.status_code)
        self.assertEqual(
            response.get(""Content-Disposition""),
            'attachment; filename=""Resource Grid (a4).pdf""'
        )

    def test_generate_view_valid_slug_production_cache(self):
        os.environ[""DJANGO_PRODUCTION""] = ""TRUE""
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        get_parameters = {
            ""paper_size"": ""a4""
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        url += query_string(get_parameters)
        response = self.client.get(url)
        self.assertEqual(302, response.status_code)
        self.assertEqual(
            response.url,
            ""/staticfiles/resources/Resource%20Grid%20(a4).pdf""
        )

    def test_generate_view_valid_slug_missing_parameter(self):
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        response = self.client.get(url)
        self.assertEqual(404, response.status_code)

    def test_generate_view_valid_slug_invalid_parameter(self):
        resource = self.test_data.create_resource(
            ""grid"",
            ""Grid"",
            ""resources/grid.html"",
            ""GridResourceGenerator"",
        )
        kwargs = {
            ""resource_slug"": resource.slug,
        }
        get_parameters = {
            ""paper_size"": ""b7""
        }
        url = reverse(""resources:generate"", kwargs=kwargs)
        url += query_string(get_parameters)
        response = self.client.get(url)
        self.assertEqual(404, response.status_code)

```"
a558b9efe5f20af2b5b4e588add8648bfba22c2c,app/scripts/benchmark_match_accuracy.py,app/scripts/benchmark_match_accuracy.py,,"from tabulate import tabulate
import csv
import yaml
import sys
import os.path as path

base_directory = path.dirname(path.dirname(path.abspath(__file__)))
sys.path.append(base_directory)

import deparse
from segment import Segment


def load_segments(filename):
    '''Load a segment feature matrix from a CSV file, returning a list of
    dictionaries with information about each segment.

    '''
    with open(filename, 'r') as f:
        return [segment for segment in csv.DictReader(f)]


def load_feature_strings(filename):
    '''Load a feature string list from a CSV file, returning a list of
    lists where the first item in each is the IPA string and the
    second is the corresponding feature string.

    '''
    with open(filename, 'r') as f:
        return [line for line in csv.reader(f)]


def load_diacritics(filename):
    '''Load diacritic data from a YAML file, returning a list of
    dictionaries with information about each diacritic.

    '''
    with open(filename, 'r') as f:
        return yaml.load(f)


def main():
    segments = load_segments(path.join(base_directory, 'data', 'features.csv'))
    diacritics = load_diacritics(path.join(base_directory, 'data', 'diacritics.yaml'))

    datasets = ['feature-strings', 'hayes-feature-strings',
                'feature-strings-with-diacritics']

    print('Beginning benchmark\n===================\n')

    results = []

    for dataset in datasets:
        print('Running dataset: {0}'.format(dataset))
        filename = '{0}.csv'.format(dataset)

        accuracy = benchmark_match_accuracy(segments, diacritics, filename)

        results.append([dataset, accuracy])

    print('Finished!\n')
    print(tabulate(results, headers=['Dataset', 'Accuracy']))


def benchmark_match_accuracy(segments, diacritics, filename):
    '''Convert all given segments to feature strings, then convert back to
    segments. Use the given feature string file. Return the percentage accuracy
    of the conversion.

    '''
    feature_strings = load_feature_strings(path.join(base_directory, 'data',
                                                     filename))

    matches = []

    for segment in segments:
        base_segment = Segment.from_dictionary(segment)
        matches.append((segment['IPA'],
                        deparse.segment_match(feature_strings, base_segment)))

        for diacritic in diacritics:
            IPA_representation = segment['IPA'] + diacritic['IPA']

            if base_segment.meets_conditions(diacritic['conditions']):
                diacritic_segment = base_segment + Segment(diacritic['applies'].get('positive', []),
                                                           diacritic['applies'].get('negative', []))

                matches.append((IPA_representation,
                                deparse.segment_match(feature_strings,
                                                      diacritic_segment)))

    successes = len([match for match in matches if match[0] == match[1]])

    return (successes / len(matches))


if __name__ == '__main__':
    main()
",Add match accuracy benchmark script,"Add match accuracy benchmark script
",Python,mit,"kdelwat/LangEvolve,kdelwat/LangEvolve,kdelwat/LangEvolve",99,"```python
from tabulate import tabulate
import csv
import yaml
import sys
import os.path as path

base_directory = path.dirname(path.dirname(path.abspath(__file__)))
sys.path.append(base_directory)

import deparse
from segment import Segment


def load_segments(filename):
    '''Load a segment feature matrix from a CSV file, returning a list of
    dictionaries with information about each segment.

    '''
    with open(filename, 'r') as f:
        return [segment for segment in csv.DictReader(f)]


def load_feature_strings(filename):
    '''Load a feature string list from a CSV file, returning a list of
    lists where the first item in each is the IPA string and the
    second is the corresponding feature string.

    '''
    with open(filename, 'r') as f:
        return [line for line in csv.reader(f)]


def load_diacritics(filename):
    '''Load diacritic data from a YAML file, returning a list of
    dictionaries with information about each diacritic.

    '''
    with open(filename, 'r') as f:
        return yaml.load(f)


def main():
    segments = load_segments(path.join(base_directory, 'data', 'features.csv'))
    diacritics = load_diacritics(path.join(base_directory, 'data', 'diacritics.yaml'))

    datasets = ['feature-strings', 'hayes-feature-strings',
                'feature-strings-with-diacritics']

    print('Beginning benchmark\n===================\n')

    results = []

    for dataset in datasets:
        print('Running dataset: {0}'.format(dataset))
        filename = '{0}.csv'.format(dataset)

        accuracy = benchmark_match_accuracy(segments, diacritics, filename)

        results.append([dataset, accuracy])

    print('Finished!\n')
    print(tabulate(results, headers=['Dataset', 'Accuracy']))


def benchmark_match_accuracy(segments, diacritics, filename):
    '''Convert all given segments to feature strings, then convert back to
    segments. Use the given feature string file. Return the percentage accuracy
    of the conversion.

    '''
    feature_strings = load_feature_strings(path.join(base_directory, 'data',
                                                     filename))

    matches = []

    for segment in segments:
        base_segment = Segment.from_dictionary(segment)
        matches.append((segment['IPA'],
                        deparse.segment_match(feature_strings, base_segment)))

        for diacritic in diacritics:
            IPA_representation = segment['IPA'] + diacritic['IPA']

            if base_segment.meets_conditions(diacritic['conditions']):
                diacritic_segment = base_segment + Segment(diacritic['applies'].get('positive', []),
                                                           diacritic['applies'].get('negative', []))

                matches.append((IPA_representation,
                                deparse.segment_match(feature_strings,
                                                      diacritic_segment)))

    successes = len([match for match in matches if match[0] == match[1]])

    return (successes / len(matches))


if __name__ == '__main__':
    main()

```"
c7d32d2ed2483ce34c7a901c84d128c50f03c472,golddust/__init__.py,golddust/__init__.py,,"# Copyright 2015-2016 John ""LuaMilkshake"" Marion
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""GoldDust package management library.

GoldDust manages modded installations of the game Minecraft. This
package implements a package manager for mod files (including
Minecraft Forge and texture packs).
""""""


import os
import platform


def default_home_dir():
    """"""Get the default home directory path on this platform.

    This is an absolute form of ""~/.golddust"" on all platforms except
    for Windows, where it is ""%APPDATA%/.golddust"".

    In the strange case where APPDATA isn't set on Windows, will default
    to the non-Windows behavior.

    Returns str, the absolute path of ""~/.golddust"" (non-Windows),
        ""%APPDATA/.golddust"" (Windows).
    """"""
    path = ""~/.golddust""

    if platform.system() == ""Windows"" and ""APPDATA"" in os.environ:
        path = os.path.join(os.environ[""APPDATA""], "".golddust"")

    return os.path.abspath(os.path.expanduser(path))


def write_default_config(path):
    """"""Write out a default GoldDust configuration.
    """"""
    config = open(path, ""w+"")
    # TODO Actually serialize a configuration
    config.write(""{}"")
    config.close()


def install_home_dir(path):
    """"""Install the GoldDust home directory.

    The home directory is used for global configuration, local package caches,
    and instance information.

    Takes:
        path (str): Path to install the home directory to. Should not exist.

    Raises:
        FileExistsError: The path supplied already exists.
    """"""

    if os.path.isdir(path):
        raise FileExistsError(""Installation target directory shouldn't exist."")

    gdhome = os.path.abspath(os.path.expanduser(path))

    # Create directories and config file
    os.mkdir(gdhome, mode=0o755)
    os.mkdir(os.path.join(gdhome, ""pkgcache""))
    os.mkdir(os.path.join(gdhome, ""instances""))
    write_default_config(os.path.join(gdhome, ""config.json""))


class Package:
    """"""A package managed by GoldDust""""""
    def __init__(self):
        self.name = """"
        self.version = """"

    @property
    def tarball(self):
        return ""{}-{}.tar.bz2"".format(self.name, self.version)

    @property
    def sig_file(self):
        return ""{}.sig"".format(self.tarball)
",Add current state of golddust package,"Add current state of golddust package
",Python,apache-2.0,Packeteers/GoldDust,94,"```python
# Copyright 2015-2016 John ""LuaMilkshake"" Marion
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

""""""GoldDust package management library.

GoldDust manages modded installations of the game Minecraft. This
package implements a package manager for mod files (including
Minecraft Forge and texture packs).
""""""


import os
import platform


def default_home_dir():
    """"""Get the default home directory path on this platform.

    This is an absolute form of ""~/.golddust"" on all platforms except
    for Windows, where it is ""%APPDATA%/.golddust"".

    In the strange case where APPDATA isn't set on Windows, will default
    to the non-Windows behavior.

    Returns str, the absolute path of ""~/.golddust"" (non-Windows),
        ""%APPDATA/.golddust"" (Windows).
    """"""
    path = ""~/.golddust""

    if platform.system() == ""Windows"" and ""APPDATA"" in os.environ:
        path = os.path.join(os.environ[""APPDATA""], "".golddust"")

    return os.path.abspath(os.path.expanduser(path))


def write_default_config(path):
    """"""Write out a default GoldDust configuration.
    """"""
    config = open(path, ""w+"")
    # TODO Actually serialize a configuration
    config.write(""{}"")
    config.close()


def install_home_dir(path):
    """"""Install the GoldDust home directory.

    The home directory is used for global configuration, local package caches,
    and instance information.

    Takes:
        path (str): Path to install the home directory to. Should not exist.

    Raises:
        FileExistsError: The path supplied already exists.
    """"""

    if os.path.isdir(path):
        raise FileExistsError(""Installation target directory shouldn't exist."")

    gdhome = os.path.abspath(os.path.expanduser(path))

    # Create directories and config file
    os.mkdir(gdhome, mode=0o755)
    os.mkdir(os.path.join(gdhome, ""pkgcache""))
    os.mkdir(os.path.join(gdhome, ""instances""))
    write_default_config(os.path.join(gdhome, ""config.json""))


class Package:
    """"""A package managed by GoldDust""""""
    def __init__(self):
        self.name = """"
        self.version = """"

    @property
    def tarball(self):
        return ""{}-{}.tar.bz2"".format(self.name, self.version)

    @property
    def sig_file(self):
        return ""{}.sig"".format(self.tarball)

```"
9e8b32928c068237e34a7c319564333bbed59cb7,pox/messenger/test_client.py,pox/messenger/test_client.py,,"#!/usr/bin/env python

# Copyright 2012 James McCauley
#
# This file is part of POX.
#
# POX is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# POX is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with POX.  If not, see <http://www.gnu.org/licenses/>.

""""""
This is NOT a POX component.  It's a little tool to test out the messenger.
""""""

import socket
import threading
import json

class JSONDestreamer (object):
  import json
  decoder = json.JSONDecoder()
  def __init__ (self, callback = None):
    self.data = ''
    self.callback = callback if callback else self.rx

  def push (self, data):
    if len(self.data) == 0:
      data = data.lstrip()
    self.data += data
    try:
      while len(self.data) > 0:
        r,off = self.decoder.raw_decode(self.data)

        self.data = self.data[off:].lstrip()
        self.callback(r)
    except ValueError:
      pass

  def rx (self, data):
    import json
    print ""Recv:"", json.dumps(data, indent=4)

jd = JSONDestreamer()
done = False

def reader (socket):
  global done
  while True:
    d = socket.recv(1024)
    if d == """":
      done = True
      break
    jd.push(d)

cur_chan = None
def channel (ch):
  global cur_chan
  cur_chan = ch

import readline

def main (addr = ""127.0.0.1"", port = 7790):
  print ""Connecting to %s:%i"" % (addr,port)
  port = int(port)

  sock = socket.create_connection((addr, port))

  t = threading.Thread(target=reader, args=(sock,))
  t.daemon = True
  t.start()

  while not done:
    try:
      #print "">"",
      m = raw_input()
      if len(m) == 0: continue
      m = eval(m)
      if not isinstance(m, dict):
        continue
      if cur_chan is not None and 'CHANNEL' not in m:
        m['CHANNEL'] = cur_chan
      m = json.dumps(m)
      sock.send(m)
    except EOFError:
      break
    except KeyboardInterrupt:
      break
    except:
      import traceback
      traceback.print_exc()

if __name__ == ""__main__"":
  import sys
  main(*sys.argv[1:])
",Add a small test client program,"messenger: Add a small test client program
",Python,apache-2.0,"waltznetworks/pox,waltznetworks/pox,PrincetonUniversity/pox,andiwundsam/_of_normalize,VamsikrishnaNallabothu/pox,jacobq/csci5221-viro-project,chenyuntc/pox,diogommartins/pox,carlye566/IoT-POX,pthien92/sdn,adusia/pox,pthien92/sdn,pthien92/sdn,carlye566/IoT-POX,pthien92/sdn,waltznetworks/pox,kulawczukmarcin/mypox,PrincetonUniversity/pox,MurphyMc/pox,diogommartins/pox,carlye566/IoT-POX,waltznetworks/pox,VamsikrishnaNallabothu/pox,kavitshah8/SDNDeveloper,denovogroup/pox,andiwundsam/_of_normalize,xAKLx/pox,VamsikrishnaNallabothu/pox,kulawczukmarcin/mypox,adusia/pox,andiwundsam/_of_normalize,noxrepo/pox,kpengboy/pox-exercise,andiwundsam/_of_normalize,diogommartins/pox,noxrepo/pox,kpengboy/pox-exercise,carlye566/IoT-POX,VamsikrishnaNallabothu/pox,PrincetonUniversity/pox,adusia/pox,xAKLx/pox,kulawczukmarcin/mypox,VamsikrishnaNallabothu/pox,MurphyMc/pox,chenyuntc/pox,jacobq/csci5221-viro-project,PrincetonUniversity/pox,xAKLx/pox,denovogroup/pox,kavitshah8/SDNDeveloper,kulawczukmarcin/mypox,chenyuntc/pox,diogommartins/pox,kpengboy/pox-exercise,pthien92/sdn,xAKLx/pox,MurphyMc/pox,chenyuntc/pox,xAKLx/pox,kavitshah8/SDNDeveloper,jacobq/csci5221-viro-project,carlye566/IoT-POX,jacobq/csci5221-viro-project,kpengboy/pox-exercise,noxrepo/pox,PrincetonUniversity/pox,adusia/pox,denovogroup/pox,MurphyMc/pox,adusia/pox,denovogroup/pox,noxrepo/pox,jacobq/csci5221-viro-project,chenyuntc/pox,diogommartins/pox,waltznetworks/pox,MurphyMc/pox,kavitshah8/SDNDeveloper,kpengboy/pox-exercise,kulawczukmarcin/mypox,denovogroup/pox",104,"```python
#!/usr/bin/env python

# Copyright 2012 James McCauley
#
# This file is part of POX.
#
# POX is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# POX is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with POX.  If not, see <http://www.gnu.org/licenses/>.

""""""
This is NOT a POX component.  It's a little tool to test out the messenger.
""""""

import socket
import threading
import json

class JSONDestreamer (object):
  import json
  decoder = json.JSONDecoder()
  def __init__ (self, callback = None):
    self.data = ''
    self.callback = callback if callback else self.rx

  def push (self, data):
    if len(self.data) == 0:
      data = data.lstrip()
    self.data += data
    try:
      while len(self.data) > 0:
        r,off = self.decoder.raw_decode(self.data)

        self.data = self.data[off:].lstrip()
        self.callback(r)
    except ValueError:
      pass

  def rx (self, data):
    import json
    print ""Recv:"", json.dumps(data, indent=4)

jd = JSONDestreamer()
done = False

def reader (socket):
  global done
  while True:
    d = socket.recv(1024)
    if d == """":
      done = True
      break
    jd.push(d)

cur_chan = None
def channel (ch):
  global cur_chan
  cur_chan = ch

import readline

def main (addr = ""127.0.0.1"", port = 7790):
  print ""Connecting to %s:%i"" % (addr,port)
  port = int(port)

  sock = socket.create_connection((addr, port))

  t = threading.Thread(target=reader, args=(sock,))
  t.daemon = True
  t.start()

  while not done:
    try:
      #print "">"",
      m = raw_input()
      if len(m) == 0: continue
      m = eval(m)
      if not isinstance(m, dict):
        continue
      if cur_chan is not None and 'CHANNEL' not in m:
        m['CHANNEL'] = cur_chan
      m = json.dumps(m)
      sock.send(m)
    except EOFError:
      break
    except KeyboardInterrupt:
      break
    except:
      import traceback
      traceback.print_exc()

if __name__ == ""__main__"":
  import sys
  main(*sys.argv[1:])

```"
55a17865393f8c4d489f41dccbcc656670c81f2b,bika/lims/fields.py,bika/lims/fields.py,,"""""""Generic field extensions
""""""
from Acquisition import aq_inner
from Acquisition import aq_parent
from Acquisition import Implicit
from Acquisition import ImplicitAcquisitionWrapper
from archetypes.schemaextender.field import ExtensionField
from archetypes.schemaextender.field import ExtensionField
from archetypes.schemaextender.interfaces import IExtensionField
from Products.Archetypes.public import *
from Products.ATExtensions.ateapi import DateTimeField
from Products.ATExtensions.ateapi import RecordField, RecordsField
from zope.interface import implements


class ExtBooleanField(ExtensionField, BooleanField):

    ""Field extender""


class ExtComputedField(ExtensionField, ComputedField):

    ""Field extender""


class ExtDateTimeField(ExtensionField, DateTimeField):

    ""Field extender""


class ExtIntegerField(ExtensionField, IntegerField):

    ""Field extender""


class ExtLinesField(ExtensionField, LinesField):

    ""Field extender""


class ExtRecordField(ExtensionField, RecordField):

    ""Field extender""


class ExtRecordsField(ExtensionField, RecordsField):

    ""Field extender""


class ExtReferenceField(ExtensionField, ReferenceField):

    ""Field extender""


class ExtStringField(ExtensionField, StringField):

    ""Field extender""


class ExtTextField(ExtensionField, TextField):

    ""Field extender""

#
# Method Initialization
# apply default getters and setters to schemaextender fields.
#


def apply_default_methods(context):

    for field in context.schema.fields():
        fieldname = field.getName()
        if getattr(context, 'get'+fieldname, None) is None:
            setattr(context, 'get'+fieldname, field_getter(context, fieldname))
        if getattr(context, 'set'+fieldname, None) is None:
            setattr(context, 'set'+fieldname, field_setter(context, fieldname))


class field_getter:

    def __init__(self, context, fieldname, **kwargs):
        self.context = context
        self.fieldname = fieldname

    def __call__(self):
        return self.context.Schema()[self.fieldname].getAccessor(self.context)(**kwargs)


class field_setter:

    def __init__(self, context, fieldname):
        self.context = context
        self.fieldname = fieldname

    def __call__(self, value, **kwargs):
        return self.context.Schema()[self.fieldname].getMutator(self.context)(value, **kwargs)



",Move schemaextender automatic getter and setter methods,"Move schemaextender automatic getter and setter methods
",Python,agpl-3.0,"anneline/Bika-LIMS,labsanmartin/Bika-LIMS,rockfruit/bika.lims,labsanmartin/Bika-LIMS,DeBortoliWines/Bika-LIMS,DeBortoliWines/Bika-LIMS,rockfruit/bika.lims,veroc/Bika-LIMS,veroc/Bika-LIMS,labsanmartin/Bika-LIMS,anneline/Bika-LIMS,DeBortoliWines/Bika-LIMS,anneline/Bika-LIMS,veroc/Bika-LIMS",102,"```python
""""""Generic field extensions
""""""
from Acquisition import aq_inner
from Acquisition import aq_parent
from Acquisition import Implicit
from Acquisition import ImplicitAcquisitionWrapper
from archetypes.schemaextender.field import ExtensionField
from archetypes.schemaextender.field import ExtensionField
from archetypes.schemaextender.interfaces import IExtensionField
from Products.Archetypes.public import *
from Products.ATExtensions.ateapi import DateTimeField
from Products.ATExtensions.ateapi import RecordField, RecordsField
from zope.interface import implements


class ExtBooleanField(ExtensionField, BooleanField):

    ""Field extender""


class ExtComputedField(ExtensionField, ComputedField):

    ""Field extender""


class ExtDateTimeField(ExtensionField, DateTimeField):

    ""Field extender""


class ExtIntegerField(ExtensionField, IntegerField):

    ""Field extender""


class ExtLinesField(ExtensionField, LinesField):

    ""Field extender""


class ExtRecordField(ExtensionField, RecordField):

    ""Field extender""


class ExtRecordsField(ExtensionField, RecordsField):

    ""Field extender""


class ExtReferenceField(ExtensionField, ReferenceField):

    ""Field extender""


class ExtStringField(ExtensionField, StringField):

    ""Field extender""


class ExtTextField(ExtensionField, TextField):

    ""Field extender""

#
# Method Initialization
# apply default getters and setters to schemaextender fields.
#


def apply_default_methods(context):

    for field in context.schema.fields():
        fieldname = field.getName()
        if getattr(context, 'get'+fieldname, None) is None:
            setattr(context, 'get'+fieldname, field_getter(context, fieldname))
        if getattr(context, 'set'+fieldname, None) is None:
            setattr(context, 'set'+fieldname, field_setter(context, fieldname))


class field_getter:

    def __init__(self, context, fieldname, **kwargs):
        self.context = context
        self.fieldname = fieldname

    def __call__(self):
        return self.context.Schema()[self.fieldname].getAccessor(self.context)(**kwargs)


class field_setter:

    def __init__(self, context, fieldname):
        self.context = context
        self.fieldname = fieldname

    def __call__(self, value, **kwargs):
        return self.context.Schema()[self.fieldname].getMutator(self.context)(value, **kwargs)




```"
67b90064cfaa09c55ea138895eb6c7ed9513bc96,Python-demo/test-demo/test_loop_with_else.py,Python-demo/test-demo/test_loop_with_else.py,,"# coding=utf-8

""""""
 # test_loop_with_else.py
 # 
 # Copyright(C) By AbsentM. 2018
 # 
 # Author: AbsentM
 # Date: 2018/03/18
 # 
 # Description: Test python loop with else demo
 #
 # Change Log:
 # 2018/03/18 AbsentM Create the file
 # 2018/03/18 AbsentM Add test demo
 #
""""""

def run_for_else_validation(fruits):
	""""""
	Test python for else
	:param fruits: A string of fruits split with ','
	""""""
	if fruits is None:
		fruits_result = ""*""
	else:
		fruits = fruits.split("","")
		print ""fruits >>> {}"".format(fruits)

		for item in fruits:
			print ""item >>> {}"".format(item)
		else:
			fruits_result = fruits

	print ""total >>> {}"".format(fruits_result)


def run_while_else_validation():
	""""""
	Test python while else
	""""""
	index = 0
	while index <= 10:
		index += 1
		print ""index {}: {} "".format(index, index)
	else:
		print ""in while else""


if __name__ == '__main__':
	print ""---------------------------""
	print ""Run first test...""
	test_1 = None
	run_for_else_validation(test_1)
	print ""Run first test finished""

	print ""---------------------------""
	print ""Run second test...""
	test_2 = ""apple""
	run_for_else_validation(test_2)
	print ""Run second test finished""

	print ""---------------------------""
	print ""Run third test...""
	test_3 = ""apple,pear,orange""
	run_for_else_validation(test_3)
	print ""Run third test finished""

	print ""---------------------------""
	print ""Run fourth test...""
	run_while_else_validation()
	print ""Run fourth test finished""


""""""
The output as follow:

---------------------------
Run first test...
total >>> *
Run first test finished
---------------------------
Run second test...
fruits >>> ['apple']
item >>> apple
total >>> ['apple']
Run second test finished
---------------------------
Run third test...
fruits >>> ['apple', 'pear', 'orange']
item >>> apple
item >>> pear
item >>> orange
total >>> ['apple', 'pear', 'orange']
Run third test finished
---------------------------
Run fourth test...
index 1: 1 
index 2: 2 
index 3: 3 
index 4: 4 
index 5: 5 
index 6: 6 
index 7: 7 
index 8: 8 
index 9: 9 
index 10: 10 
index 11: 11 
in while else
Run fourth test finished
[Finished in 0.3s]

""""""
",Add python loop with else demo,"Add python loop with else demo
",Python,mit,"absentm/Demo,absentm/Demo,absentm/Demo,absentm/Demo,absentm/Demo,absentm/Demo",114,"```python
# coding=utf-8

""""""
 # test_loop_with_else.py
 # 
 # Copyright(C) By AbsentM. 2018
 # 
 # Author: AbsentM
 # Date: 2018/03/18
 # 
 # Description: Test python loop with else demo
 #
 # Change Log:
 # 2018/03/18 AbsentM Create the file
 # 2018/03/18 AbsentM Add test demo
 #
""""""

def run_for_else_validation(fruits):
	""""""
	Test python for else
	:param fruits: A string of fruits split with ','
	""""""
	if fruits is None:
		fruits_result = ""*""
	else:
		fruits = fruits.split("","")
		print ""fruits >>> {}"".format(fruits)

		for item in fruits:
			print ""item >>> {}"".format(item)
		else:
			fruits_result = fruits

	print ""total >>> {}"".format(fruits_result)


def run_while_else_validation():
	""""""
	Test python while else
	""""""
	index = 0
	while index <= 10:
		index += 1
		print ""index {}: {} "".format(index, index)
	else:
		print ""in while else""


if __name__ == '__main__':
	print ""---------------------------""
	print ""Run first test...""
	test_1 = None
	run_for_else_validation(test_1)
	print ""Run first test finished""

	print ""---------------------------""
	print ""Run second test...""
	test_2 = ""apple""
	run_for_else_validation(test_2)
	print ""Run second test finished""

	print ""---------------------------""
	print ""Run third test...""
	test_3 = ""apple,pear,orange""
	run_for_else_validation(test_3)
	print ""Run third test finished""

	print ""---------------------------""
	print ""Run fourth test...""
	run_while_else_validation()
	print ""Run fourth test finished""


""""""
The output as follow:

---------------------------
Run first test...
total >>> *
Run first test finished
---------------------------
Run second test...
fruits >>> ['apple']
item >>> apple
total >>> ['apple']
Run second test finished
---------------------------
Run third test...
fruits >>> ['apple', 'pear', 'orange']
item >>> apple
item >>> pear
item >>> orange
total >>> ['apple', 'pear', 'orange']
Run third test finished
---------------------------
Run fourth test...
index 1: 1 
index 2: 2 
index 3: 3 
index 4: 4 
index 5: 5 
index 6: 6 
index 7: 7 
index 8: 8 
index 9: 9 
index 10: 10 
index 11: 11 
in while else
Run fourth test finished
[Finished in 0.3s]

""""""

```"
da50bb53f05e487a00b3db2feb97ce37cf449afd,tests/squid.py,tests/squid.py,,"""""""
    Squid Proxy Detector
    ********************

""""""
import os
import httplib
import urllib2
from urlparse import urlparse

from plugoo import gen_headers
from plugoo.assets import Asset
from plugoo.tests import Test

__plugoo__ = ""SquidProxy""
__desc__ = ""This Test aims at detecting the squid transparent proxy""

class SquidAsset(Asset):
    """"""
    This is the asset that should be used by the Test. It will
    contain all the code responsible for parsing the asset file
    and should be passed on instantiation to the test.
    """"""
    def __init__(self, file=None):
        self = Asset.__init__(self, file)


class Squid(Test):
    """"""
    Squid Proxy testing class.
    """"""
    def _http_request(self, method, url,
                      path=None, headers=None):
        """"""
        Perform an HTTP Request.
        XXX move this function to the core OONI
        code.
        """"""
        url = urlparse(url)
        host = url.netloc

        conn = httplib.HTTPConnection(host, 80)
        conn.connect()

        if path is None:
            path = purl.path

        conn.putrequest(method, path)

        for h in gen_headers():
            conn.putheaders(h[0], h[1])
        conn.endheaders()

        send_browser_headers(self, None, conn)

        response = conn.getresponse()

        headers = dict(response.getheaders())

        self.headers = headers
        self.data = response.read()
        return True

    def invalid_request(self):
        """"""
        This will trigger squids ""Invalid Request"" error.
        """"""
        pass

    def cache_object(self):
        """"""
        This attempts to do a GET cache_object://localhost/info on
        any destination and checks to see if the response contains
        is that of Squid.
        """"""

        pass

    def experiment(self, *a, **kw):
        """"""
        Fill this up with the tasks that should be performed
        on the ""dirty"" network and should be compared with the
        control.
        """"""


    def control(self):
        """"""
        Fill this up with the control related code.
        """"""
        return True

def run(ooni):
    """"""
    This is the function that will be called by OONI
    and it is responsible for instantiating and passing
    the arguments to the Test class.
    """"""
    config = ooni.config

    # This the assets array to be passed to the run function of
    # the test
    assets = [TestTemplateAsset(os.path.join(config.main.assetdir, \
                                            ""someasset.txt""))]

    # Instantiate the Test
    thetest = TestTemplate(ooni)
    ooni.logger.info(""starting SquidProxyTest..."")
    # Run the test with argument assets
    thetest.run(assets)
    ooni.logger.info(""finished."")


",Add some scaffolding for Squid Proxy detection test,"Add some scaffolding for Squid Proxy detection test
",Python,bsd-2-clause,"juga0/ooni-probe,Karthikeyan-kkk/ooni-probe,0xPoly/ooni-probe,0xPoly/ooni-probe,lordappsec/ooni-probe,0xPoly/ooni-probe,Karthikeyan-kkk/ooni-probe,kdmurray91/ooni-probe,Karthikeyan-kkk/ooni-probe,0xPoly/ooni-probe,juga0/ooni-probe,Karthikeyan-kkk/ooni-probe,kdmurray91/ooni-probe,juga0/ooni-probe,kdmurray91/ooni-probe,hackerberry/ooni-probe,hackerberry/ooni-probe,lordappsec/ooni-probe,lordappsec/ooni-probe,juga0/ooni-probe,lordappsec/ooni-probe,kdmurray91/ooni-probe",114,"```python
""""""
    Squid Proxy Detector
    ********************

""""""
import os
import httplib
import urllib2
from urlparse import urlparse

from plugoo import gen_headers
from plugoo.assets import Asset
from plugoo.tests import Test

__plugoo__ = ""SquidProxy""
__desc__ = ""This Test aims at detecting the squid transparent proxy""

class SquidAsset(Asset):
    """"""
    This is the asset that should be used by the Test. It will
    contain all the code responsible for parsing the asset file
    and should be passed on instantiation to the test.
    """"""
    def __init__(self, file=None):
        self = Asset.__init__(self, file)


class Squid(Test):
    """"""
    Squid Proxy testing class.
    """"""
    def _http_request(self, method, url,
                      path=None, headers=None):
        """"""
        Perform an HTTP Request.
        XXX move this function to the core OONI
        code.
        """"""
        url = urlparse(url)
        host = url.netloc

        conn = httplib.HTTPConnection(host, 80)
        conn.connect()

        if path is None:
            path = purl.path

        conn.putrequest(method, path)

        for h in gen_headers():
            conn.putheaders(h[0], h[1])
        conn.endheaders()

        send_browser_headers(self, None, conn)

        response = conn.getresponse()

        headers = dict(response.getheaders())

        self.headers = headers
        self.data = response.read()
        return True

    def invalid_request(self):
        """"""
        This will trigger squids ""Invalid Request"" error.
        """"""
        pass

    def cache_object(self):
        """"""
        This attempts to do a GET cache_object://localhost/info on
        any destination and checks to see if the response contains
        is that of Squid.
        """"""

        pass

    def experiment(self, *a, **kw):
        """"""
        Fill this up with the tasks that should be performed
        on the ""dirty"" network and should be compared with the
        control.
        """"""


    def control(self):
        """"""
        Fill this up with the control related code.
        """"""
        return True

def run(ooni):
    """"""
    This is the function that will be called by OONI
    and it is responsible for instantiating and passing
    the arguments to the Test class.
    """"""
    config = ooni.config

    # This the assets array to be passed to the run function of
    # the test
    assets = [TestTemplateAsset(os.path.join(config.main.assetdir, \
                                            ""someasset.txt""))]

    # Instantiate the Test
    thetest = TestTemplate(ooni)
    ooni.logger.info(""starting SquidProxyTest..."")
    # Run the test with argument assets
    thetest.run(assets)
    ooni.logger.info(""finished."")



```"
1d2463d7aa476608b95dc1ca37ced23e7dcb13d4,tests/handlers/test_analyses.py,tests/handlers/test_analyses.py,,"import pytest


@pytest.mark.parametrize(""not_found"", [False, True], ids=[""200"", ""404""])
async def test_get(mocker, not_found, spawn_client):
    client = await spawn_client(authorize=True)

    document = {
        ""_id"": ""foobar"",
        ""formatted"": False
    }

    if not not_found:
        await client.db.analyses.insert_one(document)

    m = mocker.stub(name=""format_analysis"")

    return_value = dict(document, formatted=True)

    async def format_analysis(db, document):
        m(db, document)
        return return_value

    mocker.patch(""virtool.sample_analysis.format_analysis"", new=format_analysis)

    resp = await client.get(""/api/analyses/foobar"")

    if not_found:
        assert resp.status == 404
    else:
        assert resp.status == 200

        assert await resp.json() == {
            ""id"": ""foobar"",
            ""formatted"": True
        }

        assert m.call_args[0] == (
            client.db,
            document
        )


@pytest.mark.parametrize(""has_sample"", [True, False], ids=[""with_sample"", ""without_sample""])
@pytest.mark.parametrize(""status"", [204, 404, 409])
async def test_remove(has_sample, status, spawn_client, resp_is, test_dispatch):
    client = await spawn_client(authorize=True)

    sample_document = None

    if has_sample:
        sample_document = {
            ""_id"": ""baz"",
            ""name"": ""Baz""
        }

        await client.db.samples.insert_one(sample_document)

    if status != 404:
        analysis_document = {
            ""_id"": ""foobar"",
            ""ready"": status == 204,
            ""sample"": {
                ""id"": ""baz"",
                ""name"": ""Baz""
            },
            ""job"": {
                ""id"": ""hello""
            }
        }

        await client.db.analyses.insert_one(analysis_document)

    resp = await client.delete(""/api/analyses/foobar"")

    assert resp.status == status

    if status == 409:
        assert await resp_is.conflict(resp, ""Analysis is still running. Cancel job 'hello' instead"")

    elif status == 404:
        assert await resp_is.not_found(resp)

    else:
        if has_sample:
            assert test_dispatch.stub.call_args[0] == (
                ""samples"",
                ""update"",
                {
                    ""id"": ""baz"",
                    ""name"": ""Baz""
                }
            )
        else:
            assert not test_dispatch.stub.called

",Add some tests for analyses handlers,"Add some tests for analyses handlers

",Python,mit,"igboyes/virtool,virtool/virtool,igboyes/virtool,virtool/virtool",97,"```python
import pytest


@pytest.mark.parametrize(""not_found"", [False, True], ids=[""200"", ""404""])
async def test_get(mocker, not_found, spawn_client):
    client = await spawn_client(authorize=True)

    document = {
        ""_id"": ""foobar"",
        ""formatted"": False
    }

    if not not_found:
        await client.db.analyses.insert_one(document)

    m = mocker.stub(name=""format_analysis"")

    return_value = dict(document, formatted=True)

    async def format_analysis(db, document):
        m(db, document)
        return return_value

    mocker.patch(""virtool.sample_analysis.format_analysis"", new=format_analysis)

    resp = await client.get(""/api/analyses/foobar"")

    if not_found:
        assert resp.status == 404
    else:
        assert resp.status == 200

        assert await resp.json() == {
            ""id"": ""foobar"",
            ""formatted"": True
        }

        assert m.call_args[0] == (
            client.db,
            document
        )


@pytest.mark.parametrize(""has_sample"", [True, False], ids=[""with_sample"", ""without_sample""])
@pytest.mark.parametrize(""status"", [204, 404, 409])
async def test_remove(has_sample, status, spawn_client, resp_is, test_dispatch):
    client = await spawn_client(authorize=True)

    sample_document = None

    if has_sample:
        sample_document = {
            ""_id"": ""baz"",
            ""name"": ""Baz""
        }

        await client.db.samples.insert_one(sample_document)

    if status != 404:
        analysis_document = {
            ""_id"": ""foobar"",
            ""ready"": status == 204,
            ""sample"": {
                ""id"": ""baz"",
                ""name"": ""Baz""
            },
            ""job"": {
                ""id"": ""hello""
            }
        }

        await client.db.analyses.insert_one(analysis_document)

    resp = await client.delete(""/api/analyses/foobar"")

    assert resp.status == status

    if status == 409:
        assert await resp_is.conflict(resp, ""Analysis is still running. Cancel job 'hello' instead"")

    elif status == 404:
        assert await resp_is.not_found(resp)

    else:
        if has_sample:
            assert test_dispatch.stub.call_args[0] == (
                ""samples"",
                ""update"",
                {
                    ""id"": ""baz"",
                    ""name"": ""Baz""
                }
            )
        else:
            assert not test_dispatch.stub.called


```"
fbbd0a3f55c1e79ebf6ae7b872697611740edb24,foobar.py,foobar.py,,"# -*- coding: utf-8 -*-


class TreeElement(object):
    def __repr__(self):
        return u'{}'.format(self.__class__.__name__)


class Token(TreeElement):
    def __init__(self, value):
        if isinstance(value, TreeElement):
            raise TypeError
        self._value = value

    def __repr__(self):
        return u'{}({})'.format(self.__class__.__name__, self._value)

    def __gt__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) > hash(other)
        return self._value > other._value

    def __ge__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) >= hash(other)
        return self._value >= other._value

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self._value == other._value

    def __hash__(self):
        return hash((self.__class__.__name__, self._value))

    def tokens(self):
        yield self._value


class Not(TreeElement):
    def __init__(self, child):
        if not isinstance(child, TreeElement):
            raise TypeError
        self._child = child

    def __repr__(self):
        return u'{}({})'.format(self.__class__.__name__, self._child)

    def __gt__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) > hash(other)
        return self._child > other._child

    def __ge__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) >= hash(other)
        return self._child >= other._child

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self._child == other._child

    def __hash__(self):
        return hash((self.__class__.__name__, self._child))

    def tokens(self):
        for token in self._child.tokens():
            yield token


class GroupMixin(object):
    def __init__(self, a, b, *others):
        children = (a, b) + others

        for child in children:
            if not isinstance(child, TreeElement):
                raise TypeError
        self._children = tuple(sorted(children))

    def __repr__(self):
        return u'{}({})'.format(
            self.__class__.__name__,
            u', '.join(repr(child) for child in self._children)
        )

    def __gt__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) > hash(other)
        return self._children > other._children

    def __ge__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) >= hash(other)
        return self._children >= other._children

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self._children == other._children

    def __hash__(self):
        return hash(tuple([self.__class__.__name__]) + self._children)

    def tokens(self):
        for child in self._children:
            for token in child.tokens():
                yield token


class And(GroupMixin, TreeElement):
    pass


class Or(GroupMixin, TreeElement):
    pass
","Implement Token and Not, And, Or classes","Implement Token and Not, And, Or classes
",Python,mit,hackebrot/i-am-bool,117,"```python
# -*- coding: utf-8 -*-


class TreeElement(object):
    def __repr__(self):
        return u'{}'.format(self.__class__.__name__)


class Token(TreeElement):
    def __init__(self, value):
        if isinstance(value, TreeElement):
            raise TypeError
        self._value = value

    def __repr__(self):
        return u'{}({})'.format(self.__class__.__name__, self._value)

    def __gt__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) > hash(other)
        return self._value > other._value

    def __ge__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) >= hash(other)
        return self._value >= other._value

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self._value == other._value

    def __hash__(self):
        return hash((self.__class__.__name__, self._value))

    def tokens(self):
        yield self._value


class Not(TreeElement):
    def __init__(self, child):
        if not isinstance(child, TreeElement):
            raise TypeError
        self._child = child

    def __repr__(self):
        return u'{}({})'.format(self.__class__.__name__, self._child)

    def __gt__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) > hash(other)
        return self._child > other._child

    def __ge__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) >= hash(other)
        return self._child >= other._child

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self._child == other._child

    def __hash__(self):
        return hash((self.__class__.__name__, self._child))

    def tokens(self):
        for token in self._child.tokens():
            yield token


class GroupMixin(object):
    def __init__(self, a, b, *others):
        children = (a, b) + others

        for child in children:
            if not isinstance(child, TreeElement):
                raise TypeError
        self._children = tuple(sorted(children))

    def __repr__(self):
        return u'{}({})'.format(
            self.__class__.__name__,
            u', '.join(repr(child) for child in self._children)
        )

    def __gt__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) > hash(other)
        return self._children > other._children

    def __ge__(self, other):
        if not isinstance(other, self.__class__):
            return hash(self) >= hash(other)
        return self._children >= other._children

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self._children == other._children

    def __hash__(self):
        return hash(tuple([self.__class__.__name__]) + self._children)

    def tokens(self):
        for child in self._children:
            for token in child.tokens():
                yield token


class And(GroupMixin, TreeElement):
    pass


class Or(GroupMixin, TreeElement):
    pass

```"
67120e72883d4a5fd86dca2fba26599e65e7ea39,every_election/apps/elections/management/commands/add_referendum.py,every_election/apps/elections/management/commands/add_referendum.py,,"from django.core.management import BaseCommand

from elections.models import (
    Election,
    ElectionType,
    ModerationHistory,
    ModerationStatuses,
)
from organisations.models.organisations import Organisation


class Command(BaseCommand):
    help = """"""
    Adds an election with an election type of referendum
    Example usage:
    python manage.py add_referendum --date 2021-10-07 \
        --council croydon \
        --election-title ""Governance referendum"" \
        --official-identifier CRY \
        --org-type local-authority
    """"""

    def add_arguments(self, parser):
        parser.add_argument(
            ""-d"",
            ""--date"",
            action=""store"",
            dest=""date"",
            help=""The date the referendum is taking place"",
            required=True,
        )
        parser.add_argument(
            ""-c"",
            ""--council"",
            action=""store"",
            dest=""council"",
            help=""The council area the referendum is taking place"",
            required=True,
        )
        parser.add_argument(
            ""-t"",
            ""--election-title"",
            action=""store"",
            dest=""election_title"",
            help=""The election title to be used"",
            required=True,
        )
        parser.add_argument(
            ""--official-identifier"",
            action=""store"",
            dest=""official_identifier"",
            help=""The official identifier for the related Organisation. Election will cover the whole organisation unless a division ID is passed"",
            required=True,
        )
        parser.add_argument(
            ""--org-type"",
            action=""store"",
            dest=""org_type"",
            help=""The council area the referendum is taking place"",
            required=True,
        )
        parser.add_argument(
            ""--division-id"",
            action=""store"",
            dest=""division_id"",
            help=""ID of a OrganisationDivision if applicable"",
            required=False,
        )

    def handle(self, *args, **options):
        election_date = options[""date""]
        election_id = f""ref.{options['council']}.{election_date}""

        referendum_type = ElectionType.objects.get(election_type=""ref"")
        ref_election, created = Election.private_objects.update_or_create(
            election_id=election_id,
            defaults={
                ""election_type"": referendum_type,
                ""poll_open_date"": election_date,
                ""election_title"": options[""election_title""],
                ""current"": True,
                ""division_id"": options[""division_id""],
            },
        )

        self.stdout.write(f""{'Created' if created else 'Updated'} {election_id}"")

        ModerationHistory.objects.get_or_create(
            status_id=ModerationStatuses.approved.value,
            election=ref_election,
        )

        org = Organisation.objects.get_by_date(
            date=election_date,
            official_identifier=options[""official_identifier""],
            organisation_type=options[""org_type""],
        )
        ref_election.organisation = org
        ref_election.organisation_geography = org.geographies.latest()
        if ref_election.division:
            ref_election.division_geography = ref_election.division.geography

        ref_election.save()
",Add management command to add a refendeum election,"Add management command to add a refendeum election

- Added to allow us to add Croydon referendum initially, with
the possibility of being able to use again in the future as required
",Python,bsd-3-clause,"DemocracyClub/EveryElection,DemocracyClub/EveryElection,DemocracyClub/EveryElection",104,"```python
from django.core.management import BaseCommand

from elections.models import (
    Election,
    ElectionType,
    ModerationHistory,
    ModerationStatuses,
)
from organisations.models.organisations import Organisation


class Command(BaseCommand):
    help = """"""
    Adds an election with an election type of referendum
    Example usage:
    python manage.py add_referendum --date 2021-10-07 \
        --council croydon \
        --election-title ""Governance referendum"" \
        --official-identifier CRY \
        --org-type local-authority
    """"""

    def add_arguments(self, parser):
        parser.add_argument(
            ""-d"",
            ""--date"",
            action=""store"",
            dest=""date"",
            help=""The date the referendum is taking place"",
            required=True,
        )
        parser.add_argument(
            ""-c"",
            ""--council"",
            action=""store"",
            dest=""council"",
            help=""The council area the referendum is taking place"",
            required=True,
        )
        parser.add_argument(
            ""-t"",
            ""--election-title"",
            action=""store"",
            dest=""election_title"",
            help=""The election title to be used"",
            required=True,
        )
        parser.add_argument(
            ""--official-identifier"",
            action=""store"",
            dest=""official_identifier"",
            help=""The official identifier for the related Organisation. Election will cover the whole organisation unless a division ID is passed"",
            required=True,
        )
        parser.add_argument(
            ""--org-type"",
            action=""store"",
            dest=""org_type"",
            help=""The council area the referendum is taking place"",
            required=True,
        )
        parser.add_argument(
            ""--division-id"",
            action=""store"",
            dest=""division_id"",
            help=""ID of a OrganisationDivision if applicable"",
            required=False,
        )

    def handle(self, *args, **options):
        election_date = options[""date""]
        election_id = f""ref.{options['council']}.{election_date}""

        referendum_type = ElectionType.objects.get(election_type=""ref"")
        ref_election, created = Election.private_objects.update_or_create(
            election_id=election_id,
            defaults={
                ""election_type"": referendum_type,
                ""poll_open_date"": election_date,
                ""election_title"": options[""election_title""],
                ""current"": True,
                ""division_id"": options[""division_id""],
            },
        )

        self.stdout.write(f""{'Created' if created else 'Updated'} {election_id}"")

        ModerationHistory.objects.get_or_create(
            status_id=ModerationStatuses.approved.value,
            election=ref_election,
        )

        org = Organisation.objects.get_by_date(
            date=election_date,
            official_identifier=options[""official_identifier""],
            organisation_type=options[""org_type""],
        )
        ref_election.organisation = org
        ref_election.organisation_geography = org.geographies.latest()
        if ref_election.division:
            ref_election.division_geography = ref_election.division.geography

        ref_election.save()

```"
1dd8120dab6cdec4097bea1193a5b5b68d3bfe4f,salt/modules/win_groupadd.py,salt/modules/win_groupadd.py,,"'''
Manage groups on Windows
'''

def __virtual__():
    '''
    Set the group module if the kernel is Windows
    '''
    return 'group' if __grains__['kernel'] == 'Windows' else False


def add(name):
    '''
    Add the specified group

    CLI Example::

        salt '*' group.add foo 
    '''
    cmd = 'net localgroup {0} /add'.format(name)

    ret = __salt__['cmd.run_all'](cmd)

    return not ret['retcode']


def delete(name):
    '''
    Remove the named group

    CLI Example::

        salt '*' group.delete foo
    '''
    ret = __salt__['cmd.run_all']('net localgroup {0} /delete'.format(name))

    return not ret['retcode']


def info(name):
    '''
    Return information about a group

    CLI Example::

        salt '*' group.info foo
    '''
    lines = __salt__['cmd.run']('net localgroup {0}'.format(name)).split('\n')
    memberline = False
    gr_mem = []
    gr_name = ''
    for line in lines:
        if 'Alias name' in line:
            comps = line.split('  ', 1)
            gr_name = comps[1].strip()
        if 'successfully' in line:
            memberline = False
        if memberline:
            gr_mem.append(line.strip())    
        if '---' in line:
            memberline = True
    if not gr_name:
        return False

    return {'name': gr_name,
            'passwd': None,
            'gid': None,
            'members': gr_mem}


def getent():
    '''
    Return info on all groups

    CLI Example::

        salt '*' group.getent
    '''
    ret = []
    lines = __salt__['cmd.run']('net localgroup').split('\n')
    groupline = False
    for line in lines:
        if 'successfully' in line:
            groupline = False
        if groupline:
            ret.append(line.strip('*').strip())    
        if '---' in line:
            groupline = True

    return ret
","Add Windows support to group add Add, remove and get info on Local Windows Groups","Add Windows support to group add
Add, remove and get info on  Local Windows Groups
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",91,"```python
'''
Manage groups on Windows
'''

def __virtual__():
    '''
    Set the group module if the kernel is Windows
    '''
    return 'group' if __grains__['kernel'] == 'Windows' else False


def add(name):
    '''
    Add the specified group

    CLI Example::

        salt '*' group.add foo 
    '''
    cmd = 'net localgroup {0} /add'.format(name)

    ret = __salt__['cmd.run_all'](cmd)

    return not ret['retcode']


def delete(name):
    '''
    Remove the named group

    CLI Example::

        salt '*' group.delete foo
    '''
    ret = __salt__['cmd.run_all']('net localgroup {0} /delete'.format(name))

    return not ret['retcode']


def info(name):
    '''
    Return information about a group

    CLI Example::

        salt '*' group.info foo
    '''
    lines = __salt__['cmd.run']('net localgroup {0}'.format(name)).split('\n')
    memberline = False
    gr_mem = []
    gr_name = ''
    for line in lines:
        if 'Alias name' in line:
            comps = line.split('  ', 1)
            gr_name = comps[1].strip()
        if 'successfully' in line:
            memberline = False
        if memberline:
            gr_mem.append(line.strip())    
        if '---' in line:
            memberline = True
    if not gr_name:
        return False

    return {'name': gr_name,
            'passwd': None,
            'gid': None,
            'members': gr_mem}


def getent():
    '''
    Return info on all groups

    CLI Example::

        salt '*' group.getent
    '''
    ret = []
    lines = __salt__['cmd.run']('net localgroup').split('\n')
    groupline = False
    for line in lines:
        if 'successfully' in line:
            groupline = False
        if groupline:
            ret.append(line.strip('*').strip())    
        if '---' in line:
            groupline = True

    return ret

```"
b6353857dc124c8a9295cab865418ee888adf1e9,examples/plot_mne_example.py,examples/plot_mne_example.py,,"""""""
Using NeuroDSP with MNE
=======================

This example explores some example analyses using NeuroDSP, integrated with MNE.
""""""

###################################################################################################

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from mne.datasets import sample

from neurodsp.spectral import compute_spectrum, trim_spectrum
from neurodsp.burst import detect_bursts_dual_threshold

###################################################################################################

# Get the data path for the MNE example data
raw_fname = sample.data_path() + '/MEG/sample/sample_audvis_filt-0-40_raw.fif'

# Load the file of example MNE data
raw = io.read_raw_fif(raw_fname, preload=True, verbose=False)

###################################################################################################

# Select EEG channels from the dataset
raw = raw.pick_types(meg=False, eeg=True, eog=True, exclude='bads')

# Grab the sampling rate from the data
fs = raw.info['sfreq']

###################################################################################################

# Settings
ch_label = 'EEG 058'
t_start = 20000
t_stop = int(t_start + (10 * fs))

###################################################################################################

# Extract an example channel to explore
sig, times = raw.get_data(mne.pick_channels(raw.ch_names, [ch_label]), start=t_start, stop=t_stop, return_times=True)
sig = np.squeeze(sig)

###################################################################################################

# Plot a segment of the extracted time series data
plt.figure(figsize=(16, 3))
plt.plot(times, sig, 'k')

###################################################################################################

# Calculate the power spectrum, using a median welch & extract a frequency range of interest
freqs, powers = compute_spectrum(sig, fs, method='median')
freqs, powers = trim_spectrum(freqs, powers, [3, 30])

###################################################################################################

# Check where the peak power is, in center frequency
peak_cf = freqs[np.argmax(powers)]
print(peak_cf)

###################################################################################################

# Plot the power spectra
plt.figure(figsize=(8, 8))
plt.semilogy(freqs, powers)
plt.plot(freqs[np.argmax(powers)], np.max(powers), '.r', ms=12)

###################################################################################################

# Burst Settings
amp_dual_thresh = (1., 1.5)
f_range = (peak_cf-2, peak_cf+2)

###################################################################################################

# Detect bursts of high amplitude oscillations in the extracted signal
bursting = detect_bursts_dual_threshold(sig, fs, f_range, amp_dual_thresh)

###################################################################################################

# Plot original signal and burst activity
plt.figure(figsize=(16, 3))
plt.plot(times, sig, 'k', label='Raw Data')
plt.plot(times[bursting], sig[bursting], 'r', label='Detected Bursts')
plt.legend(loc='best')

###################################################################################################
",Add draft of MNE example,"Add draft of MNE example
",Python,apache-2.0,"srcole/neurodsp,srcole/neurodsp,voytekresearch/neurodsp",94,"```python
""""""
Using NeuroDSP with MNE
=======================

This example explores some example analyses using NeuroDSP, integrated with MNE.
""""""

###################################################################################################

import numpy as np
import matplotlib.pyplot as plt

import mne
from mne import io
from mne.datasets import sample

from neurodsp.spectral import compute_spectrum, trim_spectrum
from neurodsp.burst import detect_bursts_dual_threshold

###################################################################################################

# Get the data path for the MNE example data
raw_fname = sample.data_path() + '/MEG/sample/sample_audvis_filt-0-40_raw.fif'

# Load the file of example MNE data
raw = io.read_raw_fif(raw_fname, preload=True, verbose=False)

###################################################################################################

# Select EEG channels from the dataset
raw = raw.pick_types(meg=False, eeg=True, eog=True, exclude='bads')

# Grab the sampling rate from the data
fs = raw.info['sfreq']

###################################################################################################

# Settings
ch_label = 'EEG 058'
t_start = 20000
t_stop = int(t_start + (10 * fs))

###################################################################################################

# Extract an example channel to explore
sig, times = raw.get_data(mne.pick_channels(raw.ch_names, [ch_label]), start=t_start, stop=t_stop, return_times=True)
sig = np.squeeze(sig)

###################################################################################################

# Plot a segment of the extracted time series data
plt.figure(figsize=(16, 3))
plt.plot(times, sig, 'k')

###################################################################################################

# Calculate the power spectrum, using a median welch & extract a frequency range of interest
freqs, powers = compute_spectrum(sig, fs, method='median')
freqs, powers = trim_spectrum(freqs, powers, [3, 30])

###################################################################################################

# Check where the peak power is, in center frequency
peak_cf = freqs[np.argmax(powers)]
print(peak_cf)

###################################################################################################

# Plot the power spectra
plt.figure(figsize=(8, 8))
plt.semilogy(freqs, powers)
plt.plot(freqs[np.argmax(powers)], np.max(powers), '.r', ms=12)

###################################################################################################

# Burst Settings
amp_dual_thresh = (1., 1.5)
f_range = (peak_cf-2, peak_cf+2)

###################################################################################################

# Detect bursts of high amplitude oscillations in the extracted signal
bursting = detect_bursts_dual_threshold(sig, fs, f_range, amp_dual_thresh)

###################################################################################################

# Plot original signal and burst activity
plt.figure(figsize=(16, 3))
plt.plot(times, sig, 'k', label='Raw Data')
plt.plot(times[bursting], sig[bursting], 'r', label='Detected Bursts')
plt.legend(loc='best')

###################################################################################################

```"
dec17e4d8eb88610fbe81aeef84ea41b76b0e398,ListItems.py,ListItems.py,,"import os
import json
import re

###############################################
# Run this from the root of the assets folder #
###############################################

# Some code from http://www.lifl.fr/~riquetd/parse-a-json-file-with-comments.html
# ""Parse a JSON file with comments""

comment_re = re.compile(
    '(^)?[^\S\n]*/(?:\*(.*?)\*/[^\S\n]*|/[^\n]*)($)?',
    re.DOTALL | re.MULTILINE
)

def parse_json(content):
    """""" Parse a JSON file
        First remove comments and then use the json module package
    """"""
    ## Looking for comments
    match = comment_re.search(content)
    while match:
        # single line comment
        content = content[:match.start()] + content[match.end():]
        match = comment_re.search(content)

    # Replace -. with -0.
    content = content.replace(""-."", ""-0."")

    # Return json file
    return json.loads(content)

def getPath(root, file):
    path = os.path.join(root, file)
    return path[2:].replace(""\\"", ""/"")

imageKeys = [""inventoryIcon"", ""image""]
def getImageFromData(data):
    # Not all items just have an ""inventoryIcon""
    for key in imageKeys:
        if key in data:
            return data[key]
    return None

def getItemDetails(root, filename):
    itemPath = getPath(root, filename)
    #print(itemPath)

    fh = open(itemPath, ""r"")
    data = parse_json(fh.read())

    # It will either have objectName or itemName
    try:
        itemName = data[""itemName""]
    except KeyError:
        itemName = data[""objectName""]

    # Not all items have a static inventory icon
    image = getImageFromData(data)
    if image is not None:
        iconPath = getPath(root, image)
    else:
        print(""No image for: "" + itemPath)
        iconPath = None

    return itemName, { ""itemPath"": itemPath, ""iconPath"": iconPath }

def getAllItems(itemType):
    itemDict = {}
    extension = ""."" + itemType
    for root, dirs, files in os.walk("".""):
        for file in files:
            if file.endswith(extension):
                itemName, details = getItemDetails(root, file)
                itemDict[itemName] = details

    return itemDict

allItems = {}

# ""generatedsword"", ""generatedgun"", ""generatedshield"", ""codexitem""
itemTypes = [
    ""item"", 
    ""matitem"", ""miningtool"", ""flashlight"", ""wiretool"", ""beamaxe"", 
    ""tillingtool"", ""painttool"", ""gun"", ""sword"", ""harvestingtool"", 
    ""head"", ""chest"", ""legs"", ""back"", ""coinitem"", ""consumable"", 
    ""blueprint"", ""techitem"", ""instrument"", ""grapplinghook"", 
    ""thrownitem"", ""celestial"", ""object""
]

for itemType in itemTypes:
    allItems[itemType] = getAllItems(itemType)

outFile = open(""items.json"", ""w"")
json.dump(allItems, outFile)

print(""All items written to items.json"")
",Add a python script to list all items,"Add a python script to list all items

Saves the item name, item path, and image path into ""items.json""
",Python,mit,McSimp/starbound-research,99,"```python
import os
import json
import re

###############################################
# Run this from the root of the assets folder #
###############################################

# Some code from http://www.lifl.fr/~riquetd/parse-a-json-file-with-comments.html
# ""Parse a JSON file with comments""

comment_re = re.compile(
    '(^)?[^\S\n]*/(?:\*(.*?)\*/[^\S\n]*|/[^\n]*)($)?',
    re.DOTALL | re.MULTILINE
)

def parse_json(content):
    """""" Parse a JSON file
        First remove comments and then use the json module package
    """"""
    ## Looking for comments
    match = comment_re.search(content)
    while match:
        # single line comment
        content = content[:match.start()] + content[match.end():]
        match = comment_re.search(content)

    # Replace -. with -0.
    content = content.replace(""-."", ""-0."")

    # Return json file
    return json.loads(content)

def getPath(root, file):
    path = os.path.join(root, file)
    return path[2:].replace(""\\"", ""/"")

imageKeys = [""inventoryIcon"", ""image""]
def getImageFromData(data):
    # Not all items just have an ""inventoryIcon""
    for key in imageKeys:
        if key in data:
            return data[key]
    return None

def getItemDetails(root, filename):
    itemPath = getPath(root, filename)
    #print(itemPath)

    fh = open(itemPath, ""r"")
    data = parse_json(fh.read())

    # It will either have objectName or itemName
    try:
        itemName = data[""itemName""]
    except KeyError:
        itemName = data[""objectName""]

    # Not all items have a static inventory icon
    image = getImageFromData(data)
    if image is not None:
        iconPath = getPath(root, image)
    else:
        print(""No image for: "" + itemPath)
        iconPath = None

    return itemName, { ""itemPath"": itemPath, ""iconPath"": iconPath }

def getAllItems(itemType):
    itemDict = {}
    extension = ""."" + itemType
    for root, dirs, files in os.walk("".""):
        for file in files:
            if file.endswith(extension):
                itemName, details = getItemDetails(root, file)
                itemDict[itemName] = details

    return itemDict

allItems = {}

# ""generatedsword"", ""generatedgun"", ""generatedshield"", ""codexitem""
itemTypes = [
    ""item"", 
    ""matitem"", ""miningtool"", ""flashlight"", ""wiretool"", ""beamaxe"", 
    ""tillingtool"", ""painttool"", ""gun"", ""sword"", ""harvestingtool"", 
    ""head"", ""chest"", ""legs"", ""back"", ""coinitem"", ""consumable"", 
    ""blueprint"", ""techitem"", ""instrument"", ""grapplinghook"", 
    ""thrownitem"", ""celestial"", ""object""
]

for itemType in itemTypes:
    allItems[itemType] = getAllItems(itemType)

outFile = open(""items.json"", ""w"")
json.dump(allItems, outFile)

print(""All items written to items.json"")

```"
d59103daa62897996b3585c2a826b092caf95a76,non_deterministic.py,non_deterministic.py,,"# Non-Deterministic Turing Machine Simulator

class Queue():
    def __init__(self):
        self.queue = []

    def enqueue(self, state, head, string, iter_count):
        self.queue.append((state, head, string, iter_count))

    def dequeue(self):
        item = self.queue[0]
        self.queue = self.queue[1:]
        return item

    def is_empty(self):
        return len(self.queue) == 0

class TuringMachine():
    def __init__(self, transitions, accepted_states, max_iterations):
        self.transitions = transitions
        self.accepted_states = accepted_states
        self.max_iterations = max_iterations

    def validate_string(self, string):
        head = 0
        state = self.transitions[0][0]
        iter_count = 1

        self.queue = Queue()
        self.queue.enqueue(head, state, string, iter_count)

        outputs = self.validate_symbol()
        return self.output(outputs)

    def validate_symbol(self):
        if self.queue.is_empty():
            return [0]

        
        (state, head, string, iter_count) = self.queue.dequeue()
        outputs = []
        symbol = string[head]

        for (current_state, current_symbol, next_symbol, move, next_state) in self.transitions:
            if state == current_state and (symbol == current_symbol or current_symbol == 'x'):
                if next_state in accepted_states and head == len(string) - 1:
                    return [1]

                if iter_count > max_iterations:
                    outputs = outputs + ['u']
                else:
                    head_copy, string_copy = head, string

                    if next_symbol != 'x':
                        string_copy[head] = next_symbol

                    (head_copy, string_copy) = self.update_values(head_copy, string_copy, move)

                    self.queue.enqueue(next_state, head_copy, string_copy, iter_count + 1)

        outputs = outputs + self.validate_symbol()
        return outputs

    def update_values(self, head, string, move):
        if move == 'r':
            head += 1
            if head == len(string):
                string = string + ['_']
        elif move == 'l':
            head -= 1
            if head == 0:
                string = ['_'] + string
        return (head, string)

    def output(self, outputs):
        if 1 in outputs:
            return 'Accepted'
        if 0 in outputs:
            return 'Rejected'
        return 'Undefined'


# Example: Automata that accepts strings that ends with 'b'
transitions = [(0, 'x', 'x', 'r', 0), (0, 'b', 'b', 's', 1)]
accepted_states = [1]
max_iterations = 50
input_strings = ['aa', 'ab', 'aaab', 'ababab', 'bba']

turing_machine = TuringMachine(transitions, accepted_states, max_iterations)
for string in input_strings:
    output = turing_machine.validate_string(list(string))
    print(string, output)


",Implement non-deterministic turing machine in Python,"Implement non-deterministic turing machine in Python
",Python,mit,"yedhukrishnan/turing-machine,yedhukrishnan/turing-machine",95,"```python
# Non-Deterministic Turing Machine Simulator

class Queue():
    def __init__(self):
        self.queue = []

    def enqueue(self, state, head, string, iter_count):
        self.queue.append((state, head, string, iter_count))

    def dequeue(self):
        item = self.queue[0]
        self.queue = self.queue[1:]
        return item

    def is_empty(self):
        return len(self.queue) == 0

class TuringMachine():
    def __init__(self, transitions, accepted_states, max_iterations):
        self.transitions = transitions
        self.accepted_states = accepted_states
        self.max_iterations = max_iterations

    def validate_string(self, string):
        head = 0
        state = self.transitions[0][0]
        iter_count = 1

        self.queue = Queue()
        self.queue.enqueue(head, state, string, iter_count)

        outputs = self.validate_symbol()
        return self.output(outputs)

    def validate_symbol(self):
        if self.queue.is_empty():
            return [0]

        
        (state, head, string, iter_count) = self.queue.dequeue()
        outputs = []
        symbol = string[head]

        for (current_state, current_symbol, next_symbol, move, next_state) in self.transitions:
            if state == current_state and (symbol == current_symbol or current_symbol == 'x'):
                if next_state in accepted_states and head == len(string) - 1:
                    return [1]

                if iter_count > max_iterations:
                    outputs = outputs + ['u']
                else:
                    head_copy, string_copy = head, string

                    if next_symbol != 'x':
                        string_copy[head] = next_symbol

                    (head_copy, string_copy) = self.update_values(head_copy, string_copy, move)

                    self.queue.enqueue(next_state, head_copy, string_copy, iter_count + 1)

        outputs = outputs + self.validate_symbol()
        return outputs

    def update_values(self, head, string, move):
        if move == 'r':
            head += 1
            if head == len(string):
                string = string + ['_']
        elif move == 'l':
            head -= 1
            if head == 0:
                string = ['_'] + string
        return (head, string)

    def output(self, outputs):
        if 1 in outputs:
            return 'Accepted'
        if 0 in outputs:
            return 'Rejected'
        return 'Undefined'


# Example: Automata that accepts strings that ends with 'b'
transitions = [(0, 'x', 'x', 'r', 0), (0, 'b', 'b', 's', 1)]
accepted_states = [1]
max_iterations = 50
input_strings = ['aa', 'ab', 'aaab', 'ababab', 'bba']

turing_machine = TuringMachine(transitions, accepted_states, max_iterations)
for string in input_strings:
    output = turing_machine.validate_string(list(string))
    print(string, output)



```"
a5f380db22e20265b4d543827f052300b2fb3fa4,tests/test_choose.py,tests/test_choose.py,,"from tests.base import IntegrationTest
from time import sleep


class TestChooseProject(IntegrationTest):

    viminput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    vimoutput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    tasks = [
        dict(description=""test task 1"", project=""Home""),
        dict(description=""test task 2""),
    ]

    def execute(self):
        self.client.normal('2gg')
        sleep(1)

        self.command(""TaskWikiChooseProject"")
        sleep(1)

        self.client.normal('5gg')
        sleep(0.5)
        self.client.feedkeys(""\\<CR>"")
        sleep(1)

        for task in self.tasks:
            task.refresh()

        assert self.tasks[0]['project'] == ""Home""
        assert self.tasks[1]['project'] == ""Home""


class TestChooseProjectUnset(IntegrationTest):

    viminput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    vimoutput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    tasks = [
        dict(description=""test task 1"", project=""Home""),
        dict(description=""test task 2""),
    ]

    def execute(self):
        self.client.normal('1gg')
        sleep(0.5)

        self.command(""TaskWikiChooseProject"")
        sleep(0.5)

        self.client.normal('4gg')
        sleep(0.5)
        self.client.feedkeys(""\\<CR>"")
        sleep(0.5)

        for task in self.tasks:
            task.refresh()

        assert self.tasks[0]['project'] == None
        assert self.tasks[1]['project'] == None


class TestChooseProjectCanceled(IntegrationTest):

    viminput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    vimoutput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    tasks = [
        dict(description=""test task 1"", project=""Home""),
        dict(description=""test task 2""),
    ]

    def execute(self):
        self.client.normal('1gg')
        sleep(0.5)

        self.command(""TaskWikiChooseProject"")
        sleep(0.5)

        self.client.normal('4gg')
        sleep(0.5)
        self.client.feedkeys(""q"")
        sleep(0.5)

        for task in self.tasks:
            task.refresh()

        assert self.tasks[0]['project'] == ""Home""
        assert self.tasks[1]['project'] == None

",Add tests for TaskWikiChooseProject command,"tests: Add tests for TaskWikiChooseProject command
",Python,mit,"Spirotot/taskwiki,phha/taskwiki",112,"```python
from tests.base import IntegrationTest
from time import sleep


class TestChooseProject(IntegrationTest):

    viminput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    vimoutput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    tasks = [
        dict(description=""test task 1"", project=""Home""),
        dict(description=""test task 2""),
    ]

    def execute(self):
        self.client.normal('2gg')
        sleep(1)

        self.command(""TaskWikiChooseProject"")
        sleep(1)

        self.client.normal('5gg')
        sleep(0.5)
        self.client.feedkeys(""\\<CR>"")
        sleep(1)

        for task in self.tasks:
            task.refresh()

        assert self.tasks[0]['project'] == ""Home""
        assert self.tasks[1]['project'] == ""Home""


class TestChooseProjectUnset(IntegrationTest):

    viminput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    vimoutput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    tasks = [
        dict(description=""test task 1"", project=""Home""),
        dict(description=""test task 2""),
    ]

    def execute(self):
        self.client.normal('1gg')
        sleep(0.5)

        self.command(""TaskWikiChooseProject"")
        sleep(0.5)

        self.client.normal('4gg')
        sleep(0.5)
        self.client.feedkeys(""\\<CR>"")
        sleep(0.5)

        for task in self.tasks:
            task.refresh()

        assert self.tasks[0]['project'] == None
        assert self.tasks[1]['project'] == None


class TestChooseProjectCanceled(IntegrationTest):

    viminput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    vimoutput = """"""
    * [ ] test task 1  #{uuid}
    * [ ] test task 2  #{uuid}
    """"""

    tasks = [
        dict(description=""test task 1"", project=""Home""),
        dict(description=""test task 2""),
    ]

    def execute(self):
        self.client.normal('1gg')
        sleep(0.5)

        self.command(""TaskWikiChooseProject"")
        sleep(0.5)

        self.client.normal('4gg')
        sleep(0.5)
        self.client.feedkeys(""q"")
        sleep(0.5)

        for task in self.tasks:
            task.refresh()

        assert self.tasks[0]['project'] == ""Home""
        assert self.tasks[1]['project'] == None


```"
365d61ee5620f0743ffcdeb9c6b09f2b4d66940c,grab.py,grab.py,,"#!/usr/bin/python3


import json
import requests
import argparse
from typing import Tuple
from os.path import exists


BASE_URL = 'https://leetcode.com/problems/'
GRAPHQL_API_URL = 'https://leetcode.com/graphql'
QUERY = '''query questionData($titleSlug: String!) {
                question(titleSlug: $titleSlug) {
                    questionId
                    questionFrontendId
                    boundTopicId
                    title
                    titleSlug
                    content
                    translatedTitle
                    translatedContent
                    difficulty
                    exampleTestcases
                    codeSnippets {
                        lang
                        langSlug
                        code
                    }
                }
            }'''


def get_url() -> str:
    parser = argparse.ArgumentParser(description='Grab leetcode problem')
    parser.add_argument(
        'slug', metavar='slug', type=str, nargs='+',
        help='Slug of the leetcode problem e.g.: two-sum',
    )
    parser.add_argument(
        '--force', action='store_true',
        help='Overwrite the file if it already exists',
    )
    args = parser.parse_args()
    return args.slug[0], args.force


def get_data(slug: str) -> Tuple[str, int, str, str, str]:
    resp = requests.post(GRAPHQL_API_URL, json={
        'query': QUERY,
        'variables': {
            'titleSlug': slug,
        }
    })
    question = json.loads(resp.text)['data']['question']
    difficulty = question['difficulty'].lower()
    nr = question['questionId']
    title = question['title']
    title_slug = question['titleSlug']
    url = f'{BASE_URL}{title_slug}/'

    code = ''
    for snippet in question['codeSnippets']:
        if snippet['langSlug'] == 'python3':
            code = snippet['code']
    return difficulty, nr, title, url, code


def create_file(difficulty: str, nr: int, title: str, url: str, code: str, *, force: bool) -> None:
    filename = f'{difficulty}/{nr}.py'

    if exists(filename) and not force:
        print(f'\n{filename} already exists! Use --force to overwrite.\n')
        return

    with open(filename, 'w') as f:
        f.write('""""""\n')
        f.write(f'{nr}. {title}\n')
        f.write('\n')
        f.write(f'{url}\n')
        f.write('""""""\n')
        f.write('\n\n')
        f.write(code)
        f.write('...\n')
        f.write((
            '\n\n'
            'def main():\n'
            '   s = Solution()\n'
            '   print(s.xxx())\n'
            '\n\n'
            ""if __name__ == '__main__':\n""
            '   raise(SystemExit(main()))'
        ))


def main() -> int:
    slug, force = get_url()
    difficulty, nr, title, url, code = get_data(slug)
    create_file(difficulty, nr, title, url, code, force=force)
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
",Create program that fethes code from leetcode.,"Create program that fethes code from leetcode.
",Python,mit,pisskidney/leetcode,105,"```python
#!/usr/bin/python3


import json
import requests
import argparse
from typing import Tuple
from os.path import exists


BASE_URL = 'https://leetcode.com/problems/'
GRAPHQL_API_URL = 'https://leetcode.com/graphql'
QUERY = '''query questionData($titleSlug: String!) {
                question(titleSlug: $titleSlug) {
                    questionId
                    questionFrontendId
                    boundTopicId
                    title
                    titleSlug
                    content
                    translatedTitle
                    translatedContent
                    difficulty
                    exampleTestcases
                    codeSnippets {
                        lang
                        langSlug
                        code
                    }
                }
            }'''


def get_url() -> str:
    parser = argparse.ArgumentParser(description='Grab leetcode problem')
    parser.add_argument(
        'slug', metavar='slug', type=str, nargs='+',
        help='Slug of the leetcode problem e.g.: two-sum',
    )
    parser.add_argument(
        '--force', action='store_true',
        help='Overwrite the file if it already exists',
    )
    args = parser.parse_args()
    return args.slug[0], args.force


def get_data(slug: str) -> Tuple[str, int, str, str, str]:
    resp = requests.post(GRAPHQL_API_URL, json={
        'query': QUERY,
        'variables': {
            'titleSlug': slug,
        }
    })
    question = json.loads(resp.text)['data']['question']
    difficulty = question['difficulty'].lower()
    nr = question['questionId']
    title = question['title']
    title_slug = question['titleSlug']
    url = f'{BASE_URL}{title_slug}/'

    code = ''
    for snippet in question['codeSnippets']:
        if snippet['langSlug'] == 'python3':
            code = snippet['code']
    return difficulty, nr, title, url, code


def create_file(difficulty: str, nr: int, title: str, url: str, code: str, *, force: bool) -> None:
    filename = f'{difficulty}/{nr}.py'

    if exists(filename) and not force:
        print(f'\n{filename} already exists! Use --force to overwrite.\n')
        return

    with open(filename, 'w') as f:
        f.write('""""""\n')
        f.write(f'{nr}. {title}\n')
        f.write('\n')
        f.write(f'{url}\n')
        f.write('""""""\n')
        f.write('\n\n')
        f.write(code)
        f.write('...\n')
        f.write((
            '\n\n'
            'def main():\n'
            '   s = Solution()\n'
            '   print(s.xxx())\n'
            '\n\n'
            ""if __name__ == '__main__':\n""
            '   raise(SystemExit(main()))'
        ))


def main() -> int:
    slug, force = get_url()
    difficulty, nr, title, url, code = get_data(slug)
    create_file(difficulty, nr, title, url, code, force=force)
    return 0


if __name__ == '__main__':
    raise SystemExit(main())

```"
086aad14864ddeae8cf050f5ed2b0f57773a4fad,tests/link_tests/substitution_tests.py,tests/link_tests/substitution_tests.py,,"from utils import LinkTest, main
import re

class SubstitutionTest(LinkTest):
    def testSubstititonFunction(self):
        text = """"""
        <pattern>
        """"""
        subs = [
            (r""<pattern>"", lambda m: 'Hello world!')
        ]
        expect = """"""
        Hello world!
        """"""
        self.assertSubstitution(text, expect, subs)

    def testSubstitionString(self):
        text = """"""
        <pattern>
        """"""
        subs = [
            (r""<pattern>"", 'Hello world!')
        ]
        expect = """"""
        Hello world!
        """"""
        self.assertSubstitution(text, expect, subs)

    def testOrdering(self):
        text = """"""
        <pattern>
        """"""
        subs = [
            (r""<pattern>"", 'Hello world!'),
            (r""<.+>"", 'Bye!')
        ]
        expect = """"""
        Hello world!
        """"""
        self.assertSubstitution(text, expect, subs)

    def testCounter(self):
        text = """"""
        <question>
        <question>
        <question>
        """"""
        count = 0
        def sub(m):
            nonlocal count
            count += 1
            return '<h1>Question {}</h1>'.format(count)
        subs = [
            (r""<question>"", sub),
        ]
        expect = """"""
        <h1>Question 1</h1>
        <h1>Question 2</h1>
        <h1>Question 3</h1>
        """"""
        self.assertSubstitution(text, expect, subs)

class ScrapeHeadersTest(LinkTest):
    def testBasic(self):
        text = """"""
        + Header 1
        + Not a header
        + Header 2
        + Header 3
        """"""
        regex = r""\+ Header (\d+)""
        translate = lambda m: 'Title ' + m.group(1)
        expect = [
            'Title 1',
            'Title 2',
            'Title 3',
        ]
        self.assertHeaders(text, regex, translate, expect)

    def testNoHeaders(self):
        text = """"""
        No headers
        to be found!
        """"""
        regex = r""\+ Header (\d+)""
        translate = lambda m: 'Title ' + m.group(1)
        expect = []
        self.assertHeaders(text, regex, translate, expect)

    def testCounter(self):
        text = """"""
        + Header To be
        + Header Or not to be
        + Header That is the question
        """"""
        regex = r""\+ Header ([\w ]+)""
        count = 0
        def translate(m):
            nonlocal count
            count += 1
            return ""Section {}: {}"".format(count, m.group(1))
        expect = [
            'Section 1: To be',
            'Section 2: Or not to be',
            'Section 3: That is the question',
        ]
        self.assertHeaders(text, regex, translate, expect)


if __name__ == '__main__':
    main()

",Add substitution and scrape_header tests,"Add substitution and scrape_header tests
",Python,mit,"albert12132/templar,albert12132/templar",113,"```python
from utils import LinkTest, main
import re

class SubstitutionTest(LinkTest):
    def testSubstititonFunction(self):
        text = """"""
        <pattern>
        """"""
        subs = [
            (r""<pattern>"", lambda m: 'Hello world!')
        ]
        expect = """"""
        Hello world!
        """"""
        self.assertSubstitution(text, expect, subs)

    def testSubstitionString(self):
        text = """"""
        <pattern>
        """"""
        subs = [
            (r""<pattern>"", 'Hello world!')
        ]
        expect = """"""
        Hello world!
        """"""
        self.assertSubstitution(text, expect, subs)

    def testOrdering(self):
        text = """"""
        <pattern>
        """"""
        subs = [
            (r""<pattern>"", 'Hello world!'),
            (r""<.+>"", 'Bye!')
        ]
        expect = """"""
        Hello world!
        """"""
        self.assertSubstitution(text, expect, subs)

    def testCounter(self):
        text = """"""
        <question>
        <question>
        <question>
        """"""
        count = 0
        def sub(m):
            nonlocal count
            count += 1
            return '<h1>Question {}</h1>'.format(count)
        subs = [
            (r""<question>"", sub),
        ]
        expect = """"""
        <h1>Question 1</h1>
        <h1>Question 2</h1>
        <h1>Question 3</h1>
        """"""
        self.assertSubstitution(text, expect, subs)

class ScrapeHeadersTest(LinkTest):
    def testBasic(self):
        text = """"""
        + Header 1
        + Not a header
        + Header 2
        + Header 3
        """"""
        regex = r""\+ Header (\d+)""
        translate = lambda m: 'Title ' + m.group(1)
        expect = [
            'Title 1',
            'Title 2',
            'Title 3',
        ]
        self.assertHeaders(text, regex, translate, expect)

    def testNoHeaders(self):
        text = """"""
        No headers
        to be found!
        """"""
        regex = r""\+ Header (\d+)""
        translate = lambda m: 'Title ' + m.group(1)
        expect = []
        self.assertHeaders(text, regex, translate, expect)

    def testCounter(self):
        text = """"""
        + Header To be
        + Header Or not to be
        + Header That is the question
        """"""
        regex = r""\+ Header ([\w ]+)""
        count = 0
        def translate(m):
            nonlocal count
            count += 1
            return ""Section {}: {}"".format(count, m.group(1))
        expect = [
            'Section 1: To be',
            'Section 2: Or not to be',
            'Section 3: That is the question',
        ]
        self.assertHeaders(text, regex, translate, expect)


if __name__ == '__main__':
    main()


```"
fba9cb1278afab3c16ec4077770a33c09950e1a7,waterbutler/core/path.py,waterbutler/core/path.py,,"import os

class WaterButlerPathPart:
    DECODE = lambda x: x
    ENCODE = lambda x: x

    def __init__(self, part, _id=None):
        self._id = _id
        self._part = part

    @property
    def identifier(self):
        return self._id

    @property
    def value(self):
        return self.__class__.DECODE(self._part)

    @property
    def raw(self):
        return self._part

    def __repr__(self):
        return '{}({!r})'.format(self.__class__.__name__, self._part)


class WaterButlerPath:
    """"""
    A standardized and validated immutable WaterButler path.
    """"""

    PART_CLASS = WaterButlerPathPart

    @classmethod
    def from_parts(cls, parts, folder=False):
        _ids, _parts = [], ['']
        for part in parts:
            _ids.append(part.identifier)
            _parts.append(part.raw)

        return cls('/'.join(_parts), _ids=_ids, folder=folder)

    def __init__(self, path, _ids=(), prepend=None, folder=None):
        self._generic_path_validation(path)

        self._orig_path = path
        path = path.strip('/').split('/')

        _ids = [None] * len(_ids) - len(path) + _ids

        self._parts = [
            self.PART_CLASS(part, _id)
            for part, _id
            in zip(path, _ids)
        ]

        if folder is not None:
            self.is_folder = bool(folder)
        else:
            self.is_folder = self.path.endswith('/')

    @property
    def parts(self):
        return self._parts

    @property
    def name(self):
        return self._parts[-1].value

    @property
    def identifier(self):
        return self._parts[-1].identifier

    @property
    def path(self):
        return '/'.join([x.value for x in self.parts])

    @property
    def parent(self):
        return self.__class__.from_parts(self.parts[:-1])

    def _generic_path_validation(self, path):
        pass

    def __str__(self):
        return '/'.join([''] + [x.raw for x in self.parts]) + ('/' if self.is_folder else '')

    def __repr__(self):
        return '{}({!r})'.format(self.__class__.__name__, self._orig_path)
",Add a new variant of WaterButlerPath,"Add a new variant of WaterButlerPath
",Python,apache-2.0,"rdhyee/waterbutler,rafaeldelucena/waterbutler,CenterForOpenScience/waterbutler,TomBaxter/waterbutler,felliott/waterbutler,Johnetordoff/waterbutler,cosenal/waterbutler,icereval/waterbutler,chrisseto/waterbutler,hmoco/waterbutler,RCOSDP/waterbutler,Ghalko/waterbutler,kwierman/waterbutler",90,"```python
import os

class WaterButlerPathPart:
    DECODE = lambda x: x
    ENCODE = lambda x: x

    def __init__(self, part, _id=None):
        self._id = _id
        self._part = part

    @property
    def identifier(self):
        return self._id

    @property
    def value(self):
        return self.__class__.DECODE(self._part)

    @property
    def raw(self):
        return self._part

    def __repr__(self):
        return '{}({!r})'.format(self.__class__.__name__, self._part)


class WaterButlerPath:
    """"""
    A standardized and validated immutable WaterButler path.
    """"""

    PART_CLASS = WaterButlerPathPart

    @classmethod
    def from_parts(cls, parts, folder=False):
        _ids, _parts = [], ['']
        for part in parts:
            _ids.append(part.identifier)
            _parts.append(part.raw)

        return cls('/'.join(_parts), _ids=_ids, folder=folder)

    def __init__(self, path, _ids=(), prepend=None, folder=None):
        self._generic_path_validation(path)

        self._orig_path = path
        path = path.strip('/').split('/')

        _ids = [None] * len(_ids) - len(path) + _ids

        self._parts = [
            self.PART_CLASS(part, _id)
            for part, _id
            in zip(path, _ids)
        ]

        if folder is not None:
            self.is_folder = bool(folder)
        else:
            self.is_folder = self.path.endswith('/')

    @property
    def parts(self):
        return self._parts

    @property
    def name(self):
        return self._parts[-1].value

    @property
    def identifier(self):
        return self._parts[-1].identifier

    @property
    def path(self):
        return '/'.join([x.value for x in self.parts])

    @property
    def parent(self):
        return self.__class__.from_parts(self.parts[:-1])

    def _generic_path_validation(self, path):
        pass

    def __str__(self):
        return '/'.join([''] + [x.raw for x in self.parts]) + ('/' if self.is_folder else '')

    def __repr__(self):
        return '{}({!r})'.format(self.__class__.__name__, self._orig_path)

```"
1ec0c1d949e9379fc2a01bf480a782bb4d75afb9,test.py,test.py,,"#!/bin/env python3
# -*- coding: utf-8 -*-

""""""
Test the 'send_morse' module.
""""""

import sys
import os
import getopt
import threading
sys.path.append('..')
from sound_morse import SoundMorse


# get program name from sys.argv
prog_name = sys.argv[0]
if prog_name.endswith('.py'):
    prog_name = prog_name[:-3]


def usage(msg=None):
    if msg:
        print(('*'*80 + '\n%s\n' + '*'*80) % msg)
    print(""\n""
          ""CLI program to continually send a morse string.\n\n""
          ""Usage: %s [-h] [-s c,w] <string>\n\n""
          ""where -h      means print this help and stop\n""
          ""      -s c,w  means set char and word speeds\n\n""
          ""and   <string>  is the morse string to repeatedly send\n""
          ""The morse sound is created in a separate thread."" % prog_name)

def send_morse(string, sound_object):
    # sound each character in the string
    # we do this in this strange way so setting a global from main code will stop the thread
    for ch in string:
        if StopThread:
            return
        sound_object.send(ch)


# parse the CLI params
argv = sys.argv[1:]

try:
    (opts, args) = getopt.getopt(argv, 'hs:', ['help', '--speed='])
except getopt.GetoptError as err:
    usage(err)
    sys.exit(1)

morse_string = ''.join(args)

cwpm = 25
wpm = 15
for (opt, param) in opts:
    if opt in ['-h', '--help']:
        usage()
        sys.exit(0)
    elif opt in ['-s', '--speed']:
        speeds = param.split(',')
        if len(speeds) not in (1, 2):
            usage(""-s option must be followed by one or two speeds, eg: '-s 20' or '- 10,5'"")
        cwpm = speeds[0]
        wpm = cwpm
        if len(speeds) == 2:
            (cwpm, wpm) = speeds
        cwpm = int(cwpm)
        wpm = int(wpm)

morse = SoundMorse()
morse.set_speeds(cwpm=cwpm, wpm=wpm)

StopThread = False

while True:
    for ch in morse_string:
        try:
            thread = threading.Thread(target=send_morse, args=(ch, morse))
            thread.start()
            thread.join()
            thread = None
        except KeyboardInterrupt:
            print('')
            StopThread = True
            if thread:
                thread.join()
            break
    print('*', end='')
    sys.stdout.flush()

",Test program for thread debugging,"Test program for thread debugging
",Python,mit,"rzzzwilson/morse_trainer,rzzzwilson/morse_trainer",91,"```python
#!/bin/env python3
# -*- coding: utf-8 -*-

""""""
Test the 'send_morse' module.
""""""

import sys
import os
import getopt
import threading
sys.path.append('..')
from sound_morse import SoundMorse


# get program name from sys.argv
prog_name = sys.argv[0]
if prog_name.endswith('.py'):
    prog_name = prog_name[:-3]


def usage(msg=None):
    if msg:
        print(('*'*80 + '\n%s\n' + '*'*80) % msg)
    print(""\n""
          ""CLI program to continually send a morse string.\n\n""
          ""Usage: %s [-h] [-s c,w] <string>\n\n""
          ""where -h      means print this help and stop\n""
          ""      -s c,w  means set char and word speeds\n\n""
          ""and   <string>  is the morse string to repeatedly send\n""
          ""The morse sound is created in a separate thread."" % prog_name)

def send_morse(string, sound_object):
    # sound each character in the string
    # we do this in this strange way so setting a global from main code will stop the thread
    for ch in string:
        if StopThread:
            return
        sound_object.send(ch)


# parse the CLI params
argv = sys.argv[1:]

try:
    (opts, args) = getopt.getopt(argv, 'hs:', ['help', '--speed='])
except getopt.GetoptError as err:
    usage(err)
    sys.exit(1)

morse_string = ''.join(args)

cwpm = 25
wpm = 15
for (opt, param) in opts:
    if opt in ['-h', '--help']:
        usage()
        sys.exit(0)
    elif opt in ['-s', '--speed']:
        speeds = param.split(',')
        if len(speeds) not in (1, 2):
            usage(""-s option must be followed by one or two speeds, eg: '-s 20' or '- 10,5'"")
        cwpm = speeds[0]
        wpm = cwpm
        if len(speeds) == 2:
            (cwpm, wpm) = speeds
        cwpm = int(cwpm)
        wpm = int(wpm)

morse = SoundMorse()
morse.set_speeds(cwpm=cwpm, wpm=wpm)

StopThread = False

while True:
    for ch in morse_string:
        try:
            thread = threading.Thread(target=send_morse, args=(ch, morse))
            thread.start()
            thread.join()
            thread = None
        except KeyboardInterrupt:
            print('')
            StopThread = True
            if thread:
                thread.join()
            break
    print('*', end='')
    sys.stdout.flush()


```"
6c53c55204955cf0682c655321219b4e67481030,lab/10/template_10_c.py,lab/10/template_10_c.py,,"# Template Binary Search Tree

class Node:
    
    '''
    Class untuk node dari Binary Search Tree
    Terdiri dari value dari node
    dan reference ke left child dan right child
    '''
    
    def __init__(self, value):
        self.value = value
        self.left = None
        self.right = None

# Benny-ry Search Tree
# *binary
class BST:
    
    '''
    Class untuk Binary Search Tree
    Terdiri dari kumpulan-kumpulan node
    yang sudah tersusun dalam bentuk tree
    '''
    
    def __init__(self):
        self.root = None

    '''
    Method untuk set root dari tree
    '''
    def set_root(self, value):
        self.root = Node(value)

    '''
    Method insert
    Digunakan untuk memasukkan nilai ke dalam tree
    Jika tree masih kosong, nilai yang dimasukkan menjadi root
    Jika tree sudah terisi, nilai yang dimasukkan akan dicek,
    kalau lebih kecil daripada elemen yang akan dibandingkan akan dimasukkan
    sebagai left child, sebaliknya akan ke right child
    -------------------------------------------------------------------------
    @param value
    '''
    def insert(self, value):
        if(self.root is None):
            self.set_root(value)
        else:
            self.insert_node(self.root, value)

    def insert_node(self, current_node, value):
        if(value < current_node.value):
            if(current_node.left):
                self.insert_node(current_node.left, value)
            else:
                current_node.left = Node(value)
        elif(value > current_node.value):
            if(current_node.right):
                self.insert_node(current_node.right, value)
            else:
                current_node.right = Node(value)

    '''
    Method find
    Digunakan untuk mencari sebuah nilai di dalam tree
    --------------------------------------------------
    @param value
    @return boolean
    '''
    def find(self, value):
        return self.find_node(self.root, value)

    def find_node(self, current_node, value):        
        # TODO implementasikan code ini menggunakan recursion
        # current_node adalah node yang sedang ditunjuk
        # value adalah nilai yang dicari
        # method ini mengembalikan True jika value terdapat di dalam BST,
        # atau False jika value tidak terdapat di dalam BST
        pass


def main():
    # Inisiasi binary search tree
    bst = BST()

    # Memasukkan elemen ke dalam tree
    angka = [50,40,60,30,45,57,75,21,44,47,65,90]
    for i in angka:
        bst.insert(i)

    while(True):
    # Meminta input dari user, angka yang yang akan dicek apakah ada dalam tree
        x = int(input('Masukkan sebuah integer yang ingin dicari di dalam tree: '))
        if(bst.find(x)):
            print(""{} terdapat dalam tree"".format(x))
        else:
            print(""{} tidak ada dalam tree"".format(x))

main()
",Add lab 10 template for class C,"Add lab 10 template for class C
",Python,mit,"giovanism/TarungLab,laymonage/TarungLab",100,"```python
# Template Binary Search Tree

class Node:
    
    '''
    Class untuk node dari Binary Search Tree
    Terdiri dari value dari node
    dan reference ke left child dan right child
    '''
    
    def __init__(self, value):
        self.value = value
        self.left = None
        self.right = None

# Benny-ry Search Tree
# *binary
class BST:
    
    '''
    Class untuk Binary Search Tree
    Terdiri dari kumpulan-kumpulan node
    yang sudah tersusun dalam bentuk tree
    '''
    
    def __init__(self):
        self.root = None

    '''
    Method untuk set root dari tree
    '''
    def set_root(self, value):
        self.root = Node(value)

    '''
    Method insert
    Digunakan untuk memasukkan nilai ke dalam tree
    Jika tree masih kosong, nilai yang dimasukkan menjadi root
    Jika tree sudah terisi, nilai yang dimasukkan akan dicek,
    kalau lebih kecil daripada elemen yang akan dibandingkan akan dimasukkan
    sebagai left child, sebaliknya akan ke right child
    -------------------------------------------------------------------------
    @param value
    '''
    def insert(self, value):
        if(self.root is None):
            self.set_root(value)
        else:
            self.insert_node(self.root, value)

    def insert_node(self, current_node, value):
        if(value < current_node.value):
            if(current_node.left):
                self.insert_node(current_node.left, value)
            else:
                current_node.left = Node(value)
        elif(value > current_node.value):
            if(current_node.right):
                self.insert_node(current_node.right, value)
            else:
                current_node.right = Node(value)

    '''
    Method find
    Digunakan untuk mencari sebuah nilai di dalam tree
    --------------------------------------------------
    @param value
    @return boolean
    '''
    def find(self, value):
        return self.find_node(self.root, value)

    def find_node(self, current_node, value):        
        # TODO implementasikan code ini menggunakan recursion
        # current_node adalah node yang sedang ditunjuk
        # value adalah nilai yang dicari
        # method ini mengembalikan True jika value terdapat di dalam BST,
        # atau False jika value tidak terdapat di dalam BST
        pass


def main():
    # Inisiasi binary search tree
    bst = BST()

    # Memasukkan elemen ke dalam tree
    angka = [50,40,60,30,45,57,75,21,44,47,65,90]
    for i in angka:
        bst.insert(i)

    while(True):
    # Meminta input dari user, angka yang yang akan dicek apakah ada dalam tree
        x = int(input('Masukkan sebuah integer yang ingin dicari di dalam tree: '))
        if(bst.find(x)):
            print(""{} terdapat dalam tree"".format(x))
        else:
            print(""{} tidak ada dalam tree"".format(x))

main()

```"
76ced10ebc0c7ef36924ca4e123ba637a53e0784,test_server.py,test_server.py,,"import datetime

from flask.ext.testing import TestCase
import mongomock
import mock

from server import app, get_db

class BaseTest(TestCase):
    def create_app(self):
        app.config['TESTING'] = True
        return app

    def setUp(self):
        self.mongo_patcher = mock.patch('pymongo.MongoClient', mongomock.MongoClient)
        self.mongo_patcher.start()

    def tearDown(self):
        self.mongo_patcher.stop()


class TestGeoNotesApi(BaseTest):
    NOTE1 = {
        'txt': ""test"",
        'lat': 0,
        'lng': 0,
        'dt': datetime.datetime(2012, 11, 10, 0, 0),
    }

    NOTE2 = {
        'txt': ""test"",
        'lat': 0,
        'lng': 0,
        'dt': datetime.datetime(2012, 11, 10, 0, 0),
    }

    def test_get_empty(self):
        resp = self.client.get(""/geonotes"")
        self.assert200(resp)
        self.assertEquals(resp.json, [])

    def test_get(self):
        _id = get_db().insert(self.NOTE1)
        resp = self.client.get(""/geonotes"")
        self.assert200(resp)
        self.assertEquals(len(resp.json), 1)
        self.assertEqual(resp.json[0]['id'], str(_id))

    def test_get_several(self):
        _id1 = get_db().insert(self.NOTE1)
        _id2 = get_db().insert(self.NOTE2)
        resp = self.client.get(""/geonotes"")
        self.assert200(resp)
        self.assertEquals(len(resp.json), 2)

    def test_post_one_error(self):
        resp = self.client.post(""/geonotes"", {})
        self.assert400(resp)

        resp = self.client.post(""/geonotes"", {'lat': 9})
        self.assert400(resp)

    def test_post_one(self):
        to_post = {
            'text_content': 'test',
            'lat': 5,
            'lng': 9,
            'date': datetime.datetime(2013, 11, 10, 0, 0).isoformat()
        }
        resp = self.client.post(""/geonotes"", data=to_post)
        self.assertStatus(resp, 201)
        self.assertTrue('id' in resp.json)


class TestGeoNoteApi(BaseTest):
    NOTE1 = {
        'txt': ""test"",
        'lat': 0,
        'lng': 0,
        'dt': datetime.datetime(2012, 11, 10, 0, 0),
    }

    def test_get(self):
        id_ = get_db().insert(self.NOTE1)
        resp = self.client.get('/geonotes/%s' % id_)
        self.assert200(resp)
        self.assertEqual(resp.json['id'], str(id_))


",Add tests for RESTful server,"Add tests for RESTful server
",Python,bsd-2-clause,"AAzza/geonaut,AAzza/geonaut",90,"```python
import datetime

from flask.ext.testing import TestCase
import mongomock
import mock

from server import app, get_db

class BaseTest(TestCase):
    def create_app(self):
        app.config['TESTING'] = True
        return app

    def setUp(self):
        self.mongo_patcher = mock.patch('pymongo.MongoClient', mongomock.MongoClient)
        self.mongo_patcher.start()

    def tearDown(self):
        self.mongo_patcher.stop()


class TestGeoNotesApi(BaseTest):
    NOTE1 = {
        'txt': ""test"",
        'lat': 0,
        'lng': 0,
        'dt': datetime.datetime(2012, 11, 10, 0, 0),
    }

    NOTE2 = {
        'txt': ""test"",
        'lat': 0,
        'lng': 0,
        'dt': datetime.datetime(2012, 11, 10, 0, 0),
    }

    def test_get_empty(self):
        resp = self.client.get(""/geonotes"")
        self.assert200(resp)
        self.assertEquals(resp.json, [])

    def test_get(self):
        _id = get_db().insert(self.NOTE1)
        resp = self.client.get(""/geonotes"")
        self.assert200(resp)
        self.assertEquals(len(resp.json), 1)
        self.assertEqual(resp.json[0]['id'], str(_id))

    def test_get_several(self):
        _id1 = get_db().insert(self.NOTE1)
        _id2 = get_db().insert(self.NOTE2)
        resp = self.client.get(""/geonotes"")
        self.assert200(resp)
        self.assertEquals(len(resp.json), 2)

    def test_post_one_error(self):
        resp = self.client.post(""/geonotes"", {})
        self.assert400(resp)

        resp = self.client.post(""/geonotes"", {'lat': 9})
        self.assert400(resp)

    def test_post_one(self):
        to_post = {
            'text_content': 'test',
            'lat': 5,
            'lng': 9,
            'date': datetime.datetime(2013, 11, 10, 0, 0).isoformat()
        }
        resp = self.client.post(""/geonotes"", data=to_post)
        self.assertStatus(resp, 201)
        self.assertTrue('id' in resp.json)


class TestGeoNoteApi(BaseTest):
    NOTE1 = {
        'txt': ""test"",
        'lat': 0,
        'lng': 0,
        'dt': datetime.datetime(2012, 11, 10, 0, 0),
    }

    def test_get(self):
        id_ = get_db().insert(self.NOTE1)
        resp = self.client.get('/geonotes/%s' % id_)
        self.assert200(resp)
        self.assertEqual(resp.json['id'], str(id_))



```"
f26e8927f83c3a897d4f474762bca9775467e74e,src/helpers/vyos-load-config.py,src/helpers/vyos-load-config.py,,"#!/usr/bin/env python3
#
# Copyright (C) 2019 VyOS maintainers and contributors
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 2 or later as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#

""""""Load config file from within config session.
Config file specified by URI or path (without scheme prefix).
Example: load https://somewhere.net/some.config
        or
         load /tmp/some.config
""""""

import sys
import tempfile
import vyos.defaults
import vyos.remote
from vyos.config import Config, VyOSError
from vyos.migrator import Migrator, MigratorError

system_config_file = 'config.boot'

class LoadConfig(Config):
    """"""A subclass for calling 'loadFile'.
    This does not belong in config.py, and only has a single caller.
    """"""
    def load_config(self, file_path):
        cmd = [self._cli_shell_api, 'loadFile', file_path]
        self._run(cmd)

if len(sys.argv) > 1:
    file_name = sys.argv[1]
else:
    file_name = system_config_file

configdir = vyos.defaults.directories['config']

protocols = ['scp', 'sftp', 'http', 'https', 'ftp', 'tftp']

if any(x in file_name for x in protocols):
    config_file = vyos.remote.get_remote_config(file_name)
    if not config_file:
        sys.exit(""No config file by that name."")
else:
    canonical_path = '{0}/{1}'.format(configdir, file_name)
    try:
        with open(canonical_path, 'r') as f:
            config_file = f.read()
    except OSError as err1:
        try:
            with open(file_name, 'r') as f:
                config_file = f.read()
        except OSError as err2:
            sys.exit('{0}\n{1}'.format(err1, err2))

config = LoadConfig()

print(""Loading configuration from '{}'"".format(file_name))

with tempfile.NamedTemporaryFile() as fp:
    with open(fp.name, 'w') as fd:
        fd.write(config_file)

    migration = Migrator(fp.name)
    try:
        migration.run()
    except MigratorError as err:
        sys.exit('{}'.format(err))

    try:
        config.load_config(fp.name)
    except VyOSError as err:
        sys.exit('{}'.format(err))

if config.session_changed():
    print(""Load complete. Use 'commit' to make changes effective."")
else:
    print(""No configuration changes to commit."")
",Rewrite the config load script,"T1424: Rewrite the config load script

Rewrite of the load functionality of vyatta-load-config.pl, removing the
dependency on Vyatta::Config.
",Python,lgpl-2.1,"vyos/vyos-1x,vyos/vyos-1x,vyos/vyos-1x,vyos/vyos-1x",91,"```python
#!/usr/bin/env python3
#
# Copyright (C) 2019 VyOS maintainers and contributors
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License version 2 or later as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.
#
#

""""""Load config file from within config session.
Config file specified by URI or path (without scheme prefix).
Example: load https://somewhere.net/some.config
        or
         load /tmp/some.config
""""""

import sys
import tempfile
import vyos.defaults
import vyos.remote
from vyos.config import Config, VyOSError
from vyos.migrator import Migrator, MigratorError

system_config_file = 'config.boot'

class LoadConfig(Config):
    """"""A subclass for calling 'loadFile'.
    This does not belong in config.py, and only has a single caller.
    """"""
    def load_config(self, file_path):
        cmd = [self._cli_shell_api, 'loadFile', file_path]
        self._run(cmd)

if len(sys.argv) > 1:
    file_name = sys.argv[1]
else:
    file_name = system_config_file

configdir = vyos.defaults.directories['config']

protocols = ['scp', 'sftp', 'http', 'https', 'ftp', 'tftp']

if any(x in file_name for x in protocols):
    config_file = vyos.remote.get_remote_config(file_name)
    if not config_file:
        sys.exit(""No config file by that name."")
else:
    canonical_path = '{0}/{1}'.format(configdir, file_name)
    try:
        with open(canonical_path, 'r') as f:
            config_file = f.read()
    except OSError as err1:
        try:
            with open(file_name, 'r') as f:
                config_file = f.read()
        except OSError as err2:
            sys.exit('{0}\n{1}'.format(err1, err2))

config = LoadConfig()

print(""Loading configuration from '{}'"".format(file_name))

with tempfile.NamedTemporaryFile() as fp:
    with open(fp.name, 'w') as fd:
        fd.write(config_file)

    migration = Migrator(fp.name)
    try:
        migration.run()
    except MigratorError as err:
        sys.exit('{}'.format(err))

    try:
        config.load_config(fp.name)
    except VyOSError as err:
        sys.exit('{}'.format(err))

if config.session_changed():
    print(""Load complete. Use 'commit' to make changes effective."")
else:
    print(""No configuration changes to commit."")

```"
612d2e3f749244c00404d97d93982c88538ce8aa,mk_json_query.py,mk_json_query.py,,"#python mk_json_quert.py & excute it in backgroud
# excute ES query and export it to apache directory for data visulization
import datetime
from elasticsearch import Elasticsearch
from elasticsearch import helpers
import os.path
import time
import json

#JSON export for further data visulization
FILE_PATH_JSON1 = ""/var/www/cache_info1.json""

#ES information
ES_HOST = {
    ""host"" : ""10.0.0.158"", 
    ""port"" : 9200
}
INDEX_NAME = 'ats'
TYPE_NAME = 'accesslog'
POLL_INTERVAL = 10 #10 seconds

mBody = {
  ""query"": {
    ""filtered"": {
      ""query"": {
        ""match_all"": {}
      },
      ""filter"": {
        ""range"": {
          ""accessTime"": {
            ""gte"": ""now-1d/d""
          }
        }
      }
    }
  },
  ""size"": 0,
  ""aggregations"": {
    ""accessTime_1h"": {
      ""date_histogram"": {
        ""field"": ""accessTime"",
        ""interval"": ""1h"",
        ""order"": {
          ""_key"": ""desc""
        },
        ""min_doc_count"": 0
      },
      ""aggs"": {
        ""hit_ratio"": {
          ""avg"": {
            ""field"": ""cacheCode""
          }
        },
        ""size_access_info"": {
          ""terms"": {
            ""field"": ""cacheCode"",
            ""size"": 2
          },
          ""aggs"": {
            ""sum_cache_size"": {
              ""sum"": {
                ""field"": ""contentLength""
              }
            }
          }
        }
      }
    },
    ""accessTime_5m"": {
      ""date_histogram"": {
        ""field"": ""accessTime"",
        ""interval"": ""5m"",
        ""order"": {
          ""_key"": ""desc""
        },
        ""min_doc_count"": 0
      },
      ""aggs"": {
        ""hit_ratio"": {
          ""avg"": {
            ""field"": ""cacheCode""
          }
        },
        ""size_access_info"": {
          ""terms"": {
            ""field"": ""cacheCode"",
            ""size"": 2
          },
          ""aggs"": {
            ""sum_cache_size"": {
              ""sum"": {
                ""field"": ""contentLength""
              }
            }
          }
        }
      }
    }
  }
}


# create ES client, create index
es = Elasticsearch(hosts = [ES_HOST])

def exportInfo(mBody,filePath):
    #print (mBody)
    res = es.search(index = INDEX_NAME, size=0, body = mBody)
    #print(res)
    f = open(filePath,'w') #clear the contents
    f.write(json.dumps(res))
    f.close()

#Main function    
if __name__ == '__main__':
    while (True):  
        exportInfo(mBody,FILE_PATH_JSON1)
        time.sleep(POLL_INTERVAL)


        


",Add export data into apache folder script,"Add export data into apache folder script
",Python,mit,"yuecong/tools,yuecong/tools,yuecong/tools,yuecong/tools",124,"```python
#python mk_json_quert.py & excute it in backgroud
# excute ES query and export it to apache directory for data visulization
import datetime
from elasticsearch import Elasticsearch
from elasticsearch import helpers
import os.path
import time
import json

#JSON export for further data visulization
FILE_PATH_JSON1 = ""/var/www/cache_info1.json""

#ES information
ES_HOST = {
    ""host"" : ""10.0.0.158"", 
    ""port"" : 9200
}
INDEX_NAME = 'ats'
TYPE_NAME = 'accesslog'
POLL_INTERVAL = 10 #10 seconds

mBody = {
  ""query"": {
    ""filtered"": {
      ""query"": {
        ""match_all"": {}
      },
      ""filter"": {
        ""range"": {
          ""accessTime"": {
            ""gte"": ""now-1d/d""
          }
        }
      }
    }
  },
  ""size"": 0,
  ""aggregations"": {
    ""accessTime_1h"": {
      ""date_histogram"": {
        ""field"": ""accessTime"",
        ""interval"": ""1h"",
        ""order"": {
          ""_key"": ""desc""
        },
        ""min_doc_count"": 0
      },
      ""aggs"": {
        ""hit_ratio"": {
          ""avg"": {
            ""field"": ""cacheCode""
          }
        },
        ""size_access_info"": {
          ""terms"": {
            ""field"": ""cacheCode"",
            ""size"": 2
          },
          ""aggs"": {
            ""sum_cache_size"": {
              ""sum"": {
                ""field"": ""contentLength""
              }
            }
          }
        }
      }
    },
    ""accessTime_5m"": {
      ""date_histogram"": {
        ""field"": ""accessTime"",
        ""interval"": ""5m"",
        ""order"": {
          ""_key"": ""desc""
        },
        ""min_doc_count"": 0
      },
      ""aggs"": {
        ""hit_ratio"": {
          ""avg"": {
            ""field"": ""cacheCode""
          }
        },
        ""size_access_info"": {
          ""terms"": {
            ""field"": ""cacheCode"",
            ""size"": 2
          },
          ""aggs"": {
            ""sum_cache_size"": {
              ""sum"": {
                ""field"": ""contentLength""
              }
            }
          }
        }
      }
    }
  }
}


# create ES client, create index
es = Elasticsearch(hosts = [ES_HOST])

def exportInfo(mBody,filePath):
    #print (mBody)
    res = es.search(index = INDEX_NAME, size=0, body = mBody)
    #print(res)
    f = open(filePath,'w') #clear the contents
    f.write(json.dumps(res))
    f.close()

#Main function    
if __name__ == '__main__':
    while (True):  
        exportInfo(mBody,FILE_PATH_JSON1)
        time.sleep(POLL_INTERVAL)


        



```"
1ced12173272a670b001cf17aa0beb08bae2eb8a,scipy/fftpack/realtransforms.py,scipy/fftpack/realtransforms.py,,"""""""
Real spectrum tranforms (DCT, DST, MDCT)
""""""

__all__ = ['dct1', 'dct2']

import numpy as np
from scipy.fftpack import _fftpack

import atexit
atexit.register(_fftpack.destroy_dct1_cache)
atexit.register(_fftpack.destroy_dct2_cache)

def dct1(x, n=None):
    """"""
    Return Discrete Cosine Transform (type I) of arbitrary type sequence x.

    Parameters
    ----------
    x : array-like
        input array.
    n : int, optional
        Length of the transform.

    Returns
    -------
    y : real ndarray
    """"""
    return _dct(x, 1, n)

def dct2(x, n=None):
    """"""
    Return Discrete Cosine Transform (type II) of arbitrary type sequence x.

    Parameters
    ----------
    x : array-like
        input array.
    n : int, optional
        Length of the transform.

    Returns
    -------
    y : real ndarray
    """"""
    return _dct(x, 2, n)

def _dct(x, type, n=None, axis=-1, overwrite_x=0):
    """"""
    Return Discrete Cosine Transform of arbitrary type sequence x.

    Parameters
    ----------
    x : array-like
        input array.
    n : int, optional
        Length of the transform.
    axis : int, optional
        Axis along which the dct is computed. (default=-1)
    overwrite_x : bool, optional
        If True the contents of x can be destroyed. (default=False)

    Returns
    -------
    z : real ndarray

    """"""
    tmp = np.asarray(x)
    if not np.isrealobj(tmp):
        raise TypeError,""1st argument must be real sequence""

    if n is None:
        n = tmp.shape[axis]
    else:
        raise NotImplemented(""Padding/truncating not yet implemented"")

    if type == 1:
        f = _fftpack.dct1
    elif type == 2:
        f = _fftpack.dct2
    else:
        raise ValueError(""Type %d not understood"" % type)

    if axis == -1 or axis == len(tmp.shape) - 1:
        return f(tmp, n, 0, overwrite_x)
    else:
        raise NotImplementedError(""Axis arg not yet implemented"")

    #tmp = swapaxes(tmp, axis, -1)
    #tmp = work_function(tmp,n,1,0,overwrite_x)
    #return swapaxes(tmp, axis, -1)
",Add python wrapper around fftpack dct.,"Add python wrapper around fftpack dct.
",Python,bsd-3-clause,"ilayn/scipy,ortylp/scipy,kalvdans/scipy,perimosocordiae/scipy,surhudm/scipy,witcxc/scipy,befelix/scipy,pbrod/scipy,minhlongdo/scipy,efiring/scipy,sauliusl/scipy,vanpact/scipy,cpaulik/scipy,sriki18/scipy,endolith/scipy,jjhelmus/scipy,giorgiop/scipy,vigna/scipy,apbard/scipy,WarrenWeckesser/scipy,larsmans/scipy,woodscn/scipy,zaxliu/scipy,anielsen001/scipy,newemailjdm/scipy,lukauskas/scipy,matthew-brett/scipy,mikebenfield/scipy,Srisai85/scipy,perimosocordiae/scipy,sonnyhu/scipy,WarrenWeckesser/scipy,aarchiba/scipy,andyfaff/scipy,trankmichael/scipy,anntzer/scipy,ortylp/scipy,kleskjr/scipy,jamestwebber/scipy,fredrikw/scipy,anntzer/scipy,ogrisel/scipy,mikebenfield/scipy,sargas/scipy,ChanderG/scipy,Eric89GXL/scipy,witcxc/scipy,nonhermitian/scipy,gfyoung/scipy,maniteja123/scipy,josephcslater/scipy,richardotis/scipy,gfyoung/scipy,zaxliu/scipy,felipebetancur/scipy,pyramania/scipy,richardotis/scipy,ilayn/scipy,niknow/scipy,richardotis/scipy,mikebenfield/scipy,Stefan-Endres/scipy,vanpact/scipy,befelix/scipy,sauliusl/scipy,pschella/scipy,ilayn/scipy,piyush0609/scipy,mdhaber/scipy,richardotis/scipy,gef756/scipy,jjhelmus/scipy,Srisai85/scipy,newemailjdm/scipy,anntzer/scipy,pnedunuri/scipy,futurulus/scipy,minhlongdo/scipy,tylerjereddy/scipy,fernand/scipy,mtrbean/scipy,zerothi/scipy,gdooper/scipy,Newman101/scipy,trankmichael/scipy,njwilson23/scipy,nvoron23/scipy,woodscn/scipy,Dapid/scipy,vberaudi/scipy,rmcgibbo/scipy,jor-/scipy,FRidh/scipy,gdooper/scipy,WarrenWeckesser/scipy,andim/scipy,person142/scipy,gertingold/scipy,Eric89GXL/scipy,apbard/scipy,Srisai85/scipy,woodscn/scipy,Srisai85/scipy,lhilt/scipy,andyfaff/scipy,e-q/scipy,kleskjr/scipy,sargas/scipy,argriffing/scipy,Gillu13/scipy,sonnyhu/scipy,zaxliu/scipy,anielsen001/scipy,tylerjereddy/scipy,argriffing/scipy,josephcslater/scipy,WillieMaddox/scipy,giorgiop/scipy,vhaasteren/scipy,nonhermitian/scipy,jseabold/scipy,perimosocordiae/scipy,Stefan-Endres/scipy,kalvdans/scipy,chatcannon/scipy,e-q/scipy,surhudm/scipy,dch312/scipy,aman-iitj/scipy,jsilter/scipy,sonnyhu/scipy,mortonjt/scipy,grlee77/scipy,mtrbean/scipy,pyramania/scipy,niknow/scipy,rgommers/scipy,aman-iitj/scipy,kalvdans/scipy,tylerjereddy/scipy,witcxc/scipy,dominicelse/scipy,Shaswat27/scipy,gef756/scipy,pbrod/scipy,ndchorley/scipy,juliantaylor/scipy,rmcgibbo/scipy,trankmichael/scipy,vhaasteren/scipy,efiring/scipy,ChanderG/scipy,Newman101/scipy,witcxc/scipy,jseabold/scipy,jakevdp/scipy,WarrenWeckesser/scipy,dch312/scipy,pizzathief/scipy,njwilson23/scipy,arokem/scipy,lhilt/scipy,fernand/scipy,zxsted/scipy,trankmichael/scipy,WillieMaddox/scipy,Dapid/scipy,behzadnouri/scipy,gertingold/scipy,arokem/scipy,rmcgibbo/scipy,andyfaff/scipy,raoulbq/scipy,dch312/scipy,fernand/scipy,felipebetancur/scipy,chatcannon/scipy,Shaswat27/scipy,vhaasteren/scipy,mortada/scipy,juliantaylor/scipy,dominicelse/scipy,haudren/scipy,lukauskas/scipy,andim/scipy,pnedunuri/scipy,jseabold/scipy,mdhaber/scipy,josephcslater/scipy,WarrenWeckesser/scipy,fredrikw/scipy,haudren/scipy,futurulus/scipy,haudren/scipy,aeklant/scipy,mtrbean/scipy,mgaitan/scipy,lukauskas/scipy,Kamp9/scipy,endolith/scipy,Srisai85/scipy,cpaulik/scipy,jjhelmus/scipy,mhogg/scipy,vanpact/scipy,mgaitan/scipy,scipy/scipy,zxsted/scipy,nmayorov/scipy,gfyoung/scipy,ndchorley/scipy,jseabold/scipy,andyfaff/scipy,Shaswat27/scipy,lukauskas/scipy,jonycgn/scipy,perimosocordiae/scipy,matthewalbani/scipy,Kamp9/scipy,sriki18/scipy,Dapid/scipy,jamestwebber/scipy,maniteja123/scipy,jseabold/scipy,Gillu13/scipy,Kamp9/scipy,scipy/scipy,efiring/scipy,arokem/scipy,FRidh/scipy,gfyoung/scipy,dominicelse/scipy,mhogg/scipy,zaxliu/scipy,jonycgn/scipy,jonycgn/scipy,fredrikw/scipy,Srisai85/scipy,Stefan-Endres/scipy,pschella/scipy,mgaitan/scipy,maciejkula/scipy,ales-erjavec/scipy,aeklant/scipy,piyush0609/scipy,bkendzior/scipy,nmayorov/scipy,hainm/scipy,matthew-brett/scipy,lhilt/scipy,lukauskas/scipy,fredrikw/scipy,sauliusl/scipy,vhaasteren/scipy,raoulbq/scipy,vanpact/scipy,dch312/scipy,zxsted/scipy,mortonjt/scipy,pizzathief/scipy,pschella/scipy,raoulbq/scipy,surhudm/scipy,nvoron23/scipy,kleskjr/scipy,vanpact/scipy,efiring/scipy,Gillu13/scipy,perimosocordiae/scipy,WarrenWeckesser/scipy,jakevdp/scipy,aeklant/scipy,fernand/scipy,gef756/scipy,vhaasteren/scipy,aman-iitj/scipy,andim/scipy,maniteja123/scipy,teoliphant/scipy,haudren/scipy,Shaswat27/scipy,sonnyhu/scipy,jor-/scipy,grlee77/scipy,kalvdans/scipy,teoliphant/scipy,aeklant/scipy,mortada/scipy,zerothi/scipy,endolith/scipy,mhogg/scipy,rmcgibbo/scipy,nmayorov/scipy,maciejkula/scipy,gef756/scipy,Dapid/scipy,maniteja123/scipy,mdhaber/scipy,jonycgn/scipy,mgaitan/scipy,mhogg/scipy,pyramania/scipy,niknow/scipy,mdhaber/scipy,nonhermitian/scipy,chatcannon/scipy,zerothi/scipy,anielsen001/scipy,mhogg/scipy,ChanderG/scipy,ogrisel/scipy,surhudm/scipy,minhlongdo/scipy,andim/scipy,mortonjt/scipy,pizzathief/scipy,argriffing/scipy,futurulus/scipy,niknow/scipy,woodscn/scipy,gef756/scipy,ales-erjavec/scipy,pizzathief/scipy,chatcannon/scipy,piyush0609/scipy,sauliusl/scipy,raoulbq/scipy,bkendzior/scipy,apbard/scipy,scipy/scipy,Dapid/scipy,Gillu13/scipy,teoliphant/scipy,niknow/scipy,ogrisel/scipy,petebachant/scipy,befelix/scipy,minhlongdo/scipy,tylerjereddy/scipy,WillieMaddox/scipy,sargas/scipy,fernand/scipy,anntzer/scipy,Gillu13/scipy,nvoron23/scipy,haudren/scipy,woodscn/scipy,jsilter/scipy,Eric89GXL/scipy,ales-erjavec/scipy,petebachant/scipy,haudren/scipy,bkendzior/scipy,vberaudi/scipy,lhilt/scipy,mgaitan/scipy,newemailjdm/scipy,behzadnouri/scipy,pschella/scipy,trankmichael/scipy,Newman101/scipy,vberaudi/scipy,njwilson23/scipy,cpaulik/scipy,vigna/scipy,mingwpy/scipy,rmcgibbo/scipy,sargas/scipy,behzadnouri/scipy,Stefan-Endres/scipy,jakevdp/scipy,mingwpy/scipy,surhudm/scipy,kleskjr/scipy,zaxliu/scipy,aarchiba/scipy,pyramania/scipy,newemailjdm/scipy,jamestwebber/scipy,Newman101/scipy,ortylp/scipy,aarchiba/scipy,zxsted/scipy,cpaulik/scipy,maciejkula/scipy,sriki18/scipy,mtrbean/scipy,anielsen001/scipy,pnedunuri/scipy,ogrisel/scipy,sauliusl/scipy,behzadnouri/scipy,jamestwebber/scipy,rmcgibbo/scipy,jor-/scipy,jjhelmus/scipy,bkendzior/scipy,WillieMaddox/scipy,aman-iitj/scipy,giorgiop/scipy,mtrbean/scipy,zaxliu/scipy,woodscn/scipy,mortonjt/scipy,hainm/scipy,cpaulik/scipy,jsilter/scipy,e-q/scipy,person142/scipy,e-q/scipy,zxsted/scipy,ortylp/scipy,matthew-brett/scipy,gfyoung/scipy,njwilson23/scipy,niknow/scipy,larsmans/scipy,raoulbq/scipy,gdooper/scipy,zxsted/scipy,endolith/scipy,andim/scipy,ilayn/scipy,ogrisel/scipy,larsmans/scipy,mingwpy/scipy,kleskjr/scipy,behzadnouri/scipy,hainm/scipy,mingwpy/scipy,mortonjt/scipy,WillieMaddox/scipy,futurulus/scipy,matthewalbani/scipy,nonhermitian/scipy,felipebetancur/scipy,jseabold/scipy,juliantaylor/scipy,sonnyhu/scipy,kleskjr/scipy,ales-erjavec/scipy,ales-erjavec/scipy,Kamp9/scipy,jamestwebber/scipy,rgommers/scipy,vigna/scipy,minhlongdo/scipy,futurulus/scipy,jonycgn/scipy,vigna/scipy,tylerjereddy/scipy,vanpact/scipy,piyush0609/scipy,petebachant/scipy,larsmans/scipy,befelix/scipy,mikebenfield/scipy,ilayn/scipy,scipy/scipy,sriki18/scipy,jor-/scipy,person142/scipy,newemailjdm/scipy,grlee77/scipy,jonycgn/scipy,pnedunuri/scipy,aman-iitj/scipy,trankmichael/scipy,argriffing/scipy,andim/scipy,behzadnouri/scipy,mortonjt/scipy,pbrod/scipy,efiring/scipy,ortylp/scipy,maciejkula/scipy,scipy/scipy,nvoron23/scipy,arokem/scipy,person142/scipy,sriki18/scipy,felipebetancur/scipy,mikebenfield/scipy,petebachant/scipy,gertingold/scipy,matthewalbani/scipy,cpaulik/scipy,sriki18/scipy,maciejkula/scipy,bkendzior/scipy,andyfaff/scipy,zerothi/scipy,fredrikw/scipy,pyramania/scipy,teoliphant/scipy,e-q/scipy,witcxc/scipy,argriffing/scipy,Shaswat27/scipy,juliantaylor/scipy,gdooper/scipy,petebachant/scipy,Gillu13/scipy,teoliphant/scipy,endolith/scipy,gdooper/scipy,vigna/scipy,vberaudi/scipy,rgommers/scipy,grlee77/scipy,nvoron23/scipy,ChanderG/scipy,mortada/scipy,dominicelse/scipy,matthew-brett/scipy,ChanderG/scipy,ndchorley/scipy,nmayorov/scipy,mgaitan/scipy,gertingold/scipy,efiring/scipy,vberaudi/scipy,FRidh/scipy,pnedunuri/scipy,ndchorley/scipy,maniteja123/scipy,dch312/scipy,mdhaber/scipy,ndchorley/scipy,gef756/scipy,hainm/scipy,dominicelse/scipy,vberaudi/scipy,mortada/scipy,hainm/scipy,pnedunuri/scipy,piyush0609/scipy,larsmans/scipy,Kamp9/scipy,Dapid/scipy,aman-iitj/scipy,raoulbq/scipy,mtrbean/scipy,scipy/scipy,ales-erjavec/scipy,fernand/scipy,matthewalbani/scipy,Kamp9/scipy,futurulus/scipy,gertingold/scipy,endolith/scipy,mingwpy/scipy,piyush0609/scipy,nmayorov/scipy,surhudm/scipy,kalvdans/scipy,Eric89GXL/scipy,jsilter/scipy,josephcslater/scipy,ortylp/scipy,aarchiba/scipy,newemailjdm/scipy,mhogg/scipy,mdhaber/scipy,mortada/scipy,befelix/scipy,apbard/scipy,Shaswat27/scipy,jsilter/scipy,josephcslater/scipy,fredrikw/scipy,felipebetancur/scipy,jor-/scipy,anielsen001/scipy,njwilson23/scipy,ilayn/scipy,lhilt/scipy,Stefan-Endres/scipy,ndchorley/scipy,richardotis/scipy,matthew-brett/scipy,lukauskas/scipy,arokem/scipy,pbrod/scipy,Eric89GXL/scipy,chatcannon/scipy,sargas/scipy,pbrod/scipy,minhlongdo/scipy,giorgiop/scipy,aeklant/scipy,grlee77/scipy,sauliusl/scipy,WillieMaddox/scipy,mingwpy/scipy,Newman101/scipy,hainm/scipy,FRidh/scipy,anntzer/scipy,larsmans/scipy,mortada/scipy,rgommers/scipy,giorgiop/scipy,andyfaff/scipy,zerothi/scipy,juliantaylor/scipy,njwilson23/scipy,ChanderG/scipy,FRidh/scipy,jakevdp/scipy,vhaasteren/scipy,giorgiop/scipy,perimosocordiae/scipy,Eric89GXL/scipy,chatcannon/scipy,FRidh/scipy,pschella/scipy,anntzer/scipy,apbard/scipy,pbrod/scipy,maniteja123/scipy,felipebetancur/scipy,Newman101/scipy,rgommers/scipy,zerothi/scipy,petebachant/scipy,jakevdp/scipy,jjhelmus/scipy,sonnyhu/scipy,anielsen001/scipy,pizzathief/scipy,nonhermitian/scipy,nvoron23/scipy,person142/scipy,aarchiba/scipy,richardotis/scipy,matthewalbani/scipy,Stefan-Endres/scipy,argriffing/scipy",92,"```python
""""""
Real spectrum tranforms (DCT, DST, MDCT)
""""""

__all__ = ['dct1', 'dct2']

import numpy as np
from scipy.fftpack import _fftpack

import atexit
atexit.register(_fftpack.destroy_dct1_cache)
atexit.register(_fftpack.destroy_dct2_cache)

def dct1(x, n=None):
    """"""
    Return Discrete Cosine Transform (type I) of arbitrary type sequence x.

    Parameters
    ----------
    x : array-like
        input array.
    n : int, optional
        Length of the transform.

    Returns
    -------
    y : real ndarray
    """"""
    return _dct(x, 1, n)

def dct2(x, n=None):
    """"""
    Return Discrete Cosine Transform (type II) of arbitrary type sequence x.

    Parameters
    ----------
    x : array-like
        input array.
    n : int, optional
        Length of the transform.

    Returns
    -------
    y : real ndarray
    """"""
    return _dct(x, 2, n)

def _dct(x, type, n=None, axis=-1, overwrite_x=0):
    """"""
    Return Discrete Cosine Transform of arbitrary type sequence x.

    Parameters
    ----------
    x : array-like
        input array.
    n : int, optional
        Length of the transform.
    axis : int, optional
        Axis along which the dct is computed. (default=-1)
    overwrite_x : bool, optional
        If True the contents of x can be destroyed. (default=False)

    Returns
    -------
    z : real ndarray

    """"""
    tmp = np.asarray(x)
    if not np.isrealobj(tmp):
        raise TypeError,""1st argument must be real sequence""

    if n is None:
        n = tmp.shape[axis]
    else:
        raise NotImplemented(""Padding/truncating not yet implemented"")

    if type == 1:
        f = _fftpack.dct1
    elif type == 2:
        f = _fftpack.dct2
    else:
        raise ValueError(""Type %d not understood"" % type)

    if axis == -1 or axis == len(tmp.shape) - 1:
        return f(tmp, n, 0, overwrite_x)
    else:
        raise NotImplementedError(""Axis arg not yet implemented"")

    #tmp = swapaxes(tmp, axis, -1)
    #tmp = work_function(tmp,n,1,0,overwrite_x)
    #return swapaxes(tmp, axis, -1)

```"
c0ba8348f614f2ef6c14db9335ba3d1a6f3d29af,p3/management/commands/create_bulk_coupons.py,p3/management/commands/create_bulk_coupons.py,,"
"""""" Create a batch of single use discount coupons from a CSV file.

    Parameters: <conference> <csv-file>

    Creates coupons based on the CSV file contents:
    
	code		- coupon code
        max_usage 	- max. number of uses
	items_per_usage - max number of items per use
	value		- value of the coupon in percent
	description     - description
	fares 		- comma separated list of included fares

    Use --dry-run to test drive the script.

""""""
import sys
import csv

from django.core.management.base import BaseCommand, CommandError
from django.db import transaction

from conference import models as cmodels
from assopy.models import Coupon

###

class Command(BaseCommand):

    args = '<conference> <count>'

    # Dry run ?
    dry_run = False

    def add_arguments(self, parser):

        # Positional arguments
        parser.add_argument('conference')
        parser.add_argument('csv')

        # Named (optional) arguments
        parser.add_argument('--dry-run',
                            action='store_true',
                            dest='dry_run',
                            default=False,
                            help='Do everything except create the coupons')

    @transaction.atomic
    def handle(self, *args, **options):

        conference = cmodels.Conference.objects.get(code=options['conference'])
        self.dry_run = options.get('dry_run', False)
        csv_filename = options['csv']

        # Get set of existing coupon codes
        all_codes = set(c['code'] for c in Coupon.objects\
            .filter(conference=conference.code)\
            .values('code'))

        # Valid fares (conference fares only)
        all_fares = cmodels.Fare.objects\
            .filter(conference=conference.code)
        
        # Create coupons
        if csv_filename == 'stdin':
            csv_file = sys.stdin
        else:
            csv_file = open(csv_filename)
        with csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                code = row['code'].strip()
                if not code:
                    # Skip lines without code
                    continue
                if code in all_codes:
                    # Skip coupons which already exist
                    print ('Coupon %r already exists - skipping' % code)
                    continue
                c = Coupon(conference=conference)
                c.code = code
                c.max_usage = int(row.get('max_usage', 1))
                c.items_per_usage = int(row.get('items_per_usage', 1))
                c.value = row['value']
                c.description = row.get('description', '')
                if not self.dry_run:
                    c.save()
                    c.fares = all_fares.filter(
                        code__in = [x.strip()
                                    for x in row['fares'].split(',')])
                print ('Coupond %r created' % c.code)
",Add new coupon script to generate coupons using a CSV file.,"Add new coupon script to generate coupons using a CSV file.
",Python,bsd-2-clause,"EuroPython/epcon,EuroPython/epcon,EuroPython/epcon,EuroPython/epcon",93,"```python

"""""" Create a batch of single use discount coupons from a CSV file.

    Parameters: <conference> <csv-file>

    Creates coupons based on the CSV file contents:
    
	code		- coupon code
        max_usage 	- max. number of uses
	items_per_usage - max number of items per use
	value		- value of the coupon in percent
	description     - description
	fares 		- comma separated list of included fares

    Use --dry-run to test drive the script.

""""""
import sys
import csv

from django.core.management.base import BaseCommand, CommandError
from django.db import transaction

from conference import models as cmodels
from assopy.models import Coupon

###

class Command(BaseCommand):

    args = '<conference> <count>'

    # Dry run ?
    dry_run = False

    def add_arguments(self, parser):

        # Positional arguments
        parser.add_argument('conference')
        parser.add_argument('csv')

        # Named (optional) arguments
        parser.add_argument('--dry-run',
                            action='store_true',
                            dest='dry_run',
                            default=False,
                            help='Do everything except create the coupons')

    @transaction.atomic
    def handle(self, *args, **options):

        conference = cmodels.Conference.objects.get(code=options['conference'])
        self.dry_run = options.get('dry_run', False)
        csv_filename = options['csv']

        # Get set of existing coupon codes
        all_codes = set(c['code'] for c in Coupon.objects\
            .filter(conference=conference.code)\
            .values('code'))

        # Valid fares (conference fares only)
        all_fares = cmodels.Fare.objects\
            .filter(conference=conference.code)
        
        # Create coupons
        if csv_filename == 'stdin':
            csv_file = sys.stdin
        else:
            csv_file = open(csv_filename)
        with csv_file:
            reader = csv.DictReader(csv_file)
            for row in reader:
                code = row['code'].strip()
                if not code:
                    # Skip lines without code
                    continue
                if code in all_codes:
                    # Skip coupons which already exist
                    print ('Coupon %r already exists - skipping' % code)
                    continue
                c = Coupon(conference=conference)
                c.code = code
                c.max_usage = int(row.get('max_usage', 1))
                c.items_per_usage = int(row.get('items_per_usage', 1))
                c.value = row['value']
                c.description = row.get('description', '')
                if not self.dry_run:
                    c.save()
                    c.fares = all_fares.filter(
                        code__in = [x.strip()
                                    for x in row['fares'].split(',')])
                print ('Coupond %r created' % c.code)

```"
b9f2a5816c55334013a8447da705066f783dfda2,paperwork_parser/base.py,paperwork_parser/base.py,,"import inspect

from enum import IntEnum
from pdfquery import PDFQuery


class DocFieldType(IntEnum):
    NUMBER = 1
    TEXT = 2
    CUSTOM = 3  # TODO: Forget this and have 'type' take a callable instead?


class DocField(object):
    def __init__(self, bbox, type=DocFieldType.TEXT, required=False,
                 description=None):
        self.bbox = bbox
        self.type = type
        self.required = required
        self.description = description


class DocSchema(object):

    @classmethod
    def as_pdf_selectors(cls, field_name=None):
        """"""Return pdfminer selector for specified field. If no field is
        specified, then selectors for all fields are returned.
        """"""
        if field_name is not None:
            field = getattr(cls, field_name, None)
            if (field is None) or (not isinstance(field, DocField)):
                raise ValueError(
                    '{field} is not a DocField attribute on {klass}'.format(
                        field=field_name, klass=cls.__name__
                    )
                )
            pdf_fields = [('assessment_year', field)]
        else:
            pdf_fields = inspect.getmembers(
                cls, lambda f: isinstance(f, DocField)
            )

        selectors = [('with_formatter', 'text')]
        selectors.extend(
            (key, 'LTTextLineHorizontal:in_bbox(""{bbox}"")'.format(
                bbox=', '.join(str(coord) for coord in field.bbox)
            ))
            for key, field in pdf_fields
        )
        return selectors


class Document(object):

    variants = []

    def __init__(self, file):
        # TODO: Check for str or actual file inst?
        self._file = PDFQuery(file)
        self._data = {}

        self._check_configuration()

    @property
    def data(self):
        """"""Read only property that is loaded with document data once
        `extract()` is called.
        """"""
        return self._data

    def detect_variant(self):
        raise NotImplementedError('Subclass Document and override this method')

    def extract(self):
        self._file.load()

        variant = self.detect_variant()

        selectors = variant.as_pdf_selectors()
        extracted = self._file.extract(selectors)

        self._data = extracted

    def _check_configuration(self):
        if not self.variants:
            raise ValueError(
                ""The class '{name}' hasn't been configured with any variants.""
                "" Set {name}.variants to a list of DocSchema types."".format(
                    name=self.__class__.__name__
                )
            )
",Add primitives for pdf parsing,"Add primitives for pdf parsing
",Python,mit,loanzen/zen_document_parser,92,"```python
import inspect

from enum import IntEnum
from pdfquery import PDFQuery


class DocFieldType(IntEnum):
    NUMBER = 1
    TEXT = 2
    CUSTOM = 3  # TODO: Forget this and have 'type' take a callable instead?


class DocField(object):
    def __init__(self, bbox, type=DocFieldType.TEXT, required=False,
                 description=None):
        self.bbox = bbox
        self.type = type
        self.required = required
        self.description = description


class DocSchema(object):

    @classmethod
    def as_pdf_selectors(cls, field_name=None):
        """"""Return pdfminer selector for specified field. If no field is
        specified, then selectors for all fields are returned.
        """"""
        if field_name is not None:
            field = getattr(cls, field_name, None)
            if (field is None) or (not isinstance(field, DocField)):
                raise ValueError(
                    '{field} is not a DocField attribute on {klass}'.format(
                        field=field_name, klass=cls.__name__
                    )
                )
            pdf_fields = [('assessment_year', field)]
        else:
            pdf_fields = inspect.getmembers(
                cls, lambda f: isinstance(f, DocField)
            )

        selectors = [('with_formatter', 'text')]
        selectors.extend(
            (key, 'LTTextLineHorizontal:in_bbox(""{bbox}"")'.format(
                bbox=', '.join(str(coord) for coord in field.bbox)
            ))
            for key, field in pdf_fields
        )
        return selectors


class Document(object):

    variants = []

    def __init__(self, file):
        # TODO: Check for str or actual file inst?
        self._file = PDFQuery(file)
        self._data = {}

        self._check_configuration()

    @property
    def data(self):
        """"""Read only property that is loaded with document data once
        `extract()` is called.
        """"""
        return self._data

    def detect_variant(self):
        raise NotImplementedError('Subclass Document and override this method')

    def extract(self):
        self._file.load()

        variant = self.detect_variant()

        selectors = variant.as_pdf_selectors()
        extracted = self._file.extract(selectors)

        self._data = extracted

    def _check_configuration(self):
        if not self.variants:
            raise ValueError(
                ""The class '{name}' hasn't been configured with any variants.""
                "" Set {name}.variants to a list of DocSchema types."".format(
                    name=self.__class__.__name__
                )
            )

```"
e271d355e057f72c85efe3f1a407c55e54faca74,migrations/008_convert_ga_buckets_to_utc.py,migrations/008_convert_ga_buckets_to_utc.py,,"""""""
Convert Google Analytics buckets from Europe/London to UTC
""""""
import logging
import copy
from itertools import imap, ifilter
from datetime import timedelta

from backdrop.core.records import Record
from backdrop.core.timeutils import utc


log = logging.getLogger(__name__)

GA_BUCKETS_TO_MIGRATE = [
    ""carers_allowance_journey"",
    ""deposit_foreign_marriage_journey"",
    ""pay_foreign_marriage_certificates_journey"",
    ""pay_legalisation_drop_off_journey"",
    ""pay_legalisation_post_journey"",
    ""pay_register_birth_abroad_journey"",
    ""pay_register_death_abroad_journey"",
    ""lpa_journey"",
    ""licensing_journey"",
]


def fix_timestamp(document):
    """"""Return a new dict with the _timestamp field fixed
    """"""
    document = copy.deepcopy(document)
    document['_timestamp'] = document['_timestamp'].replace(tzinfo=None) + \
        timedelta(hours=1)
    return document


def strip_internal_fields(document):
    """"""Return a new dict with all internal fields removed

    Leaves _timestamp and _id in place
    """"""
    def allowed_field(key):
        return not key.startswith('_') or key in ['_timestamp', '_id']

    return dict(
        (key, value) for key, value in document.items() if allowed_field(key))


def is_bst(document):
    """"""Return true if a document looks like it's BST""""""
    return document['_timestamp'].hour == 23


def create_record(document):
    """"""Return a dict with internal fields applied""""""
    return Record(document).to_mongo()


def fix_id(document):
    """"""Return a new dict with the _id field recalculated""""""
    def _format(timestamp):
        return to_utc(timestamp).strftime(""%Y%m%d%H%M%S"")

    def data_id(data_type, timestamp, period):
        """"""_id generation function copied from backdrop-ga-collector""""""
        return base64.urlsafe_b64encode(""_"".join(
            [data_type, _format(timestamp), period]))

    document = copy.deepcopy(document)
    document['_id'] = data_id(
        document['dataType'], document['_timestamp'],
        document['timeSpan'])

    return document


def up(db):
    for name in GA_BUCKETS_TO_MIGRATE:
        collection = db.get_repository(name)

        documents = collection.find({})

        documents = ifilter(is_bst, documents)
        documents = imap(strip_internal_fields, documents)
        documents = imap(fix_timestamp, documents)
        documents = map(create_record, documents)

        if len(documents) > 0:
            log.info(""Convert GA timezone: {0}"".format(name))

        map(collection.save, documents)
",Add migration to convert GA timezone to UTC,"Add migration to convert GA timezone to UTC

We are ignoring the timezone from GA requests so previous data needs to
be converted to account for this.

See https://www.pivotaltracker.com/story/show/62779118 for details.
",Python,mit,"alphagov/backdrop,alphagov/backdrop,alphagov/backdrop",92,"```python
""""""
Convert Google Analytics buckets from Europe/London to UTC
""""""
import logging
import copy
from itertools import imap, ifilter
from datetime import timedelta

from backdrop.core.records import Record
from backdrop.core.timeutils import utc


log = logging.getLogger(__name__)

GA_BUCKETS_TO_MIGRATE = [
    ""carers_allowance_journey"",
    ""deposit_foreign_marriage_journey"",
    ""pay_foreign_marriage_certificates_journey"",
    ""pay_legalisation_drop_off_journey"",
    ""pay_legalisation_post_journey"",
    ""pay_register_birth_abroad_journey"",
    ""pay_register_death_abroad_journey"",
    ""lpa_journey"",
    ""licensing_journey"",
]


def fix_timestamp(document):
    """"""Return a new dict with the _timestamp field fixed
    """"""
    document = copy.deepcopy(document)
    document['_timestamp'] = document['_timestamp'].replace(tzinfo=None) + \
        timedelta(hours=1)
    return document


def strip_internal_fields(document):
    """"""Return a new dict with all internal fields removed

    Leaves _timestamp and _id in place
    """"""
    def allowed_field(key):
        return not key.startswith('_') or key in ['_timestamp', '_id']

    return dict(
        (key, value) for key, value in document.items() if allowed_field(key))


def is_bst(document):
    """"""Return true if a document looks like it's BST""""""
    return document['_timestamp'].hour == 23


def create_record(document):
    """"""Return a dict with internal fields applied""""""
    return Record(document).to_mongo()


def fix_id(document):
    """"""Return a new dict with the _id field recalculated""""""
    def _format(timestamp):
        return to_utc(timestamp).strftime(""%Y%m%d%H%M%S"")

    def data_id(data_type, timestamp, period):
        """"""_id generation function copied from backdrop-ga-collector""""""
        return base64.urlsafe_b64encode(""_"".join(
            [data_type, _format(timestamp), period]))

    document = copy.deepcopy(document)
    document['_id'] = data_id(
        document['dataType'], document['_timestamp'],
        document['timeSpan'])

    return document


def up(db):
    for name in GA_BUCKETS_TO_MIGRATE:
        collection = db.get_repository(name)

        documents = collection.find({})

        documents = ifilter(is_bst, documents)
        documents = imap(strip_internal_fields, documents)
        documents = imap(fix_timestamp, documents)
        documents = map(create_record, documents)

        if len(documents) > 0:
            log.info(""Convert GA timezone: {0}"".format(name))

        map(collection.save, documents)

```"
48b1479a84fcb6d16d8aea8ca256d01e9898eeea,tools/fake_robot.py,tools/fake_robot.py,,"import json

from time import time
from threading import Timer

from tornado.ioloop import IOLoop
from tornado.web import Application
from tornado.websocket import WebSocketHandler


class RepeatedTimer(object):
    def __init__(self, interval, function, *args, **kwargs):
        self._timer = None
        self.interval = interval
        self.function = function
        self.args = args
        self.kwargs = kwargs
        self.is_running = False
        self.start()

    def _run(self):
        self.is_running = False
        self.function(*self.args, **self.kwargs)
        self.start()

    def start(self):
        if not self.is_running:
            self._timer = Timer(self.interval, self._run)
            self._timer.start()
            self.is_running = True

    def stop(self):
        self._timer.cancel()
        self.is_running = False


class FakeRobot(WebSocketHandler):
    period = 1 / 10
    verbose = False

    def open(self):
        if self.verbose:
            print('WebSocket connection open.')

        self.rt = RepeatedTimer(self.period, self.pub_state)

    def on_message(self, message):
        if self.verbose:
            print('{}: Received {}'.format(time(), message))

        self.handle_command(json.loads(message))

    def on_close(self):
        if self.verbose:
            print('WebSocket closed {}.'.format(self.close_reason))

        self.rt.stop()

    def pub_state(self):
        state = {
            'modules': [
                {
                    'alias': 'my_led',
                    'id': 2,
                    'type': 'led',
                },
                {
                    'alias': 'my_motor',
                    'id': 3,
                    'type': 'motor',
                },
                {
                    'alias': 'my_button',
                    'id': 4,
                    'type': 'button',
                    'value': 0,
                },
                {
                    'alias': 'my_potentiometer',
                    'id': 5,
                    'type': 'potard',
                    'value': 50,
                },
                {
                    'alias': 'my_relay',
                    'id': 6,
                    'type': 'relay',
                },
                {
                    'alias': 'my_distance',
                    'id': 7,
                    'type': 'distance',
                    'value': 12,
                },
            ]
        }
        self.write_message(json.dumps(state))

    def handle_command(self, message):
        pass


if __name__ == '__main__':
    port = 9342
    FakeRobot.verbose = True

    loop = IOLoop()
    app = Application([
        (r'/', FakeRobot)
    ])

    app.listen(port)
    url = 'ws://{}:{}'.format('127.0.0.1', port)
    print('Fake robot serving on {}'.format(url))
    loop.start()
",Add fake robot utility tool.,"Add fake robot utility tool.
",Python,mit,pollen/pyrobus,116,"```python
import json

from time import time
from threading import Timer

from tornado.ioloop import IOLoop
from tornado.web import Application
from tornado.websocket import WebSocketHandler


class RepeatedTimer(object):
    def __init__(self, interval, function, *args, **kwargs):
        self._timer = None
        self.interval = interval
        self.function = function
        self.args = args
        self.kwargs = kwargs
        self.is_running = False
        self.start()

    def _run(self):
        self.is_running = False
        self.function(*self.args, **self.kwargs)
        self.start()

    def start(self):
        if not self.is_running:
            self._timer = Timer(self.interval, self._run)
            self._timer.start()
            self.is_running = True

    def stop(self):
        self._timer.cancel()
        self.is_running = False


class FakeRobot(WebSocketHandler):
    period = 1 / 10
    verbose = False

    def open(self):
        if self.verbose:
            print('WebSocket connection open.')

        self.rt = RepeatedTimer(self.period, self.pub_state)

    def on_message(self, message):
        if self.verbose:
            print('{}: Received {}'.format(time(), message))

        self.handle_command(json.loads(message))

    def on_close(self):
        if self.verbose:
            print('WebSocket closed {}.'.format(self.close_reason))

        self.rt.stop()

    def pub_state(self):
        state = {
            'modules': [
                {
                    'alias': 'my_led',
                    'id': 2,
                    'type': 'led',
                },
                {
                    'alias': 'my_motor',
                    'id': 3,
                    'type': 'motor',
                },
                {
                    'alias': 'my_button',
                    'id': 4,
                    'type': 'button',
                    'value': 0,
                },
                {
                    'alias': 'my_potentiometer',
                    'id': 5,
                    'type': 'potard',
                    'value': 50,
                },
                {
                    'alias': 'my_relay',
                    'id': 6,
                    'type': 'relay',
                },
                {
                    'alias': 'my_distance',
                    'id': 7,
                    'type': 'distance',
                    'value': 12,
                },
            ]
        }
        self.write_message(json.dumps(state))

    def handle_command(self, message):
        pass


if __name__ == '__main__':
    port = 9342
    FakeRobot.verbose = True

    loop = IOLoop()
    app = Application([
        (r'/', FakeRobot)
    ])

    app.listen(port)
    url = 'ws://{}:{}'.format('127.0.0.1', port)
    print('Fake robot serving on {}'.format(url))
    loop.start()

```"
ca9a19f721b06b619ffdab5bda1814667294d505,tests/test_librato_uptime.py,tests/test_librato_uptime.py,,"import os
import sys
import unittest
import json

sys.path.insert(0, os.path.abspath('./situation'))
sys.path.insert(0, os.path.abspath('./'))

from situation.librato_uptime import Calculator
from google.appengine.ext import testbed
import mock


UPTIME_CFG = {
    'root_uri': 'https://metrics-api.librato.com/v1/metrics/',
    'username': 'MOCK_USERNAME',
    'password': 'MOCK_PWD',
    'services': {
        'API': {
            'SOURCE': '*bapi-live*',
            'TOTAL_TARGETS': [
                'MOCK_TOTAL_TARGET_A',
                'MOCK_TOTAL_TARGET_B',
            ],
            'ERROR_TARGETS': [
                'MOCK_ERROR_TARGET_A',
                'MOCK_ERROR_TARGET_B',
                'MOCK_ERROR_TARGET_C',
            ]
        },
    }
}


class TestLibratoUptime(unittest.TestCase):

    def setUp(self):
        # First, create an instance of the Testbed class.
        self.testbed = testbed.Testbed()
        # Then activate the testbed, which prepares the service stubs for use.
        self.testbed.activate()

    def tearDown(self):
        self.testbed.deactivate()

    def test_refresh(self):
        index = [0]
        resps = [
            # TOTAL A
            dict(
                measurements=dict(
                    foobar=[dict(count=1), dict(count=7788), dict(count=123)],
                ),
                query=dict(next_time=999),
            ),
            dict(
                measurements=dict(
                    foobar=[dict(count=888)],
                ),
            ),
            # TOTAL B
            dict(
                measurements=dict(
                    foobar=[dict(count=3), dict(count=4)],
                )
            ),
            # ERROR A
            dict(
                measurements=dict(
                    foobar=[dict(count=5)],
                    barfoo=[dict(count=78)],
                )
            ),
            # ERROR B
            dict(
                measurements=dict(
                    foobar=[dict(count=6)],
                    barfoo=[dict(count=78)],
                )
            ),
            # ERROR C
            dict(
                measurements=dict(
                    foobar=[dict(count=0)],
                )
            ),
        ]

        def read():
            result = resps[index[0]]
            index[0] += 1
            return json.dumps(result)

        calculator = Calculator(**UPTIME_CFG)
        calculator.opener = mock.Mock()
        calculator.opener.open.return_value = mock.Mock()
        calculator.opener.open.return_value.read.side_effect = read

        result = calculator._for_service(calculator.services['API'], 5)
        total_number = (
            (1 + 7788 + 123) +
            (888) +
            (3 + 4)
        )
        error_number = (
            (5 + 78) +
            (6 + 78) +
            (0)
        )
        expected_result = ((total_number - float(error_number)) / total_number) * 100
        self.assertEqual(result, expected_result)
",Add test for librato uptime,"Add test for librato uptime
",Python,mit,"balanced/status.balancedpayments.com,chriskuehl/kloudless-status,chriskuehl/kloudless-status,balanced/status.balancedpayments.com,balanced/status.balancedpayments.com,chriskuehl/kloudless-status",112,"```python
import os
import sys
import unittest
import json

sys.path.insert(0, os.path.abspath('./situation'))
sys.path.insert(0, os.path.abspath('./'))

from situation.librato_uptime import Calculator
from google.appengine.ext import testbed
import mock


UPTIME_CFG = {
    'root_uri': 'https://metrics-api.librato.com/v1/metrics/',
    'username': 'MOCK_USERNAME',
    'password': 'MOCK_PWD',
    'services': {
        'API': {
            'SOURCE': '*bapi-live*',
            'TOTAL_TARGETS': [
                'MOCK_TOTAL_TARGET_A',
                'MOCK_TOTAL_TARGET_B',
            ],
            'ERROR_TARGETS': [
                'MOCK_ERROR_TARGET_A',
                'MOCK_ERROR_TARGET_B',
                'MOCK_ERROR_TARGET_C',
            ]
        },
    }
}


class TestLibratoUptime(unittest.TestCase):

    def setUp(self):
        # First, create an instance of the Testbed class.
        self.testbed = testbed.Testbed()
        # Then activate the testbed, which prepares the service stubs for use.
        self.testbed.activate()

    def tearDown(self):
        self.testbed.deactivate()

    def test_refresh(self):
        index = [0]
        resps = [
            # TOTAL A
            dict(
                measurements=dict(
                    foobar=[dict(count=1), dict(count=7788), dict(count=123)],
                ),
                query=dict(next_time=999),
            ),
            dict(
                measurements=dict(
                    foobar=[dict(count=888)],
                ),
            ),
            # TOTAL B
            dict(
                measurements=dict(
                    foobar=[dict(count=3), dict(count=4)],
                )
            ),
            # ERROR A
            dict(
                measurements=dict(
                    foobar=[dict(count=5)],
                    barfoo=[dict(count=78)],
                )
            ),
            # ERROR B
            dict(
                measurements=dict(
                    foobar=[dict(count=6)],
                    barfoo=[dict(count=78)],
                )
            ),
            # ERROR C
            dict(
                measurements=dict(
                    foobar=[dict(count=0)],
                )
            ),
        ]

        def read():
            result = resps[index[0]]
            index[0] += 1
            return json.dumps(result)

        calculator = Calculator(**UPTIME_CFG)
        calculator.opener = mock.Mock()
        calculator.opener.open.return_value = mock.Mock()
        calculator.opener.open.return_value.read.side_effect = read

        result = calculator._for_service(calculator.services['API'], 5)
        total_number = (
            (1 + 7788 + 123) +
            (888) +
            (3 + 4)
        )
        error_number = (
            (5 + 78) +
            (6 + 78) +
            (0)
        )
        expected_result = ((total_number - float(error_number)) / total_number) * 100
        self.assertEqual(result, expected_result)

```"
5d4e8bb02dd78f8ba6eb386478f25a6e60c80240,atoman/slowtests/test_filtering.py,atoman/slowtests/test_filtering.py,,"  
""""""
Slow tests for filtering systems
   
""""""
import os
import tempfile
import shutil

import numpy as np

from . import base
from ..gui import mainWindow
from ..system.lattice import Lattice


def path_to_file(path):
    return os.path.join(os.path.dirname(__file__), "".."", "".."", ""testing"", path)


class TestFilteringKennyLattice(base.UsesQApplication):
    """"""
    Test filtering a system
       
    """"""
    def setUp(self):
        """"""
        Set up the test
        
        """"""
        super(TestFilteringKennyLattice, self).setUp()
        
        # tmp dir
        self.tmpLocation = tempfile.mkdtemp(prefix=""atomanTest"")
        
        # main window
        self.mw = mainWindow.MainWindow(None, testing=True)
        self.mw.preferences.renderingForm.maxAtomsAutoRun = 0
        self.mw.show()
        
        # copy a lattice to tmpLocation
        self.fn = os.path.join(self.tmpLocation, ""testLattice.dat"")
        shutil.copy(path_to_file(""kenny_lattice.dat""), self.fn)
        
        # load Lattice
        try:
            self.mw.systemsDialog.load_system_form.readerForm.openFile(self.fn)
            state = self.mw.mainToolbar.pipelineList[0].inputState
            err = False
            if not isinstance(state, Lattice):
                err = True
            elif state.NAtoms != 1140:
                err = True
            if err:
                self.fail(""Loading Lattice failed"")
        except:
            self.fail(""Loading Lattice failed"")
    
    def tearDown(self):
        """"""
        Tidy up
        
        """"""
        super(TestFilteringKennyLattice, self).tearDown()
        
        # remove refs
        self.fn = None
        self.mw.close()
        self.mw = None
        
        # remove tmp dir
        shutil.rmtree(self.tmpLocation)
    
    def test_filterAtomID(self):
        """"""
        GUI: filter atom ID
           
        """"""
        # add the atom ID filter
        pp = self.mw.mainToolbar.pipelineList[0]
        flist = pp.filterLists[0]
        flist.addFilter(filterName=""Atom ID"")
        item = flist.listItems.item(0)
        item.filterSettings.lineEdit.setText(""104,1,4-7"")
        item.filterSettings.lineEdit.editingFinished.emit()
        
        # run the filter
        pp.runAllFilterLists()
        
        # check the result
        flt = flist.filterer
        atomids = (104, 1, 4, 5, 6, 7)
        self.assertEqual(len(flt.visibleAtoms), 6)
        for i in xrange(6):
            self.assertTrue(pp.inputState.atomID[flt.visibleAtoms[i]] in atomids)
",Add file for running gui filtering tests.,"Add file for running gui filtering tests.
",Python,mit,"chrisdjscott/Atoman,chrisdjscott/Atoman,chrisdjscott/Atoman,chrisdjscott/Atoman,chrisdjscott/Atoman",96,"```python
  
""""""
Slow tests for filtering systems
   
""""""
import os
import tempfile
import shutil

import numpy as np

from . import base
from ..gui import mainWindow
from ..system.lattice import Lattice


def path_to_file(path):
    return os.path.join(os.path.dirname(__file__), "".."", "".."", ""testing"", path)


class TestFilteringKennyLattice(base.UsesQApplication):
    """"""
    Test filtering a system
       
    """"""
    def setUp(self):
        """"""
        Set up the test
        
        """"""
        super(TestFilteringKennyLattice, self).setUp()
        
        # tmp dir
        self.tmpLocation = tempfile.mkdtemp(prefix=""atomanTest"")
        
        # main window
        self.mw = mainWindow.MainWindow(None, testing=True)
        self.mw.preferences.renderingForm.maxAtomsAutoRun = 0
        self.mw.show()
        
        # copy a lattice to tmpLocation
        self.fn = os.path.join(self.tmpLocation, ""testLattice.dat"")
        shutil.copy(path_to_file(""kenny_lattice.dat""), self.fn)
        
        # load Lattice
        try:
            self.mw.systemsDialog.load_system_form.readerForm.openFile(self.fn)
            state = self.mw.mainToolbar.pipelineList[0].inputState
            err = False
            if not isinstance(state, Lattice):
                err = True
            elif state.NAtoms != 1140:
                err = True
            if err:
                self.fail(""Loading Lattice failed"")
        except:
            self.fail(""Loading Lattice failed"")
    
    def tearDown(self):
        """"""
        Tidy up
        
        """"""
        super(TestFilteringKennyLattice, self).tearDown()
        
        # remove refs
        self.fn = None
        self.mw.close()
        self.mw = None
        
        # remove tmp dir
        shutil.rmtree(self.tmpLocation)
    
    def test_filterAtomID(self):
        """"""
        GUI: filter atom ID
           
        """"""
        # add the atom ID filter
        pp = self.mw.mainToolbar.pipelineList[0]
        flist = pp.filterLists[0]
        flist.addFilter(filterName=""Atom ID"")
        item = flist.listItems.item(0)
        item.filterSettings.lineEdit.setText(""104,1,4-7"")
        item.filterSettings.lineEdit.editingFinished.emit()
        
        # run the filter
        pp.runAllFilterLists()
        
        # check the result
        flt = flist.filterer
        atomids = (104, 1, 4, 5, 6, 7)
        self.assertEqual(len(flt.visibleAtoms), 6)
        for i in xrange(6):
            self.assertTrue(pp.inputState.atomID[flt.visibleAtoms[i]] in atomids)

```"
17401f8fad648cbe4258f257fdc288b327aed9ab,integration/simple_test_module.py,integration/simple_test_module.py,,"import sys
import argparse
import operator
import threading
import time

from jnius import autoclass, cast
from TripsModule.trips_module import TripsModule

# Declare KQML java classes
KQMLPerformative = autoclass('TRIPS.KQML.KQMLPerformative')
KQMLList = autoclass('TRIPS.KQML.KQMLList')
KQMLObject = autoclass('TRIPS.KQML.KQMLObject')

class FIFO(object):
    def __init__(self, lst=None):
        if lst is None:
            self.lst = []
        else:
            self.lst = lst

    def pop(self):
        return self.lst.pop()

    def push(self, e):
        self.lst = [e] + self.lst

    def is_empty(self):
        if not self.lst:
            return True
        return False

class TestModule(TripsModule):
    '''
    The Test module is a TRIPS module built to run unit test. It will
    ts role is to receive and decode messages and send responses from
    and to other agents in the system.
    '''
    def __init__(self, argv):
        # Call the constructor of TripsModule
        super(TestModule, self).__init__(argv)
        self.expected = FIFO()
        self.sent = FIFO()
        # TODO:make this an input argument
        self.test_file = 'integration/test.in'

    def init(self):
        '''
        Initialize TRIPS module
        '''
        super(TestModule, self).init()
        # Send ready message
        self.ready()
        self.run_tests(self.test_file)
        return None

    def run_tests(self, test_file):
        fh = open(test_file, 'rt')
        messages = fh.readlines()
        send_msg = messages[0::2]
        expect_msg = messages[1::2]
        msg_id = 1
        for sm, em in zip(send_msg, expect_msg):
            msg_id_str = 'IO-%d' % msg_id
            # TODO: allow non-request messages?
            perf  = KQMLPerformative.fromString(
                '(request :reply-with %s :content %s)' % (msg_id_str, sm))
            self.sent.push(sm)
            self.expected.push(em)
            self.send(perf)
            msg_id += 1

    def receive_reply(self, msg, content):
        '''
        Handle a ""reply"" message is received.
        '''
        sent = self.sent.pop().strip()
        expected_content = self.expected.pop().strip()
        actual_content = content.toString().strip()
        print 'sent:     ', sent
        print 'expected: ', expected_content
        print 'actual:   ', actual_content
        print '---'
        assert(expected_content == actual_content)
        if self.expected.is_empty():
            sys.exit(0)

if __name__ == ""__main__"":
    dm = TestModule(['-name', 'Test'] + sys.argv[1:])
    dm.run()
",Add simple test module for integration testing,"Add simple test module for integration testing
",Python,bsd-2-clause,"sorgerlab/bioagents,bgyori/bioagents",91,"```python
import sys
import argparse
import operator
import threading
import time

from jnius import autoclass, cast
from TripsModule.trips_module import TripsModule

# Declare KQML java classes
KQMLPerformative = autoclass('TRIPS.KQML.KQMLPerformative')
KQMLList = autoclass('TRIPS.KQML.KQMLList')
KQMLObject = autoclass('TRIPS.KQML.KQMLObject')

class FIFO(object):
    def __init__(self, lst=None):
        if lst is None:
            self.lst = []
        else:
            self.lst = lst

    def pop(self):
        return self.lst.pop()

    def push(self, e):
        self.lst = [e] + self.lst

    def is_empty(self):
        if not self.lst:
            return True
        return False

class TestModule(TripsModule):
    '''
    The Test module is a TRIPS module built to run unit test. It will
    ts role is to receive and decode messages and send responses from
    and to other agents in the system.
    '''
    def __init__(self, argv):
        # Call the constructor of TripsModule
        super(TestModule, self).__init__(argv)
        self.expected = FIFO()
        self.sent = FIFO()
        # TODO:make this an input argument
        self.test_file = 'integration/test.in'

    def init(self):
        '''
        Initialize TRIPS module
        '''
        super(TestModule, self).init()
        # Send ready message
        self.ready()
        self.run_tests(self.test_file)
        return None

    def run_tests(self, test_file):
        fh = open(test_file, 'rt')
        messages = fh.readlines()
        send_msg = messages[0::2]
        expect_msg = messages[1::2]
        msg_id = 1
        for sm, em in zip(send_msg, expect_msg):
            msg_id_str = 'IO-%d' % msg_id
            # TODO: allow non-request messages?
            perf  = KQMLPerformative.fromString(
                '(request :reply-with %s :content %s)' % (msg_id_str, sm))
            self.sent.push(sm)
            self.expected.push(em)
            self.send(perf)
            msg_id += 1

    def receive_reply(self, msg, content):
        '''
        Handle a ""reply"" message is received.
        '''
        sent = self.sent.pop().strip()
        expected_content = self.expected.pop().strip()
        actual_content = content.toString().strip()
        print 'sent:     ', sent
        print 'expected: ', expected_content
        print 'actual:   ', actual_content
        print '---'
        assert(expected_content == actual_content)
        if self.expected.is_empty():
            sys.exit(0)

if __name__ == ""__main__"":
    dm = TestModule(['-name', 'Test'] + sys.argv[1:])
    dm.run()

```"
c5852893e3b3dccf94e4c2845d5cb773b07d084f,gtkmvco/tests/container_observation.py,gtkmvco/tests/container_observation.py,,"# This tests the observation of observables into lists and maps.
# This test should be converted to unittest

import _importer
from gtkmvc import Model, Observer, Observable


# ----------------------------------------------------------------------
# An ad-hoc class which has a chaging method 'change'
class MyObservable (Observable):
    def __init__(self, name):
        # name is used to distinguish instances in verbosity
        Observable.__init__(self)
        self.name = name
        return
    
    @Observable.observed
    def change(self):
        print ""called change:"", self.name
        return
    pass # end of class
# ----------------------------------------------------------------------

class MyModel (Model):

    # this model contains only dynamically declared observables in to
    # the list and the map.
  
    def __init__(self):
        Model.__init__(self)

        # self.list and self.map are not observable here, althought
        # that might be observable of course.
        
        self.list = [ MyObservable(""ob.%02d"" % i) for i in range(5) ]

        self.map = { 'k0' : MyObservable(""k0""),
                     'k1' : [MyObservable(""k1[0]""), MyObservable(""k1[1]"")],
                     }
        
        for i in range(len(self.list)):
            self.register_property(""list[%d]"" % i)
            pass

        # notice tha way keys are represented: map[k0], and not
        # map['k0']. This may change (TBD):        
        self.register_property(""map[k0]"")
        self.register_property(""map[k1][0]"") 
        self.register_property(""map[k1][1]"")
        return
    pass # end of class
# ----------------------------------------------------------------------


class MyObserver (Observer):

    # The observer exploits both dynamic and static declaration of
    # notification methods.
    
    def __init__(self, m):
        # notice that the observation of is delayed here, as in 1.99.1
        # dynamic observation work only before the model registration.
        Observer.__init__(self) 

        # dynamically observes list[0]
        self.observe(self.content_changed, ""list[0]"", before=True)

        # dynamically observes map[k0]
        self.observe(self.content_changed, ""map[k0]"", before=True)

        self.observe_model(m)
        return

    # statically observes list[1] and list[3]
    @Observer.observe(""list[1]"", after=True)
    @Observer.observe(""list[3]"", before=True)
    @Observer.observe(""map[k1][0]"", after=True)
    def content_changed(self, model, name, info):
        print ""Observer:"", model, name, info
        return

    pass # end of class
# ----------------------------------------------------------------------


if ""__main__"" == __name__:
    m = MyModel()
    o = MyObserver(m)

    # change the list's content
    for o in m.list: o.change()

    # change the map's content
    m.map['k0'].change()
    for i in range(2): m.map['k1'][i].change()
    pass

","TEST (Still not automatic, to be converted eventually.)","TEST
(Still not automatic, to be converted eventually.)

Test/example for experimental feature introduced in r283


",Python,lgpl-2.1,"roboogle/gtkmvc3,roboogle/gtkmvc3",98,"```python
# This tests the observation of observables into lists and maps.
# This test should be converted to unittest

import _importer
from gtkmvc import Model, Observer, Observable


# ----------------------------------------------------------------------
# An ad-hoc class which has a chaging method 'change'
class MyObservable (Observable):
    def __init__(self, name):
        # name is used to distinguish instances in verbosity
        Observable.__init__(self)
        self.name = name
        return
    
    @Observable.observed
    def change(self):
        print ""called change:"", self.name
        return
    pass # end of class
# ----------------------------------------------------------------------

class MyModel (Model):

    # this model contains only dynamically declared observables in to
    # the list and the map.
  
    def __init__(self):
        Model.__init__(self)

        # self.list and self.map are not observable here, althought
        # that might be observable of course.
        
        self.list = [ MyObservable(""ob.%02d"" % i) for i in range(5) ]

        self.map = { 'k0' : MyObservable(""k0""),
                     'k1' : [MyObservable(""k1[0]""), MyObservable(""k1[1]"")],
                     }
        
        for i in range(len(self.list)):
            self.register_property(""list[%d]"" % i)
            pass

        # notice tha way keys are represented: map[k0], and not
        # map['k0']. This may change (TBD):        
        self.register_property(""map[k0]"")
        self.register_property(""map[k1][0]"") 
        self.register_property(""map[k1][1]"")
        return
    pass # end of class
# ----------------------------------------------------------------------


class MyObserver (Observer):

    # The observer exploits both dynamic and static declaration of
    # notification methods.
    
    def __init__(self, m):
        # notice that the observation of is delayed here, as in 1.99.1
        # dynamic observation work only before the model registration.
        Observer.__init__(self) 

        # dynamically observes list[0]
        self.observe(self.content_changed, ""list[0]"", before=True)

        # dynamically observes map[k0]
        self.observe(self.content_changed, ""map[k0]"", before=True)

        self.observe_model(m)
        return

    # statically observes list[1] and list[3]
    @Observer.observe(""list[1]"", after=True)
    @Observer.observe(""list[3]"", before=True)
    @Observer.observe(""map[k1][0]"", after=True)
    def content_changed(self, model, name, info):
        print ""Observer:"", model, name, info
        return

    pass # end of class
# ----------------------------------------------------------------------


if ""__main__"" == __name__:
    m = MyModel()
    o = MyObserver(m)

    # change the list's content
    for o in m.list: o.change()

    # change the map's content
    m.map['k0'].change()
    for i in range(2): m.map['k1'][i].change()
    pass


```"
2dc2b301edd7fa6451399a726f8ba7328865a4c5,django/website/contacts/migrations/0006_auto_20160713_1115.py,django/website/contacts/migrations/0006_auto_20160713_1115.py,,"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('contacts', '0005_auto_20160621_1456'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='user',
            name='area_of_specialisation',
        ),
        migrations.RemoveField(
            model_name='user',
            name='business_address',
        ),
        migrations.RemoveField(
            model_name='user',
            name='business_tel',
        ),
        migrations.RemoveField(
            model_name='user',
            name='contact_type',
        ),
        migrations.RemoveField(
            model_name='user',
            name='country',
        ),
        migrations.RemoveField(
            model_name='user',
            name='cv',
        ),
        migrations.RemoveField(
            model_name='user',
            name='fax',
        ),
        migrations.RemoveField(
            model_name='user',
            name='gender',
        ),
        migrations.RemoveField(
            model_name='user',
            name='home_address',
        ),
        migrations.RemoveField(
            model_name='user',
            name='home_tel',
        ),
        migrations.RemoveField(
            model_name='user',
            name='job_title',
        ),
        migrations.RemoveField(
            model_name='user',
            name='mobile',
        ),
        migrations.RemoveField(
            model_name='user',
            name='msn_id',
        ),
        migrations.RemoveField(
            model_name='user',
            name='nationality',
        ),
        migrations.RemoveField(
            model_name='user',
            name='notes',
        ),
        migrations.RemoveField(
            model_name='user',
            name='personal_email',
        ),
        migrations.RemoveField(
            model_name='user',
            name='picture',
        ),
        migrations.RemoveField(
            model_name='user',
            name='skype_id',
        ),
        migrations.RemoveField(
            model_name='user',
            name='title',
        ),
        migrations.RemoveField(
            model_name='user',
            name='yahoo_messenger',
        ),
    ]
",Add migration to remove fields from contacts,Add migration to remove fields from contacts,Python,agpl-3.0,"aptivate/alfie,aptivate/kashana,aptivate/alfie,aptivate/kashana,aptivate/kashana,aptivate/alfie,aptivate/alfie,daniell/kashana,daniell/kashana,aptivate/kashana,daniell/kashana,daniell/kashana",95,"```python
# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('contacts', '0005_auto_20160621_1456'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='user',
            name='area_of_specialisation',
        ),
        migrations.RemoveField(
            model_name='user',
            name='business_address',
        ),
        migrations.RemoveField(
            model_name='user',
            name='business_tel',
        ),
        migrations.RemoveField(
            model_name='user',
            name='contact_type',
        ),
        migrations.RemoveField(
            model_name='user',
            name='country',
        ),
        migrations.RemoveField(
            model_name='user',
            name='cv',
        ),
        migrations.RemoveField(
            model_name='user',
            name='fax',
        ),
        migrations.RemoveField(
            model_name='user',
            name='gender',
        ),
        migrations.RemoveField(
            model_name='user',
            name='home_address',
        ),
        migrations.RemoveField(
            model_name='user',
            name='home_tel',
        ),
        migrations.RemoveField(
            model_name='user',
            name='job_title',
        ),
        migrations.RemoveField(
            model_name='user',
            name='mobile',
        ),
        migrations.RemoveField(
            model_name='user',
            name='msn_id',
        ),
        migrations.RemoveField(
            model_name='user',
            name='nationality',
        ),
        migrations.RemoveField(
            model_name='user',
            name='notes',
        ),
        migrations.RemoveField(
            model_name='user',
            name='personal_email',
        ),
        migrations.RemoveField(
            model_name='user',
            name='picture',
        ),
        migrations.RemoveField(
            model_name='user',
            name='skype_id',
        ),
        migrations.RemoveField(
            model_name='user',
            name='title',
        ),
        migrations.RemoveField(
            model_name='user',
            name='yahoo_messenger',
        ),
    ]

```"
bb3bab2b8efb49d8a3ac9070289681d12fb8c7e2,modules/performanceplatform/files/sensu-elasticsearch.py,modules/performanceplatform/files/sensu-elasticsearch.py,,"#!/usr/bin/env python
# encoding: utf-8

import datetime
import json
import requests


JSON_REQUEST = {
  ""query"": {
    ""filtered"": {
      ""query"": {
        ""bool"": {
          ""should"": [
            {
              ""query_string"": {
                ""query"": ""*""
              }
            }
          ]
        }
      },
      ""filter"": {
        ""bool"": {
          ""must"": [
            {
              ""match_all"": {

              }
            },
            {
              ""range"": {
                ""@timestamp"": {
                  ""from"": ""now-1h"",
                  ""to"": ""now""
                }
              }
            },
            {
              ""fquery"": {
                ""query"": {
                  ""field"": {
                    ""@fields.levelname"": {
                      ""query"": ""\""ERROR\""""
                    }
                  }
                },
                ""_cache"": True
              }
            },
            {
              ""bool"": {
                ""must"": [
                  {
                    ""match_all"": {

                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  },
  ""highlight"": {
    ""fields"": {

    },
    ""fragment_size"": 2147483647,
    ""pre_tags"": [
      ""@start-highlight@""
    ],
    ""post_tags"": [
      ""@end-highlight@""
    ]
  },
  ""size"": 500,
  ""sort"": [
    {
      ""@timestamp"": {
        ""order"": ""desc""
      }
    }
  ]
}


def main():
    now = datetime.datetime.now().date()
    es_host = 'elasticsearch:9200'
    es_index = 'logstash-{year:04}.{month:02}.{day:02}'.format(
        year=now.year, month=now.month, day=now.day)

    response = requests.post(
        'http://{}/{}/_search'.format(es_host, es_index),
        headers={'Content-Type': 'application/json'},
        data=json.dumps(JSON_REQUEST))
    response.raise_for_status()
    response_data = json.loads(response.content)
    from pprint import pprint
    hits = response_data['hits']['hits']
    print(""{} log matches"".format(len(hits)))
    for i, hit in enumerate(hits):
        print(""--- Log message #{} --- "".format(i + 1))
        pprint(hits[0]['_source'])

    return 2 if len(hits) > 0 else 0

if __name__ == '__main__':
    main()
",Add a version of sensu checking script which works on monitoring-1,"Add a version of sensu checking script which works on monitoring-1
",Python,mit,"alphagov/pp-puppet,alphagov/pp-puppet,alphagov/pp-puppet,alphagov/pp-puppet",113,"```python
#!/usr/bin/env python
# encoding: utf-8

import datetime
import json
import requests


JSON_REQUEST = {
  ""query"": {
    ""filtered"": {
      ""query"": {
        ""bool"": {
          ""should"": [
            {
              ""query_string"": {
                ""query"": ""*""
              }
            }
          ]
        }
      },
      ""filter"": {
        ""bool"": {
          ""must"": [
            {
              ""match_all"": {

              }
            },
            {
              ""range"": {
                ""@timestamp"": {
                  ""from"": ""now-1h"",
                  ""to"": ""now""
                }
              }
            },
            {
              ""fquery"": {
                ""query"": {
                  ""field"": {
                    ""@fields.levelname"": {
                      ""query"": ""\""ERROR\""""
                    }
                  }
                },
                ""_cache"": True
              }
            },
            {
              ""bool"": {
                ""must"": [
                  {
                    ""match_all"": {

                    }
                  }
                ]
              }
            }
          ]
        }
      }
    }
  },
  ""highlight"": {
    ""fields"": {

    },
    ""fragment_size"": 2147483647,
    ""pre_tags"": [
      ""@start-highlight@""
    ],
    ""post_tags"": [
      ""@end-highlight@""
    ]
  },
  ""size"": 500,
  ""sort"": [
    {
      ""@timestamp"": {
        ""order"": ""desc""
      }
    }
  ]
}


def main():
    now = datetime.datetime.now().date()
    es_host = 'elasticsearch:9200'
    es_index = 'logstash-{year:04}.{month:02}.{day:02}'.format(
        year=now.year, month=now.month, day=now.day)

    response = requests.post(
        'http://{}/{}/_search'.format(es_host, es_index),
        headers={'Content-Type': 'application/json'},
        data=json.dumps(JSON_REQUEST))
    response.raise_for_status()
    response_data = json.loads(response.content)
    from pprint import pprint
    hits = response_data['hits']['hits']
    print(""{} log matches"".format(len(hits)))
    for i, hit in enumerate(hits):
        print(""--- Log message #{} --- "".format(i + 1))
        pprint(hits[0]['_source'])

    return 2 if len(hits) > 0 else 0

if __name__ == '__main__':
    main()

```"
160049859e40855082f7482421df383a5ed80df4,examples/fips.py,examples/fips.py,,"#!/usr/bin/env python3
'''
Suppose you're trying to estimate someone's median household income
based on their current location. Perhaps they posted a photograph on
Twitter that has latitude and longitude in its EXIF data. You might go
to the FCC census block conversions API (https://www.fcc.gov/general
/census-block-conversions-api) to figure out in which census block the
photo was taken.
'''

from destructure import match, MatchError, Binding, Switch
import json
from urllib.request import urlopen
from urllib.parse import urlencode



url = 'http://data.fcc.gov/api/block/find?'
params = {'format': 'json', 'showall': 'true',
          # 'latitude': 28.35975, 'longitude': -81.421988}
          'latitude': 28.359, 'longitude': -81.421}



results = Binding()

schema_one = \
{
    ""County"": {
        ""name"": results.county,
        ""FIPS"": str,
    },
    ""State"": {
        ""name"": results.state,
        ""code"": str,
        ""FIPS"": str,
    },
    ""Block"": {
        ""FIPS"": results.fips,
    },
    ""executionTime"": str,
    ""status"": ""OK"",
}

schema_intersection = \
{
    ""executionTime"": str,
    ""County"": {
        ""FIPS"": str,
        ""name"": results.county
    },
    ""messages"": [
        ""FCC0001: The coordinate lies on the boundary of mulitple blocks, first FIPS is displayed. For a complete list use showall=true to display 'intersection' element in the Block""
    ],
    ""Block"": {
        ""FIPS"": str,
        ""intersection"": results.intersection
    },
    ""status"": ""OK"",
    ""State"": {
        ""code"": str,
        ""FIPS"": str,
        ""name"": results.state
    }
}



with urlopen(url + urlencode(params)) as response:
    data = response.read()

text = data.decode('utf-8')
mapping = json.loads(text)



s = Switch(data=mapping, binding=results)

if s.case(schema_one):
    codes = [results.fips]

elif s.case(schema_intersection):
    codes = [block['FIPS'] for block in results.intersection]

else:
    raise MatchError('Could not match any schemas')



if not codes or None in codes:
    fmt = 'No FIPS found for {latitude}, {longitude}'
    raise ValueError(fmt.format(**params))

for fips in codes:
    print(fips)

# From there, it's on to http://api.census.gov to finish the task.


",Add simple switch/case usage example,"Add simple switch/case usage example
",Python,mit,selik/destructure,100,"```python
#!/usr/bin/env python3
'''
Suppose you're trying to estimate someone's median household income
based on their current location. Perhaps they posted a photograph on
Twitter that has latitude and longitude in its EXIF data. You might go
to the FCC census block conversions API (https://www.fcc.gov/general
/census-block-conversions-api) to figure out in which census block the
photo was taken.
'''

from destructure import match, MatchError, Binding, Switch
import json
from urllib.request import urlopen
from urllib.parse import urlencode



url = 'http://data.fcc.gov/api/block/find?'
params = {'format': 'json', 'showall': 'true',
          # 'latitude': 28.35975, 'longitude': -81.421988}
          'latitude': 28.359, 'longitude': -81.421}



results = Binding()

schema_one = \
{
    ""County"": {
        ""name"": results.county,
        ""FIPS"": str,
    },
    ""State"": {
        ""name"": results.state,
        ""code"": str,
        ""FIPS"": str,
    },
    ""Block"": {
        ""FIPS"": results.fips,
    },
    ""executionTime"": str,
    ""status"": ""OK"",
}

schema_intersection = \
{
    ""executionTime"": str,
    ""County"": {
        ""FIPS"": str,
        ""name"": results.county
    },
    ""messages"": [
        ""FCC0001: The coordinate lies on the boundary of mulitple blocks, first FIPS is displayed. For a complete list use showall=true to display 'intersection' element in the Block""
    ],
    ""Block"": {
        ""FIPS"": str,
        ""intersection"": results.intersection
    },
    ""status"": ""OK"",
    ""State"": {
        ""code"": str,
        ""FIPS"": str,
        ""name"": results.state
    }
}



with urlopen(url + urlencode(params)) as response:
    data = response.read()

text = data.decode('utf-8')
mapping = json.loads(text)



s = Switch(data=mapping, binding=results)

if s.case(schema_one):
    codes = [results.fips]

elif s.case(schema_intersection):
    codes = [block['FIPS'] for block in results.intersection]

else:
    raise MatchError('Could not match any schemas')



if not codes or None in codes:
    fmt = 'No FIPS found for {latitude}, {longitude}'
    raise ValueError(fmt.format(**params))

for fips in codes:
    print(fips)

# From there, it's on to http://api.census.gov to finish the task.



```"
96616feb46dc8342f742bc63f075477dc1b325e6,modules/data_generator.py,modules/data_generator.py,,"from __future__ import print_function
import requests
import json

class Table(object):
    def __init__(self, x, y, width, height, name):
        self.name = name
        self.position_x = x
        self.position_y = y
        self.width = width
        self.height = height

    def json(self):
        return json.dumps(self.__dict__)


class DataTableGenerator(object):

    def __init__(self):
        self.url = ""http://bleepr.io/tables""

    def get_tables(self):
        r = requests.get(self.url)
        print(r.status_code)
        print(r.headers['content-type'])
        return r.json()

    def insert_table(self, table):
        print(table.json())

        headers = {'Content-type': 'application/json',
                   'Accept': 'text/plain'}

        r = requests.post(self.url, data=table.json(),
                          headers=headers)

        if(r.status_code != 406):
            raise Exception(""POST response received was %s"" % r.status_code)


    def edit_table(self, id):
        return

    def delete_table(self, id):
        r = requests.delete(self.url + ""/"" + str(id))
        if (r.status_code != 204):
            raise Exception(""DELETE response received was %s"" % r.status_code)

    def clear_database(self):
        data = self.get_tables()
        for d in data:
            self.delete_table(d[""id""])


class CostumerDataGenerator(object):

    def get_costumers(self):
        return None

    def insert_costumer(self, costumer):
        return None

    def edit_costumer(self, id):
        return None

    def delete_costumer(self, id):
        return None


class CardsDataGenerator(object):

    def get_cards(self):
        return None

    def insert_card(self, card):
        return None

    def edit_card(self, id):
        return None

    def delete_card(self, id):
        return


def slip():
    dt = DataTableGenerator()
    dt.clear_database()
    t = Table(45,249,80,485, ""One"")
    dt.insert_table(t)
    t = Table(300,60,180,100, ""Two"")
    dt.insert_table(t)
    t = Table(480,60,180,100, ""Three"")
    dt.insert_table(t)
    t = Table(220,440,180,100, ""Four"")
    dt.insert_table(t)
    t = Table(400,440,180,100, ""Five"")
    dt.insert_table(t)
    t = Table(300,250,180,100, ""Six"")
    dt.insert_table(t)

def main():
    slip()

if __name__ == ""__main__"":
    main()
",Add generator for tables plus SLIP room config,"Add generator for tables plus SLIP room config
",Python,mit,"bleepr/bleepr-manage,bleepr/bleepr-manage",106,"```python
from __future__ import print_function
import requests
import json

class Table(object):
    def __init__(self, x, y, width, height, name):
        self.name = name
        self.position_x = x
        self.position_y = y
        self.width = width
        self.height = height

    def json(self):
        return json.dumps(self.__dict__)


class DataTableGenerator(object):

    def __init__(self):
        self.url = ""http://bleepr.io/tables""

    def get_tables(self):
        r = requests.get(self.url)
        print(r.status_code)
        print(r.headers['content-type'])
        return r.json()

    def insert_table(self, table):
        print(table.json())

        headers = {'Content-type': 'application/json',
                   'Accept': 'text/plain'}

        r = requests.post(self.url, data=table.json(),
                          headers=headers)

        if(r.status_code != 406):
            raise Exception(""POST response received was %s"" % r.status_code)


    def edit_table(self, id):
        return

    def delete_table(self, id):
        r = requests.delete(self.url + ""/"" + str(id))
        if (r.status_code != 204):
            raise Exception(""DELETE response received was %s"" % r.status_code)

    def clear_database(self):
        data = self.get_tables()
        for d in data:
            self.delete_table(d[""id""])


class CostumerDataGenerator(object):

    def get_costumers(self):
        return None

    def insert_costumer(self, costumer):
        return None

    def edit_costumer(self, id):
        return None

    def delete_costumer(self, id):
        return None


class CardsDataGenerator(object):

    def get_cards(self):
        return None

    def insert_card(self, card):
        return None

    def edit_card(self, id):
        return None

    def delete_card(self, id):
        return


def slip():
    dt = DataTableGenerator()
    dt.clear_database()
    t = Table(45,249,80,485, ""One"")
    dt.insert_table(t)
    t = Table(300,60,180,100, ""Two"")
    dt.insert_table(t)
    t = Table(480,60,180,100, ""Three"")
    dt.insert_table(t)
    t = Table(220,440,180,100, ""Four"")
    dt.insert_table(t)
    t = Table(400,440,180,100, ""Five"")
    dt.insert_table(t)
    t = Table(300,250,180,100, ""Six"")
    dt.insert_table(t)

def main():
    slip()

if __name__ == ""__main__"":
    main()

```"
c93da26c35607518f286dbdf9023034288074fab,tests/test_unix.py,tests/test_unix.py,,"import asyncio
import os
import socket
import tempfile
import uvloop

from uvloop import _testbase as tb


class _TestUnix:
    def test_create_server_1(self):
        CNT = 0           # number of clients that were successful
        TOTAL_CNT = 100   # total number of clients that test will create
        TIMEOUT = 5.0     # timeout for this test

        async def handle_client(reader, writer):
            nonlocal CNT

            data = await reader.readexactly(4)
            self.assertEqual(data, b'AAAA')
            writer.write(b'OK')

            data = await reader.readexactly(4)
            self.assertEqual(data, b'BBBB')
            writer.write(b'SPAM')

            await writer.drain()
            writer.close()

            CNT += 1

        async def test_client(addr):
            sock = socket.socket(socket.AF_UNIX)
            with sock:
                sock.setblocking(False)
                await self.loop.sock_connect(sock, addr)

                await self.loop.sock_sendall(sock, b'AAAA')
                data = await self.loop.sock_recv(sock, 2)
                self.assertEqual(data, b'OK')

                await self.loop.sock_sendall(sock, b'BBBB')
                data = await self.loop.sock_recv(sock, 4)
                self.assertEqual(data, b'SPAM')

        async def start_server():
            with tempfile.TemporaryDirectory() as td:
                sock_name = os.path.join(td, 'sock')
                try:
                    srv = await asyncio.start_unix_server(
                        handle_client,
                        sock_name,
                        loop=self.loop)

                    try:
                        srv_socks = srv.sockets
                        self.assertTrue(srv_socks)

                        tasks = []
                        for _ in range(TOTAL_CNT):
                            tasks.append(test_client(sock_name))

                        try:
                            await asyncio.wait_for(
                                asyncio.gather(*tasks, loop=self.loop),
                                TIMEOUT, loop=self.loop)
                        finally:
                            self.loop.stop()

                    finally:
                        srv.close()

                        # Check that the server cleaned-up proxy-sockets
                        for srv_sock in srv_socks:
                            self.assertEqual(srv_sock.fileno(), -1)

                except:
                    self.loop.stop()  # We don't want this test to stuck when
                                      # it fails.
                    raise

        self.loop.create_task(start_server())
        self.loop.run_forever()
        self.assertEqual(CNT, TOTAL_CNT)


class Test_UV_Unix(_TestUnix, tb.UVTestCase):
    pass


class Test_AIO_Unix(_TestUnix, tb.AIOTestCase):
    pass
",Add a test for loop.create_unix_server,"tests: Add a test for loop.create_unix_server
",Python,apache-2.0,"MagicStack/uvloop,1st1/uvloop,MagicStack/uvloop",93,"```python
import asyncio
import os
import socket
import tempfile
import uvloop

from uvloop import _testbase as tb


class _TestUnix:
    def test_create_server_1(self):
        CNT = 0           # number of clients that were successful
        TOTAL_CNT = 100   # total number of clients that test will create
        TIMEOUT = 5.0     # timeout for this test

        async def handle_client(reader, writer):
            nonlocal CNT

            data = await reader.readexactly(4)
            self.assertEqual(data, b'AAAA')
            writer.write(b'OK')

            data = await reader.readexactly(4)
            self.assertEqual(data, b'BBBB')
            writer.write(b'SPAM')

            await writer.drain()
            writer.close()

            CNT += 1

        async def test_client(addr):
            sock = socket.socket(socket.AF_UNIX)
            with sock:
                sock.setblocking(False)
                await self.loop.sock_connect(sock, addr)

                await self.loop.sock_sendall(sock, b'AAAA')
                data = await self.loop.sock_recv(sock, 2)
                self.assertEqual(data, b'OK')

                await self.loop.sock_sendall(sock, b'BBBB')
                data = await self.loop.sock_recv(sock, 4)
                self.assertEqual(data, b'SPAM')

        async def start_server():
            with tempfile.TemporaryDirectory() as td:
                sock_name = os.path.join(td, 'sock')
                try:
                    srv = await asyncio.start_unix_server(
                        handle_client,
                        sock_name,
                        loop=self.loop)

                    try:
                        srv_socks = srv.sockets
                        self.assertTrue(srv_socks)

                        tasks = []
                        for _ in range(TOTAL_CNT):
                            tasks.append(test_client(sock_name))

                        try:
                            await asyncio.wait_for(
                                asyncio.gather(*tasks, loop=self.loop),
                                TIMEOUT, loop=self.loop)
                        finally:
                            self.loop.stop()

                    finally:
                        srv.close()

                        # Check that the server cleaned-up proxy-sockets
                        for srv_sock in srv_socks:
                            self.assertEqual(srv_sock.fileno(), -1)

                except:
                    self.loop.stop()  # We don't want this test to stuck when
                                      # it fails.
                    raise

        self.loop.create_task(start_server())
        self.loop.run_forever()
        self.assertEqual(CNT, TOTAL_CNT)


class Test_UV_Unix(_TestUnix, tb.UVTestCase):
    pass


class Test_AIO_Unix(_TestUnix, tb.AIOTestCase):
    pass

```"
e51e1c14b1375249af90eff21978a316471c16b9,ichnaea/tests/test_migration.py,ichnaea/tests/test_migration.py,,"from alembic import command as alembic_command
from alembic.config import Config
from alembic.script import ScriptDirectory
from sqlalchemy import inspect
from sqlalchemy.schema import (
    MetaData,
    Table,
)

# make sure all models are imported
from ichnaea import models  # NOQA
from ichnaea.content import models  # NOQA

from ichnaea.tests.base import (
    _make_db,
    DBIsolation,
    setup_package,
    SQL_BASE_STRUCTURE,
    TestCase,
)


class TestMigration(TestCase):

    def setUp(self):
        self.db = _make_db()
        # capture state of fresh database
        self.head_tables = self.inspect_tables()
        DBIsolation.cleanup_tables(self.db.engine)

    def tearDown(self):
        self.db.engine.pool.dispose()
        del self.db
        # setup normal database schema again
        setup_package(None)

    def alembic_config(self):
        alembic_cfg = Config()
        alembic_cfg.set_section_option(
            'alembic', 'script_location', 'alembic')
        alembic_cfg.set_section_option(
            'alembic', 'sqlalchemy.url', str(self.db.engine.url))
        return alembic_cfg

    def alembic_script(self):
        return ScriptDirectory.from_config(self.alembic_config())

    def current_db_revision(self):
        with self.db.engine.connect() as conn:
            result = conn.execute('select version_num from alembic_version')
            alembic_rev = result.first()
        if alembic_rev is None:
            return None
        return alembic_rev[0]

    def inspect_tables(self):
        metadata = MetaData()
        inspector = inspect(self.db.engine)
        tables = {}
        for name in inspector.get_table_names():
            tables[name] = Table(name, metadata)
        return tables

    def setup_base_db(self):
        with open(SQL_BASE_STRUCTURE) as fd:
            sql_text = fd.read()
        with self.db.engine.connect() as conn:
            conn.execute(sql_text)

    def run_migration(self, target='head'):
        engine = self.db.engine
        with engine.connect() as conn:
            trans = conn.begin()
            alembic_command.upgrade(self.alembic_config(), target)
            trans.commit()

    def test_migration(self):
        self.setup_base_db()
        # we have no alembic base revision
        self.assertTrue(self.current_db_revision() is None)

        self.run_migration()
        # after the migration, the DB is stamped
        db_revision = self.current_db_revision()
        self.assertTrue(db_revision is not None)

        # db revision matches latest alembic revision
        alembic_head = self.alembic_script().get_current_head()
        self.assertEqual(db_revision, alembic_head)

        # compare the tables from a migrated database to those
        # created fresh from the model definitions
        migrated_tables = self.inspect_tables()
        head_tables = self.head_tables
        self.assertEqual(set(head_tables.keys()),
                         set(migrated_tables.keys()))
",Add a test to execute all migrations.,"Add a test to execute all migrations.
",Python,apache-2.0,"mozilla/ichnaea,therewillbecode/ichnaea,mozilla/ichnaea,mozilla/ichnaea,mozilla/ichnaea,therewillbecode/ichnaea,therewillbecode/ichnaea",97,"```python
from alembic import command as alembic_command
from alembic.config import Config
from alembic.script import ScriptDirectory
from sqlalchemy import inspect
from sqlalchemy.schema import (
    MetaData,
    Table,
)

# make sure all models are imported
from ichnaea import models  # NOQA
from ichnaea.content import models  # NOQA

from ichnaea.tests.base import (
    _make_db,
    DBIsolation,
    setup_package,
    SQL_BASE_STRUCTURE,
    TestCase,
)


class TestMigration(TestCase):

    def setUp(self):
        self.db = _make_db()
        # capture state of fresh database
        self.head_tables = self.inspect_tables()
        DBIsolation.cleanup_tables(self.db.engine)

    def tearDown(self):
        self.db.engine.pool.dispose()
        del self.db
        # setup normal database schema again
        setup_package(None)

    def alembic_config(self):
        alembic_cfg = Config()
        alembic_cfg.set_section_option(
            'alembic', 'script_location', 'alembic')
        alembic_cfg.set_section_option(
            'alembic', 'sqlalchemy.url', str(self.db.engine.url))
        return alembic_cfg

    def alembic_script(self):
        return ScriptDirectory.from_config(self.alembic_config())

    def current_db_revision(self):
        with self.db.engine.connect() as conn:
            result = conn.execute('select version_num from alembic_version')
            alembic_rev = result.first()
        if alembic_rev is None:
            return None
        return alembic_rev[0]

    def inspect_tables(self):
        metadata = MetaData()
        inspector = inspect(self.db.engine)
        tables = {}
        for name in inspector.get_table_names():
            tables[name] = Table(name, metadata)
        return tables

    def setup_base_db(self):
        with open(SQL_BASE_STRUCTURE) as fd:
            sql_text = fd.read()
        with self.db.engine.connect() as conn:
            conn.execute(sql_text)

    def run_migration(self, target='head'):
        engine = self.db.engine
        with engine.connect() as conn:
            trans = conn.begin()
            alembic_command.upgrade(self.alembic_config(), target)
            trans.commit()

    def test_migration(self):
        self.setup_base_db()
        # we have no alembic base revision
        self.assertTrue(self.current_db_revision() is None)

        self.run_migration()
        # after the migration, the DB is stamped
        db_revision = self.current_db_revision()
        self.assertTrue(db_revision is not None)

        # db revision matches latest alembic revision
        alembic_head = self.alembic_script().get_current_head()
        self.assertEqual(db_revision, alembic_head)

        # compare the tables from a migrated database to those
        # created fresh from the model definitions
        migrated_tables = self.inspect_tables()
        head_tables = self.head_tables
        self.assertEqual(set(head_tables.keys()),
                         set(migrated_tables.keys()))

```"
9031a8def9b797cbd8280a29e62c436e168f4096,txircd/modules/rfc/cmd_nick.py,txircd/modules/rfc/cmd_nick.py,,"from twisted.plugin import IPlugin
from twisted.words.protocols import irc
from txircd.module_interface import Command, ICommand, IModuleData, ModuleData
from txircd.utils import ircLower, isValidNick
from zope.interface import implements
from datetime import datetime

class NickCommand(ModuleData):
    implements(IPlugin, IModuleData)
    
    name = ""NickCommand""
    core = True
    
    def hookIRCd(self, ircd):
        self.ircd = ircd
    
    def userCommands(self):
        return [ (""NICK"", 1, NickUserCommand(self.ircd)) ]
    
    def serverCommands(self):
        return [ (""NICK"", 1, NickServerCommand(self.ircd)) ]

class NickUserCommand(Command):
    implements(ICommand)
    
    forRegisteredUsers = None
    
    def __init__(self, ircd):
        self.ircd = ircd
    
    def parseParams(self, user, params, prefix, tags):
        if not params or not params[0]:
            user.sendMessage(irc.ERR_NEEDMOREPARAMS, ""NICK"", "":Not enough parameters"")
            return None
        if not isValidNick(params[0]):
            user.sendMessage(irc.ERR_ERRONEUSNICKNAME, params[0], "":Erroneous nickname"")
            return None
        if params[0] in self.ircd.userNicks:
            otherUser = self.ircd.users[self.ircd.userNicks]
            if user != otherUser:
                user.sendMessage(irc.ERR_NICKNAMEINUSE, nick, "":Nickname is already in use"")
                return None
        return {
            ""nick"": params[0]
        }
    
    def execute(self, user, data):
        user.changeNick(data[""nick""])
        if not user.isRegistered():
            user.register(""NICK"")
        return True

class NickServerCommand(Command):
    implements(ICommand)
    
    def __init__(self, ircd):
        self.ircd = ircd
    
    def parseParams(self, server, params, prefix, tags):
        if len(params) != 2:
            return None
        if prefix not in self.ircd.users:
            self.disconnect(""Desync: User list"")
            return None
        user = self.ircd.users[prefix]
        try:
            time = datetime.utcfromtimestamp(params[0])
        except ValueError:
            return None
        if params[1] in self.ircd.userNicks:
            localUser = self.ircd.users[self.ircd.userNicks[params[1]]]
            if localUser != user:
                if localUser.localOnly:
                    if ""localnickcollision"" in self.ircd.actions:
                        for action in self.ircd.actions[""localnickcollision""]:
                            if action[0](user):
                                break
                        else:
                            return None
                        return {
                            ""user"": user,
                            ""time"": time,
                            ""nick"": params[1]
                        }
                    return None
                self.disconnect(""Desync: User data (nicknames)"")
                return None
        return {
            ""user"": user,
            ""time"": time,
            ""nick"": params[1]
        }
    
    def execute(self, server, data):
        user = data[""user""]
        user.changeNick(data[""nick""])
        user.nickSince = data[""time""]
        return True

cmd_nick = NickCommand()",Implement NICK command for both users and servers,"Implement NICK command for both users and servers
",Python,bsd-3-clause,"ElementalAlchemist/txircd,Heufneutje/txircd",100,"```python
from twisted.plugin import IPlugin
from twisted.words.protocols import irc
from txircd.module_interface import Command, ICommand, IModuleData, ModuleData
from txircd.utils import ircLower, isValidNick
from zope.interface import implements
from datetime import datetime

class NickCommand(ModuleData):
    implements(IPlugin, IModuleData)
    
    name = ""NickCommand""
    core = True
    
    def hookIRCd(self, ircd):
        self.ircd = ircd
    
    def userCommands(self):
        return [ (""NICK"", 1, NickUserCommand(self.ircd)) ]
    
    def serverCommands(self):
        return [ (""NICK"", 1, NickServerCommand(self.ircd)) ]

class NickUserCommand(Command):
    implements(ICommand)
    
    forRegisteredUsers = None
    
    def __init__(self, ircd):
        self.ircd = ircd
    
    def parseParams(self, user, params, prefix, tags):
        if not params or not params[0]:
            user.sendMessage(irc.ERR_NEEDMOREPARAMS, ""NICK"", "":Not enough parameters"")
            return None
        if not isValidNick(params[0]):
            user.sendMessage(irc.ERR_ERRONEUSNICKNAME, params[0], "":Erroneous nickname"")
            return None
        if params[0] in self.ircd.userNicks:
            otherUser = self.ircd.users[self.ircd.userNicks]
            if user != otherUser:
                user.sendMessage(irc.ERR_NICKNAMEINUSE, nick, "":Nickname is already in use"")
                return None
        return {
            ""nick"": params[0]
        }
    
    def execute(self, user, data):
        user.changeNick(data[""nick""])
        if not user.isRegistered():
            user.register(""NICK"")
        return True

class NickServerCommand(Command):
    implements(ICommand)
    
    def __init__(self, ircd):
        self.ircd = ircd
    
    def parseParams(self, server, params, prefix, tags):
        if len(params) != 2:
            return None
        if prefix not in self.ircd.users:
            self.disconnect(""Desync: User list"")
            return None
        user = self.ircd.users[prefix]
        try:
            time = datetime.utcfromtimestamp(params[0])
        except ValueError:
            return None
        if params[1] in self.ircd.userNicks:
            localUser = self.ircd.users[self.ircd.userNicks[params[1]]]
            if localUser != user:
                if localUser.localOnly:
                    if ""localnickcollision"" in self.ircd.actions:
                        for action in self.ircd.actions[""localnickcollision""]:
                            if action[0](user):
                                break
                        else:
                            return None
                        return {
                            ""user"": user,
                            ""time"": time,
                            ""nick"": params[1]
                        }
                    return None
                self.disconnect(""Desync: User data (nicknames)"")
                return None
        return {
            ""user"": user,
            ""time"": time,
            ""nick"": params[1]
        }
    
    def execute(self, server, data):
        user = data[""user""]
        user.changeNick(data[""nick""])
        user.nickSince = data[""time""]
        return True

cmd_nick = NickCommand()
```"
5f0788be20bad0cef4a31a88b7513da58822a157,buildbucket.py,buildbucket.py,,"#!/usr/bin/env python
# Copyright (c) 2015 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Tool for interacting with Buildbucket.

Usage:
  $ depot-tools-auth login https://cr-buildbucket.appspot.com
  $ buildbucket.py \
    put \
    --bucket master.tryserver.chromium.linux \
    --builder my-builder \

  Puts a build into buildbucket for my-builder on tryserver.chromium.linux.
""""""

import argparse
import json
import urlparse
import os
import sys

from third_party import httplib2

import auth


BUILDBUCKET_URL = 'https://cr-buildbucket.appspot.com'
PUT_BUILD_URL = urlparse.urljoin(
  BUILDBUCKET_URL,
  '_ah/api/buildbucket/v1/builds',
)


def main(argv):
  parser = argparse.ArgumentParser()
  parser.add_argument(
    '-v',
    '--verbose',
    action='store_true',
  )
  subparsers = parser.add_subparsers(dest='command')
  put_parser = subparsers.add_parser('put')
  put_parser.add_argument(
    '--bucket',
    help=(
      'The bucket to schedule the build on. Typically the master name, e.g.'
      ' master.tryserver.chromium.linux.'
    ),
    required=True,
  )
  put_parser.add_argument(
    '-n',
    '--builder-name',
    help='The builder to schedule the build on.',
    required=True,
  )
  put_parser.add_argument(
    '-p',
    '--properties',
    help='A file to load a JSON dict of properties from.',
  )
  args = parser.parse_args()
  # TODO(smut): When more commands are implemented, refactor this.
  assert args.command == 'put'

  properties = {}
  if args.properties:
    try:
      with open(args.properties) as fp:
        properties.update(json.load(fp))
    except (TypeError, ValueError):
      sys.stderr.write('%s contained invalid JSON dict.\n' % args.properties)
      raise

  authenticator = auth.get_authenticator_for_host(
    BUILDBUCKET_URL,
    auth.make_auth_config(use_oauth2=True),
  )
  http = authenticator.authorize(httplib2.Http())
  http.force_exception_to_status_code = True
  response, content = http.request(
    PUT_BUILD_URL,
    'PUT',
    body=json.dumps({
      'bucket': args.bucket,
      'parameters_json': json.dumps({
        'builder_name': args.builder_name,
        'properties': properties,
      }),
    }),
    headers={'Content-Type': 'application/json'},
  )

  if args.verbose:
    print content

  return response.status != 200


if __name__ == '__main__':
  sys.exit(main(sys.argv))
",Add script for triggering Buildbucket builds,"Add script for triggering Buildbucket builds

BUG=493885
TESTED=See https://paste.googleplex.com/5622248052359168

Review URL: https://codereview.chromium.org/1164363003

git-svn-id: fd409f4bdeea2bb50a5d34bb4d4bfc2046a5a3dd@295569 0039d316-1c4b-4281-b951-d872f2087c98
",Python,bsd-3-clause,"duongbaoduy/gtools,mlufei/depot_tools,aleonliao/depot_tools,azunite/chrome_build,azunite/chrome_build,disigma/depot_tools,hsharsha/depot_tools,chinmaygarde/depot_tools,kaiix/depot_tools,fracting/depot_tools,disigma/depot_tools,fracting/depot_tools,mlufei/depot_tools,aleonliao/depot_tools,ajohnson23/depot_tools,mlufei/depot_tools,Midrya/chromium,CoherentLabs/depot_tools,kaiix/depot_tools,azureplus/chromium_depot_tools,aleonliao/depot_tools,duongbaoduy/gtools,CoherentLabs/depot_tools,duanwujie/depot_tools,gcodetogit/depot_tools,SuYiling/chrome_depot_tools,chinmaygarde/depot_tools,azureplus/chromium_depot_tools,disigma/depot_tools,primiano/depot_tools,chinmaygarde/depot_tools,gcodetogit/depot_tools,SuYiling/chrome_depot_tools,primiano/depot_tools,hsharsha/depot_tools,azunite/chrome_build,fracting/depot_tools,duanwujie/depot_tools,duanwujie/depot_tools,primiano/depot_tools,cpanelli/-git-clone-https-chromium.googlesource.com-chromium-tools-depot_tools,Midrya/chromium,duongbaoduy/gtools,kaiix/depot_tools,hsharsha/depot_tools,SuYiling/chrome_depot_tools,Midrya/chromium,cpanelli/-git-clone-https-chromium.googlesource.com-chromium-tools-depot_tools,gcodetogit/depot_tools,ajohnson23/depot_tools,ajohnson23/depot_tools,azureplus/chromium_depot_tools,cpanelli/-git-clone-https-chromium.googlesource.com-chromium-tools-depot_tools",104,"```python
#!/usr/bin/env python
# Copyright (c) 2015 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Tool for interacting with Buildbucket.

Usage:
  $ depot-tools-auth login https://cr-buildbucket.appspot.com
  $ buildbucket.py \
    put \
    --bucket master.tryserver.chromium.linux \
    --builder my-builder \

  Puts a build into buildbucket for my-builder on tryserver.chromium.linux.
""""""

import argparse
import json
import urlparse
import os
import sys

from third_party import httplib2

import auth


BUILDBUCKET_URL = 'https://cr-buildbucket.appspot.com'
PUT_BUILD_URL = urlparse.urljoin(
  BUILDBUCKET_URL,
  '_ah/api/buildbucket/v1/builds',
)


def main(argv):
  parser = argparse.ArgumentParser()
  parser.add_argument(
    '-v',
    '--verbose',
    action='store_true',
  )
  subparsers = parser.add_subparsers(dest='command')
  put_parser = subparsers.add_parser('put')
  put_parser.add_argument(
    '--bucket',
    help=(
      'The bucket to schedule the build on. Typically the master name, e.g.'
      ' master.tryserver.chromium.linux.'
    ),
    required=True,
  )
  put_parser.add_argument(
    '-n',
    '--builder-name',
    help='The builder to schedule the build on.',
    required=True,
  )
  put_parser.add_argument(
    '-p',
    '--properties',
    help='A file to load a JSON dict of properties from.',
  )
  args = parser.parse_args()
  # TODO(smut): When more commands are implemented, refactor this.
  assert args.command == 'put'

  properties = {}
  if args.properties:
    try:
      with open(args.properties) as fp:
        properties.update(json.load(fp))
    except (TypeError, ValueError):
      sys.stderr.write('%s contained invalid JSON dict.\n' % args.properties)
      raise

  authenticator = auth.get_authenticator_for_host(
    BUILDBUCKET_URL,
    auth.make_auth_config(use_oauth2=True),
  )
  http = authenticator.authorize(httplib2.Http())
  http.force_exception_to_status_code = True
  response, content = http.request(
    PUT_BUILD_URL,
    'PUT',
    body=json.dumps({
      'bucket': args.bucket,
      'parameters_json': json.dumps({
        'builder_name': args.builder_name,
        'properties': properties,
      }),
    }),
    headers={'Content-Type': 'application/json'},
  )

  if args.verbose:
    print content

  return response.status != 200


if __name__ == '__main__':
  sys.exit(main(sys.argv))

```"
dfa6d3b3fe148895599360d01195200b06e7583d,receiver-dump.py,receiver-dump.py,,"#******************************************************************************
#
#   receiver-dump.py
#
#******************************************************************************
#
#   Author:         Werner Lane
#   E-mail:         laneboysrc@gmail.com
#
#******************************************************************************

import serial
import sys
import time

jitterString0x07 = [
    '|*  ..   |',
    '| * ..   |',
    '|  *..   |',
    '|   *.   |',
    '|   .*   |',
    '|   ..*  |',
    '|   .. * |',
    '|   ..  *|'
]

jitterString0x0f = [
    '|*      ..       |',
    '| *     ..       |',
    '|  *    ..       |',
    '|   *   ..       |',
    '|    *  ..       |',
    '|     * ..       |',
    '|      *..       |',
    '|       *.       |',
    '|       .*       |',
    '|       ..*      |',
    '|       .. *     |',
    '|       ..  *    |',
    '|       ..   *   |',
    '|       ..    *  |',
    '|       ..     * |',
    '|       ..      *|'
]

def int2bin(n, count=32):
    """"""returns the binary of integer n, using count number of digits""""""
    return """".join([str((n >> y) & 1) for y in range(count-1, -1, -1)])

def dump(port):
    try:
        s = serial.Serial(port, 38400)
    except serial.SerialException, e:
        print(""Unable to open port %s.\nError message: %s"" % (port, e))
        sys.exit(0)

    oldValue = 0
    oldData = 0
    startTime = time.time()
    bigDiff = False
    while True:
        c = s.read(1)
        numString = """"
        while c != ""\n"":
            numString = numString + c
            c = s.read(1)

        try:        
            value = int(numString, 10)
        except ValueError:
            value = 0
        if abs(oldValue - value) > 4:
            #print  abs(oldValue - value)
            if bigDiff or abs(oldValue - value) > 32:
                oldValue = value
                bigDiff = False
            else:
                bigDiff = True
                #print ""GLITCH"", abs(oldValue - value)
        else:
            oldValue = value
            #print ""%s  %d"" % (int2bin(value >> 3, 7), value & 0x7)
            
            jitter = value & 0x7
            print jitterString0x07[jitter],
            
            data = value >> 3
            if oldData != data:
                now = time.time()
                print ""%3d %s"" % (int((now - startTime) * 1000), int2bin(data, 6)),
                startTime = now
            oldData = data
            print


if __name__ == '__main__':
    try:
        port = sys.argv[1]
    except IndexError:
        port = '/dev/ttyUSB0'

    try:
        dump(port)
    except KeyboardInterrupt:
        print """"
        sys.exit(0)
","Add receiver dump program, output of firmware","Add receiver dump program, output of firmware
",Python,unlicense,"laneboysrc/hk310-expansion,laneboysrc/hk310-expansion,laneboysrc/hk310-expansion",107,"```python
#******************************************************************************
#
#   receiver-dump.py
#
#******************************************************************************
#
#   Author:         Werner Lane
#   E-mail:         laneboysrc@gmail.com
#
#******************************************************************************

import serial
import sys
import time

jitterString0x07 = [
    '|*  ..   |',
    '| * ..   |',
    '|  *..   |',
    '|   *.   |',
    '|   .*   |',
    '|   ..*  |',
    '|   .. * |',
    '|   ..  *|'
]

jitterString0x0f = [
    '|*      ..       |',
    '| *     ..       |',
    '|  *    ..       |',
    '|   *   ..       |',
    '|    *  ..       |',
    '|     * ..       |',
    '|      *..       |',
    '|       *.       |',
    '|       .*       |',
    '|       ..*      |',
    '|       .. *     |',
    '|       ..  *    |',
    '|       ..   *   |',
    '|       ..    *  |',
    '|       ..     * |',
    '|       ..      *|'
]

def int2bin(n, count=32):
    """"""returns the binary of integer n, using count number of digits""""""
    return """".join([str((n >> y) & 1) for y in range(count-1, -1, -1)])

def dump(port):
    try:
        s = serial.Serial(port, 38400)
    except serial.SerialException, e:
        print(""Unable to open port %s.\nError message: %s"" % (port, e))
        sys.exit(0)

    oldValue = 0
    oldData = 0
    startTime = time.time()
    bigDiff = False
    while True:
        c = s.read(1)
        numString = """"
        while c != ""\n"":
            numString = numString + c
            c = s.read(1)

        try:        
            value = int(numString, 10)
        except ValueError:
            value = 0
        if abs(oldValue - value) > 4:
            #print  abs(oldValue - value)
            if bigDiff or abs(oldValue - value) > 32:
                oldValue = value
                bigDiff = False
            else:
                bigDiff = True
                #print ""GLITCH"", abs(oldValue - value)
        else:
            oldValue = value
            #print ""%s  %d"" % (int2bin(value >> 3, 7), value & 0x7)
            
            jitter = value & 0x7
            print jitterString0x07[jitter],
            
            data = value >> 3
            if oldData != data:
                now = time.time()
                print ""%3d %s"" % (int((now - startTime) * 1000), int2bin(data, 6)),
                startTime = now
            oldData = data
            print


if __name__ == '__main__':
    try:
        port = sys.argv[1]
    except IndexError:
        port = '/dev/ttyUSB0'

    try:
        dump(port)
    except KeyboardInterrupt:
        print """"
        sys.exit(0)

```"
a78f918849e35dca110eec38741001ab11279c65,sara_flexbe_states/src/sara_flexbe_states/WonderlandAddUpdatePeople.py,sara_flexbe_states/src/sara_flexbe_states/WonderlandAddUpdatePeople.py,,"#!/usr/bin/env python
# encoding=utf8

import json

import requests
from flexbe_core import EventState, Logger

""""""
Created on 17/05/2018

@author: Lucas Maurice
""""""


class WonderlandAddUpdatePeople(EventState):
    '''
    Add or update all known persons in wonderland.

    <= done                     return when the add correctly append
    '''

    def __init__(self):
        # See example_state.py for basic explanations.
        super(WonderlandAddUpdatePeople, self).__init__(outcomes=['done'])
        self.url = ""http://wonderland:8000/api/people/""

    def execute(self, userdata):
        # Generate URL to contact
        s = 0

    def addPerson(self, entity):
        if entity.face.id is None:
            Logger.logwarn('Need face ID !')
            return 'bad_request'

        if entity.wonderlandId is None and entity.face.id is None:
            Logger.logwarn('Need wonderland ID or face ID !')
            return 'bad_request'

        data = {'peopleRecognitionId': entity.face.id}

        if entity.color is not None:
            data.update({'peopleColor': entity.color})

        if entity.pose is not None:
            data.update({'peoplePose': entity.pose})

        if entity.poseProbability is not None:
            data.update({'peoplePoseAccuracy': entity.poseProbability})

        if entity.face.gender is not None:
            data.update({'peopleGender': entity.face.gender})

        if entity.face.genderProbability is not None:
            data.update({'peopleGenderAccuracy': entity.face.genderProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.isOperator is None:
            data.update({'peopleIsOperator': False})
        else:
            data.update({'peopleIsOperator': entity.isOperator})

        if len(entity.aliases) > 0:
            data.update({'peopleName': entity.aliases[0]})

        # try the request
        try:
            response = requests.post(self.url, data=data)
            if response.status_code == 201:
                return 'done'

            elif 400 <= response.status_code < 500:
                Logger.logwarn(response.status_code)
                data = json.loads(response.content)
                if 'peopleRecognitionId' in data and data['peopleRecognitionId'][0] \
                        == u'people with this peopleRecognitionId already exists.':
                    return 'already_exit'
                else:
                    return 'bad_request'
            else:
                Logger.logerr(response.status_code)
                return 'error'

        except requests.exceptions.RequestException as e:
            Logger.logerr(e)
            return 'error'",Add a state for update entire list of persons.,"Add a state for update entire list of persons.
",Python,bsd-3-clause,"WalkingMachine/sara_behaviors,WalkingMachine/sara_behaviors",98,"```python
#!/usr/bin/env python
# encoding=utf8

import json

import requests
from flexbe_core import EventState, Logger

""""""
Created on 17/05/2018

@author: Lucas Maurice
""""""


class WonderlandAddUpdatePeople(EventState):
    '''
    Add or update all known persons in wonderland.

    <= done                     return when the add correctly append
    '''

    def __init__(self):
        # See example_state.py for basic explanations.
        super(WonderlandAddUpdatePeople, self).__init__(outcomes=['done'])
        self.url = ""http://wonderland:8000/api/people/""

    def execute(self, userdata):
        # Generate URL to contact
        s = 0

    def addPerson(self, entity):
        if entity.face.id is None:
            Logger.logwarn('Need face ID !')
            return 'bad_request'

        if entity.wonderlandId is None and entity.face.id is None:
            Logger.logwarn('Need wonderland ID or face ID !')
            return 'bad_request'

        data = {'peopleRecognitionId': entity.face.id}

        if entity.color is not None:
            data.update({'peopleColor': entity.color})

        if entity.pose is not None:
            data.update({'peoplePose': entity.pose})

        if entity.poseProbability is not None:
            data.update({'peoplePoseAccuracy': entity.poseProbability})

        if entity.face.gender is not None:
            data.update({'peopleGender': entity.face.gender})

        if entity.face.genderProbability is not None:
            data.update({'peopleGenderAccuracy': entity.face.genderProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.isOperator is None:
            data.update({'peopleIsOperator': False})
        else:
            data.update({'peopleIsOperator': entity.isOperator})

        if len(entity.aliases) > 0:
            data.update({'peopleName': entity.aliases[0]})

        # try the request
        try:
            response = requests.post(self.url, data=data)
            if response.status_code == 201:
                return 'done'

            elif 400 <= response.status_code < 500:
                Logger.logwarn(response.status_code)
                data = json.loads(response.content)
                if 'peopleRecognitionId' in data and data['peopleRecognitionId'][0] \
                        == u'people with this peopleRecognitionId already exists.':
                    return 'already_exit'
                else:
                    return 'bad_request'
            else:
                Logger.logerr(response.status_code)
                return 'error'

        except requests.exceptions.RequestException as e:
            Logger.logerr(e)
            return 'error'
```"
1ff0b5fe7651b836ac9010b2970ad6b45a0739e7,tools/verify_simulator_result.py,tools/verify_simulator_result.py,,"#! /usr/bin/env python

import os
import os.path
import sys
import json

success = 0

def _extractGenome(header):
    headers = header.split('|')
    genome = headers[4].strip()
    pos = genome.find("","")        
    if pos > 0:
        genome = genome[:pos]

    pos = genome.find("" chromosome"")
    if pos > 0:
        genome = genome[:pos]
    return genome

def _parseClassified(path):
    f = open(path)
    for line in f:
        j = json.loads(line)

        query = j['query']
        query_header = j['query_header']
        result = j['result']
        ctype = j['type']

        if ctype == ""CLASSIFIED"":
            r = result[0]
            #print ""> "" + ctype + ""\t"" + r['header'] + ""\t"" + str(r['score'])
            genome = _extractGenome(r['header'])
            source = _extractGenome(query_header)
            if genome == source:
                global success
                success = success + 1

    f.close()

def _parseVague(path):
    count=0
    f = open(path)
    for line in f:
        j = json.loads(line)

        query = j['query']
        query_header = j['query_header']
        result = j['result']
        ctype = j['type']

        """"""
        if ctype == ""VAGUE"":
            print ""> ""
            for r in result:
                print ctype + ""\t"" + r['header'] + ""\t"" + str(r['score'])
        """"""

    f.close()

def _parseUnknown(path):
    count=0
    f = open(path)
    for line in f:
        j = json.loads(line)

        query = j['query']
        query_header = j['query_header']
        result = j['result']
        ctype = j['type']

        """"""
        if ctype == ""UNKNOWN"":
            print ""> "" + ctype + query_header
        """"""

    f.close()

def _parse(path):
    print ""parsing"", path
    _parseClassified(path);
    _parseVague(path);
    _parseUnknown(path);

def parse(path):
    if os.path.isdir(path):
        for p in path:
            _parse(p)
    else:
        _parse(path)

    print ""total successful classification ="", success

def main():
    parse(sys.argv[1])

if __name__ == ""__main__"":
    main()
",Add simulator result parser tool,"Add simulator result parser tool
",Python,apache-2.0,"iychoi/biospectra,iychoi/biospectra,iychoi/biospectra",101,"```python
#! /usr/bin/env python

import os
import os.path
import sys
import json

success = 0

def _extractGenome(header):
    headers = header.split('|')
    genome = headers[4].strip()
    pos = genome.find("","")        
    if pos > 0:
        genome = genome[:pos]

    pos = genome.find("" chromosome"")
    if pos > 0:
        genome = genome[:pos]
    return genome

def _parseClassified(path):
    f = open(path)
    for line in f:
        j = json.loads(line)

        query = j['query']
        query_header = j['query_header']
        result = j['result']
        ctype = j['type']

        if ctype == ""CLASSIFIED"":
            r = result[0]
            #print ""> "" + ctype + ""\t"" + r['header'] + ""\t"" + str(r['score'])
            genome = _extractGenome(r['header'])
            source = _extractGenome(query_header)
            if genome == source:
                global success
                success = success + 1

    f.close()

def _parseVague(path):
    count=0
    f = open(path)
    for line in f:
        j = json.loads(line)

        query = j['query']
        query_header = j['query_header']
        result = j['result']
        ctype = j['type']

        """"""
        if ctype == ""VAGUE"":
            print ""> ""
            for r in result:
                print ctype + ""\t"" + r['header'] + ""\t"" + str(r['score'])
        """"""

    f.close()

def _parseUnknown(path):
    count=0
    f = open(path)
    for line in f:
        j = json.loads(line)

        query = j['query']
        query_header = j['query_header']
        result = j['result']
        ctype = j['type']

        """"""
        if ctype == ""UNKNOWN"":
            print ""> "" + ctype + query_header
        """"""

    f.close()

def _parse(path):
    print ""parsing"", path
    _parseClassified(path);
    _parseVague(path);
    _parseUnknown(path);

def parse(path):
    if os.path.isdir(path):
        for p in path:
            _parse(p)
    else:
        _parse(path)

    print ""total successful classification ="", success

def main():
    parse(sys.argv[1])

if __name__ == ""__main__"":
    main()

```"
59d92d862d5e744a11a59b41ca8e01acd6c2b105,tests/test_command_cluster.py,tests/test_command_cluster.py,,"import os
import tempfile

from buddy.command.cluster import cli
import pytest
import vcr
import yaml
import boto3


def teardown():
    ecs_client = boto3.client('ecs')
    ecs_client.delete_service(cluster='CLUSTERNAME', service='SERVICENAME')
    ecs_client.delete_cluster(cluster='CLUSTERNAME')


def setup():
    ecs_client = boto3.client('ecs')

    containers = [
        {
            'name': 'NAME',
            'image': 'nginx',
            'memory': 10,
        }
    ]
    response = ecs_client.register_task_definition(
        family='TASKNAME',
        containerDefinitions=containers,
    )
    task_definition_arn = response['taskDefinition']['taskDefinitionArn']

    response = ecs_client.create_cluster(clusterName='CLUSTERNAME')
    ecs_cluster = response['cluster']['clusterName']

    response = ecs_client.create_service(
        cluster=ecs_cluster,
        serviceName='SERVICENAME',
        taskDefinition=task_definition_arn,
        desiredCount=0,

    )
    ecs_service = response['service']['serviceName']

    return ecs_cluster, ecs_service


def make_deploy_config_data(cluster, service):
    return {
        'targets': {
            'production': {
                'cluster': cluster,
                'service': service,
                'task': 'TASKNAME',
                'environment': 'ENVNAME',
            },
        },
        'tasks': {
            'TASKNAME': {
                'containers': ['CONTAINERNAME'],
            },
        },
        'environments': {
            'ENVNAME': {
                'VARIABLE_NAME': 'VARIABLE_VALUE',
            },
        },
        'containers': {
            'CONTAINERNAME': {
                'properties': {
                    'cpu': 10,
                    'memory': 20,
                    'command': ['prog', 'arg1', 'arg2'],
                    'logConfiguration': {
                        'logDriver': 'awslogs',
                    },
                },
                'environment': ['VARIABLE_NAME'],
            },
        },
    }


def deploy_config(ecs_service, ecs_cluster):
    data = make_deploy_config_data(cluster=ecs_cluster, service=ecs_service)
    fh, name = tempfile.mkstemp()
    fh.write(yaml.safe_dump(data))
    fh.close()
    return name


@vcr.use_cassette('tests/vcr/deploy.yaml')
def test_deploy():
    cluster, service = setup()
    config = deploy_config(cluster, service)

    try:
        args = ['deploy', config, 'production', 'image:tag', 'rev']
        # result = runner.invoke(cli, args, catch_exceptions=True)

        # print(result.output)

        # assert result.exit_code == 0

        # assert 'CREATE_COMPLETE' in result.output
    except:
        try:
            teardown()
        except:
            pass
        raise
",Add a test for cluster command using VCR,"Add a test for cluster command using VCR
",Python,mit,pior/buddy,112,"```python
import os
import tempfile

from buddy.command.cluster import cli
import pytest
import vcr
import yaml
import boto3


def teardown():
    ecs_client = boto3.client('ecs')
    ecs_client.delete_service(cluster='CLUSTERNAME', service='SERVICENAME')
    ecs_client.delete_cluster(cluster='CLUSTERNAME')


def setup():
    ecs_client = boto3.client('ecs')

    containers = [
        {
            'name': 'NAME',
            'image': 'nginx',
            'memory': 10,
        }
    ]
    response = ecs_client.register_task_definition(
        family='TASKNAME',
        containerDefinitions=containers,
    )
    task_definition_arn = response['taskDefinition']['taskDefinitionArn']

    response = ecs_client.create_cluster(clusterName='CLUSTERNAME')
    ecs_cluster = response['cluster']['clusterName']

    response = ecs_client.create_service(
        cluster=ecs_cluster,
        serviceName='SERVICENAME',
        taskDefinition=task_definition_arn,
        desiredCount=0,

    )
    ecs_service = response['service']['serviceName']

    return ecs_cluster, ecs_service


def make_deploy_config_data(cluster, service):
    return {
        'targets': {
            'production': {
                'cluster': cluster,
                'service': service,
                'task': 'TASKNAME',
                'environment': 'ENVNAME',
            },
        },
        'tasks': {
            'TASKNAME': {
                'containers': ['CONTAINERNAME'],
            },
        },
        'environments': {
            'ENVNAME': {
                'VARIABLE_NAME': 'VARIABLE_VALUE',
            },
        },
        'containers': {
            'CONTAINERNAME': {
                'properties': {
                    'cpu': 10,
                    'memory': 20,
                    'command': ['prog', 'arg1', 'arg2'],
                    'logConfiguration': {
                        'logDriver': 'awslogs',
                    },
                },
                'environment': ['VARIABLE_NAME'],
            },
        },
    }


def deploy_config(ecs_service, ecs_cluster):
    data = make_deploy_config_data(cluster=ecs_cluster, service=ecs_service)
    fh, name = tempfile.mkstemp()
    fh.write(yaml.safe_dump(data))
    fh.close()
    return name


@vcr.use_cassette('tests/vcr/deploy.yaml')
def test_deploy():
    cluster, service = setup()
    config = deploy_config(cluster, service)

    try:
        args = ['deploy', config, 'production', 'image:tag', 'rev']
        # result = runner.invoke(cli, args, catch_exceptions=True)

        # print(result.output)

        # assert result.exit_code == 0

        # assert 'CREATE_COMPLETE' in result.output
    except:
        try:
            teardown()
        except:
            pass
        raise

```"
2981f89a1ffffbd3dae78543fd1e8e257e6a8a3f,copy_ftp.py,copy_ftp.py,,"#C:\Users\smst\AppData\Local\Radio Server Player 2>
#for /d %f in (profiles\*) do \code\rsab\nowplaying\copy_ftp.py setup.ini ""%f\setup.ini""

start_time = __import__('time').time()

def selective_merge(target_ini, merge_from_ini, merge_items):
    try:
        set
    except NameError:
        from sets import Set as set
    import shutil
    import tempfile
    import time

    fm = open(merge_from_ini, 'r')
    lines_m = fm.readlines()
    fm.close()

    merge_values = {}
    merge_items = set(merge_items)

    seen_sections = set()
    seen_in_this_section = set()
    section = None
    for line in lines_m:
        line = line.strip()
        if not line:
            continue
        if line.startswith('['):
            s = line[1:].split(']')[0]
            if s not in seen_sections:
                section = s
                seen_sections.add(section)
            seen_in_this_section.clear()
            continue
        if '=' not in line:
            continue
        k, v = line.split('=', 1)
        k = k.strip()
        v = v.strip()
        if k not in seen_in_this_section:
            seen_in_this_section.add(k)
            if (section, k) in merge_items and (section, k) not in merge_values:
                merge_values[(section, k)] = v

    ft = open(target_ini, 'r')
    lines_t = ft.readlines()
    ft.close()

    out_lines = []

    section = None
    for line in lines_t:
        write_line = line
        line = line.strip()
        if line.startswith('['):
            section = line[1:].split(']')[0]
        elif '=' in line:
            k, v = line.split('=', 1)
            k = k.strip()
            v = v.strip()
            if (section, k) in merge_values:
                write_line = '%s=%s\n' % (k, merge_values[(section, k)])
        out_lines.append(write_line)

    backup_ext = time.strftime('.%Y%m%d-%H%M%S', time.localtime(start_time))
    shutil.move(target_ini, target_ini + backup_ext)
    fw = open(target_ini, 'w')
    for line in out_lines:
        fw.write(line)
    fw.close()


if __name__ == '__main__':
    import sys
    if len(sys.argv) < 3:
        print 'Usage: %s SOURCE_INI TARGET_INI' % (sys.argv[0],)
        sys.exit(0)

    merge_from_ini = sys.argv[1]
    target_ini = sys.argv[2]
    print '%s -> %s' % (merge_from_ini, target_ini)
    selective_merge(
        target_ini,
        merge_from_ini,
        [
            ('player', 'ftpxtra'),
            ('ftp', 'enable'),
            ('ftp', 'server'),
            ('ftp', 'user'),
            ('ftp', 'pass'),
            ('ftp', 'path'),
        ],
    )
",Copy the RSP2 FTP settings from one INI file into another,"Copy the RSP2 FTP settings from one INI file into another
",Python,mit,radio-st-austell-bay/helpers,95,"```python
#C:\Users\smst\AppData\Local\Radio Server Player 2>
#for /d %f in (profiles\*) do \code\rsab\nowplaying\copy_ftp.py setup.ini ""%f\setup.ini""

start_time = __import__('time').time()

def selective_merge(target_ini, merge_from_ini, merge_items):
    try:
        set
    except NameError:
        from sets import Set as set
    import shutil
    import tempfile
    import time

    fm = open(merge_from_ini, 'r')
    lines_m = fm.readlines()
    fm.close()

    merge_values = {}
    merge_items = set(merge_items)

    seen_sections = set()
    seen_in_this_section = set()
    section = None
    for line in lines_m:
        line = line.strip()
        if not line:
            continue
        if line.startswith('['):
            s = line[1:].split(']')[0]
            if s not in seen_sections:
                section = s
                seen_sections.add(section)
            seen_in_this_section.clear()
            continue
        if '=' not in line:
            continue
        k, v = line.split('=', 1)
        k = k.strip()
        v = v.strip()
        if k not in seen_in_this_section:
            seen_in_this_section.add(k)
            if (section, k) in merge_items and (section, k) not in merge_values:
                merge_values[(section, k)] = v

    ft = open(target_ini, 'r')
    lines_t = ft.readlines()
    ft.close()

    out_lines = []

    section = None
    for line in lines_t:
        write_line = line
        line = line.strip()
        if line.startswith('['):
            section = line[1:].split(']')[0]
        elif '=' in line:
            k, v = line.split('=', 1)
            k = k.strip()
            v = v.strip()
            if (section, k) in merge_values:
                write_line = '%s=%s\n' % (k, merge_values[(section, k)])
        out_lines.append(write_line)

    backup_ext = time.strftime('.%Y%m%d-%H%M%S', time.localtime(start_time))
    shutil.move(target_ini, target_ini + backup_ext)
    fw = open(target_ini, 'w')
    for line in out_lines:
        fw.write(line)
    fw.close()


if __name__ == '__main__':
    import sys
    if len(sys.argv) < 3:
        print 'Usage: %s SOURCE_INI TARGET_INI' % (sys.argv[0],)
        sys.exit(0)

    merge_from_ini = sys.argv[1]
    target_ini = sys.argv[2]
    print '%s -> %s' % (merge_from_ini, target_ini)
    selective_merge(
        target_ini,
        merge_from_ini,
        [
            ('player', 'ftpxtra'),
            ('ftp', 'enable'),
            ('ftp', 'server'),
            ('ftp', 'user'),
            ('ftp', 'pass'),
            ('ftp', 'path'),
        ],
    )

```"
37848cc0a6cffe9c12dda905715fd1e347603560,routing_table.py,routing_table.py,,"import pandas

class DynamicTable(object):
    """"""
    Dynamically sized table.
    """"""

    def __init__(self, *args, **kwargs):
        self._data = pandas.DataFrame(*args, **kwargs)

    def __getitem__(self, key):
        """"""
        Retrieve a value from the table.

        Note
        ----
        The first index in the key specifies the column.
        """"""

        if len(key) != 2:
            raise KeyError('invalid key')

        col, row = key
        return self._data.__getitem__(col).__getitem__(row)

    def __setitem__(self, key, value):
        """"""
        Set the specified entry in the table.

        Notes
        -----
        The first index in the key specifies the column.

        If the specified row or column identifiers do not exist, the
        table is expanded to include rows or columns with those
        identifiers.

        """"""

        if len(key) != 2:
            raise KeyError('invalid key')
        col, row = key
        if row not in self.data.index:
            new_row = pandas.DataFrame(index=[row],                                       
                                       columns=self._data.columns)
            self._data = pandas.concat([self._data, new_row])
        if col not in self._data.columns:
            new_col = pandas.DataFrame(index=self._data.index,              
                                       columns=[col])
            self._data = pandas.concat([self._data, new_col], axis=1)
        self._data[col][row] = value

    @property
    def table(self):
        """"""
        Return a view of the current table.
        """"""

        return self._data

    def __repr__(self):
        return self._data.__repr__()

class RoutingTable(DynamicTable):
    """"""
    Routing table.
    """"""

    def __init__(self):
        DynamicTable.__init__(self)

    def __setitem__(self, key, value):
        """"""
        Set the specified entry in the table.

        Notes
        -----
        The first index in the key specifies the column.

        If the specified row or column identifiers do not exist, the
        table is expanded to include rows or columns with those
        identifiers.

        """"""

        if len(key) != 2:
            raise KeyError('invalid key')
        col, row = key

        # Since the routing table must describe routes between all
        # recognized entities, adding a hitherto unrecognized row or
        # column identifier must cause that identifier to be added to
        # both the list of rows and columns:
        Nc = len(self._data.columns)
        Nr = len(self._data.index)
        for k in (col, row):
            if k not in self._data.index:
                new_row = pandas.DataFrame(index=[k],
                                           columns=self._data.columns)
                self._data = pandas.concat([self._data, new_row])
            if k not in self._data.columns:
                new_col = pandas.DataFrame(index=self._data.index,                      
                                           columns=[k])
                self._data = pandas.concat([self._data, new_col], axis=1)
        self._data[col][row] = value

    @property
    def ids(self):
        """"""
        Identifiers of rows and columns in the routing table.
        """"""

        return self._data.index.tolist()
",Add routing table class based on pandas.DataFrame.,"Add routing table class based on pandas.DataFrame.
",Python,bsd-3-clause,cerrno/neurokernel,114,"```python
import pandas

class DynamicTable(object):
    """"""
    Dynamically sized table.
    """"""

    def __init__(self, *args, **kwargs):
        self._data = pandas.DataFrame(*args, **kwargs)

    def __getitem__(self, key):
        """"""
        Retrieve a value from the table.

        Note
        ----
        The first index in the key specifies the column.
        """"""

        if len(key) != 2:
            raise KeyError('invalid key')

        col, row = key
        return self._data.__getitem__(col).__getitem__(row)

    def __setitem__(self, key, value):
        """"""
        Set the specified entry in the table.

        Notes
        -----
        The first index in the key specifies the column.

        If the specified row or column identifiers do not exist, the
        table is expanded to include rows or columns with those
        identifiers.

        """"""

        if len(key) != 2:
            raise KeyError('invalid key')
        col, row = key
        if row not in self.data.index:
            new_row = pandas.DataFrame(index=[row],                                       
                                       columns=self._data.columns)
            self._data = pandas.concat([self._data, new_row])
        if col not in self._data.columns:
            new_col = pandas.DataFrame(index=self._data.index,              
                                       columns=[col])
            self._data = pandas.concat([self._data, new_col], axis=1)
        self._data[col][row] = value

    @property
    def table(self):
        """"""
        Return a view of the current table.
        """"""

        return self._data

    def __repr__(self):
        return self._data.__repr__()

class RoutingTable(DynamicTable):
    """"""
    Routing table.
    """"""

    def __init__(self):
        DynamicTable.__init__(self)

    def __setitem__(self, key, value):
        """"""
        Set the specified entry in the table.

        Notes
        -----
        The first index in the key specifies the column.

        If the specified row or column identifiers do not exist, the
        table is expanded to include rows or columns with those
        identifiers.

        """"""

        if len(key) != 2:
            raise KeyError('invalid key')
        col, row = key

        # Since the routing table must describe routes between all
        # recognized entities, adding a hitherto unrecognized row or
        # column identifier must cause that identifier to be added to
        # both the list of rows and columns:
        Nc = len(self._data.columns)
        Nr = len(self._data.index)
        for k in (col, row):
            if k not in self._data.index:
                new_row = pandas.DataFrame(index=[k],
                                           columns=self._data.columns)
                self._data = pandas.concat([self._data, new_row])
            if k not in self._data.columns:
                new_col = pandas.DataFrame(index=self._data.index,                      
                                           columns=[k])
                self._data = pandas.concat([self._data, new_col], axis=1)
        self._data[col][row] = value

    @property
    def ids(self):
        """"""
        Identifiers of rows and columns in the routing table.
        """"""

        return self._data.index.tolist()

```"
b614fae93ff965abb45ee26e1a72198b4b4af8ec,saltcloud/clouds/ec2.py,saltcloud/clouds/ec2.py,,"'''
The generic libcloud template used to create the connections and deploy the
cloud virtual machines
'''

# Import python libs
import os
import tempfile
import shutil

#
# Import libcloud
from libcloud.compute.types import Provider
from libcloud.compute.providers import get_driver
from libcloud.compute.deployment import MultiStepDeployment, ScriptDeployment, SSHKeyDeployment

# Import salt libs
import saltcloud.utils
import salt.crypt


def conn(vm_):
    '''
    Return a conn object for the passed vm data
    '''
    prov = 'EC2'
    if 'location' in vm_:
        prov += '_{0}'.format(vm_['location'])
    elif 'location' in __opts__:
        if __opts__['location']:
            prov += '_{0}'.format(__opts__['location'])
    if not hasattr(Provider, prov):
        return None
    driver = get_driver('EC2')
    return driver(
            __opts__['EC2_user'],
            __opts__['EC2_key'],
            )


def ssh_pub(vm_):
    '''
    Deploy the primary ssh authentication key
    '''
    ssh = ''
    if 'ssh_auth' in vm_:
        if not os.path.isfile(vm_['ssh_auth']):
            return None
        ssh = vm_['ssh_auth']
    if not ssh:
        if not os.path.isfile(__opts__['ssh_auth']):
            return None
        ssh = __opts__['ssh_auth']

    return SSHKeyDeployment(open(os.path.expanduser(ssh)).read())


def script(vm_):
    '''
    Return the deployment object for managing a script
    '''
    os_ = ''
    if 'os' in vm_:
        os_ = vm_['os']
    if not os_:
        os_ = __opts__['os']
    return ScriptDeployment(saltcloud.utils.os_script(os_))


def image(conn, vm_):
    '''
    Return the image object to use
    '''
    images = conn.list_images()
    if not 'image' in vm_:
        return images[0]
    if isinstance(vm_['image'], int):
        return images[vm_['image']]
    for img in images:
        if img.id == vm_['image']:
            return img


def size(conn, vm_):
    '''
    Return the vm's size object
    '''
    sizes = conn.list_sizes()
    if not 'size' in vm_:
        return sizes[0]
    if isinstance(vm_['size'], int):
        return sizes[vm_['size']]
    for size in sizes:
        if size.id == vm_['size']:
            return size
        if size.name == vm_['size']:
            return size


def create(vm_):
    '''
    Create a single vm from a data dict
    '''
    connection = conn(vm_)
    msd = MultiStepDeployment([ssh_pub(vm_), script(vm_)])
    image = image(conn, vm_)
    size = size(conn, vm_)
    return conn.deploy_node(
            name=vm_['name'],
            image=image,
            size=size,
            deploy=msd)

",Add initial testing module for cloud creation,"Add initial testing module for cloud creation
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",114,"```python
'''
The generic libcloud template used to create the connections and deploy the
cloud virtual machines
'''

# Import python libs
import os
import tempfile
import shutil

#
# Import libcloud
from libcloud.compute.types import Provider
from libcloud.compute.providers import get_driver
from libcloud.compute.deployment import MultiStepDeployment, ScriptDeployment, SSHKeyDeployment

# Import salt libs
import saltcloud.utils
import salt.crypt


def conn(vm_):
    '''
    Return a conn object for the passed vm data
    '''
    prov = 'EC2'
    if 'location' in vm_:
        prov += '_{0}'.format(vm_['location'])
    elif 'location' in __opts__:
        if __opts__['location']:
            prov += '_{0}'.format(__opts__['location'])
    if not hasattr(Provider, prov):
        return None
    driver = get_driver('EC2')
    return driver(
            __opts__['EC2_user'],
            __opts__['EC2_key'],
            )


def ssh_pub(vm_):
    '''
    Deploy the primary ssh authentication key
    '''
    ssh = ''
    if 'ssh_auth' in vm_:
        if not os.path.isfile(vm_['ssh_auth']):
            return None
        ssh = vm_['ssh_auth']
    if not ssh:
        if not os.path.isfile(__opts__['ssh_auth']):
            return None
        ssh = __opts__['ssh_auth']

    return SSHKeyDeployment(open(os.path.expanduser(ssh)).read())


def script(vm_):
    '''
    Return the deployment object for managing a script
    '''
    os_ = ''
    if 'os' in vm_:
        os_ = vm_['os']
    if not os_:
        os_ = __opts__['os']
    return ScriptDeployment(saltcloud.utils.os_script(os_))


def image(conn, vm_):
    '''
    Return the image object to use
    '''
    images = conn.list_images()
    if not 'image' in vm_:
        return images[0]
    if isinstance(vm_['image'], int):
        return images[vm_['image']]
    for img in images:
        if img.id == vm_['image']:
            return img


def size(conn, vm_):
    '''
    Return the vm's size object
    '''
    sizes = conn.list_sizes()
    if not 'size' in vm_:
        return sizes[0]
    if isinstance(vm_['size'], int):
        return sizes[vm_['size']]
    for size in sizes:
        if size.id == vm_['size']:
            return size
        if size.name == vm_['size']:
            return size


def create(vm_):
    '''
    Create a single vm from a data dict
    '''
    connection = conn(vm_)
    msd = MultiStepDeployment([ssh_pub(vm_), script(vm_)])
    image = image(conn, vm_)
    size = size(conn, vm_)
    return conn.deploy_node(
            name=vm_['name'],
            image=image,
            size=size,
            deploy=msd)


```"
8d4255538203336e6f4d72845723c179c38547e0,datastore/migrations/0028_auto_20160713_0000.py,datastore/migrations/0028_auto_20160713_0000.py,,"# -*- coding: utf-8 -*-
# Generated by Django 1.9.5 on 2016-07-13 00:00
from __future__ import unicode_literals

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('datastore', '0027_auto_20160712_2251'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='dailyusagebaseline',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='dailyusagereporting',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='dailyusagesummaryactual',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='dailyusagesummarybaseline',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='dailyusagesummaryreporting',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='interpretationsummary',
            name='project_block',
        ),
        migrations.RemoveField(
            model_name='monthlyaverageusagebaseline',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='monthlyaverageusagereporting',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='monthlyusagesummaryactual',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='monthlyusagesummarybaseline',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='monthlyusagesummaryreporting',
            name='interpretation_summary',
        ),
        migrations.DeleteModel(
            name='DailyUsageBaseline',
        ),
        migrations.DeleteModel(
            name='DailyUsageReporting',
        ),
        migrations.DeleteModel(
            name='DailyUsageSummaryActual',
        ),
        migrations.DeleteModel(
            name='DailyUsageSummaryBaseline',
        ),
        migrations.DeleteModel(
            name='DailyUsageSummaryReporting',
        ),
        migrations.DeleteModel(
            name='InterpretationSummary',
        ),
        migrations.DeleteModel(
            name='MonthlyAverageUsageBaseline',
        ),
        migrations.DeleteModel(
            name='MonthlyAverageUsageReporting',
        ),
        migrations.DeleteModel(
            name='MonthlyUsageSummaryActual',
        ),
        migrations.DeleteModel(
            name='MonthlyUsageSummaryBaseline',
        ),
        migrations.DeleteModel(
            name='MonthlyUsageSummaryReporting',
        ),
    ]
",Add migration for deleted gunk,"Add migration for deleted gunk
",Python,mit,"impactlab/oeem-energy-datastore,impactlab/oeem-energy-datastore,impactlab/oeem-energy-datastore",93,"```python
# -*- coding: utf-8 -*-
# Generated by Django 1.9.5 on 2016-07-13 00:00
from __future__ import unicode_literals

from django.db import migrations


class Migration(migrations.Migration):

    dependencies = [
        ('datastore', '0027_auto_20160712_2251'),
    ]

    operations = [
        migrations.RemoveField(
            model_name='dailyusagebaseline',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='dailyusagereporting',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='dailyusagesummaryactual',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='dailyusagesummarybaseline',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='dailyusagesummaryreporting',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='interpretationsummary',
            name='project_block',
        ),
        migrations.RemoveField(
            model_name='monthlyaverageusagebaseline',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='monthlyaverageusagereporting',
            name='meter_run',
        ),
        migrations.RemoveField(
            model_name='monthlyusagesummaryactual',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='monthlyusagesummarybaseline',
            name='interpretation_summary',
        ),
        migrations.RemoveField(
            model_name='monthlyusagesummaryreporting',
            name='interpretation_summary',
        ),
        migrations.DeleteModel(
            name='DailyUsageBaseline',
        ),
        migrations.DeleteModel(
            name='DailyUsageReporting',
        ),
        migrations.DeleteModel(
            name='DailyUsageSummaryActual',
        ),
        migrations.DeleteModel(
            name='DailyUsageSummaryBaseline',
        ),
        migrations.DeleteModel(
            name='DailyUsageSummaryReporting',
        ),
        migrations.DeleteModel(
            name='InterpretationSummary',
        ),
        migrations.DeleteModel(
            name='MonthlyAverageUsageBaseline',
        ),
        migrations.DeleteModel(
            name='MonthlyAverageUsageReporting',
        ),
        migrations.DeleteModel(
            name='MonthlyUsageSummaryActual',
        ),
        migrations.DeleteModel(
            name='MonthlyUsageSummaryBaseline',
        ),
        migrations.DeleteModel(
            name='MonthlyUsageSummaryReporting',
        ),
    ]

```"
d2018ec07a79b1d0fd7c0ee3c34faa0c4c5bf723,src/python/borg/tools/plot_performance.py,src/python/borg/tools/plot_performance.py,,"""""""
@author: Bryan Silverthorn <bcs@cargo-cult.org>
""""""

if __name__ == ""__main__"":
    from borg.tools.plot_performance import main

    raise SystemExit(main())

from cargo.log import get_logger

log = get_logger(__name__, default_level = ""INFO"")

def plot_trial(session, trial_row):
    """"""
    Plot the specified trial.
    """"""

    # get the relevant attempts
    from sqlalchemy import and_
    from borg.data  import RunAttemptRow

    attempt_rows =                                        \
        session                                           \
        .query(RunAttemptRow)                             \
        .filter(
            and_(
                RunAttemptRow.trials.contains(trial_row),
                RunAttemptRow.answer != None,
                ),
            )                                             \
        .order_by(RunAttemptRow.cost)

    # break them into series
    attempts = {}
    budget   = None

    for attempt_row in attempt_rows:

        solver_name     = attempt_row.solver_name
        solver_attempts = attempts.get(solver_name, [])

        solver_attempts.append(attempt_row.cost)

        attempts[solver_name] = solver_attempts

        # determine the budget
        if budget is None:
            budget = attempt_row.budget
        else:
            if budget != attempt_row.budget:
                raise RuntimeError(""multiple budgets in trial"")

    session.commit()

    # plot the series
    import pylab

    pylab.title(""Solver Performance (Trial %s)"" % trial_row.uuid)

    for (name, costs) in attempts.iteritems():
        x_values = [0.0] + [c.as_s for c in costs] + [budget.as_s]
        y_values = range(len(costs) + 1) + [len(costs)]

        pylab.plot(x_values, y_values, label = name)

    pylab.legend()
    pylab.show()

def main():
    """"""
    Run the script.
    """"""

    # get command line arguments
    import borg.data

    from cargo.flags import parse_given

    (trial_uuid,) = parse_given(usage = ""%prog <trial_uuid> [options]"")

    # set up logging
    from cargo.log import enable_default_logging

    enable_default_logging()

    get_logger(""sqlalchemy.engine"", level = ""DETAIL"")

    # connect to the database and go
    from cargo.sql.alchemy import SQL_Engines

    with SQL_Engines.default:
        from cargo.sql.alchemy import make_session
        from borg.data         import research_connect

        ResearchSession = make_session(bind = research_connect())

        with ResearchSession() as session:
            # get the trial
            from borg.data import TrialRow

            trial_row = session.query(TrialRow).get(trial_uuid)

            if trial_row is None:
                raise ValueError(""no such trial"")

            # and plot it
            plot_trial(session, trial_row)

",Add a simple performance plotting tool.,"Add a simple performance plotting tool.
",Python,mit,borg-project/borg,110,"```python
""""""
@author: Bryan Silverthorn <bcs@cargo-cult.org>
""""""

if __name__ == ""__main__"":
    from borg.tools.plot_performance import main

    raise SystemExit(main())

from cargo.log import get_logger

log = get_logger(__name__, default_level = ""INFO"")

def plot_trial(session, trial_row):
    """"""
    Plot the specified trial.
    """"""

    # get the relevant attempts
    from sqlalchemy import and_
    from borg.data  import RunAttemptRow

    attempt_rows =                                        \
        session                                           \
        .query(RunAttemptRow)                             \
        .filter(
            and_(
                RunAttemptRow.trials.contains(trial_row),
                RunAttemptRow.answer != None,
                ),
            )                                             \
        .order_by(RunAttemptRow.cost)

    # break them into series
    attempts = {}
    budget   = None

    for attempt_row in attempt_rows:

        solver_name     = attempt_row.solver_name
        solver_attempts = attempts.get(solver_name, [])

        solver_attempts.append(attempt_row.cost)

        attempts[solver_name] = solver_attempts

        # determine the budget
        if budget is None:
            budget = attempt_row.budget
        else:
            if budget != attempt_row.budget:
                raise RuntimeError(""multiple budgets in trial"")

    session.commit()

    # plot the series
    import pylab

    pylab.title(""Solver Performance (Trial %s)"" % trial_row.uuid)

    for (name, costs) in attempts.iteritems():
        x_values = [0.0] + [c.as_s for c in costs] + [budget.as_s]
        y_values = range(len(costs) + 1) + [len(costs)]

        pylab.plot(x_values, y_values, label = name)

    pylab.legend()
    pylab.show()

def main():
    """"""
    Run the script.
    """"""

    # get command line arguments
    import borg.data

    from cargo.flags import parse_given

    (trial_uuid,) = parse_given(usage = ""%prog <trial_uuid> [options]"")

    # set up logging
    from cargo.log import enable_default_logging

    enable_default_logging()

    get_logger(""sqlalchemy.engine"", level = ""DETAIL"")

    # connect to the database and go
    from cargo.sql.alchemy import SQL_Engines

    with SQL_Engines.default:
        from cargo.sql.alchemy import make_session
        from borg.data         import research_connect

        ResearchSession = make_session(bind = research_connect())

        with ResearchSession() as session:
            # get the trial
            from borg.data import TrialRow

            trial_row = session.query(TrialRow).get(trial_uuid)

            if trial_row is None:
                raise ValueError(""no such trial"")

            # and plot it
            plot_trial(session, trial_row)


```"
32ec67c2a4fa0e225e9edef03cd59b4f1a509db8,cheroot/test/test_server.py,cheroot/test/test_server.py,,"""""""Tests for the HTTP server.""""""
# -*- coding: utf-8 -*-
# vim: set fileencoding=utf-8 :

import os
import tempfile
import threading
import time

import pytest

import cheroot.server

from cheroot.testing import (
    ANY_INTERFACE_IPV4,
    ANY_INTERFACE_IPV6,
    EPHEMERAL_PORT,
)


def make_http_server(bind_addr):
    """"""Create and start an HTTP server bound to bind_addr.""""""
    httpserver = cheroot.server.HTTPServer(
        bind_addr=bind_addr,
        gateway=cheroot.server.Gateway,
    )

    threading.Thread(target=httpserver.safe_start).start()

    while not httpserver.ready:
        time.sleep(0.1)

    return httpserver


@pytest.fixture
def http_server():
    """"""Provision a server creator as a fixture.""""""
    def start_srv():
        bind_addr = yield
        httpserver = make_http_server(bind_addr)
        yield httpserver
        yield httpserver

    srv_creator = iter(start_srv())
    next(srv_creator)
    yield srv_creator
    try:
        while True:
            httpserver = next(srv_creator)
            if httpserver is not None:
                httpserver.stop()
    except StopIteration:
        pass


@pytest.fixture
def unix_sock_file():
    """"""Check that bound UNIX socket address is stored in server.""""""
    tmp_sock_fh, tmp_sock_fname = tempfile.mkstemp()

    yield tmp_sock_fname

    os.close(tmp_sock_fh)
    os.unlink(tmp_sock_fname)


@pytest.mark.parametrize(
    'ip_addr',
    (
        ANY_INTERFACE_IPV4,
        ANY_INTERFACE_IPV6,
    )
)
def test_bind_addr_inet(http_server, ip_addr):
    """"""Check that bound IP address is stored in server.""""""
    httpserver = http_server.send((ip_addr, EPHEMERAL_PORT))

    assert httpserver.bind_addr[0] == ip_addr
    assert httpserver.bind_addr[1] != EPHEMERAL_PORT


def test_bind_addr_unix(http_server, unix_sock_file):
    """"""Check that bound UNIX socket address is stored in server.""""""
    httpserver = http_server.send(unix_sock_file)

    assert httpserver.bind_addr == unix_sock_file


@pytest.mark.skip  # FIXME: investigate binding to abstract sockets issue
def test_bind_addr_unix_abstract(http_server):
    """"""Check that bound UNIX socket address is stored in server.""""""
    unix_abstract_sock = b'\x00cheroot/test/socket/here.sock'
    httpserver = http_server.send(unix_abstract_sock)

    assert httpserver.bind_addr == unix_abstract_sock
",Add basic tests for server.HTTPServer.bind_addr,"Add basic tests for server.HTTPServer.bind_addr
",Python,bsd-3-clause,cherrypy/cheroot,97,"```python
""""""Tests for the HTTP server.""""""
# -*- coding: utf-8 -*-
# vim: set fileencoding=utf-8 :

import os
import tempfile
import threading
import time

import pytest

import cheroot.server

from cheroot.testing import (
    ANY_INTERFACE_IPV4,
    ANY_INTERFACE_IPV6,
    EPHEMERAL_PORT,
)


def make_http_server(bind_addr):
    """"""Create and start an HTTP server bound to bind_addr.""""""
    httpserver = cheroot.server.HTTPServer(
        bind_addr=bind_addr,
        gateway=cheroot.server.Gateway,
    )

    threading.Thread(target=httpserver.safe_start).start()

    while not httpserver.ready:
        time.sleep(0.1)

    return httpserver


@pytest.fixture
def http_server():
    """"""Provision a server creator as a fixture.""""""
    def start_srv():
        bind_addr = yield
        httpserver = make_http_server(bind_addr)
        yield httpserver
        yield httpserver

    srv_creator = iter(start_srv())
    next(srv_creator)
    yield srv_creator
    try:
        while True:
            httpserver = next(srv_creator)
            if httpserver is not None:
                httpserver.stop()
    except StopIteration:
        pass


@pytest.fixture
def unix_sock_file():
    """"""Check that bound UNIX socket address is stored in server.""""""
    tmp_sock_fh, tmp_sock_fname = tempfile.mkstemp()

    yield tmp_sock_fname

    os.close(tmp_sock_fh)
    os.unlink(tmp_sock_fname)


@pytest.mark.parametrize(
    'ip_addr',
    (
        ANY_INTERFACE_IPV4,
        ANY_INTERFACE_IPV6,
    )
)
def test_bind_addr_inet(http_server, ip_addr):
    """"""Check that bound IP address is stored in server.""""""
    httpserver = http_server.send((ip_addr, EPHEMERAL_PORT))

    assert httpserver.bind_addr[0] == ip_addr
    assert httpserver.bind_addr[1] != EPHEMERAL_PORT


def test_bind_addr_unix(http_server, unix_sock_file):
    """"""Check that bound UNIX socket address is stored in server.""""""
    httpserver = http_server.send(unix_sock_file)

    assert httpserver.bind_addr == unix_sock_file


@pytest.mark.skip  # FIXME: investigate binding to abstract sockets issue
def test_bind_addr_unix_abstract(http_server):
    """"""Check that bound UNIX socket address is stored in server.""""""
    unix_abstract_sock = b'\x00cheroot/test/socket/here.sock'
    httpserver = http_server.send(unix_abstract_sock)

    assert httpserver.bind_addr == unix_abstract_sock

```"
5dce9c5438b4fcb63c2b7a4a4f481cba331836ed,wger/exercises/management/commands/exercises-health-check.py,wger/exercises/management/commands/exercises-health-check.py,,"# -*- coding: utf-8 *-*

# This file is part of wger Workout Manager.
#
# wger Workout Manager is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# wger Workout Manager is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License

# Standard Library
import collections

# Django
from django.core.management.base import BaseCommand

# wger
from wger.core.models import Language
from wger.exercises.models import ExerciseBase
from wger.utils.constants import ENGLISH_SHORT_NAME


class Command(BaseCommand):
    """"""
    Performs some sanity checks on the exercise database
    """"""

    help = """"""Performs some sanity checks on the database

            At the moment this script checks the following:
            - each base has at least one exercise
            - each exercise base has a translation in English
            - exercise bases have no duplicate translations
            """"""

    def add_arguments(self, parser):

        # Add dry run argument
        parser.add_argument(
            '--delete',
            action='store_true',
            dest='delete',
            default=False,
            help='Delete problematic exercise bases from the database (use with care!!)',
        )

    def handle(self, **options):

        delete = options['delete']
        english = Language.objects.get(short_name=ENGLISH_SHORT_NAME)

        for base in ExerciseBase.objects.all():

            if not base.exercises.count():
                warning = f'Exercise base {base.uuid} has no translations!'
                self.stdout.write(self.style.WARNING(warning))

                if delete:
                    base.delete()
                    self.stdout.write('  Deleting base...')
                continue

            if not base.exercises.filter(language=english).exists():
                warning = f'Exercise base {base.uuid} has no English translation!'
                self.stdout.write(self.style.WARNING(warning))

                if delete:
                    base.delete()
                    self.stdout.write('  Deleting base...')

            exercise_languages = base.exercises.values_list('language_id', flat=True)
            duplicates = [
                item for item, count in collections.Counter(exercise_languages).items() if count > 1
            ]

            if not duplicates:
                continue

            warning = f'Exercise base {base.uuid} has duplicate translations for language IDs: {duplicates}!'
            self.stdout.write(self.style.WARNING(warning))

            if delete:
                exercises = base.exercises.filter(language_id__in=duplicates)
                for exercise in exercises[1:]:
                    exercise.delete()
                    self.stdout.write(
                        f'  Deleting translation {exercise.uuid} for language ID {exercise.language_id}...'
                    )
",Add script to check the current exercise translations,"Add script to check the current exercise translations
",Python,agpl-3.0,"wger-project/wger,wger-project/wger,wger-project/wger,wger-project/wger",95,"```python
# -*- coding: utf-8 *-*

# This file is part of wger Workout Manager.
#
# wger Workout Manager is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# wger Workout Manager is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License

# Standard Library
import collections

# Django
from django.core.management.base import BaseCommand

# wger
from wger.core.models import Language
from wger.exercises.models import ExerciseBase
from wger.utils.constants import ENGLISH_SHORT_NAME


class Command(BaseCommand):
    """"""
    Performs some sanity checks on the exercise database
    """"""

    help = """"""Performs some sanity checks on the database

            At the moment this script checks the following:
            - each base has at least one exercise
            - each exercise base has a translation in English
            - exercise bases have no duplicate translations
            """"""

    def add_arguments(self, parser):

        # Add dry run argument
        parser.add_argument(
            '--delete',
            action='store_true',
            dest='delete',
            default=False,
            help='Delete problematic exercise bases from the database (use with care!!)',
        )

    def handle(self, **options):

        delete = options['delete']
        english = Language.objects.get(short_name=ENGLISH_SHORT_NAME)

        for base in ExerciseBase.objects.all():

            if not base.exercises.count():
                warning = f'Exercise base {base.uuid} has no translations!'
                self.stdout.write(self.style.WARNING(warning))

                if delete:
                    base.delete()
                    self.stdout.write('  Deleting base...')
                continue

            if not base.exercises.filter(language=english).exists():
                warning = f'Exercise base {base.uuid} has no English translation!'
                self.stdout.write(self.style.WARNING(warning))

                if delete:
                    base.delete()
                    self.stdout.write('  Deleting base...')

            exercise_languages = base.exercises.values_list('language_id', flat=True)
            duplicates = [
                item for item, count in collections.Counter(exercise_languages).items() if count > 1
            ]

            if not duplicates:
                continue

            warning = f'Exercise base {base.uuid} has duplicate translations for language IDs: {duplicates}!'
            self.stdout.write(self.style.WARNING(warning))

            if delete:
                exercises = base.exercises.filter(language_id__in=duplicates)
                for exercise in exercises[1:]:
                    exercise.delete()
                    self.stdout.write(
                        f'  Deleting translation {exercise.uuid} for language ID {exercise.language_id}...'
                    )

```"
bb70a98437c87fa5b9677716acbcbd948d93f982,tests/syft/grid/messages/setup_msg_test.py,tests/syft/grid/messages/setup_msg_test.py,,"# syft absolute
import syft as sy
from syft.core.io.address import Address
from syft.grid.messages.setup_messages import CreateInitialSetUpMessage
from syft.grid.messages.setup_messages import CreateInitialSetUpResponse
from syft.grid.messages.setup_messages import GetSetUpMessage
from syft.grid.messages.setup_messages import GetSetUpResponse


def test_create_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = CreateInitialSetUpMessage(
        address=target,
        content= request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_create_initial_setup_response_serde() -> None:
    target = Address(name=""Alice"")

    request_content = {""msg"": ""Initial setup registered successfully!""}
    msg = CreateInitialSetUpResponse(
        address=target,
        success=True,
        content= request_content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_get_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = {}
    msg = GetSetUpMessage(
        address=target,
        content=request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_delete_worker_response_serde() -> None:
    target = Address(name=""Alice"")

    content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = GetSetUpResponse(
        success=True,
        address=target,
        content=content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2",ADD PyGrid SetupService message tests,"ADD PyGrid SetupService message tests
",Python,apache-2.0,"OpenMined/PySyft,OpenMined/PySyft,OpenMined/PySyft,OpenMined/PySyft",106,"```python
# syft absolute
import syft as sy
from syft.core.io.address import Address
from syft.grid.messages.setup_messages import CreateInitialSetUpMessage
from syft.grid.messages.setup_messages import CreateInitialSetUpResponse
from syft.grid.messages.setup_messages import GetSetUpMessage
from syft.grid.messages.setup_messages import GetSetUpResponse


def test_create_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = CreateInitialSetUpMessage(
        address=target,
        content= request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_create_initial_setup_response_serde() -> None:
    target = Address(name=""Alice"")

    request_content = {""msg"": ""Initial setup registered successfully!""}
    msg = CreateInitialSetUpResponse(
        address=target,
        success=True,
        content= request_content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_get_initial_setup_message_serde() -> None:
    bob_vm = sy.VirtualMachine(name=""Bob"")
    target = Address(name=""Alice"")

    request_content = {}
    msg = GetSetUpMessage(
        address=target,
        content=request_content,
        reply_to=bob_vm.address,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2

def test_delete_worker_response_serde() -> None:
    target = Address(name=""Alice"")

    content = { 
            ""settings"": { 
                ""cloud-admin-token"" : ""d84we35ad3a1d59a84sd9"",
                ""cloud-credentials"": ""<cloud-credentials.pem>"",
                ""infra"": {
                    ""autoscaling"": True,
                    ""triggers"": {
                        ""memory"": ""50"",
                        ""vCPU"": ""80""
                    }
                },
            }
    }
    msg = GetSetUpResponse(
        success=True,
        address=target,
        content=content,
    )

    blob = msg.serialize()
    msg2 = sy.deserialize(blob=blob)

    assert msg.id == msg2.id
    assert msg.address == target
    assert msg.content == msg2.content
    assert msg == msg2
```"
60edb041e6096f37cc451acb77a44f421c37d910,tests/cupy_tests/core_tests/test_core.py,tests/cupy_tests/core_tests/test_core.py,,"import unittest

import cupy
from cupy.core import core


class TestGetSize(unittest.TestCase):

    def test_none(self):
        self.assertEqual(core.get_size(None), ())

    def test_list(self):
        self.assertEqual(core.get_size([1, 2]), (1, 2))

    def test_tuple(self):
        self.assertEqual(core.get_size((1, 2)), (1, 2))

    def test_int(self):
        self.assertEqual(core.get_size(1), (1,))

    def test_invalid(self):
        with self.assertRaises(ValueError):
            core.get_size(1.0)


class TestInternalProd(unittest.TestCase):

    def test_empty(self):
        self.assertEqual(core.internal_prod([]), 1)

    def test_one(self):
        self.assertEqual(core.internal_prod([2]), 2)

    def test_two(self):
        self.assertEqual(core.internal_prod([2, 3]), 6)


class TestGetStridesForNocopyReshape(unittest.TestCase):

    def test_different_size(self):
        a = core.ndarray((2, 3))
        self.assertEqual(core._get_strides_for_nocopy_reshape(a, (1, 5)),
                         [])

    def test_one(self):
        a = core.ndarray((1,), dtype=cupy.int32)
        self.assertEqual(core._get_strides_for_nocopy_reshape(a, (1, 1, 1)),
                         [4, 4, 4])

    def test_normal(self):
        # TODO(nno): write test for normal case
        pass


class TestGetContiguousStrides(unittest.TestCase):

    def test_zero(self):
        self.assertEqual(core._get_contiguous_strides((), 1), [])

    def test_one(self):
        self.assertEqual(core._get_contiguous_strides((1,), 2), [2])

    def test_two(self):
        self.assertEqual(core._get_contiguous_strides((1, 2), 3), [6, 3])

    def test_three(self):
        self.assertEqual(core._get_contiguous_strides((1, 2, 3), 4),
                         [24, 12, 4])


class TestGetCContiguity(unittest.TestCase):

    def test_zero_in_shape(self):
        self.assertTrue(core._get_c_contiguity((1, 0, 1), (1, 1, 1), 3))

    def test_normal(self):
        # TODO(unno): write test for normal case
        pass


class TestInferUnknownDimension(unittest.TestCase):

    def test_known_all(self):
        self.assertEqual(core._infer_unknown_dimension((1, 2, 3), 6),
                         [1, 2, 3])

    def test_multiple_unknown(self):
        with self.assertRaises(ValueError):
            core._infer_unknown_dimension((-1, 1, -1), 10)

    def test_infer(self):
        self.assertEqual(core._infer_unknown_dimension((-1, 2, 3), 12),
                         [2, 2, 3])
",Write test for core module of cupy,"Write test for core module of cupy
",Python,mit,"delta2323/chainer,AlpacaDB/chainer,benob/chainer,okuta/chainer,niboshi/chainer,benob/chainer,muupan/chainer,hvy/chainer,t-abe/chainer,chainer/chainer,kiyukuta/chainer,cupy/cupy,muupan/chainer,AlpacaDB/chainer,ktnyt/chainer,anaruse/chainer,jnishi/chainer,ysekky/chainer,wkentaro/chainer,t-abe/chainer,chainer/chainer,jnishi/chainer,hvy/chainer,ktnyt/chainer,ronekko/chainer,keisuke-umezawa/chainer,sinhrks/chainer,truongdq/chainer,jnishi/chainer,tkerola/chainer,niboshi/chainer,kikusu/chainer,wkentaro/chainer,cupy/cupy,cemoody/chainer,keisuke-umezawa/chainer,hvy/chainer,pfnet/chainer,chainer/chainer,keisuke-umezawa/chainer,okuta/chainer,kikusu/chainer,niboshi/chainer,ktnyt/chainer,okuta/chainer,aonotas/chainer,keisuke-umezawa/chainer,okuta/chainer,ktnyt/chainer,kashif/chainer,sinhrks/chainer,jnishi/chainer,rezoo/chainer,chainer/chainer,cupy/cupy,truongdq/chainer,cupy/cupy,wkentaro/chainer,niboshi/chainer,wkentaro/chainer,hvy/chainer",94,"```python
import unittest

import cupy
from cupy.core import core


class TestGetSize(unittest.TestCase):

    def test_none(self):
        self.assertEqual(core.get_size(None), ())

    def test_list(self):
        self.assertEqual(core.get_size([1, 2]), (1, 2))

    def test_tuple(self):
        self.assertEqual(core.get_size((1, 2)), (1, 2))

    def test_int(self):
        self.assertEqual(core.get_size(1), (1,))

    def test_invalid(self):
        with self.assertRaises(ValueError):
            core.get_size(1.0)


class TestInternalProd(unittest.TestCase):

    def test_empty(self):
        self.assertEqual(core.internal_prod([]), 1)

    def test_one(self):
        self.assertEqual(core.internal_prod([2]), 2)

    def test_two(self):
        self.assertEqual(core.internal_prod([2, 3]), 6)


class TestGetStridesForNocopyReshape(unittest.TestCase):

    def test_different_size(self):
        a = core.ndarray((2, 3))
        self.assertEqual(core._get_strides_for_nocopy_reshape(a, (1, 5)),
                         [])

    def test_one(self):
        a = core.ndarray((1,), dtype=cupy.int32)
        self.assertEqual(core._get_strides_for_nocopy_reshape(a, (1, 1, 1)),
                         [4, 4, 4])

    def test_normal(self):
        # TODO(nno): write test for normal case
        pass


class TestGetContiguousStrides(unittest.TestCase):

    def test_zero(self):
        self.assertEqual(core._get_contiguous_strides((), 1), [])

    def test_one(self):
        self.assertEqual(core._get_contiguous_strides((1,), 2), [2])

    def test_two(self):
        self.assertEqual(core._get_contiguous_strides((1, 2), 3), [6, 3])

    def test_three(self):
        self.assertEqual(core._get_contiguous_strides((1, 2, 3), 4),
                         [24, 12, 4])


class TestGetCContiguity(unittest.TestCase):

    def test_zero_in_shape(self):
        self.assertTrue(core._get_c_contiguity((1, 0, 1), (1, 1, 1), 3))

    def test_normal(self):
        # TODO(unno): write test for normal case
        pass


class TestInferUnknownDimension(unittest.TestCase):

    def test_known_all(self):
        self.assertEqual(core._infer_unknown_dimension((1, 2, 3), 6),
                         [1, 2, 3])

    def test_multiple_unknown(self):
        with self.assertRaises(ValueError):
            core._infer_unknown_dimension((-1, 1, -1), 10)

    def test_infer(self):
        self.assertEqual(core._infer_unknown_dimension((-1, 2, 3), 12),
                         [2, 2, 3])

```"
c426e8845632d13f27b1cbc71d2c13292cc88711,buildbucket.py,buildbucket.py,,"#!/usr/bin/env python
# Copyright (c) 2015 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Tool for interacting with Buildbucket.

Usage:
  $ depot-tools-auth login https://cr-buildbucket.appspot.com
  $ buildbucket.py \
    put \
    --bucket master.tryserver.chromium.linux \
    --builder my-builder \

  Puts a build into buildbucket for my-builder on tryserver.chromium.linux.
""""""

import argparse
import json
import urlparse
import os
import sys

from third_party import httplib2

import auth


BUILDBUCKET_URL = 'https://cr-buildbucket.appspot.com'
PUT_BUILD_URL = urlparse.urljoin(
  BUILDBUCKET_URL,
  '_ah/api/buildbucket/v1/builds',
)


def main(argv):
  parser = argparse.ArgumentParser()
  parser.add_argument(
    '-v',
    '--verbose',
    action='store_true',
  )
  subparsers = parser.add_subparsers(dest='command')
  put_parser = subparsers.add_parser('put')
  put_parser.add_argument(
    '--bucket',
    help=(
      'The bucket to schedule the build on. Typically the master name, e.g.'
      ' master.tryserver.chromium.linux.'
    ),
    required=True,
  )
  put_parser.add_argument(
    '-n',
    '--builder-name',
    help='The builder to schedule the build on.',
    required=True,
  )
  put_parser.add_argument(
    '-p',
    '--properties',
    help='A file to load a JSON dict of properties from.',
  )
  args = parser.parse_args()
  # TODO(smut): When more commands are implemented, refactor this.
  assert args.command == 'put'

  properties = {}
  if args.properties:
    try:
      with open(args.properties) as fp:
        properties.update(json.load(fp))
    except (TypeError, ValueError):
      sys.stderr.write('%s contained invalid JSON dict.\n' % args.properties)
      raise

  authenticator = auth.get_authenticator_for_host(
    BUILDBUCKET_URL,
    auth.make_auth_config(use_oauth2=True),
  )
  http = authenticator.authorize(httplib2.Http())
  http.force_exception_to_status_code = True
  response, content = http.request(
    PUT_BUILD_URL,
    'PUT',
    body=json.dumps({
      'bucket': args.bucket,
      'parameters_json': json.dumps({
        'builder_name': args.builder_name,
        'properties': properties,
      }),
    }),
    headers={'Content-Type': 'application/json'},
  )

  if args.verbose:
    print content

  return response.status != 200


if __name__ == '__main__':
  sys.exit(main(sys.argv))
",Add script for triggering Buildbucket builds,"Add script for triggering Buildbucket builds

BUG=493885
TESTED=See https://paste.googleplex.com/5622248052359168

Review URL: https://codereview.chromium.org/1164363003

git-svn-id: bd64dd6fa6f3f0ed0c0666d1018379882b742947@295569 4ff67af0-8c30-449e-8e8b-ad334ec8d88c
",Python,bsd-3-clause,"svn2github/chromium-depot-tools,svn2github/chromium-depot-tools,svn2github/chromium-depot-tools",104,"```python
#!/usr/bin/env python
# Copyright (c) 2015 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Tool for interacting with Buildbucket.

Usage:
  $ depot-tools-auth login https://cr-buildbucket.appspot.com
  $ buildbucket.py \
    put \
    --bucket master.tryserver.chromium.linux \
    --builder my-builder \

  Puts a build into buildbucket for my-builder on tryserver.chromium.linux.
""""""

import argparse
import json
import urlparse
import os
import sys

from third_party import httplib2

import auth


BUILDBUCKET_URL = 'https://cr-buildbucket.appspot.com'
PUT_BUILD_URL = urlparse.urljoin(
  BUILDBUCKET_URL,
  '_ah/api/buildbucket/v1/builds',
)


def main(argv):
  parser = argparse.ArgumentParser()
  parser.add_argument(
    '-v',
    '--verbose',
    action='store_true',
  )
  subparsers = parser.add_subparsers(dest='command')
  put_parser = subparsers.add_parser('put')
  put_parser.add_argument(
    '--bucket',
    help=(
      'The bucket to schedule the build on. Typically the master name, e.g.'
      ' master.tryserver.chromium.linux.'
    ),
    required=True,
  )
  put_parser.add_argument(
    '-n',
    '--builder-name',
    help='The builder to schedule the build on.',
    required=True,
  )
  put_parser.add_argument(
    '-p',
    '--properties',
    help='A file to load a JSON dict of properties from.',
  )
  args = parser.parse_args()
  # TODO(smut): When more commands are implemented, refactor this.
  assert args.command == 'put'

  properties = {}
  if args.properties:
    try:
      with open(args.properties) as fp:
        properties.update(json.load(fp))
    except (TypeError, ValueError):
      sys.stderr.write('%s contained invalid JSON dict.\n' % args.properties)
      raise

  authenticator = auth.get_authenticator_for_host(
    BUILDBUCKET_URL,
    auth.make_auth_config(use_oauth2=True),
  )
  http = authenticator.authorize(httplib2.Http())
  http.force_exception_to_status_code = True
  response, content = http.request(
    PUT_BUILD_URL,
    'PUT',
    body=json.dumps({
      'bucket': args.bucket,
      'parameters_json': json.dumps({
        'builder_name': args.builder_name,
        'properties': properties,
      }),
    }),
    headers={'Content-Type': 'application/json'},
  )

  if args.verbose:
    print content

  return response.status != 200


if __name__ == '__main__':
  sys.exit(main(sys.argv))

```"
7dd898309e81feb686220757a740b6b7b934462d,pipelines/genome_merge.py,pipelines/genome_merge.py,,"import os
import luigi
import json
import subprocess

from support import merge_fasta as mf

# add parent directory to path
if __name__ == '__main__' and __package__ is None:
    os.sys.path.append(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ----------------------------------TASKS--------------------------------------


class MergeGenome(luigi.Task):
    """"""
    Task to merge genome fasta files
    """"""
    updir = luigi.parameter()

    def run(self):
        """"""
        Merge all fasta files in updir
        """"""
        mf.merge_genome_files(self.updir)

    def output(self):
        """"""
        Check genome fasta file has been generated
        """"""
        upid = os.path.split(self.updir)[1]
        genome_fasta = os.path.join(self.updir, upid+'.fa')

        return luigi.LocalTarget(genome_fasta)

# -----------------------------------------------------------------------------


class GenomeMergeEngine(luigi.Task):
    """"""
    Task to load accessions from file
    """"""
    project_dir = luigi.Parameter()
    lsf = luigi.BoolParameter(default=True,
                              description=""If used then run on lsf, otherwise run locally"")

    def run(self):
        """"""
        Call load_upid_gca_file to export all upid_gca accession pairs in
        json format.
        """"""
        id_pairs = None
        upid_gca_file_loc = os.path.join(self.project_dir, ""upid_gca_dict.json"")
        upid_gca_fp = open(upid_gca_file_loc, 'r')
        accessions = json.load(upid_gca_fp)
        upid_gca_fp.close()

        upids = accessions.keys()

        cmd = ''
        for upid in upids:

            subdir = os.path.join(self.project_dir, upid[-3:])
            updir = os.path.join(subdir, updir)

            if os.path.exists(updir):

                if self.lsf is True:
                    # TODO launch an lsf job to merge the genome fasta files
                    pass

                else:
                    cmd = ""python \""{this_file}\"" MergeGenome --updir {upid}"".format(
                        this_file=os.path.realpath(__file__),
                        updir=updir)

                    subprocess.call(cmd, shell=True)

            cmd = ''

# -----------------------------------------------------------------------------


if __name__ == '__main__':

    luigi.run()




",Add first version of genome merge pipeline,"Add first version of genome merge pipeline
",Python,apache-2.0,"Rfam/rfam-production,Rfam/rfam-production,Rfam/rfam-production",92,"```python
import os
import luigi
import json
import subprocess

from support import merge_fasta as mf

# add parent directory to path
if __name__ == '__main__' and __package__ is None:
    os.sys.path.append(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ----------------------------------TASKS--------------------------------------


class MergeGenome(luigi.Task):
    """"""
    Task to merge genome fasta files
    """"""
    updir = luigi.parameter()

    def run(self):
        """"""
        Merge all fasta files in updir
        """"""
        mf.merge_genome_files(self.updir)

    def output(self):
        """"""
        Check genome fasta file has been generated
        """"""
        upid = os.path.split(self.updir)[1]
        genome_fasta = os.path.join(self.updir, upid+'.fa')

        return luigi.LocalTarget(genome_fasta)

# -----------------------------------------------------------------------------


class GenomeMergeEngine(luigi.Task):
    """"""
    Task to load accessions from file
    """"""
    project_dir = luigi.Parameter()
    lsf = luigi.BoolParameter(default=True,
                              description=""If used then run on lsf, otherwise run locally"")

    def run(self):
        """"""
        Call load_upid_gca_file to export all upid_gca accession pairs in
        json format.
        """"""
        id_pairs = None
        upid_gca_file_loc = os.path.join(self.project_dir, ""upid_gca_dict.json"")
        upid_gca_fp = open(upid_gca_file_loc, 'r')
        accessions = json.load(upid_gca_fp)
        upid_gca_fp.close()

        upids = accessions.keys()

        cmd = ''
        for upid in upids:

            subdir = os.path.join(self.project_dir, upid[-3:])
            updir = os.path.join(subdir, updir)

            if os.path.exists(updir):

                if self.lsf is True:
                    # TODO launch an lsf job to merge the genome fasta files
                    pass

                else:
                    cmd = ""python \""{this_file}\"" MergeGenome --updir {upid}"".format(
                        this_file=os.path.realpath(__file__),
                        updir=updir)

                    subprocess.call(cmd, shell=True)

            cmd = ''

# -----------------------------------------------------------------------------


if __name__ == '__main__':

    luigi.run()





```"
29467c4a52bb5caf492460b70cb58caf5fe8f728,calaccess_processed/management/commands/loadocdmodels.py,calaccess_processed/management/commands/loadocdmodels.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""
Load data extracted from scrape and raw data snapshot into OCD models.
""""""
from django.apps import apps
from django.utils.timezone import now
from django.core.management import call_command
from calaccess_processed.models import ProcessedDataVersion
from calaccess_processed.management.commands import CalAccessCommand


class Command(CalAccessCommand):
    """"""
    Load data extracted from scrape and raw data snapshot into OCD models.
    """"""
    help = 'Load data extracted from scrape and raw data snapshot into OCD models'

    def handle(self, *args, **options):
        """"""
        Make it happen.
        """"""
        super(Command, self).handle(*args, **options)

        self.processed_version = ProcessedDataVersion.objects.latest()

        self.start_datetime = now()
        self.load()
        self.finish_datetime = now()
        self.archive()

        self.success('Done!')
        self.duration()

    def load(self):
        """"""
        Load all of the processed models.
        """"""
        call_command(
            'loadparties',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadballotmeasurecontests',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadretentioncontests',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadcandidatecontests',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'mergecandidates',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadcandidaciesfrom501s',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadincumbentofficeholders',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

    def archive(self):
        """"""
        Save a csv file for each loaded OCD model.
        """"""
        ocd_models = [
            m._meta.object_name for m in apps.get_app_config(
                'opencivicdata'
            ).get_models()
            if not m._meta.abstract and
            m.objects.count() > 0
        ]

        for m in ocd_models:
            processed_data_file, created = self.processed_version.files.get_or_create(
                file_name=m,
            )
            processed_data_file.process_start_time = self.start_datetime
            processed_data_file.save()
            call_command(
                'archivecalaccessprocessedfile',
                'opencivicdata',
                m,
            )
            processed_data_file.process_finish_time = self.finish_datetime
            processed_data_file.save()

        self.duration()
","Create ProcessedDataModels for OCD models, group loading into one cmd","Create ProcessedDataModels for OCD models, group loading into one cmd
",Python,mit,"california-civic-data-coalition/django-calaccess-processed-data,california-civic-data-coalition/django-calaccess-processed-data",115,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""
Load data extracted from scrape and raw data snapshot into OCD models.
""""""
from django.apps import apps
from django.utils.timezone import now
from django.core.management import call_command
from calaccess_processed.models import ProcessedDataVersion
from calaccess_processed.management.commands import CalAccessCommand


class Command(CalAccessCommand):
    """"""
    Load data extracted from scrape and raw data snapshot into OCD models.
    """"""
    help = 'Load data extracted from scrape and raw data snapshot into OCD models'

    def handle(self, *args, **options):
        """"""
        Make it happen.
        """"""
        super(Command, self).handle(*args, **options)

        self.processed_version = ProcessedDataVersion.objects.latest()

        self.start_datetime = now()
        self.load()
        self.finish_datetime = now()
        self.archive()

        self.success('Done!')
        self.duration()

    def load(self):
        """"""
        Load all of the processed models.
        """"""
        call_command(
            'loadparties',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadballotmeasurecontests',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadretentioncontests',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadcandidatecontests',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'mergecandidates',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadcandidaciesfrom501s',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

        call_command(
            'loadincumbentofficeholders',
            verbosity=self.verbosity,
            no_color=self.no_color,
        )
        self.duration()

    def archive(self):
        """"""
        Save a csv file for each loaded OCD model.
        """"""
        ocd_models = [
            m._meta.object_name for m in apps.get_app_config(
                'opencivicdata'
            ).get_models()
            if not m._meta.abstract and
            m.objects.count() > 0
        ]

        for m in ocd_models:
            processed_data_file, created = self.processed_version.files.get_or_create(
                file_name=m,
            )
            processed_data_file.process_start_time = self.start_datetime
            processed_data_file.save()
            call_command(
                'archivecalaccessprocessedfile',
                'opencivicdata',
                m,
            )
            processed_data_file.process_finish_time = self.finish_datetime
            processed_data_file.save()

        self.duration()

```"
1a8e623e13323187dc6b40a7358d6293d4f39706,github2jenkins.py,github2jenkins.py,,"#!/usr/bin/env python

""""""
Create Jenkins job corresponding to each Github repository.
""""""

import os
import getpass
import github3
import jenkinsapi

# Github user(s) which repositories are to be created in Jenkins
GITHUB_USERS = [""taverna""]

# Branches which existance means a corresponding Jenkins job is
# created. The job will be called $repository-$branch, except for 
# the master branch, which is simply $repository
BRANCHES = [""master"", ""maintenance""]

# Jenkins instance where jobs will be created, e.g.
# http://localhost:8080/jenkins/
JENKINS = ""http://build.mygrid.org.uk/ci/""

# Pre-existing Jenkins job which config is
# to be used as a template for any new jobs
# 
# Note: The template must be both a valid 
# Jenkins name and a Github repository
# as naive search-replace is used on the
# Jenkins Job Config XML
# The string ""master"" will be search-replaced for
# other branches
JENKINS_JOB_TEMPLATE = ""taverna-wsdl-activity""
# The pre-configured user/repo substring of the github URLs in the
# Jenkins job-template - this will be search-replaced to
# $user/$repo
JENKINS_JOB_TEMPLATE_REPO = ""taverna/taverna-wsdl-activity""

# Jenkins user with write-access in Jenkins
# The library will prompt on the console at runtime for
# the jenkins password.
#
# Set the user to None for readonly mode, in  which case
# new Jenkins jobs will not be created, but their name
# printed on the console.
# 
JENKINS_USER = os.environ.get(""JENKINS_USER"") or getpass.getuser()


class Github2JenkinsException(Exception):
    pass 


gh = github3.GitHub()


_jenkins = None

def jenkins():
    global _jenkins
    if _jenkins is not None:
        return _jenkins

    password = os.environ.get(""JENKINS_PASSWORD"")
    if JENKINS_USER and not password:
        # Need to ask for password    
        print ""Jenkins:"", JENKINS
        password = getpass.getpass(""Password for user "" + JENKINS_USER +
                                   "" [empty for read-only]: "")

    if not password: 
        _jenkins = jenkinsapi.jenkins.Jenkins(JENKINS)
    else:
        _jenkins = jenkinsapi.jenkins.Jenkins(JENKINS, JENKINS_USER, password)
    return _jenkins    

def repos(username, must_have_branch):
   for repo in gh.iter_user_repos(username):
       if repo.branch(must_have_branch):
            yield repo


_jenkins_template = None
def create_template(job_name, repository):
    global _jenkins_template
    if _jenkins_template is not None:
        return _jenkins_template
    _jenkins_template = jenkins()[JENKINS_JOB_TEMPLATE]    
    if not _jenkins_template:
        raise Github2JenkinsException(""Can't find template "" + JENKINS_JOB_TEMPLATE)
    return _jenkins_template


",Create Jenkins jobs per Github repository,"Create Jenkins jobs per Github repository
",Python,mit,stain/github2jenkins,94,"```python
#!/usr/bin/env python

""""""
Create Jenkins job corresponding to each Github repository.
""""""

import os
import getpass
import github3
import jenkinsapi

# Github user(s) which repositories are to be created in Jenkins
GITHUB_USERS = [""taverna""]

# Branches which existance means a corresponding Jenkins job is
# created. The job will be called $repository-$branch, except for 
# the master branch, which is simply $repository
BRANCHES = [""master"", ""maintenance""]

# Jenkins instance where jobs will be created, e.g.
# http://localhost:8080/jenkins/
JENKINS = ""http://build.mygrid.org.uk/ci/""

# Pre-existing Jenkins job which config is
# to be used as a template for any new jobs
# 
# Note: The template must be both a valid 
# Jenkins name and a Github repository
# as naive search-replace is used on the
# Jenkins Job Config XML
# The string ""master"" will be search-replaced for
# other branches
JENKINS_JOB_TEMPLATE = ""taverna-wsdl-activity""
# The pre-configured user/repo substring of the github URLs in the
# Jenkins job-template - this will be search-replaced to
# $user/$repo
JENKINS_JOB_TEMPLATE_REPO = ""taverna/taverna-wsdl-activity""

# Jenkins user with write-access in Jenkins
# The library will prompt on the console at runtime for
# the jenkins password.
#
# Set the user to None for readonly mode, in  which case
# new Jenkins jobs will not be created, but their name
# printed on the console.
# 
JENKINS_USER = os.environ.get(""JENKINS_USER"") or getpass.getuser()


class Github2JenkinsException(Exception):
    pass 


gh = github3.GitHub()


_jenkins = None

def jenkins():
    global _jenkins
    if _jenkins is not None:
        return _jenkins

    password = os.environ.get(""JENKINS_PASSWORD"")
    if JENKINS_USER and not password:
        # Need to ask for password    
        print ""Jenkins:"", JENKINS
        password = getpass.getpass(""Password for user "" + JENKINS_USER +
                                   "" [empty for read-only]: "")

    if not password: 
        _jenkins = jenkinsapi.jenkins.Jenkins(JENKINS)
    else:
        _jenkins = jenkinsapi.jenkins.Jenkins(JENKINS, JENKINS_USER, password)
    return _jenkins    

def repos(username, must_have_branch):
   for repo in gh.iter_user_repos(username):
       if repo.branch(must_have_branch):
            yield repo


_jenkins_template = None
def create_template(job_name, repository):
    global _jenkins_template
    if _jenkins_template is not None:
        return _jenkins_template
    _jenkins_template = jenkins()[JENKINS_JOB_TEMPLATE]    
    if not _jenkins_template:
        raise Github2JenkinsException(""Can't find template "" + JENKINS_JOB_TEMPLATE)
    return _jenkins_template



```"
6f28fc31a9734cc36f3e41759ce20852beb890f8,sara_flexbe_states/src/sara_flexbe_states/WonderlandPatchPerson.py,sara_flexbe_states/src/sara_flexbe_states/WonderlandPatchPerson.py,,"#!/usr/bin/env python
# encoding=utf8

import requests
from flexbe_core import EventState, Logger

""""""
Created on 17/05/2018

@author: Lucas Maurice
""""""


class WonderlandPatchPerson(EventState):
    '''
    Patch (update) a person.
    >#  entity                  sara_msgs/Entity

    <= done                     return when the add correctly append
    <= dont_exist                return when the entity already exist
    <= bad_request              return when error reading data
    <= error                    return when error reading data
    '''

    def __init__(self):
        # See example_state.py for basic explanations.
        super(WonderlandPatchPerson, self).__init__(input_keys=['entity'],
                                                    outcomes=['done', 'dont_exist', 'bad_request', 'error'])

    def execute(self, userdata):
        # Generate URL to contact

        url = ""http://wonderland:8000/api/people/""

        entity = userdata.entity

        data = {}

        if entity.wonderlandId is None and entity.face.id is None:
            Logger.logwarn('Need wonderland ID or face ID !')
            return 'bad_request'

        if entity.wonderlandId is not None:
            data.update({'peopleId': entity.wonderlandId})

        if entity.face.id is not None:
            data.update({'peopleRecognitionId': entity.face.id})

        if entity.color is not None:
            data.update({'peopleColor': entity.color})

        if entity.pose is not None:
            data.update({'peoplePose': entity.pose})

        if entity.poseProbability is not None:
            data.update({'peoplePoseAccuracy': entity.color})

        if entity.face.gender is not None:
            data.update({'peopleGender': entity.face.gender})

        if entity.face.genderProbability is not None:
            data.update({'peopleGenderAccuracy': entity.face.genderProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.isOperator is not None:
            data.update({'peopleIsOperator': entity.isOperator})

        if len(entity.aliases) > 0:
            data.update({'peopleName': entity.aliases[0]})

        # try the request
        try:
            response = requests.patch(url, data=data)
            if response.status_code == 200:
                return 'done'

            elif response.status_code == 404:
                return 'dont_exist'

            elif 400 <= response.status_code < 500:
                Logger.logwarn(response.status_code)
                return 'bad_request'

            else:
                Logger.logerr(response.status_code)
                return 'error'

        except requests.exceptions.RequestException as e:
            Logger.logerr(e)
            return 'error'
",Add a state for patch a person in wonderland.,"Add a state for patch a person in wonderland.
",Python,bsd-3-clause,"WalkingMachine/sara_behaviors,WalkingMachine/sara_behaviors",96,"```python
#!/usr/bin/env python
# encoding=utf8

import requests
from flexbe_core import EventState, Logger

""""""
Created on 17/05/2018

@author: Lucas Maurice
""""""


class WonderlandPatchPerson(EventState):
    '''
    Patch (update) a person.
    >#  entity                  sara_msgs/Entity

    <= done                     return when the add correctly append
    <= dont_exist                return when the entity already exist
    <= bad_request              return when error reading data
    <= error                    return when error reading data
    '''

    def __init__(self):
        # See example_state.py for basic explanations.
        super(WonderlandPatchPerson, self).__init__(input_keys=['entity'],
                                                    outcomes=['done', 'dont_exist', 'bad_request', 'error'])

    def execute(self, userdata):
        # Generate URL to contact

        url = ""http://wonderland:8000/api/people/""

        entity = userdata.entity

        data = {}

        if entity.wonderlandId is None and entity.face.id is None:
            Logger.logwarn('Need wonderland ID or face ID !')
            return 'bad_request'

        if entity.wonderlandId is not None:
            data.update({'peopleId': entity.wonderlandId})

        if entity.face.id is not None:
            data.update({'peopleRecognitionId': entity.face.id})

        if entity.color is not None:
            data.update({'peopleColor': entity.color})

        if entity.pose is not None:
            data.update({'peoplePose': entity.pose})

        if entity.poseProbability is not None:
            data.update({'peoplePoseAccuracy': entity.color})

        if entity.face.gender is not None:
            data.update({'peopleGender': entity.face.gender})

        if entity.face.genderProbability is not None:
            data.update({'peopleGenderAccuracy': entity.face.genderProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.isOperator is not None:
            data.update({'peopleIsOperator': entity.isOperator})

        if len(entity.aliases) > 0:
            data.update({'peopleName': entity.aliases[0]})

        # try the request
        try:
            response = requests.patch(url, data=data)
            if response.status_code == 200:
                return 'done'

            elif response.status_code == 404:
                return 'dont_exist'

            elif 400 <= response.status_code < 500:
                Logger.logwarn(response.status_code)
                return 'bad_request'

            else:
                Logger.logerr(response.status_code)
                return 'error'

        except requests.exceptions.RequestException as e:
            Logger.logerr(e)
            return 'error'

```"
79aa5fb22c71590f367ee4b0e8906df2a8693c27,saleor/core/tests/test_anonymize.py,saleor/core/tests/test_anonymize.py,,"from ..anonymize import obfuscate_address, obfuscate_email, obfuscate_string


def test_obfuscate_email():
    # given
    email = ""abc@gmail.com""

    # when
    result = obfuscate_email(email)

    # then
    assert result == ""a...@example.com""


def test_obfuscate_email_example_email():
    # given
    email = ""abc@example.com""

    # when
    result = obfuscate_email(email)

    # then
    assert result == ""abc@example.com""


def test_obfuscate_email_no_at_in_email():
    # given
    email = ""abcgmail.com""

    # when
    result = obfuscate_email(email)

    # then
    assert result == ""a...........""


def test_obfuscate_string():
    # given
    value = ""AbcDef""

    # when
    result = obfuscate_string(value)

    # then
    assert result == ""A.....""


def test_obfuscate_string_empty_string():
    # given
    value = """"

    # when
    result = obfuscate_string(value)

    # then
    assert result == value


def test_obfuscate_string_phone_string():
    # given
    value = ""+40123123123""

    # when
    result = obfuscate_string(value, phone=True)

    # then
    assert result == ""+40.........""


def test_obfuscate_address(address):
    # given
    first_name = address.first_name
    last_name = address.last_name
    company_name = address.company_name
    street_address_1 = address.street_address_1
    phone = str(address.phone)

    # when
    result = obfuscate_address(address)

    # then
    assert result.first_name == first_name[0] + ""."" * (len(first_name) - 1)
    assert result.last_name == last_name[0] + ""."" * (len(last_name) - 1)
    assert result.company_name == company_name[0] + ""."" * (len(company_name) - 1)
    assert result.street_address_1 == street_address_1[0] + ""."" * (
        len(street_address_1) - 1
    )
    assert result.street_address_2 == """"
    assert result.phone == phone[:3] + ""."" * (len(phone) - 3)
",Add tests for anpnymize methods,"Add tests for anpnymize methods
",Python,bsd-3-clause,"mociepka/saleor,mociepka/saleor,mociepka/saleor",90,"```python
from ..anonymize import obfuscate_address, obfuscate_email, obfuscate_string


def test_obfuscate_email():
    # given
    email = ""abc@gmail.com""

    # when
    result = obfuscate_email(email)

    # then
    assert result == ""a...@example.com""


def test_obfuscate_email_example_email():
    # given
    email = ""abc@example.com""

    # when
    result = obfuscate_email(email)

    # then
    assert result == ""abc@example.com""


def test_obfuscate_email_no_at_in_email():
    # given
    email = ""abcgmail.com""

    # when
    result = obfuscate_email(email)

    # then
    assert result == ""a...........""


def test_obfuscate_string():
    # given
    value = ""AbcDef""

    # when
    result = obfuscate_string(value)

    # then
    assert result == ""A.....""


def test_obfuscate_string_empty_string():
    # given
    value = """"

    # when
    result = obfuscate_string(value)

    # then
    assert result == value


def test_obfuscate_string_phone_string():
    # given
    value = ""+40123123123""

    # when
    result = obfuscate_string(value, phone=True)

    # then
    assert result == ""+40.........""


def test_obfuscate_address(address):
    # given
    first_name = address.first_name
    last_name = address.last_name
    company_name = address.company_name
    street_address_1 = address.street_address_1
    phone = str(address.phone)

    # when
    result = obfuscate_address(address)

    # then
    assert result.first_name == first_name[0] + ""."" * (len(first_name) - 1)
    assert result.last_name == last_name[0] + ""."" * (len(last_name) - 1)
    assert result.company_name == company_name[0] + ""."" * (len(company_name) - 1)
    assert result.street_address_1 == street_address_1[0] + ""."" * (
        len(street_address_1) - 1
    )
    assert result.street_address_2 == """"
    assert result.phone == phone[:3] + ""."" * (len(phone) - 3)

```"
d0e8a202597d5c28d7dc6efc4762040c83072223,pombola/core/management/commands/core_find_stale_elasticsearch_documents.py,pombola/core/management/commands/core_find_stale_elasticsearch_documents.py,,"import sys

from django.core.management.base import BaseCommand, CommandError

from haystack import connections as haystack_connections
from haystack.exceptions import NotHandled
from haystack.query import SearchQuerySet
from haystack.utils.app_loading import get_models, load_apps

def get_all_indexed_models():

    backends = haystack_connections.connections_info.keys()

    available_models = {}

    for backend_key in backends:
        unified_index = haystack_connections[backend_key].get_unified_index()
        for app in load_apps():
            for model in get_models(app):
                try:
                    unified_index.get_index(model)
                except NotHandled:
                    continue
                model_name = model.__module__ + '.' + model.__name__
                available_models[model_name] = {
                    'backend_key': backend_key,
                    'app': app,
                    'model': model,
                }

    return available_models

def get_models_to_check(model_names, available_models):
    
    models_to_check = []

    if model_names:
        missing_models = False
        for model_name in model_names:
            if model_name in available_models:
                models_to_check.append(model_name)
            else:
                missing_models = True
                print ""There was no model {0} with a search index"".format(model_name)
            if missing_models:
                print ""Some models were not found; they must be one of:""
                for model in sorted(available_models.keys()):
                    print "" "", model
                sys.exit(1)
    else:
        models_to_check = sorted(available_models.keys())

    return models_to_check


class Command(BaseCommand):
    args = 'MODEL ...'
    help = 'Get all search results for the given models'

    def handle(self, *args, **options):

        available_models = get_all_indexed_models()

        models_to_check = get_models_to_check(args, available_models)

        # Now we know which models to check, do that:

        for model_name in models_to_check:
            model_details = available_models[model_name]

            backend_key = model_details['backend_key']
            model = model_details['model']

            backend = haystack_connections[backend_key].get_backend()
            unified_index = haystack_connections[backend_key].get_unified_index()
            
            index = unified_index.get_index(model)

            qs = index.build_queryset()
            print ""Checking {0} ({1} objects in the database)"".format(
                model_name, qs.count()
            )
            # Get all the primary keys from the database:
            pks_in_database = set(
                unicode(pk) for pk in qs.values_list('pk', flat=True)
            )
            # Then go through every search result for that
            # model, and check that the primary key is one
            # that's in the database:
            for search_result in SearchQuerySet(using=backend.connection_alias).models(model):
                if search_result.pk not in pks_in_database:
                    print ""      stale search entry for primary key"", search_result.pk
",Add a command to help find out-of-sync objects in Elasticsearch,"Add a command to help find out-of-sync objects in Elasticsearch

This is to help trying to figure out the problems behind issue #1424.
",Python,agpl-3.0,"geoffkilpin/pombola,geoffkilpin/pombola,geoffkilpin/pombola,mysociety/pombola,geoffkilpin/pombola,mysociety/pombola,mysociety/pombola,geoffkilpin/pombola,mysociety/pombola,geoffkilpin/pombola,mysociety/pombola,mysociety/pombola",93,"```python
import sys

from django.core.management.base import BaseCommand, CommandError

from haystack import connections as haystack_connections
from haystack.exceptions import NotHandled
from haystack.query import SearchQuerySet
from haystack.utils.app_loading import get_models, load_apps

def get_all_indexed_models():

    backends = haystack_connections.connections_info.keys()

    available_models = {}

    for backend_key in backends:
        unified_index = haystack_connections[backend_key].get_unified_index()
        for app in load_apps():
            for model in get_models(app):
                try:
                    unified_index.get_index(model)
                except NotHandled:
                    continue
                model_name = model.__module__ + '.' + model.__name__
                available_models[model_name] = {
                    'backend_key': backend_key,
                    'app': app,
                    'model': model,
                }

    return available_models

def get_models_to_check(model_names, available_models):
    
    models_to_check = []

    if model_names:
        missing_models = False
        for model_name in model_names:
            if model_name in available_models:
                models_to_check.append(model_name)
            else:
                missing_models = True
                print ""There was no model {0} with a search index"".format(model_name)
            if missing_models:
                print ""Some models were not found; they must be one of:""
                for model in sorted(available_models.keys()):
                    print "" "", model
                sys.exit(1)
    else:
        models_to_check = sorted(available_models.keys())

    return models_to_check


class Command(BaseCommand):
    args = 'MODEL ...'
    help = 'Get all search results for the given models'

    def handle(self, *args, **options):

        available_models = get_all_indexed_models()

        models_to_check = get_models_to_check(args, available_models)

        # Now we know which models to check, do that:

        for model_name in models_to_check:
            model_details = available_models[model_name]

            backend_key = model_details['backend_key']
            model = model_details['model']

            backend = haystack_connections[backend_key].get_backend()
            unified_index = haystack_connections[backend_key].get_unified_index()
            
            index = unified_index.get_index(model)

            qs = index.build_queryset()
            print ""Checking {0} ({1} objects in the database)"".format(
                model_name, qs.count()
            )
            # Get all the primary keys from the database:
            pks_in_database = set(
                unicode(pk) for pk in qs.values_list('pk', flat=True)
            )
            # Then go through every search result for that
            # model, and check that the primary key is one
            # that's in the database:
            for search_result in SearchQuerySet(using=backend.connection_alias).models(model):
                if search_result.pk not in pks_in_database:
                    print ""      stale search entry for primary key"", search_result.pk

```"
94b7a913ca0d9ce5d4c58539954afef55d21c3a7,components/dash-core-components/tests/integration/dropdown/test_remove_option.py,components/dash-core-components/tests/integration/dropdown/test_remove_option.py,,"import json

from dash import Dash, html, dcc, Output, Input
from dash.exceptions import PreventUpdate


sample_dropdown_options = [
    {""label"": ""New York City"", ""value"": ""NYC""},
    {""label"": ""Montreal"", ""value"": ""MTL""},
    {""label"": ""San Francisco"", ""value"": ""SF""},
]


def test_ddro001_remove_option_single(dash_dcc):
    dropdown_options = sample_dropdown_options

    app = Dash(__name__)
    value = 'SF'

    app.layout = html.Div([
        dcc.Dropdown(
            options=dropdown_options,
            value=value,
            id='dropdown',
        ),
        html.Button('Remove option', id='remove'),
        html.Div(id='value-output')
    ])

    @app.callback(
        Output('dropdown', 'options'),
        [Input('remove', 'n_clicks')]
    )
    def on_click(n_clicks):
        if not n_clicks:
            raise PreventUpdate
        return sample_dropdown_options[:-1]

    @app.callback(
        Output('value-output', 'children'),
        [Input('dropdown', 'value')]
    )
    def on_change(val):
        if not val:
            raise PreventUpdate
        return val or 'None'

    dash_dcc.start_server(app)
    btn = dash_dcc.wait_for_element('#remove')
    btn.click()

    dash_dcc.wait_for_text_to_equal('#value-output', 'None')


def test_ddro002_remove_option_multi(dash_dcc):
    dropdown_options = sample_dropdown_options

    app = Dash(__name__)
    value = ['MTL', 'SF']

    app.layout = html.Div([
        dcc.Dropdown(
            options=dropdown_options,
            value=value,
            multi=True,
            id='dropdown',
        ),
        html.Button('Remove option', id='remove'),
        html.Div(id='value-output')
    ])

    @app.callback(
        Output('dropdown', 'options'),
        [Input('remove', 'n_clicks')]
    )
    def on_click(n_clicks):
        if not n_clicks:
            raise PreventUpdate
        return sample_dropdown_options[:-1]

    @app.callback(
        Output('value-output', 'children'),
        [Input('dropdown', 'value')]
    )
    def on_change(val):
        return json.dumps(val)

    dash_dcc.start_server(app)
    btn = dash_dcc.wait_for_element('#remove')
    btn.click()

    dash_dcc.wait_for_text_to_equal('#value-output', '[""MTL""]')
",Add test dropdown remove options.,"Add test dropdown remove options.
",Python,mit,"plotly/dash,plotly/dash,plotly/dash,plotly/dash,plotly/dash",93,"```python
import json

from dash import Dash, html, dcc, Output, Input
from dash.exceptions import PreventUpdate


sample_dropdown_options = [
    {""label"": ""New York City"", ""value"": ""NYC""},
    {""label"": ""Montreal"", ""value"": ""MTL""},
    {""label"": ""San Francisco"", ""value"": ""SF""},
]


def test_ddro001_remove_option_single(dash_dcc):
    dropdown_options = sample_dropdown_options

    app = Dash(__name__)
    value = 'SF'

    app.layout = html.Div([
        dcc.Dropdown(
            options=dropdown_options,
            value=value,
            id='dropdown',
        ),
        html.Button('Remove option', id='remove'),
        html.Div(id='value-output')
    ])

    @app.callback(
        Output('dropdown', 'options'),
        [Input('remove', 'n_clicks')]
    )
    def on_click(n_clicks):
        if not n_clicks:
            raise PreventUpdate
        return sample_dropdown_options[:-1]

    @app.callback(
        Output('value-output', 'children'),
        [Input('dropdown', 'value')]
    )
    def on_change(val):
        if not val:
            raise PreventUpdate
        return val or 'None'

    dash_dcc.start_server(app)
    btn = dash_dcc.wait_for_element('#remove')
    btn.click()

    dash_dcc.wait_for_text_to_equal('#value-output', 'None')


def test_ddro002_remove_option_multi(dash_dcc):
    dropdown_options = sample_dropdown_options

    app = Dash(__name__)
    value = ['MTL', 'SF']

    app.layout = html.Div([
        dcc.Dropdown(
            options=dropdown_options,
            value=value,
            multi=True,
            id='dropdown',
        ),
        html.Button('Remove option', id='remove'),
        html.Div(id='value-output')
    ])

    @app.callback(
        Output('dropdown', 'options'),
        [Input('remove', 'n_clicks')]
    )
    def on_click(n_clicks):
        if not n_clicks:
            raise PreventUpdate
        return sample_dropdown_options[:-1]

    @app.callback(
        Output('value-output', 'children'),
        [Input('dropdown', 'value')]
    )
    def on_change(val):
        return json.dumps(val)

    dash_dcc.start_server(app)
    btn = dash_dcc.wait_for_element('#remove')
    btn.click()

    dash_dcc.wait_for_text_to_equal('#value-output', '[""MTL""]')

```"
dfe5f213efd523891d77a8f390c690324f7bbb45,scripts/add_all_users_to_instances.py,scripts/add_all_users_to_instances.py,,"#!/usr/bin/env python
""""""Add all users into the instances given on the command line.
""""""
import os
import sys
from argparse import ArgumentParser

from paste.deploy import appconfig
from adhocracy.config.environment import load_environment
from adhocracy import model


def load_config(filename):
    conf = appconfig('config:' + os.path.abspath(filename) + '#content')
    load_environment(conf.global_conf, conf.local_conf)


def parse_args():
    parser = ArgumentParser(description=__doc__)
    parser.add_argument(""conf_file"", help=""configuration to use"")
    parser.add_argument(""instance_keys"", metavar='instance', nargs='+',
                        help=(""Instance key(s). If ALL is given, the ""
                              ""users will be added to all visible instances.""))
    return parser.parse_args()


def main():
    args = parse_args()
    load_config(args.conf_file)
    session = model.meta.Session

    num_added = {}
    keys_added = {}

    def increment(dict_, key):
        num = dict_.get(key, 0)
        num += 1
        dict_[key] = num

    # filter the instances we have to add
    instance_keys = args.instance_keys
    instances = model.Instance.all()
    if not 'ALL' in instance_keys:
        instances = [instance for instance in instances if instance.key
                     in instance_keys]

    num_instances = len(instances)
    users = model.User.all()
    num_users = len(users)
    total_added = 0

    print ""\n***Adding users***\n""
    for user in users:
        user_status = user.user_name + ': '
        added = 0
        for instance in set(instances) - set(user.instances):
            increment(keys_added, instance.key)
            user_status += instance.key + ' '
            added += 1
            total_added += 1
            membership = model.Membership(user, instance,
                                          instance.default_group)
            session.add(membership)
            if (total_added % 2) == 0:
                session.commit()
        if added:
            print ""%s (%s)"" % (user_status, added)
        increment(num_added, added)
    session.commit()

    print ""\n***Statistics***""
    print ""Instances:"", num_instances
    print ""Users:"", num_users

    print ""\n**Instances added per user**\n""
    added = 0
    for (key, count) in sorted(num_added.items()):
        added += key * count
        print ""%s membership added for %s users"" % (key, count)
    print ''
    print ""Mean number of memberships added per user: %s"" % (float(added) /
                                                             float(num_users))

    print ""\n**Users added per group**\n""
    for key in sorted(keys_added.keys()):
        print ""%s: %s"" % (key, (float(keys_added[key]) / float(num_users)))


if __name__ == '__main__':
    sys.exit(main())
",Add a command line script to ad all users to instances,"Add a command line script to ad all users to instances
",Python,agpl-3.0,"DanielNeugebauer/adhocracy,phihag/adhocracy,liqd/adhocracy,DanielNeugebauer/adhocracy,DanielNeugebauer/adhocracy,alkadis/vcv,SysTheron/adhocracy,SysTheron/adhocracy,liqd/adhocracy,phihag/adhocracy,alkadis/vcv,DanielNeugebauer/adhocracy,phihag/adhocracy,DanielNeugebauer/adhocracy,liqd/adhocracy,liqd/adhocracy,phihag/adhocracy,phihag/adhocracy,alkadis/vcv,SysTheron/adhocracy,alkadis/vcv,alkadis/vcv",91,"```python
#!/usr/bin/env python
""""""Add all users into the instances given on the command line.
""""""
import os
import sys
from argparse import ArgumentParser

from paste.deploy import appconfig
from adhocracy.config.environment import load_environment
from adhocracy import model


def load_config(filename):
    conf = appconfig('config:' + os.path.abspath(filename) + '#content')
    load_environment(conf.global_conf, conf.local_conf)


def parse_args():
    parser = ArgumentParser(description=__doc__)
    parser.add_argument(""conf_file"", help=""configuration to use"")
    parser.add_argument(""instance_keys"", metavar='instance', nargs='+',
                        help=(""Instance key(s). If ALL is given, the ""
                              ""users will be added to all visible instances.""))
    return parser.parse_args()


def main():
    args = parse_args()
    load_config(args.conf_file)
    session = model.meta.Session

    num_added = {}
    keys_added = {}

    def increment(dict_, key):
        num = dict_.get(key, 0)
        num += 1
        dict_[key] = num

    # filter the instances we have to add
    instance_keys = args.instance_keys
    instances = model.Instance.all()
    if not 'ALL' in instance_keys:
        instances = [instance for instance in instances if instance.key
                     in instance_keys]

    num_instances = len(instances)
    users = model.User.all()
    num_users = len(users)
    total_added = 0

    print ""\n***Adding users***\n""
    for user in users:
        user_status = user.user_name + ': '
        added = 0
        for instance in set(instances) - set(user.instances):
            increment(keys_added, instance.key)
            user_status += instance.key + ' '
            added += 1
            total_added += 1
            membership = model.Membership(user, instance,
                                          instance.default_group)
            session.add(membership)
            if (total_added % 2) == 0:
                session.commit()
        if added:
            print ""%s (%s)"" % (user_status, added)
        increment(num_added, added)
    session.commit()

    print ""\n***Statistics***""
    print ""Instances:"", num_instances
    print ""Users:"", num_users

    print ""\n**Instances added per user**\n""
    added = 0
    for (key, count) in sorted(num_added.items()):
        added += key * count
        print ""%s membership added for %s users"" % (key, count)
    print ''
    print ""Mean number of memberships added per user: %s"" % (float(added) /
                                                             float(num_users))

    print ""\n**Users added per group**\n""
    for key in sorted(keys_added.keys()):
        print ""%s: %s"" % (key, (float(keys_added[key]) / float(num_users)))


if __name__ == '__main__':
    sys.exit(main())

```"
9950d25a2659509c81c29f9d834dd6c39e8b4015,scripts/blog_image_refactor.py,scripts/blog_image_refactor.py,,"#!/usr/bin/python
# -*- coding: utf-8 -*-

import os


ROOT_FOLDER = '/home/mjulian/coding/figarocorso.github.io/'
POST_FOLDER = ROOT_FOLDER + '_posts/day-by-day/'
IMAGE_FOLDER = ROOT_FOLDER + 'images/blog/'


def get_file_lines(filename):
    with open(POST_FOLDER + filename) as f:
        return f.readlines()


def get_post_image_urls(lines):
    image_urls = []
    for line in lines:
        image_url = get_image_url_or_none(line)
        if image_url:
            image_urls.append(image_url)

    return image_urls


def get_image_url_or_none(line):
    # TODO: Use re
    try:
        image_url_beggining = line.index('](') + 2
        image_url_ending = line.index(')')
        line.index('![')
    except ValueError:
        return None

    return line[image_url_beggining:image_url_ending]


def url_is_blog_image(url):
    return 'blog.migueljulian.com' in url and 'uploads' in url


def get_post_image(image_urls):
    for image_url in image_urls:
        if url_is_blog_image(image_url):
            return image_url
    return None


def get_image_name(url):
    return url.split('/')[-1]


def add_image_header(file_lines, image_name):
    header = ""image: /images/blog/%s\n"" % image_name
    file_lines.insert(3, header)


def replace_image_url(file_lines, image_name):
    needle = 'http://blog.migueljulian.com/wp-content/uploads'
    line_count = 0
    for line in file_lines:
        new_line = ''
        if needle in line:
            new_line = line.replace(needle, '/images/blog')

        if new_line:
            file_lines[line_count] = new_line

        line_count += 1


def write_file(filename, lines):
    print filename
    with open(POST_FOLDER + filename, 'w') as f:
        for line in lines:
            f.write(line)


if __name__ == ""__main__"":
    for filename in os.listdir(POST_FOLDER):
        file_lines = get_file_lines(filename)
        post_images = get_post_image_urls(file_lines)
        post_image = get_post_image(post_images)
        if post_image:
            image_name = get_image_name(post_image)
            add_image_header(file_lines, image_name)
            replace_image_url(file_lines, image_name)
            write_file(filename, file_lines)
",Add image url replacement script,"Add image url replacement script
",Python,mit,"figarocorso/figarocorso.github.io,figarocorso/figarocorso.github.io,figarocorso/figarocorso.github.io,figarocorso/figarocorso.github.io",90,"```python
#!/usr/bin/python
# -*- coding: utf-8 -*-

import os


ROOT_FOLDER = '/home/mjulian/coding/figarocorso.github.io/'
POST_FOLDER = ROOT_FOLDER + '_posts/day-by-day/'
IMAGE_FOLDER = ROOT_FOLDER + 'images/blog/'


def get_file_lines(filename):
    with open(POST_FOLDER + filename) as f:
        return f.readlines()


def get_post_image_urls(lines):
    image_urls = []
    for line in lines:
        image_url = get_image_url_or_none(line)
        if image_url:
            image_urls.append(image_url)

    return image_urls


def get_image_url_or_none(line):
    # TODO: Use re
    try:
        image_url_beggining = line.index('](') + 2
        image_url_ending = line.index(')')
        line.index('![')
    except ValueError:
        return None

    return line[image_url_beggining:image_url_ending]


def url_is_blog_image(url):
    return 'blog.migueljulian.com' in url and 'uploads' in url


def get_post_image(image_urls):
    for image_url in image_urls:
        if url_is_blog_image(image_url):
            return image_url
    return None


def get_image_name(url):
    return url.split('/')[-1]


def add_image_header(file_lines, image_name):
    header = ""image: /images/blog/%s\n"" % image_name
    file_lines.insert(3, header)


def replace_image_url(file_lines, image_name):
    needle = 'http://blog.migueljulian.com/wp-content/uploads'
    line_count = 0
    for line in file_lines:
        new_line = ''
        if needle in line:
            new_line = line.replace(needle, '/images/blog')

        if new_line:
            file_lines[line_count] = new_line

        line_count += 1


def write_file(filename, lines):
    print filename
    with open(POST_FOLDER + filename, 'w') as f:
        for line in lines:
            f.write(line)


if __name__ == ""__main__"":
    for filename in os.listdir(POST_FOLDER):
        file_lines = get_file_lines(filename)
        post_images = get_post_image_urls(file_lines)
        post_image = get_post_image(post_images)
        if post_image:
            image_name = get_image_name(post_image)
            add_image_header(file_lines, image_name)
            replace_image_url(file_lines, image_name)
            write_file(filename, file_lines)

```"
e6a3e0c77ada592c06394af2e235b3528a61202a,armstrong/dev/tasks/__init__.py,armstrong/dev/tasks/__init__.py,"import pkg_resources
pkg_resources.declare_namespace(__name__)
","import pkg_resources
pkg_resources.declare_namespace(__name__)


from contextlib import contextmanager
try:
    import coverage
except ImportError:
    coverage = False
import os
from os.path import basename, dirname
import sys

from fabric.api import *
from fabric.decorators import task

if not ""fabfile"" in sys.modules:
    sys.stderr.write(""This expects to have a 'fabfile' module\n"")
    sys.stderr.write(-1)
fabfile = sys.modules[""fabfile""]


try:
    from d51.django.virtualenv.test_runner import run_tests
except ImportError, e:
    sys.stderr.write(
            ""This project requires d51.django.virtualenv.test_runner\n"")
    sys.exit(-1)


FABRIC_TASK_MODULE = True


__all__ = [""clean"", ""command"", ""create_migration"", ""pep8"", ""test"", ""runserver"",
           ""shell"", ""syncdb"", ]

@contextmanager
def html_coverage_report(directory=""./coverage""):
    # This relies on this being run from within a directory named the same as
    # the repository on GitHub.  It's fragile, but for our purposes, it works.
    if coverage:
        base_path = os.path.join(dirname(dirname(__file__)), ""armstrong"")
        files_to_cover = []
        for (dir, dirs, files) in os.walk(base_path):
            if not dir.find(""tests"") is -1:
                continue
            valid = lambda a: a[0] != ""."" and a[-3:] == "".py""
            files_to_cover += [""%s/%s"" % (dir, file) for file in files if valid(file)]
        cov = coverage.coverage(branch=True, include=files_to_cover)
        cov.start()
    yield

    if coverage:
        cov.stop()
        cov.html_report(directory=directory)
    else:
        print ""Install coverage.py to measure test coverage""


@task
def clean():
    local('find . -name ""*.py[co]"" -exec rm {} \;')


@task
def create_migration(name):
    command(((""schemamigration"", fabfile.main_app, name), {""initial"": True}))


@task
def command(*cmds):
    from d51.django.virtualenv.base import VirtualEnvironment
    runner = VirtualEnvironment()
    runner.run(fabfile.settings)
    for cmd in cmds:
        if type(cmd) is tuple:
            args, kwargs = cmd
        else:
            args = (cmd, )
            kwargs = {}
        runner.call_command(*args, **kwargs)


@task
def pep8():
    local('find ./armstrong -name ""*.py"" | xargs pep8', capture=False)


@task
def test():
    with html_coverage_report():
        run_tests(fabfile.settings, *fabfile.tested_apps)


@task
def runserver():
    command(""runserver"")


@task
def shell():
    command(""shell"")


@task
def syncdb():
    command(""syncdb"", ""migrate"")
",Create all of the tasks needed for developing in Armstrong,"Create all of the tasks needed for developing in Armstrong
",Python,apache-2.0,armstrong/armstrong.dev,108,"```python
import pkg_resources
pkg_resources.declare_namespace(__name__)


from contextlib import contextmanager
try:
    import coverage
except ImportError:
    coverage = False
import os
from os.path import basename, dirname
import sys

from fabric.api import *
from fabric.decorators import task

if not ""fabfile"" in sys.modules:
    sys.stderr.write(""This expects to have a 'fabfile' module\n"")
    sys.stderr.write(-1)
fabfile = sys.modules[""fabfile""]


try:
    from d51.django.virtualenv.test_runner import run_tests
except ImportError, e:
    sys.stderr.write(
            ""This project requires d51.django.virtualenv.test_runner\n"")
    sys.exit(-1)


FABRIC_TASK_MODULE = True


__all__ = [""clean"", ""command"", ""create_migration"", ""pep8"", ""test"", ""runserver"",
           ""shell"", ""syncdb"", ]

@contextmanager
def html_coverage_report(directory=""./coverage""):
    # This relies on this being run from within a directory named the same as
    # the repository on GitHub.  It's fragile, but for our purposes, it works.
    if coverage:
        base_path = os.path.join(dirname(dirname(__file__)), ""armstrong"")
        files_to_cover = []
        for (dir, dirs, files) in os.walk(base_path):
            if not dir.find(""tests"") is -1:
                continue
            valid = lambda a: a[0] != ""."" and a[-3:] == "".py""
            files_to_cover += [""%s/%s"" % (dir, file) for file in files if valid(file)]
        cov = coverage.coverage(branch=True, include=files_to_cover)
        cov.start()
    yield

    if coverage:
        cov.stop()
        cov.html_report(directory=directory)
    else:
        print ""Install coverage.py to measure test coverage""


@task
def clean():
    local('find . -name ""*.py[co]"" -exec rm {} \;')


@task
def create_migration(name):
    command(((""schemamigration"", fabfile.main_app, name), {""initial"": True}))


@task
def command(*cmds):
    from d51.django.virtualenv.base import VirtualEnvironment
    runner = VirtualEnvironment()
    runner.run(fabfile.settings)
    for cmd in cmds:
        if type(cmd) is tuple:
            args, kwargs = cmd
        else:
            args = (cmd, )
            kwargs = {}
        runner.call_command(*args, **kwargs)


@task
def pep8():
    local('find ./armstrong -name ""*.py"" | xargs pep8', capture=False)


@task
def test():
    with html_coverage_report():
        run_tests(fabfile.settings, *fabfile.tested_apps)


@task
def runserver():
    command(""runserver"")


@task
def shell():
    command(""shell"")


@task
def syncdb():
    command(""syncdb"", ""migrate"")

```"
1212d33d849155f8c1cdc6a610e893318937e7c5,silk/webdoc/html/v5.py,silk/webdoc/html/v5.py,"
""""""Module containing only html v5 tags. All deprecated tags have been removed.
""""""

from .common import *

del ACRONYM
del APPLET
del BASEFONT
del BIG
del CENTER
del DIR
del FONT
del FRAME
del FRAMESET
del NOFRAMES
del STRIKE
del TT
del U
","
""""""
Module containing only html v5 tags. All deprecated tags have been removed.
""""""

from .common import (  # flake8: noqa
    A,
    ABBR,
    # ACRONYM,
    ADDRESS,
    # APPLET,
    AREA,
    B,
    BASE,
    # BASEFONT,
    BDO,
    # BIG,
    BLOCKQUOTE,
    BODY,
    BR,
    BUTTON,
    Body,
    CAPTION,
    CAT,
    # CENTER,
    CITE,
    CODE,
    COL,
    COLGROUP,
    COMMENT,
    CONDITIONAL_COMMENT,
    DD,
    DEL,
    DFN,
    # DIR,
    DIV,
    DL,
    DT,
    EM,
    FIELDSET,
    # FONT,
    FORM,
    # FRAME,
    # FRAMESET,
    Form,
    H1,
    H2,
    H3,
    H4,
    H5,
    H6,
    HEAD,
    HR,
    HTML,
    HTMLDoc,
    Hyper,
    I,
    IFRAME,
    IMG,
    INPUT,
    INS,
    Image,
    Javascript,
    KBD,
    LABEL,
    LEGEND,
    LI,
    LINK,
    MAP,
    MENU,
    META,
    NBSP,
    # NOFRAMES,
    NOSCRIPT,
    OBJECT,
    OL,
    OPTGROUP,
    OPTION,
    P,
    PARAM,
    PRE,
    Q,
    S,
    SAMP,
    SCRIPT,
    SELECT,
    SMALL,
    SPAN,
    # STRIKE,
    STRONG,
    STYLE,
    SUB,
    SUP,
    TABLE,
    TBODY,
    TD,
    TEXTAREA,
    TFOOT,
    TH,
    THEAD,
    TITLE,
    TR,
    # TT,
    # U,
    UL,
    VAR,
    XML,
    XMLEntity,
    XMLNode,
    XMP,
    xmlescape,
    xmlunescape
)
",Replace import * with explicit names,"Replace import * with explicit names
",Python,bsd-3-clause,orbnauticus/silk,114,"```python

""""""
Module containing only html v5 tags. All deprecated tags have been removed.
""""""

from .common import (  # flake8: noqa
    A,
    ABBR,
    # ACRONYM,
    ADDRESS,
    # APPLET,
    AREA,
    B,
    BASE,
    # BASEFONT,
    BDO,
    # BIG,
    BLOCKQUOTE,
    BODY,
    BR,
    BUTTON,
    Body,
    CAPTION,
    CAT,
    # CENTER,
    CITE,
    CODE,
    COL,
    COLGROUP,
    COMMENT,
    CONDITIONAL_COMMENT,
    DD,
    DEL,
    DFN,
    # DIR,
    DIV,
    DL,
    DT,
    EM,
    FIELDSET,
    # FONT,
    FORM,
    # FRAME,
    # FRAMESET,
    Form,
    H1,
    H2,
    H3,
    H4,
    H5,
    H6,
    HEAD,
    HR,
    HTML,
    HTMLDoc,
    Hyper,
    I,
    IFRAME,
    IMG,
    INPUT,
    INS,
    Image,
    Javascript,
    KBD,
    LABEL,
    LEGEND,
    LI,
    LINK,
    MAP,
    MENU,
    META,
    NBSP,
    # NOFRAMES,
    NOSCRIPT,
    OBJECT,
    OL,
    OPTGROUP,
    OPTION,
    P,
    PARAM,
    PRE,
    Q,
    S,
    SAMP,
    SCRIPT,
    SELECT,
    SMALL,
    SPAN,
    # STRIKE,
    STRONG,
    STYLE,
    SUB,
    SUP,
    TABLE,
    TBODY,
    TD,
    TEXTAREA,
    TFOOT,
    TH,
    THEAD,
    TITLE,
    TR,
    # TT,
    # U,
    UL,
    VAR,
    XML,
    XMLEntity,
    XMLNode,
    XMP,
    xmlescape,
    xmlunescape
)

```"
619033bc8daf3b8f5faafa95b04c06d98c39969f,stack/vpc.py,stack/vpc.py,"from troposphere import (
    Ref,
)

from troposphere.ec2 import (
    InternetGateway,
    Route,
    RouteTable,
    VPC,
    VPCGatewayAttachment,
)

from .template import template


vpc = VPC(
    ""Vpc"",
    template=template,
    CidrBlock=""10.0.0.0/16"",
)


# Allow outgoing to outside VPC
internet_gateway = InternetGateway(
    ""InternetGateway"",
    template=template,
)


# Attach Gateway to VPC
VPCGatewayAttachment(
    ""GatewayAttachement"",
    template=template,
    VpcId=Ref(vpc),
    InternetGatewayId=Ref(internet_gateway),
)


# Public route table
public_route_table = RouteTable(
    ""PublicRouteTable"",
    template=template,
    VpcId=Ref(vpc),
)


public_route = Route(
    ""PublicRoute"",
    template=template,
    GatewayId=Ref(internet_gateway),
    DestinationCidrBlock=""0.0.0.0/0"",
    RouteTableId=Ref(public_route_table),
)
","from troposphere import (
    GetAtt,
    Ref,
)

from troposphere.ec2 import (
    EIP,
    InternetGateway,
    NatGateway,
    Route,
    RouteTable,
    Subnet,
    SubnetRouteTableAssociation,
    VPC,
    VPCGatewayAttachment,
)

from .template import template


vpc = VPC(
    ""Vpc"",
    template=template,
    CidrBlock=""10.0.0.0/16"",
)


# Allow outgoing to outside VPC
internet_gateway = InternetGateway(
    ""InternetGateway"",
    template=template,
)


# Attach Gateway to VPC
VPCGatewayAttachment(
    ""GatewayAttachement"",
    template=template,
    VpcId=Ref(vpc),
    InternetGatewayId=Ref(internet_gateway),
)


# Public route table
public_route_table = RouteTable(
    ""PublicRouteTable"",
    template=template,
    VpcId=Ref(vpc),
)


public_route = Route(
    ""PublicRoute"",
    template=template,
    GatewayId=Ref(internet_gateway),
    DestinationCidrBlock=""0.0.0.0/0"",
    RouteTableId=Ref(public_route_table),
)


# Holds public instances
public_subnet_cidr = ""10.0.1.0/24""

public_subnet = Subnet(
    ""PublicSubnet"",
    template=template,
    VpcId=Ref(vpc),
    CidrBlock=public_subnet_cidr,
)


SubnetRouteTableAssociation(
    ""PublicSubnetRouteTableAssociation"",
    template=template,
    RouteTableId=Ref(public_route_table),
    SubnetId=Ref(public_subnet),
)


# NAT
nat_ip = EIP(
    ""NatIp"",
    template=template,
    Domain=""vpc"",
)


nat_gateway = NatGateway(
    ""NatGateway"",
    template=template,
    AllocationId=GetAtt(nat_ip, ""AllocationId""),
    SubnetId=Ref(public_subnet),
)
",Add a public subnet that holds a `NAT` gateway,"Add a public subnet that holds a `NAT` gateway
",Python,mit,"caktus/aws-web-stacks,tobiasmcnulty/aws-container-basics",94,"```python
from troposphere import (
    GetAtt,
    Ref,
)

from troposphere.ec2 import (
    EIP,
    InternetGateway,
    NatGateway,
    Route,
    RouteTable,
    Subnet,
    SubnetRouteTableAssociation,
    VPC,
    VPCGatewayAttachment,
)

from .template import template


vpc = VPC(
    ""Vpc"",
    template=template,
    CidrBlock=""10.0.0.0/16"",
)


# Allow outgoing to outside VPC
internet_gateway = InternetGateway(
    ""InternetGateway"",
    template=template,
)


# Attach Gateway to VPC
VPCGatewayAttachment(
    ""GatewayAttachement"",
    template=template,
    VpcId=Ref(vpc),
    InternetGatewayId=Ref(internet_gateway),
)


# Public route table
public_route_table = RouteTable(
    ""PublicRouteTable"",
    template=template,
    VpcId=Ref(vpc),
)


public_route = Route(
    ""PublicRoute"",
    template=template,
    GatewayId=Ref(internet_gateway),
    DestinationCidrBlock=""0.0.0.0/0"",
    RouteTableId=Ref(public_route_table),
)


# Holds public instances
public_subnet_cidr = ""10.0.1.0/24""

public_subnet = Subnet(
    ""PublicSubnet"",
    template=template,
    VpcId=Ref(vpc),
    CidrBlock=public_subnet_cidr,
)


SubnetRouteTableAssociation(
    ""PublicSubnetRouteTableAssociation"",
    template=template,
    RouteTableId=Ref(public_route_table),
    SubnetId=Ref(public_subnet),
)


# NAT
nat_ip = EIP(
    ""NatIp"",
    template=template,
    Domain=""vpc"",
)


nat_gateway = NatGateway(
    ""NatGateway"",
    template=template,
    AllocationId=GetAtt(nat_ip, ""AllocationId""),
    SubnetId=Ref(public_subnet),
)

```"
33a9bd5cf465a56c2eb156dcbc0d4e61a0f590a4,osmABTS/places.py,osmABTS/places.py,"""""""
Places of interest generation
=============================

""""""
","""""""
Places of interest generation
=============================

This module defines a class for places of interest and the functions for
generating the data structure for all of them from the OSM raw data.

Each place of interest will basically just carry the information about its
location in the **network** as the identity of the network node which is
nearest to its actual location. And additionally, a name can be given for it,
as well as a weight that can be used for the random allocation for the
travellers.

The places of interest will be bundled in a dictionary, with the name of the
category as the key and a list of the actual places as the value.

""""""


class Place(object):

    """"""The places of interest for the travellers

    Since the category is going to be stored one level upper as the dictionary
    key, here just a few attributes are needed

    .. py:attribute:: node

        The node identity for the place of interest in the network

    .. py::attribute:: name

        The name of the place of interest

    .. py::attribute:: weight

        The weight for it during the place allocation. The probability of being
        selected.

    """"""

    __slots__ = [
        'node',
        'name',
        'weight',
        ]

    def __init__(self, node, name, weight):

        """"""Initializes the place instance""""""

        self.node = node
        self.name = name
        self.weight = weight


#
# Home generation
# ---------------
#
# The home generation is different from all the other places, since it is going
# to be based on the existance of residential road, rather than specific
# locations on the map.
#
# The generation code will iterate over all the nodes of the graph, find out
# the total length of residential road edges on it, and use that length as the
# weight. For nodes with no residential road, no people will live there.
#

def _gen_homes(net):

    """"""Generates a list of homes for a given network

    :param net: The NetworkX graph for the simulation
    :returns: A list of :py:class:`Place` instances for the homes

    """"""

    homes = []

    for node in net.nodes_iter():

        weight = 0.0
        for road in net[node].itervalues():
            if road['highway'] == 'residential':
                weight += road['length']

        # 0.01 is an arbitrary delta to skip nodes with few residents
        if weight > 0.01:
            homes.append(
                Place(node, 'home', weight)
                )

    return homes


#
# Some constance for place generation
# -----------------------------------
#

",Implement place class and homes generation,"Implement place class and homes generation

The Place class for places of interest has been implemented, as well as
the generation of homes, which is different from the generation of other
places of interest.
",Python,mit,tschijnmo/osmABTS,102,"```python
""""""
Places of interest generation
=============================

This module defines a class for places of interest and the functions for
generating the data structure for all of them from the OSM raw data.

Each place of interest will basically just carry the information about its
location in the **network** as the identity of the network node which is
nearest to its actual location. And additionally, a name can be given for it,
as well as a weight that can be used for the random allocation for the
travellers.

The places of interest will be bundled in a dictionary, with the name of the
category as the key and a list of the actual places as the value.

""""""


class Place(object):

    """"""The places of interest for the travellers

    Since the category is going to be stored one level upper as the dictionary
    key, here just a few attributes are needed

    .. py:attribute:: node

        The node identity for the place of interest in the network

    .. py::attribute:: name

        The name of the place of interest

    .. py::attribute:: weight

        The weight for it during the place allocation. The probability of being
        selected.

    """"""

    __slots__ = [
        'node',
        'name',
        'weight',
        ]

    def __init__(self, node, name, weight):

        """"""Initializes the place instance""""""

        self.node = node
        self.name = name
        self.weight = weight


#
# Home generation
# ---------------
#
# The home generation is different from all the other places, since it is going
# to be based on the existance of residential road, rather than specific
# locations on the map.
#
# The generation code will iterate over all the nodes of the graph, find out
# the total length of residential road edges on it, and use that length as the
# weight. For nodes with no residential road, no people will live there.
#

def _gen_homes(net):

    """"""Generates a list of homes for a given network

    :param net: The NetworkX graph for the simulation
    :returns: A list of :py:class:`Place` instances for the homes

    """"""

    homes = []

    for node in net.nodes_iter():

        weight = 0.0
        for road in net[node].itervalues():
            if road['highway'] == 'residential':
                weight += road['length']

        # 0.01 is an arbitrary delta to skip nodes with few residents
        if weight > 0.01:
            homes.append(
                Place(node, 'home', weight)
                )

    return homes


#
# Some constance for place generation
# -----------------------------------
#


```"
7f1109d38f9bc2f973410f071c97ad874dd6cb0d,minicps/topology.py,minicps/topology.py,"""""""
Recreate the SWaT network with the highest level of precision.
""""""

from mininet.net import Mininet
from mininet.topo import Topo
from minicps import constants as c


class EthRing(Topo):

    """"""Docstring for EthRing. """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)

        pass


class EthStar(Topo):

    """"""Docstring for EthStar. """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)

        pass


class Minicps(Mininet):

    """"""Docstring for Minicps. """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Mininet.__init__(self)

        pass
","""""""
Recreate the SWaT network with the highest level of precision.

DMZ AP, L3, L2 L1  wireless star networks and L0 wireless DLR
cannot be simulated because miniet lacks wireless (IEEE 802.11)
simulation support.
""""""

from mininet.net import Mininet
from mininet.topo import Topo
# from minicps import constants as c


class DLR(Topo):

    """"""Device Level Ring Topology.""""""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)

        pass


class EthStar(Topo):

    """"""Docstring for EthStar. """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)

        pass


class L3EthStar(Topo):

    """"""
    Connects Historian, Workstation and process PLCs
    using a 5-port ethernet switch.
    An industrial firewall service router filter the traffic.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class L2EthStar(Topo):

    """"""
    Connects HMI and process PLCs
    using a 5-ports ethernet switches and
    16-ports ethernet switches.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class L1EthStar(Topo):

    """"""
    Connects process PLCs
    using a 5-ports ethernet switches and
    16-ports ethernet switches.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class L0DLR(DLR):

    """"""
    One for each sub-process (6 in total)
    It connects redundant PLCs, sensors and actuators
    using a remote IO adaptor.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class Minicps(Mininet):

    """"""Docstring for Minicps. """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Mininet.__init__(self)

        pass
",Add swat network layers class.,"Add swat network layers class.
",Python,mit,"scy-phy/minicps,scy-phy/minicps,remmihsorp/minicps,remmihsorp/minicps",97,"```python
""""""
Recreate the SWaT network with the highest level of precision.

DMZ AP, L3, L2 L1  wireless star networks and L0 wireless DLR
cannot be simulated because miniet lacks wireless (IEEE 802.11)
simulation support.
""""""

from mininet.net import Mininet
from mininet.topo import Topo
# from minicps import constants as c


class DLR(Topo):

    """"""Device Level Ring Topology.""""""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)

        pass


class EthStar(Topo):

    """"""Docstring for EthStar. """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)

        pass


class L3EthStar(Topo):

    """"""
    Connects Historian, Workstation and process PLCs
    using a 5-port ethernet switch.
    An industrial firewall service router filter the traffic.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class L2EthStar(Topo):

    """"""
    Connects HMI and process PLCs
    using a 5-ports ethernet switches and
    16-ports ethernet switches.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class L1EthStar(Topo):

    """"""
    Connects process PLCs
    using a 5-ports ethernet switches and
    16-ports ethernet switches.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class L0DLR(DLR):

    """"""
    One for each sub-process (6 in total)
    It connects redundant PLCs, sensors and actuators
    using a remote IO adaptor.
    """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Topo.__init__(self)


class Minicps(Mininet):

    """"""Docstring for Minicps. """"""

    def __init__(self):
        """"""TODO: to be defined1. """"""
        Mininet.__init__(self)

        pass

```"
dfef23d834ab67acf91dcefd6fe39e089c71fb9a,quantized_mesh_tile/__init__.py,quantized_mesh_tile/__init__.py,,"""""""
This module provides high level utility functions to encode and decode a terrain tile.

Reference
---------
""""""

from .terrain import TerrainTile
from .topology import TerrainTopology


def encode(geometries, bounds=[], watermask=[], hasLighting=False, gzipped=False):
    """"""
    Function to convert geometries in a quantized-mesh encoded string buffer.

    Arguments:

    ``geometries``

        A list of shapely polygon geometries representing 3 dimensional triangles.
        or
        A list of WKT or WKB Polygons representing 3 dimensional triangles.
        or
        A list of triplet of vertices using the following structure:
        ``(((lon0/lat0/height0),(...),(lon2,lat2,height2)),(...))``

    ``bounds``

        The bounds of the terrain tile. (west, south, east, north)
        If not defined, the bounds will be computed from the provided geometries.

        Default is `[]`.

    ``hasLighting``

        Indicate whether unit vectors should be computed for the lighting extension.

        Default is `False`.

    ``watermask``

        A water mask list (Optional). Adds rendering water effect.
        The water mask list is either one byte, `[0]` for land and `[255]` for
        water, either a list of 256*256 values ranging from 0 to 255.
        Values in the mask are defined from north-to-south and west-to-east.
        Per default no watermask is applied. Note that the water mask effect depends on
        the texture of the raster layer drapped over your terrain.

        Default is `[]`.


    ``gzipped``

        Indicate if the tile content is gzipped.

        Default is `False`.

    """"""
    topology = TerrainTopology(geometries=geometries, hasLighting=hasLighting)
    if len(bounds) == 4:
        west, south, east, north = bounds
        tile = TerrainTile(watermask=watermask,
            west=west, south=south, east=east, north=north, topology=topology)
    else:
        tile = TerrainTile(watermask=watermask, topology=topology)
    return tile.toStringIO(gzipped=gzipped)


def decode(filePath, bounds, hasLighting=False, hasWatermask=False, gzipped=False):
    """"""
    Function to convert a quantized-mesh terrain tile file into a
    :class:`quantized_mesh_tile.terrain.TerrainTile` instance.

    Arguments:

    ``filePath``

        An absolute or relative path to write the terrain tile. (Required)
    
    ``bounds``

        The bounds of the terrain tile. (west, south, east, north) (Required).

    ``hasLighting``

        Indicate whether the tile has the lighting extension.

        Default is `False`.

    ``hasWatermask``

        Indicate whether the tile has the water-mask extension.

        Default is `False`.

    """"""
    west, south, east, north = bounds
    tile = TerrainTile(west=west, south=south, east=east, north=north)
    tile.fromFile(
        filePath, hasLighting=hasLighting, hasWatermask=hasWatermask, gzipped=gzipped)
    return tile
",Add higher level functions encode and decode,"Add higher level functions encode and decode
",Python,mit,loicgasser/quantized-mesh-tile,102,"```python
""""""
This module provides high level utility functions to encode and decode a terrain tile.

Reference
---------
""""""

from .terrain import TerrainTile
from .topology import TerrainTopology


def encode(geometries, bounds=[], watermask=[], hasLighting=False, gzipped=False):
    """"""
    Function to convert geometries in a quantized-mesh encoded string buffer.

    Arguments:

    ``geometries``

        A list of shapely polygon geometries representing 3 dimensional triangles.
        or
        A list of WKT or WKB Polygons representing 3 dimensional triangles.
        or
        A list of triplet of vertices using the following structure:
        ``(((lon0/lat0/height0),(...),(lon2,lat2,height2)),(...))``

    ``bounds``

        The bounds of the terrain tile. (west, south, east, north)
        If not defined, the bounds will be computed from the provided geometries.

        Default is `[]`.

    ``hasLighting``

        Indicate whether unit vectors should be computed for the lighting extension.

        Default is `False`.

    ``watermask``

        A water mask list (Optional). Adds rendering water effect.
        The water mask list is either one byte, `[0]` for land and `[255]` for
        water, either a list of 256*256 values ranging from 0 to 255.
        Values in the mask are defined from north-to-south and west-to-east.
        Per default no watermask is applied. Note that the water mask effect depends on
        the texture of the raster layer drapped over your terrain.

        Default is `[]`.


    ``gzipped``

        Indicate if the tile content is gzipped.

        Default is `False`.

    """"""
    topology = TerrainTopology(geometries=geometries, hasLighting=hasLighting)
    if len(bounds) == 4:
        west, south, east, north = bounds
        tile = TerrainTile(watermask=watermask,
            west=west, south=south, east=east, north=north, topology=topology)
    else:
        tile = TerrainTile(watermask=watermask, topology=topology)
    return tile.toStringIO(gzipped=gzipped)


def decode(filePath, bounds, hasLighting=False, hasWatermask=False, gzipped=False):
    """"""
    Function to convert a quantized-mesh terrain tile file into a
    :class:`quantized_mesh_tile.terrain.TerrainTile` instance.

    Arguments:

    ``filePath``

        An absolute or relative path to write the terrain tile. (Required)
    
    ``bounds``

        The bounds of the terrain tile. (west, south, east, north) (Required).

    ``hasLighting``

        Indicate whether the tile has the lighting extension.

        Default is `False`.

    ``hasWatermask``

        Indicate whether the tile has the water-mask extension.

        Default is `False`.

    """"""
    west, south, east, north = bounds
    tile = TerrainTile(west=west, south=south, east=east, north=north)
    tile.fromFile(
        filePath, hasLighting=hasLighting, hasWatermask=hasWatermask, gzipped=gzipped)
    return tile

```"
d76cb1aa296bc800cb24427110910a038a62a311,vctk/__init__.py,vctk/__init__.py,,"# coding: utf-8

from interface import *


class SpeechParameters(object):

    """"""
    Speech parameters
    """"""

    def __init__(self, f0, spectrum_envelope, aperiodicity):
        self.f0 = f0
        self.spectrum_envelope = spectrum_envelope
        self.aperiodicity = aperiodicity


class VoiceConverter(object):

    """"""
    Voice conversion

    This class assumes:
      - *_parameterizer implements `Parameterizer`
      - *_converter implements `Converter`
      - analyzer implements `Analyzer`
      - synthesizer implments `Synthesizer`

    analyzer and synthesizer must be specified explicitly.

    *_parameterizer and *_converter can be None.

    TODO:
    parameterizerは、デフォでTrasparentParameterizer
    （つまり特徴量をそのままパスするだけのparamterizer）にする？
    """"""

    def __init__(self,
                 f0_parameterizer=None,
                 f0_converter=None,
                 spectrum_envelope_parameterizer=None,
                 spectrum_envelope_converter=None,
                 aperiodicity_parameterizer=None,
                 aperiodicity_converter=None,
                 analyzer=None,
                 synthesizer=None
                 ):
        self.f0_converter = f0_converter
        self.f0_parameterizer = f0_parameterizer
        self.spectrum_envelope_converter = spectrum_envelope_converter
        self.spectrum_envelope_parameterizer = spectrum_envelope_parameterizer
        self.aperiodicity_converter = aperiodicity_converter
        self.aperiodicity_parameterizer = aperiodicity_parameterizer

        if analyzer == None or synthesizer == None:
            raise ""backend must be specified explicitly!""

        self.analyzer = analyzer
        self.synthesizer = synthesizer

        # speech paramters will be stored.
        self.params = None

    def analyze(self, x):
        """"""
        Decompose speech into parametric representation
        """"""
        self.params = self.analyzer.analyze(x)

    def convert(self):
        """"""
        Perform speech parameter conversion
        """"""
        if self.params == None:
            raise ""`analyze` must be called before `convert`""

        if self.f0_converter != None:
            self.params.f0 = self.f0_parameterizer.backward(
                self.f0_converter.convert(
                    self.f0_parameterizer.forward(self.params.f0)
                )
            )

        if self.spectrum_envelope_converter != None:
            self.params.spectrum_envelop = \
                self.spectrum_envelope_parameterizer.backward(
                    self.spectrum_envelope_converter.convert(
                        self.spectrum_envelope_parameterizer.forward(
                            self.params.spectrum_envelope
                        )
                    )
                )

        if self.aperiodicity_converter != None:
            self.params.aperiodicity = self.aperiodicity_parameterizer.backward(
                self.aperiodicity_converter.convert(
                    self.aperiodicity_parameterizer.forward(
                        self.params.aperiodicity)
                )
            )

    def synthesis(self):
        """"""
        Synthesize speech waveform
        """"""
        if self.params == None:
            raise ""`analyze` must be called before `synthesis`""

        return self.synthesizer.synthesis(self.params)
","Add class `VoiceConverter` that peforms all required processes in statistical voice conversion: speech analysis, feature parameterization, feature conversion and waveform syntheis","Add class `VoiceConverter` that peforms all required processes in statistical voice conversion: speech analysis, feature parameterization, feature conversion and waveform syntheis
",Python,mit,k2kobayashi/sprocket,110,"```python
# coding: utf-8

from interface import *


class SpeechParameters(object):

    """"""
    Speech parameters
    """"""

    def __init__(self, f0, spectrum_envelope, aperiodicity):
        self.f0 = f0
        self.spectrum_envelope = spectrum_envelope
        self.aperiodicity = aperiodicity


class VoiceConverter(object):

    """"""
    Voice conversion

    This class assumes:
      - *_parameterizer implements `Parameterizer`
      - *_converter implements `Converter`
      - analyzer implements `Analyzer`
      - synthesizer implments `Synthesizer`

    analyzer and synthesizer must be specified explicitly.

    *_parameterizer and *_converter can be None.

    TODO:
    parameterizerは、デフォでTrasparentParameterizer
    （つまり特徴量をそのままパスするだけのparamterizer）にする？
    """"""

    def __init__(self,
                 f0_parameterizer=None,
                 f0_converter=None,
                 spectrum_envelope_parameterizer=None,
                 spectrum_envelope_converter=None,
                 aperiodicity_parameterizer=None,
                 aperiodicity_converter=None,
                 analyzer=None,
                 synthesizer=None
                 ):
        self.f0_converter = f0_converter
        self.f0_parameterizer = f0_parameterizer
        self.spectrum_envelope_converter = spectrum_envelope_converter
        self.spectrum_envelope_parameterizer = spectrum_envelope_parameterizer
        self.aperiodicity_converter = aperiodicity_converter
        self.aperiodicity_parameterizer = aperiodicity_parameterizer

        if analyzer == None or synthesizer == None:
            raise ""backend must be specified explicitly!""

        self.analyzer = analyzer
        self.synthesizer = synthesizer

        # speech paramters will be stored.
        self.params = None

    def analyze(self, x):
        """"""
        Decompose speech into parametric representation
        """"""
        self.params = self.analyzer.analyze(x)

    def convert(self):
        """"""
        Perform speech parameter conversion
        """"""
        if self.params == None:
            raise ""`analyze` must be called before `convert`""

        if self.f0_converter != None:
            self.params.f0 = self.f0_parameterizer.backward(
                self.f0_converter.convert(
                    self.f0_parameterizer.forward(self.params.f0)
                )
            )

        if self.spectrum_envelope_converter != None:
            self.params.spectrum_envelop = \
                self.spectrum_envelope_parameterizer.backward(
                    self.spectrum_envelope_converter.convert(
                        self.spectrum_envelope_parameterizer.forward(
                            self.params.spectrum_envelope
                        )
                    )
                )

        if self.aperiodicity_converter != None:
            self.params.aperiodicity = self.aperiodicity_parameterizer.backward(
                self.aperiodicity_converter.convert(
                    self.aperiodicity_parameterizer.forward(
                        self.params.aperiodicity)
                )
            )

    def synthesis(self):
        """"""
        Synthesize speech waveform
        """"""
        if self.params == None:
            raise ""`analyze` must be called before `synthesis`""

        return self.synthesizer.synthesis(self.params)

```"
0dc3e4ffe86f25697799b8092822a8d77a22493b,pi_mqtt_gpio/__init__.py,pi_mqtt_gpio/__init__.py,"import sys
print(""FATAL ERROR: The file at pi_mqtt_gpio/__init__.py should be replaced us""
      ""ing 'make schema' before packaging."")
sys.exit(1)
","import yaml

CONFIG_SCHEMA = yaml.load(""""""
mqtt:
  type: dict
  required: yes
  schema:
    host:
      type: string
      empty: no
      required: no
      default: localhost
    port:
      type: integer
      min: 1
      max: 65535
      required: no
      default: 1883
    user:
      type: string
      required: no
      default: """"
    password:
      type: string
      required: no
      default: """"
    client_id:
      type: string
      required: no
      default: """"
    topic_prefix:
      type: string
      required: no
      default: """"
      coerce: rstrip_slash
    protocol:
      type: string
      required: no
      empty: no
      coerce: tostring
      default: ""3.1.1""
      allowed:
        - ""3.1""
        - ""3.1.1""
    status_topic:
      type: string
      required: no
      default: status
    status_payload_running:
      type: string
      required: no
      default: running
    status_payload_stopped:
      type: string
      required: no
      default: stopped
    status_payload_dead:
      type: string
      required: no
      default: dead

gpio_modules:
  type: list
  required: yes
  schema:
    type: dict
    allow_unknown: yes
    schema:
      name:
        type: string
        required: yes
        empty: no
      module:
        type: string
        required: yes
        empty: no
      cleanup:
        type: boolean
        required: no
        default: yes

digital_inputs:
  type: list
  required: no
  default: []
  schema:
    type: dict
    schema:
      name:
        type: string
        required: yes
        empty: no
      module:
        type: string
        required: yes
        empty: no
      pin:
        type: integer
        required: yes
        min: 0
      on_payload:
        type: string
        required: yes
        empty: no
      off_payload:
        type: string
        required: yes
        empty: no
      pullup:
        type: boolean
        required: no
        default: no
      pulldown:
        type: boolean
        required: no
        default: no
      retain:
        type: boolean
        required: no
        default: no

digital_outputs:
  type: list
  required: no
  default: []
  schema:
    type: dict
    schema:
      name:
        type: string
        required: yes
      module:
        type: string
        required: yes
      pin:
        type: integer
        required: yes
        min: 0
      on_payload:
        type: string
        required: no
        empty: no
      off_payload:
        type: string
        required: no
        empty: no
      inverted:
        type: boolean
        required: no
        default: no
      initial:
        type: string
        required: no
        allowed:
          - high
          - low
      retain:
        type: boolean
        required: no
        default: no


"""""")
",Add schema to repo for now,"Add schema to repo for now
",Python,mit,flyte/pi-mqtt-gpio,164,"```python
import yaml

CONFIG_SCHEMA = yaml.load(""""""
mqtt:
  type: dict
  required: yes
  schema:
    host:
      type: string
      empty: no
      required: no
      default: localhost
    port:
      type: integer
      min: 1
      max: 65535
      required: no
      default: 1883
    user:
      type: string
      required: no
      default: """"
    password:
      type: string
      required: no
      default: """"
    client_id:
      type: string
      required: no
      default: """"
    topic_prefix:
      type: string
      required: no
      default: """"
      coerce: rstrip_slash
    protocol:
      type: string
      required: no
      empty: no
      coerce: tostring
      default: ""3.1.1""
      allowed:
        - ""3.1""
        - ""3.1.1""
    status_topic:
      type: string
      required: no
      default: status
    status_payload_running:
      type: string
      required: no
      default: running
    status_payload_stopped:
      type: string
      required: no
      default: stopped
    status_payload_dead:
      type: string
      required: no
      default: dead

gpio_modules:
  type: list
  required: yes
  schema:
    type: dict
    allow_unknown: yes
    schema:
      name:
        type: string
        required: yes
        empty: no
      module:
        type: string
        required: yes
        empty: no
      cleanup:
        type: boolean
        required: no
        default: yes

digital_inputs:
  type: list
  required: no
  default: []
  schema:
    type: dict
    schema:
      name:
        type: string
        required: yes
        empty: no
      module:
        type: string
        required: yes
        empty: no
      pin:
        type: integer
        required: yes
        min: 0
      on_payload:
        type: string
        required: yes
        empty: no
      off_payload:
        type: string
        required: yes
        empty: no
      pullup:
        type: boolean
        required: no
        default: no
      pulldown:
        type: boolean
        required: no
        default: no
      retain:
        type: boolean
        required: no
        default: no

digital_outputs:
  type: list
  required: no
  default: []
  schema:
    type: dict
    schema:
      name:
        type: string
        required: yes
      module:
        type: string
        required: yes
      pin:
        type: integer
        required: yes
        min: 0
      on_payload:
        type: string
        required: no
        empty: no
      off_payload:
        type: string
        required: no
        empty: no
      inverted:
        type: boolean
        required: no
        default: no
      initial:
        type: string
        required: no
        allowed:
          - high
          - low
      retain:
        type: boolean
        required: no
        default: no


"""""")

```"
95c7037a4a1e9c3921c3b4584046824ed469ae7f,osfclient/tests/test_session.py,osfclient/tests/test_session.py,"from osfclient.models import OSFSession


def test_basic_auth():
    session = OSFSession()
    session.basic_auth('joe@example.com', 'secret_password')
    assert session.auth == ('joe@example.com', 'secret_password')
    assert 'Authorization' not in session.headers


def test_basic_build_url():
    session = OSFSession()
    url = session.build_url(""some"", ""path"")
    assert url.startswith(session.base_url)
    assert url.endswith(""/some/path/"")
","from unittest.mock import patch
from unittest.mock import MagicMock

import pytest

from osfclient.models import OSFSession
from osfclient.exceptions import UnauthorizedException


def test_basic_auth():
    session = OSFSession()
    session.basic_auth('joe@example.com', 'secret_password')
    assert session.auth == ('joe@example.com', 'secret_password')
    assert 'Authorization' not in session.headers


def test_basic_build_url():
    session = OSFSession()
    url = session.build_url(""some"", ""path"")
    assert url.startswith(session.base_url)
    assert url.endswith(""/some/path/"")


@patch('osfclient.models.session.requests.Session.put')
def test_unauthorized_put(mock_put):
    mock_response = MagicMock()
    mock_response.status_code = 401

    mock_put.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    with pytest.raises(UnauthorizedException):
        session.put(url)

    mock_put.assert_called_once_with(url)


@patch('osfclient.models.session.requests.Session.get')
def test_unauthorized_get(mock_get):
    mock_response = MagicMock()
    mock_response.status_code = 401

    mock_get.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    with pytest.raises(UnauthorizedException):
        session.get(url)

    mock_get.assert_called_once_with(url)


@patch('osfclient.models.session.requests.Session.put')
def test_put(mock_put):
    mock_response = MagicMock()
    mock_response.status_code = 200

    mock_put.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    response = session.put(url)

    assert response == mock_response
    mock_put.assert_called_once_with(url)


@patch('osfclient.models.session.requests.Session.get')
def test_get(mock_get):
    mock_response = MagicMock()
    mock_response.status_code = 200

    mock_get.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    response = session.get(url)

    assert response == mock_response
    mock_get.assert_called_once_with(url)
",Add test for osfclient's session object,"Add test for osfclient's session object

Check that exceptions are raised on unauthed HTTP put/get
",Python,bsd-3-clause,"betatim/osf-cli,betatim/osf-cli",90,"```python
from unittest.mock import patch
from unittest.mock import MagicMock

import pytest

from osfclient.models import OSFSession
from osfclient.exceptions import UnauthorizedException


def test_basic_auth():
    session = OSFSession()
    session.basic_auth('joe@example.com', 'secret_password')
    assert session.auth == ('joe@example.com', 'secret_password')
    assert 'Authorization' not in session.headers


def test_basic_build_url():
    session = OSFSession()
    url = session.build_url(""some"", ""path"")
    assert url.startswith(session.base_url)
    assert url.endswith(""/some/path/"")


@patch('osfclient.models.session.requests.Session.put')
def test_unauthorized_put(mock_put):
    mock_response = MagicMock()
    mock_response.status_code = 401

    mock_put.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    with pytest.raises(UnauthorizedException):
        session.put(url)

    mock_put.assert_called_once_with(url)


@patch('osfclient.models.session.requests.Session.get')
def test_unauthorized_get(mock_get):
    mock_response = MagicMock()
    mock_response.status_code = 401

    mock_get.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    with pytest.raises(UnauthorizedException):
        session.get(url)

    mock_get.assert_called_once_with(url)


@patch('osfclient.models.session.requests.Session.put')
def test_put(mock_put):
    mock_response = MagicMock()
    mock_response.status_code = 200

    mock_put.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    response = session.put(url)

    assert response == mock_response
    mock_put.assert_called_once_with(url)


@patch('osfclient.models.session.requests.Session.get')
def test_get(mock_get):
    mock_response = MagicMock()
    mock_response.status_code = 200

    mock_get.return_value = mock_response

    url = 'http://example.com/foo'

    session = OSFSession()

    response = session.get(url)

    assert response == mock_response
    mock_get.assert_called_once_with(url)

```"
eb368c344075ce78606d4656ebfb19c7e7ccdf50,src/054.py,src/054.py,"from path import dirpath


def ans():
    lines = open(dirpath() + '054.txt').readlines()
    cards = [line.strip().split() for line in lines]
    
    return None
    

if __name__ == '__main__':
    print(ans())
","from collections import (
    defaultdict,
    namedtuple,
)
from path import dirpath


def _value(rank):
    try:
        return int(rank)
    except ValueError:
        return 10 + 'TJQKA'.index(rank)


def _sort_by_rank(hand):
    return list(reversed(sorted(
        hand,
        key=lambda card: _value(card[0]),
    )))

    
def _of_a_kind(hand, count):
    counts = defaultdict(list)
    for card in hand:
        counts[card[0]].append(card)
    filtered = {
        rank: cards for
        rank, cards in counts.items() if
        count <= len(cards)
    }
    if len(filtered) < 1:
        return None
    return max(
        filtered.values(),
        key=lambda cards: _value(cards[0][0])
    )


def high_card(hand):
    return _of_a_kind(hand, 1)


def two_of_a_kind(hand):
    return _of_a_kind(hand, 2)


def three_of_a_kind(hand):
    return _of_a_kind(hand, 3)


def four_of_a_kind(hand):
    return _of_a_kind(hand, 4)


def full_house(hand):
    three = three_of_a_kind(hand)
    if not three:
        return None
    pair = two_of_a_kind([card for card in hand if card not in three])
    if not pair:
        return None
    return three + pair


def straight(hand):
    sorted_ = sorted([_value(card[0]) for card in hand])
    if sorted_ == list(range(sorted_[0], sorted_[-1] + 1)):
        return _sort_by_rank(hand)
    return None


def flush(hand):
    counts = defaultdict(list)
    for card in hand:
        counts[card[1]].append(card)
    for cards in counts.values():
        if len(cards) == 5:
            return _sort_by_rank(cards)
    return None


def straight_flush(hand):
    return flush(hand) if straight(hand) else None


def ans():
    lines = open(dirpath() + '054.txt').readlines()
    turns = [line.strip().split() for line in lines]
    num_wins = 0
    for cards in turns:
        one = cards[:5]
        two = cards[5:]
    
    return None
    

if __name__ == '__main__':
    print(ans())
",Write some logic for 54,"Write some logic for 54
",Python,mit,mackorone/euler,99,"```python
from collections import (
    defaultdict,
    namedtuple,
)
from path import dirpath


def _value(rank):
    try:
        return int(rank)
    except ValueError:
        return 10 + 'TJQKA'.index(rank)


def _sort_by_rank(hand):
    return list(reversed(sorted(
        hand,
        key=lambda card: _value(card[0]),
    )))

    
def _of_a_kind(hand, count):
    counts = defaultdict(list)
    for card in hand:
        counts[card[0]].append(card)
    filtered = {
        rank: cards for
        rank, cards in counts.items() if
        count <= len(cards)
    }
    if len(filtered) < 1:
        return None
    return max(
        filtered.values(),
        key=lambda cards: _value(cards[0][0])
    )


def high_card(hand):
    return _of_a_kind(hand, 1)


def two_of_a_kind(hand):
    return _of_a_kind(hand, 2)


def three_of_a_kind(hand):
    return _of_a_kind(hand, 3)


def four_of_a_kind(hand):
    return _of_a_kind(hand, 4)


def full_house(hand):
    three = three_of_a_kind(hand)
    if not three:
        return None
    pair = two_of_a_kind([card for card in hand if card not in three])
    if not pair:
        return None
    return three + pair


def straight(hand):
    sorted_ = sorted([_value(card[0]) for card in hand])
    if sorted_ == list(range(sorted_[0], sorted_[-1] + 1)):
        return _sort_by_rank(hand)
    return None


def flush(hand):
    counts = defaultdict(list)
    for card in hand:
        counts[card[1]].append(card)
    for cards in counts.values():
        if len(cards) == 5:
            return _sort_by_rank(cards)
    return None


def straight_flush(hand):
    return flush(hand) if straight(hand) else None


def ans():
    lines = open(dirpath() + '054.txt').readlines()
    turns = [line.strip().split() for line in lines]
    num_wins = 0
    for cards in turns:
        one = cards[:5]
        two = cards[5:]
    
    return None
    

if __name__ == '__main__':
    print(ans())

```"
8a16f7d51d34573d41846da25664b084ff6eb071,publication_figures.py,publication_figures.py,,"""""""
 Create nice looking publication figures
""""""

import numpy as np
import networkx as nx

import seaborn as sns
import matplotlib as mpl
import matplotlib.pylab as plt

from tqdm import tqdm

from main import analyze_system
from setup import generate_basic_system
from nm_data_generator import add_node_to_system


def visualize_node_influence():
    """""" Compare examples where fourth node perturbs system and where it doesn't
    """"""
    def simulate(syst, reps=1000):
        matrices = []
        with tqdm(total=reps) as pbar:
            while reps >= 0:
                _, mat, _ = analyze_system(syst, repetition_num=1)
                if mat is None:
                    continue
                pbar.update()
                reps -= 1

                if mat.shape == (4, 4):
                    mat = mat[:-1, :-1]
                assert mat.shape == (3, 3)

                matrices.append(mat)
        return np.asarray(matrices)

    def plot_correlation_hist(matrices, ax):
        for i, row in enumerate(matrices.T):
            for j, series in enumerate(row):
                if i == j: break

                sns.distplot(series, ax=ax, label=r'$c_{{{},{}}}$'.format(i,j))

        ax.set_xlabel('correlation')
        ax.set_ylabel('count')
        ax.set_xlim((-1,1))
        ax.legend(loc='best')

    def plot_system(syst, ax):
        graph = nx.from_numpy_matrix(syst.jacobian.T, create_using=nx.DiGraph())
        nx.draw(
            graph, ax=ax,
            with_labels=True)
        ax.axis('off')
        ax.set_xticks([], [])
        ax.set_yticks([], [])

    # generate systems
    basic_system = generate_basic_system()
    more_systs = add_node_to_system(basic_system)

    similar_system = more_systs[42]
    different_system = more_systs[22] # 52

    systems = {
        'basic': basic_system,
        'similar': similar_system,
        'different_system': different_system
    }

    # simulations
    matrices = {}
    for name, syst in systems.items():
        matrices[name] = (syst, simulate(syst))

    # plot result
    for name, (syst, mats) in matrices.items():
        plt.figure()
        plot_correlation_hist(mats, plt.gca())
        plot_system(syst, plt.axes([.3,.5,.3,.3]))
        plt.savefig(f'images/node_influence_{name}.pdf')

def main():
    visualize_node_influence()


if __name__ == '__main__':
    sns.set_style('white')
    plt.style.use('seaborn-poster')

    main()
",Create some more nice figures,"Create some more nice figures
",Python,mit,"kpj/SDEMotif,kpj/SDEMotif",94,"```python
""""""
 Create nice looking publication figures
""""""

import numpy as np
import networkx as nx

import seaborn as sns
import matplotlib as mpl
import matplotlib.pylab as plt

from tqdm import tqdm

from main import analyze_system
from setup import generate_basic_system
from nm_data_generator import add_node_to_system


def visualize_node_influence():
    """""" Compare examples where fourth node perturbs system and where it doesn't
    """"""
    def simulate(syst, reps=1000):
        matrices = []
        with tqdm(total=reps) as pbar:
            while reps >= 0:
                _, mat, _ = analyze_system(syst, repetition_num=1)
                if mat is None:
                    continue
                pbar.update()
                reps -= 1

                if mat.shape == (4, 4):
                    mat = mat[:-1, :-1]
                assert mat.shape == (3, 3)

                matrices.append(mat)
        return np.asarray(matrices)

    def plot_correlation_hist(matrices, ax):
        for i, row in enumerate(matrices.T):
            for j, series in enumerate(row):
                if i == j: break

                sns.distplot(series, ax=ax, label=r'$c_{{{},{}}}$'.format(i,j))

        ax.set_xlabel('correlation')
        ax.set_ylabel('count')
        ax.set_xlim((-1,1))
        ax.legend(loc='best')

    def plot_system(syst, ax):
        graph = nx.from_numpy_matrix(syst.jacobian.T, create_using=nx.DiGraph())
        nx.draw(
            graph, ax=ax,
            with_labels=True)
        ax.axis('off')
        ax.set_xticks([], [])
        ax.set_yticks([], [])

    # generate systems
    basic_system = generate_basic_system()
    more_systs = add_node_to_system(basic_system)

    similar_system = more_systs[42]
    different_system = more_systs[22] # 52

    systems = {
        'basic': basic_system,
        'similar': similar_system,
        'different_system': different_system
    }

    # simulations
    matrices = {}
    for name, syst in systems.items():
        matrices[name] = (syst, simulate(syst))

    # plot result
    for name, (syst, mats) in matrices.items():
        plt.figure()
        plot_correlation_hist(mats, plt.gca())
        plot_system(syst, plt.axes([.3,.5,.3,.3]))
        plt.savefig(f'images/node_influence_{name}.pdf')

def main():
    visualize_node_influence()


if __name__ == '__main__':
    sns.set_style('white')
    plt.style.use('seaborn-poster')

    main()

```"
91c35078c7a8aad153d9aabe0b02fc3c48cfc76a,hesiod.py,hesiod.py,"#!/usr/bin/env python

from _hesiod import bind, resolve
","#!/usr/bin/env python

""""""
Present both functional and object-oriented interfaces for executing
lookups in Hesiod, Project Athena's service name resolution protocol.
""""""

from _hesiod import bind, resolve

from pwd import struct_passwd

class HesiodParseError(Exception):
    pass

class Lookup(object):
    """"""
    A Generic Hesiod lookup
    """"""
    def __init__(self, hes_name, hes_type):
        self.results = resolve(hes_name, hes_type)
        self.parseRecords()
    
    def parseRecords(self):
        pass

class FilsysLookup(Lookup):
    def __init__(self, name):
        Lookup.__init__(self, name, 'filsys')
    
    def parseRecords(self):
        Lookup.parseRecords(self)
        
        self.filsys = []
        self.multiRecords = (len(self.results) > 1)
        
        for result in self.results:
            priority = 0
            if self.multiRecords:
                result, priority = result.rsplit("" "", 1)
                priority = int(priority)
            
            parts = result.split("" "")
            type = parts[0]
            if type == 'AFS':
                self.filsys.append(dict(type=type,
                                        location=parts[1],
                                        mode=parts[2],
                                        mountpoint=parts[3],
                                        priority=priority))
            elif type == 'NFS':
                self.filsys.append(dict(type=type,
                                        remote_location=parts[1],
                                        server=parts[2],
                                        mode=parts[3],
                                        mountpoint=parts[4],
                                        priority=priority))
            elif type == 'ERR':
                self.filsys.append(dict(type=type,
                                        message=parts[1],
                                        priority=priority))
            elif type == 'UFS':
                self.filsys.append(dict(type=type,
                                        device=parts[1],
                                        mode=parts[2],
                                        mountpoint=parts[3],
                                        priority=priority))
            elif type == 'LOC':
                self.filsys.append(dict(type=type,
                                        location=parts[1],
                                        mode=parts[2],
                                        mountpoint=parts[3],
                                        priority=priority))
            else:
                raise HesiodParseError('Unknown filsys type: %s' % type)

class PasswdLookup(Lookup):
    def __init__(self, name):
        Lookup.__init__(self, name, 'passwd')
    
    def parseRecords(self):
        self.passwd = struct_passwd(self.results[0].split(':'))

class UidLookup(PasswdLookup):
    def __init__(self, uid):
        Lookup.__init__(self, uid, 'uid')

__all__ = ['bind', 'resolve',
           'Lookup', 'FilsysLookup', 'PasswdLookup', 'UidLookup',
           'HesiodParseError']
","Add object-oriented-style lookups for filsys, passwd, and uid lookups","Add object-oriented-style lookups for filsys, passwd, and uid lookups

The filsys entry parsing code is taken from pyHesiodFS and was explicitly
relicensed under the MIT license by Quentin Smith <quentin@mit.edu>
",Python,mit,ebroder/python-hesiod,90,"```python
#!/usr/bin/env python

""""""
Present both functional and object-oriented interfaces for executing
lookups in Hesiod, Project Athena's service name resolution protocol.
""""""

from _hesiod import bind, resolve

from pwd import struct_passwd

class HesiodParseError(Exception):
    pass

class Lookup(object):
    """"""
    A Generic Hesiod lookup
    """"""
    def __init__(self, hes_name, hes_type):
        self.results = resolve(hes_name, hes_type)
        self.parseRecords()
    
    def parseRecords(self):
        pass

class FilsysLookup(Lookup):
    def __init__(self, name):
        Lookup.__init__(self, name, 'filsys')
    
    def parseRecords(self):
        Lookup.parseRecords(self)
        
        self.filsys = []
        self.multiRecords = (len(self.results) > 1)
        
        for result in self.results:
            priority = 0
            if self.multiRecords:
                result, priority = result.rsplit("" "", 1)
                priority = int(priority)
            
            parts = result.split("" "")
            type = parts[0]
            if type == 'AFS':
                self.filsys.append(dict(type=type,
                                        location=parts[1],
                                        mode=parts[2],
                                        mountpoint=parts[3],
                                        priority=priority))
            elif type == 'NFS':
                self.filsys.append(dict(type=type,
                                        remote_location=parts[1],
                                        server=parts[2],
                                        mode=parts[3],
                                        mountpoint=parts[4],
                                        priority=priority))
            elif type == 'ERR':
                self.filsys.append(dict(type=type,
                                        message=parts[1],
                                        priority=priority))
            elif type == 'UFS':
                self.filsys.append(dict(type=type,
                                        device=parts[1],
                                        mode=parts[2],
                                        mountpoint=parts[3],
                                        priority=priority))
            elif type == 'LOC':
                self.filsys.append(dict(type=type,
                                        location=parts[1],
                                        mode=parts[2],
                                        mountpoint=parts[3],
                                        priority=priority))
            else:
                raise HesiodParseError('Unknown filsys type: %s' % type)

class PasswdLookup(Lookup):
    def __init__(self, name):
        Lookup.__init__(self, name, 'passwd')
    
    def parseRecords(self):
        self.passwd = struct_passwd(self.results[0].split(':'))

class UidLookup(PasswdLookup):
    def __init__(self, uid):
        Lookup.__init__(self, uid, 'uid')

__all__ = ['bind', 'resolve',
           'Lookup', 'FilsysLookup', 'PasswdLookup', 'UidLookup',
           'HesiodParseError']

```"
d8a46686f12de76381bf9c2cfde0e9482acc2259,Toolkit/PlayPen/rooster.py,Toolkit/PlayPen/rooster.py,"import os
import sys
import math

def main():

    # do while len(spot.xds) > 100 (say)
    # index, no unit cell / symmetry set
    # copy out indexed spots, orientation matrix
    # done

    # compare list of orientation matrices - how are they related?
    # graphical representation?
","import os
import sys
import math
import shutil

def split_spot_file(j):
    '''Split the spot file into those which have been indexed (goes to SPOT.j)
    and those which have not, which gets returned to SPOT.XDS. Returns number
    of unindexed reflections.'''

    spot_j = open('SPOT.%d' % j, 'w')
    spot_0 = open('SPOT.0', 'w')
    spot = open('SPOT.XDS', 'r')

    n_j = 0
    n_0 = 0

    for record in spot:
        if '     0       0       0' in record:
            spot_0.write(record)
            n_0 += 1
        else:
            spot_j.write(record)
            n_j += 1

    spot_j.close()
    spot_0.close()
    spot.close()

    shutil.move('SPOT.0', 'SPOT.XDS')

    return n_0, n_j

def prepare_xds_inp():

    xds_inp = open('XDS.0', 'w')

    for record in open('XDS.INP'):
        if 'SPACE_GROUP_NUMBER' in record:
            continue
        if 'UNIT_CELL_CONSTANTS' in record:
            continue
        xds_inp.write(record)

    xds_inp.close()

    shutil.move('XDS.0', 'XDS.INP')

    return

def run_xds(j):

    os.system('xds_par > /dev/null')

    shutil.copyfile('IDXREF.LP', 'IDXREF.%d' % j)
    shutil.copyfile('XPARM.XDS', 'XPARM.%d' % j)
    
    n_unindexed, n_indexed = split_spot_file(j)

    return n_unindexed, n_indexed

def get_unit_cell():
    unit_cell = map(float, open('XPARM.XDS', 'r').read().split()[27:33])
    return tuple(unit_cell)

def num_spots():
    j = 0

    for record in open('SPOT.XDS', 'r'):
        if record.strip():
            j += 1

    return j

def main():

    prepare_xds_inp()

    n_unindexed = num_spots()

    j = 0

    while n_unindexed > 100:
        j += 1
        n_unindexed, n_indexed = run_xds(j)
        if n_indexed == 0:
            break
        unit_cell = get_unit_cell()
        print '%5d %5d' % (n_indexed, n_unindexed), \
              '%6.2f %6.2f %6.2f %6.2f %6.2f %6.2f' % unit_cell
        


if __name__ == '__main__':
    main()
",Make a start on the recursive reindexing,Make a start on the recursive reindexing,Python,bsd-3-clause,"xia2/xia2,xia2/xia2",96,"```python
import os
import sys
import math
import shutil

def split_spot_file(j):
    '''Split the spot file into those which have been indexed (goes to SPOT.j)
    and those which have not, which gets returned to SPOT.XDS. Returns number
    of unindexed reflections.'''

    spot_j = open('SPOT.%d' % j, 'w')
    spot_0 = open('SPOT.0', 'w')
    spot = open('SPOT.XDS', 'r')

    n_j = 0
    n_0 = 0

    for record in spot:
        if '     0       0       0' in record:
            spot_0.write(record)
            n_0 += 1
        else:
            spot_j.write(record)
            n_j += 1

    spot_j.close()
    spot_0.close()
    spot.close()

    shutil.move('SPOT.0', 'SPOT.XDS')

    return n_0, n_j

def prepare_xds_inp():

    xds_inp = open('XDS.0', 'w')

    for record in open('XDS.INP'):
        if 'SPACE_GROUP_NUMBER' in record:
            continue
        if 'UNIT_CELL_CONSTANTS' in record:
            continue
        xds_inp.write(record)

    xds_inp.close()

    shutil.move('XDS.0', 'XDS.INP')

    return

def run_xds(j):

    os.system('xds_par > /dev/null')

    shutil.copyfile('IDXREF.LP', 'IDXREF.%d' % j)
    shutil.copyfile('XPARM.XDS', 'XPARM.%d' % j)
    
    n_unindexed, n_indexed = split_spot_file(j)

    return n_unindexed, n_indexed

def get_unit_cell():
    unit_cell = map(float, open('XPARM.XDS', 'r').read().split()[27:33])
    return tuple(unit_cell)

def num_spots():
    j = 0

    for record in open('SPOT.XDS', 'r'):
        if record.strip():
            j += 1

    return j

def main():

    prepare_xds_inp()

    n_unindexed = num_spots()

    j = 0

    while n_unindexed > 100:
        j += 1
        n_unindexed, n_indexed = run_xds(j)
        if n_indexed == 0:
            break
        unit_cell = get_unit_cell()
        print '%5d %5d' % (n_indexed, n_unindexed), \
              '%6.2f %6.2f %6.2f %6.2f %6.2f %6.2f' % unit_cell
        


if __name__ == '__main__':
    main()

```"
52f510b64e4ded6e159119b2fa544de577b7d949,classyfd/directory/directory.py,classyfd/directory/directory.py,"""""""Contains a Directory class to represent real directories""""""

from ..base import _BaseFileAndDirectoryInterface


class Directory(_BaseFileAndDirectoryInterface):
    """"""A class that groups together the (meta)data and behavior of 
    directories""""""
    def __init__(self, path):
        """"""
        Construct the object

        Parameters:
        path -- (str) where the directory is (or will be) located at. An 
                exception is raised if the path refers to a file, and also if an
                empty string is given.

        """"""        
        
        return
    
    # Special Methods
    def __repr__(self):
        pass

    def __str__(self):
        pass
    
    # Properties
    @property
    def name(self):
        pass
    
    @property
    def path(self):
        pass              
    
    @property
    def exists(self):
        pass
    
    @property
    def created_on(self):
        pass    
    
    @property
    def size(self):
        pass    
    
    @property
    def parent(self):
        pass  
    
    @property
    def owner(self):
        pass  
    
    @property
    def group(self):
        pass   
    
    # Regular Methods
    def get_parent(self):
        pass         
    
    def create(self):
        pass         
    
    def get_permissions(self):
        pass  
    
    def change_permissions(self):
        pass      
    
    def chmod(self):
        pass 
    
    def change_owner(self):
        pass       
    
    def change_group(self):
        pass             
    
    def copy(self):
        pass
    
    def move(self):
        pass
    
    def rename(self):
        pass 
    
    def remove(self):
        pass    ","""""""Contains a Directory class to represent real directories""""""

from ..base import _BaseFileAndDirectoryInterface
from ..exceptions import InvalidDirectoryValueError


class Directory(_BaseFileAndDirectoryInterface):
    """"""A class that groups together the (meta)data and behavior of 
    directories""""""
    def __init__(self, path):
        """"""
        Construct the object

        Parameters:
        path -- (str) where the directory is (or will be) located at. An 
                exception is raised if the path refers to a file, and also if an
                empty string is given.

        """"""
        if not path:
            # No point in continuing since the methods of this class assume
            # that a path will be given upon instantiation.
            raise InvalidDirectoryValueError(""No directory path was given"")        
        
        return
    
    # Special Methods
    def __repr__(self):
        pass

    def __str__(self):
        pass
    
    # Properties
    @property
    def name(self):
        pass
    
    @property
    def path(self):
        pass              
    
    @property
    def exists(self):
        pass
    
    @property
    def created_on(self):
        pass    
    
    @property
    def size(self):
        pass    
    
    @property
    def parent(self):
        pass  
    
    @property
    def owner(self):
        pass  
    
    @property
    def group(self):
        pass   
    
    # Regular Methods
    def get_parent(self):
        pass         
    
    def create(self):
        pass         
    
    def get_permissions(self):
        pass  
    
    def change_permissions(self):
        pass      
    
    def chmod(self):
        pass 
    
    def change_owner(self):
        pass       
    
    def change_group(self):
        pass             
    
    def copy(self):
        pass
    
    def move(self):
        pass
    
    def rename(self):
        pass 
    
    def remove(self):
        pass    ",Add custom exception logic to the Directory class,"Add custom exception logic to the Directory class

If an empty string was passed to the Directory class, then an exception
needs to be raised as all of its methods assume a value for the path.
",Python,mit,SizzlingVortex/classyfd,99,"```python
""""""Contains a Directory class to represent real directories""""""

from ..base import _BaseFileAndDirectoryInterface
from ..exceptions import InvalidDirectoryValueError


class Directory(_BaseFileAndDirectoryInterface):
    """"""A class that groups together the (meta)data and behavior of 
    directories""""""
    def __init__(self, path):
        """"""
        Construct the object

        Parameters:
        path -- (str) where the directory is (or will be) located at. An 
                exception is raised if the path refers to a file, and also if an
                empty string is given.

        """"""
        if not path:
            # No point in continuing since the methods of this class assume
            # that a path will be given upon instantiation.
            raise InvalidDirectoryValueError(""No directory path was given"")        
        
        return
    
    # Special Methods
    def __repr__(self):
        pass

    def __str__(self):
        pass
    
    # Properties
    @property
    def name(self):
        pass
    
    @property
    def path(self):
        pass              
    
    @property
    def exists(self):
        pass
    
    @property
    def created_on(self):
        pass    
    
    @property
    def size(self):
        pass    
    
    @property
    def parent(self):
        pass  
    
    @property
    def owner(self):
        pass  
    
    @property
    def group(self):
        pass   
    
    # Regular Methods
    def get_parent(self):
        pass         
    
    def create(self):
        pass         
    
    def get_permissions(self):
        pass  
    
    def change_permissions(self):
        pass      
    
    def chmod(self):
        pass 
    
    def change_owner(self):
        pass       
    
    def change_group(self):
        pass             
    
    def copy(self):
        pass
    
    def move(self):
        pass
    
    def rename(self):
        pass 
    
    def remove(self):
        pass    
```"
56b4532bd330ad4075f882511c87cb97eaeff10e,jujupy/__init__.py,jujupy/__init__.py,"from jujupy.client import *
from jujupy.client import _temp_env

__all__ = ['_temp_env']
","from jujupy.client import (
    AgentsNotStarted,
    AuthNotAccepted,
    AGENTS_READY,
    client_from_config,
    ConditionList,
    coalesce_agent_status,
    describe_substrate,
    EnvJujuClient,
    EnvJujuClient1X,
    EnvJujuClient25,
    ensure_dir,
    get_cache_path,
    get_client_class,
    get_local_root,
    get_machine_dns_name,
    get_timeout_path,
    get_timeout_prefix,
    GroupReporter,
    IncompatibleConfigClass,
    InvalidEndpoint,
    jes_home_path,
    JESNotSupported,
    JujuData,
    JUJU_DEV_FEATURE_FLAGS,
    Juju2Backend,
    KILL_CONTROLLER,
    KVM_MACHINE,
    LXC_MACHINE,
    LXD_MACHINE,
    Machine,
    NameNotAccepted,
    NoProvider,
    parse_new_state_server_from_error,
    SimpleEnvironment,
    SoftDeadlineExceeded,
    Status,
    temp_bootstrap_env,
    _temp_env,
    temp_yaml_file,
    TypeNotAccepted,
    uniquify_local,
    until_timeout,
    )

__all__ = [
    'AgentsNotStarted',
    'AuthNotAccepted',
    'AGENTS_READY',
    'client_from_config',
    'ConditionList',
    'coalesce_agent_status',
    'describe_substrate',
    'EnvJujuClient',
    'EnvJujuClient1X',
    'EnvJujuClient25',
    'ensure_dir',
    'get_cache_path',
    'get_client_class',
    'get_local_root',
    'get_machine_dns_name',
    'get_timeout_path',
    'get_timeout_prefix',
    'GroupReporter',
    'IncompatibleConfigClass',
    'InvalidEndpoint',
    'jes_home_path',
    'JESNotSupported',
    'JujuData',
    'JUJU_DEV_FEATURE_FLAGS',
    'Juju2Backend',
    'KILL_CONTROLLER',
    'KVM_MACHINE',
    'LXC_MACHINE',
    'LXD_MACHINE',
    'Machine',
    'NameNotAccepted',
    'NoProvider',
    'parse_new_state_server_from_error',
    'SimpleEnvironment',
    'SoftDeadlineExceeded',
    'Status',
    'temp_bootstrap_env',
    '_temp_env',
    'temp_yaml_file',
    'TypeNotAccepted',
    'uniquify_local',
    'until_timeout',
    ]
",Switch to explicit imports for jujupy.,Switch to explicit imports for jujupy.,Python,agpl-3.0,"mjs/juju,mjs/juju,mjs/juju,mjs/juju,mjs/juju,mjs/juju,mjs/juju",90,"```python
from jujupy.client import (
    AgentsNotStarted,
    AuthNotAccepted,
    AGENTS_READY,
    client_from_config,
    ConditionList,
    coalesce_agent_status,
    describe_substrate,
    EnvJujuClient,
    EnvJujuClient1X,
    EnvJujuClient25,
    ensure_dir,
    get_cache_path,
    get_client_class,
    get_local_root,
    get_machine_dns_name,
    get_timeout_path,
    get_timeout_prefix,
    GroupReporter,
    IncompatibleConfigClass,
    InvalidEndpoint,
    jes_home_path,
    JESNotSupported,
    JujuData,
    JUJU_DEV_FEATURE_FLAGS,
    Juju2Backend,
    KILL_CONTROLLER,
    KVM_MACHINE,
    LXC_MACHINE,
    LXD_MACHINE,
    Machine,
    NameNotAccepted,
    NoProvider,
    parse_new_state_server_from_error,
    SimpleEnvironment,
    SoftDeadlineExceeded,
    Status,
    temp_bootstrap_env,
    _temp_env,
    temp_yaml_file,
    TypeNotAccepted,
    uniquify_local,
    until_timeout,
    )

__all__ = [
    'AgentsNotStarted',
    'AuthNotAccepted',
    'AGENTS_READY',
    'client_from_config',
    'ConditionList',
    'coalesce_agent_status',
    'describe_substrate',
    'EnvJujuClient',
    'EnvJujuClient1X',
    'EnvJujuClient25',
    'ensure_dir',
    'get_cache_path',
    'get_client_class',
    'get_local_root',
    'get_machine_dns_name',
    'get_timeout_path',
    'get_timeout_prefix',
    'GroupReporter',
    'IncompatibleConfigClass',
    'InvalidEndpoint',
    'jes_home_path',
    'JESNotSupported',
    'JujuData',
    'JUJU_DEV_FEATURE_FLAGS',
    'Juju2Backend',
    'KILL_CONTROLLER',
    'KVM_MACHINE',
    'LXC_MACHINE',
    'LXD_MACHINE',
    'Machine',
    'NameNotAccepted',
    'NoProvider',
    'parse_new_state_server_from_error',
    'SimpleEnvironment',
    'SoftDeadlineExceeded',
    'Status',
    'temp_bootstrap_env',
    '_temp_env',
    'temp_yaml_file',
    'TypeNotAccepted',
    'uniquify_local',
    'until_timeout',
    ]

```"
9f0b9b68a3c9dfaa64942e55fc97e435b8eb6f50,bayespy/nodes/__init__.py,bayespy/nodes/__init__.py,"################################################################################
# Copyright (C) 2013 Jaakko Luttinen
#
# This file is licensed under the MIT License.
################################################################################


""""""
Package for nodes used to construct the model.

Stochastic nodes
================

.. currentmodule:: bayespy.nodes

Nodes for Gaussian variables:

.. autosummary::
   :toctree: generated/

   Gaussian
   GaussianARD

Nodes for precision and scale variables:

.. autosummary::
   :toctree: generated/

   Gamma
   Wishart
   Exponential

Nodes for modelling Gaussian and precision variables jointly (useful as prior
for Gaussian nodes):

.. autosummary::
   :toctree: generated/

   GaussianGammaISO
   GaussianGammaARD
   GaussianWishart

Nodes for discrete count variables:

.. autosummary::
   :toctree: generated/

   Bernoulli
   Binomial
   Categorical
   Multinomial
   Poisson

Nodes for probabilities:

.. autosummary::
   :toctree: generated/

   Beta
   Dirichlet

Nodes for dynamic variables:

.. autosummary::
   :toctree: generated/

   CategoricalMarkovChain
   GaussianMarkovChain
   SwitchingGaussianMarkovChain
   VaryingGaussianMarkovChain

Other stochastic nodes:

.. autosummary::
   :toctree: generated/

   Mixture



Deterministic nodes
===================

.. autosummary::
   :toctree: generated/

   Dot
   SumMultiply
   Gate
""""""

# Currently, model construction and the inference network are not separated so
# the model is constructed using variational message passing nodes.
from bayespy.inference.vmp.nodes import *

","################################################################################
# Copyright (C) 2013 Jaakko Luttinen
#
# This file is licensed under the MIT License.
################################################################################


""""""
Package for nodes used to construct the model.

Stochastic nodes
================

.. currentmodule:: bayespy.nodes

Nodes for Gaussian variables:

.. autosummary::
   :toctree: generated/

   Gaussian
   GaussianARD

Nodes for precision and scale variables:

.. autosummary::
   :toctree: generated/

   Gamma
   Wishart
   Exponential

Nodes for modelling Gaussian and precision variables jointly (useful as prior
for Gaussian nodes):

.. autosummary::
   :toctree: generated/

   GaussianGammaISO
   GaussianGammaARD
   GaussianWishart

Nodes for discrete count variables:

.. autosummary::
   :toctree: generated/

   Bernoulli
   Binomial
   Categorical
   Multinomial
   Poisson

Nodes for probabilities:

.. autosummary::
   :toctree: generated/

   Beta
   Dirichlet

Nodes for dynamic variables:

.. autosummary::
   :toctree: generated/

   CategoricalMarkovChain
   GaussianMarkovChain
   SwitchingGaussianMarkovChain
   VaryingGaussianMarkovChain

Other stochastic nodes:

.. autosummary::
   :toctree: generated/

   Mixture



Deterministic nodes
===================

.. autosummary::
   :toctree: generated/

   Dot
   SumMultiply
   Add
   Gate
""""""

# Currently, model construction and the inference network are not separated so
# the model is constructed using variational message passing nodes.
from bayespy.inference.vmp.nodes import *

",Include Add node in user API documentation,"DOC: Include Add node in user API documentation
",Python,mit,"bayespy/bayespy,jluttine/bayespy",97,"```python
################################################################################
# Copyright (C) 2013 Jaakko Luttinen
#
# This file is licensed under the MIT License.
################################################################################


""""""
Package for nodes used to construct the model.

Stochastic nodes
================

.. currentmodule:: bayespy.nodes

Nodes for Gaussian variables:

.. autosummary::
   :toctree: generated/

   Gaussian
   GaussianARD

Nodes for precision and scale variables:

.. autosummary::
   :toctree: generated/

   Gamma
   Wishart
   Exponential

Nodes for modelling Gaussian and precision variables jointly (useful as prior
for Gaussian nodes):

.. autosummary::
   :toctree: generated/

   GaussianGammaISO
   GaussianGammaARD
   GaussianWishart

Nodes for discrete count variables:

.. autosummary::
   :toctree: generated/

   Bernoulli
   Binomial
   Categorical
   Multinomial
   Poisson

Nodes for probabilities:

.. autosummary::
   :toctree: generated/

   Beta
   Dirichlet

Nodes for dynamic variables:

.. autosummary::
   :toctree: generated/

   CategoricalMarkovChain
   GaussianMarkovChain
   SwitchingGaussianMarkovChain
   VaryingGaussianMarkovChain

Other stochastic nodes:

.. autosummary::
   :toctree: generated/

   Mixture



Deterministic nodes
===================

.. autosummary::
   :toctree: generated/

   Dot
   SumMultiply
   Add
   Gate
""""""

# Currently, model construction and the inference network are not separated so
# the model is constructed using variational message passing nodes.
from bayespy.inference.vmp.nodes import *


```"
c1e9d369680e779d481aa7db17be9348d56ec29d,test_linked_list.py,test_linked_list.py,"from __future__ import unicode_literals
import linked_list

# def func(x):
#     return x + 1

# def tdest_answer():
#     assert func(3) == 5


# init

a = linked_list.LinkedList()


def test_size():
    assert a.size is 0


def test_head():
    assert a.head is None


def test_init():
    assert type(a) is linked_list.LinkedList


","""""""Pytest file for linked_list.py

Run this with the command 'py.test test_linked_list.py'
""""""


from __future__ import unicode_literals
import linked_list
import copy


# init method

a = linked_list.LinkedList()


def test_init_size():
    assert a.sizeOfList is 0
    assert type(a.sizeOfList) is int


def test_init_head():
    assert a.head is None


def test_init_type():
    assert type(a) is linked_list.LinkedList


# insert method

b = copy.copy(a)   # make a copy every time a change is made
b.insert(5)        # so the test can handle different values


def test_insert_size():
    assert b.sizeOfList is 1


def test_insert_head():
    assert b.head.value is 5


def test_insert_next():
    assert b.head.next_node is None


c = copy.copy(b)
c.insert(6)


def test_insert_new_size():
    assert c.sizeOfList is 2


def test_insert_new_head():
    assert c.head.value is 6


def test_insert_pointer():
    assert c.head.next_node.value is 5


# size method

def test_size():
    assert c.size() is 2


# search method

def test_search_value_in_list():
    assert c.search(5).value is 5


def test_search_value_not_in_list():
    assert c.search(7) is None       # 7 is not in the list


# remove method

d = copy.copy(c)
d.remove(d.search(6))


def test_remove_value():
    assert d.search(6) is None


def test_remove_size():
    assert d.size() is 1


# display method

def test_display():
    assert d.display() == (5,)    # test to make sure they are equivalent
",Add comments to test file,"Add comments to test file

Add comments after all tests passed
",Python,mit,jesseklein406/data-structures,98,"```python
""""""Pytest file for linked_list.py

Run this with the command 'py.test test_linked_list.py'
""""""


from __future__ import unicode_literals
import linked_list
import copy


# init method

a = linked_list.LinkedList()


def test_init_size():
    assert a.sizeOfList is 0
    assert type(a.sizeOfList) is int


def test_init_head():
    assert a.head is None


def test_init_type():
    assert type(a) is linked_list.LinkedList


# insert method

b = copy.copy(a)   # make a copy every time a change is made
b.insert(5)        # so the test can handle different values


def test_insert_size():
    assert b.sizeOfList is 1


def test_insert_head():
    assert b.head.value is 5


def test_insert_next():
    assert b.head.next_node is None


c = copy.copy(b)
c.insert(6)


def test_insert_new_size():
    assert c.sizeOfList is 2


def test_insert_new_head():
    assert c.head.value is 6


def test_insert_pointer():
    assert c.head.next_node.value is 5


# size method

def test_size():
    assert c.size() is 2


# search method

def test_search_value_in_list():
    assert c.search(5).value is 5


def test_search_value_not_in_list():
    assert c.search(7) is None       # 7 is not in the list


# remove method

d = copy.copy(c)
d.remove(d.search(6))


def test_remove_value():
    assert d.search(6) is None


def test_remove_size():
    assert d.size() is 1


# display method

def test_display():
    assert d.display() == (5,)    # test to make sure they are equivalent

```"
3a7b1ff25c5ff3a1bd86efc7f70582c8268a968d,application.py,application.py,"from flask import Flask
application = Flask(__name__)


@application.route('/')
def hello_world():

    return 'Please use /api to use the DataNorth API.'

@application.route('/api')
def api_intro():

    intro = \
    """"""
    <h2> Welcome to the DataNorth API!  </h2>
    <h4> The following endpoints are available: </h4>

    <ul>
      <li>/api/crime</li>
      <li>/api/energy</li>
      <li>/api/housing</li>
    </ul>
    """"""

    return intro


if __name__ == ""__main__"":
    application.debug = True
    application.run()","from flask import Flask
from flask import jsonify
import boto3
import json
import decimal
from boto3.dynamodb.conditions import Key, Attr
import logging

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# Helper class to convert a DynamoDB item to JSON.
class DecimalEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, decimal.Decimal):
            if o % 1 > 0:
                return float(o)
            else:
                return int(o)
        return super(DecimalEncoder, self).default(o)

app.json_encoder = DecimalEncoder

dynamodb = boto3.resource(service_name='dynamodb',
                          region_name='us-east-1',
                          # endpoint_url=""http://localhost:8000""
                          )

table = dynamodb.Table('Movies')


@app.route('/')
def hello_world():

    return 'Please use /api to use the DataNorth API.'


@app.route('/api')
def api_intro():

    intro = \
    """"""
    <h2> Welcome to the DataNorth API!  </h2>
    <h4> The following endpoints are available: </h4>

    <ul>
      <li>/api/movies</li>
    </ul>
    """"""

    return intro


@app.route('/api/movies/<year>/')
def movies(year):
    """""" Sample movies endpoint. """"""
    fe = Key('year').eq(int(year));
    pe = ""#yr, title, info.rating""
    # Expression Attribute Names for Projection Expression only.
    ean = { ""#yr"": ""year"", }
    esk = None


    response = table.scan(
        FilterExpression=fe,
        ProjectionExpression=pe,
        ExpressionAttributeNames=ean
        )

    results = [i for i in response['Items']]

    # for i in response['Items']:
    #     print(json.dumps(i, cls=DecimalEncoder))

    while 'LastEvaluatedKey' in response:
        response = table.scan(
            ProjectionExpression=pe,
            FilterExpression=fe,
            ExpressionAttributeNames= ean,
            ExclusiveStartKey=response['LastEvaluatedKey']
            )

        for i in response['Items']:
            # print(json.dumps(i, cls=DecimalEncoder))
            results.append(i)

    return jsonify(items=results)


if __name__ == ""__main__"":
    app.debug = True
    app.run()",Add /movies/<year> endpoint for dynamoDB,"Add /movies/<year> endpoint for dynamoDB
",Python,mit,data-north/datanorth-api,92,"```python
from flask import Flask
from flask import jsonify
import boto3
import json
import decimal
from boto3.dynamodb.conditions import Key, Attr
import logging

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

# Helper class to convert a DynamoDB item to JSON.
class DecimalEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, decimal.Decimal):
            if o % 1 > 0:
                return float(o)
            else:
                return int(o)
        return super(DecimalEncoder, self).default(o)

app.json_encoder = DecimalEncoder

dynamodb = boto3.resource(service_name='dynamodb',
                          region_name='us-east-1',
                          # endpoint_url=""http://localhost:8000""
                          )

table = dynamodb.Table('Movies')


@app.route('/')
def hello_world():

    return 'Please use /api to use the DataNorth API.'


@app.route('/api')
def api_intro():

    intro = \
    """"""
    <h2> Welcome to the DataNorth API!  </h2>
    <h4> The following endpoints are available: </h4>

    <ul>
      <li>/api/movies</li>
    </ul>
    """"""

    return intro


@app.route('/api/movies/<year>/')
def movies(year):
    """""" Sample movies endpoint. """"""
    fe = Key('year').eq(int(year));
    pe = ""#yr, title, info.rating""
    # Expression Attribute Names for Projection Expression only.
    ean = { ""#yr"": ""year"", }
    esk = None


    response = table.scan(
        FilterExpression=fe,
        ProjectionExpression=pe,
        ExpressionAttributeNames=ean
        )

    results = [i for i in response['Items']]

    # for i in response['Items']:
    #     print(json.dumps(i, cls=DecimalEncoder))

    while 'LastEvaluatedKey' in response:
        response = table.scan(
            ProjectionExpression=pe,
            FilterExpression=fe,
            ExpressionAttributeNames= ean,
            ExclusiveStartKey=response['LastEvaluatedKey']
            )

        for i in response['Items']:
            # print(json.dumps(i, cls=DecimalEncoder))
            results.append(i)

    return jsonify(items=results)


if __name__ == ""__main__"":
    app.debug = True
    app.run()
```"
5c214680889a40a2963572e1163b8aa6beeaebc4,bayespy/nodes/__init__.py,bayespy/nodes/__init__.py,"################################################################################
# Copyright (C) 2013 Jaakko Luttinen
#
# This file is licensed under the MIT License.
################################################################################


""""""
Package for nodes used to construct the model.

Stochastic nodes
================

.. currentmodule:: bayespy.nodes

Nodes for Gaussian variables:

.. autosummary::
   :toctree: generated/

   Gaussian
   GaussianARD

Nodes for precision and scale variables:

.. autosummary::
   :toctree: generated/

   Gamma
   Wishart
   Exponential

Nodes for modelling Gaussian and precision variables jointly (useful as prior
for Gaussian nodes):

.. autosummary::
   :toctree: generated/

   GaussianGammaISO
   GaussianGammaARD
   GaussianWishart

Nodes for discrete count variables:

.. autosummary::
   :toctree: generated/

   Bernoulli
   Binomial
   Categorical
   Multinomial
   Poisson

Nodes for probabilities:

.. autosummary::
   :toctree: generated/

   Beta
   Dirichlet

Nodes for dynamic variables:

.. autosummary::
   :toctree: generated/

   CategoricalMarkovChain
   GaussianMarkovChain
   SwitchingGaussianMarkovChain
   VaryingGaussianMarkovChain

Other stochastic nodes:

.. autosummary::
   :toctree: generated/

   Mixture



Deterministic nodes
===================

.. autosummary::
   :toctree: generated/

   Dot
   SumMultiply
   Add
   Gate
""""""

# Currently, model construction and the inference network are not separated so
# the model is constructed using variational message passing nodes.
from bayespy.inference.vmp.nodes import *

","################################################################################
# Copyright (C) 2013 Jaakko Luttinen
#
# This file is licensed under the MIT License.
################################################################################


""""""
Package for nodes used to construct the model.

Stochastic nodes
================

.. currentmodule:: bayespy.nodes

Nodes for Gaussian variables:

.. autosummary::
   :toctree: generated/

   Gaussian
   GaussianARD

Nodes for precision and scale variables:

.. autosummary::
   :toctree: generated/

   Gamma
   Wishart
   Exponential

Nodes for modelling Gaussian and precision variables jointly (useful as prior
for Gaussian nodes):

.. autosummary::
   :toctree: generated/

   GaussianGammaISO
   GaussianGammaARD
   GaussianWishart

Nodes for discrete count variables:

.. autosummary::
   :toctree: generated/

   Bernoulli
   Binomial
   Categorical
   Multinomial
   Poisson

Nodes for probabilities:

.. autosummary::
   :toctree: generated/

   Beta
   Dirichlet

Nodes for dynamic variables:

.. autosummary::
   :toctree: generated/

   CategoricalMarkovChain
   GaussianMarkovChain
   SwitchingGaussianMarkovChain
   VaryingGaussianMarkovChain

Other stochastic nodes:

.. autosummary::
   :toctree: generated/

   Mixture



Deterministic nodes
===================

.. autosummary::
   :toctree: generated/

   Dot
   SumMultiply
   Add
   Gate
   Take
""""""

# Currently, model construction and the inference network are not separated so
# the model is constructed using variational message passing nodes.
from bayespy.inference.vmp.nodes import *

",Add Take node to the node list in API doc,"DOC: Add Take node to the node list in API doc
",Python,mit,"jluttine/bayespy,bayespy/bayespy",98,"```python
################################################################################
# Copyright (C) 2013 Jaakko Luttinen
#
# This file is licensed under the MIT License.
################################################################################


""""""
Package for nodes used to construct the model.

Stochastic nodes
================

.. currentmodule:: bayespy.nodes

Nodes for Gaussian variables:

.. autosummary::
   :toctree: generated/

   Gaussian
   GaussianARD

Nodes for precision and scale variables:

.. autosummary::
   :toctree: generated/

   Gamma
   Wishart
   Exponential

Nodes for modelling Gaussian and precision variables jointly (useful as prior
for Gaussian nodes):

.. autosummary::
   :toctree: generated/

   GaussianGammaISO
   GaussianGammaARD
   GaussianWishart

Nodes for discrete count variables:

.. autosummary::
   :toctree: generated/

   Bernoulli
   Binomial
   Categorical
   Multinomial
   Poisson

Nodes for probabilities:

.. autosummary::
   :toctree: generated/

   Beta
   Dirichlet

Nodes for dynamic variables:

.. autosummary::
   :toctree: generated/

   CategoricalMarkovChain
   GaussianMarkovChain
   SwitchingGaussianMarkovChain
   VaryingGaussianMarkovChain

Other stochastic nodes:

.. autosummary::
   :toctree: generated/

   Mixture



Deterministic nodes
===================

.. autosummary::
   :toctree: generated/

   Dot
   SumMultiply
   Add
   Gate
   Take
""""""

# Currently, model construction and the inference network are not separated so
# the model is constructed using variational message passing nodes.
from bayespy.inference.vmp.nodes import *


```"
0b7cdb4b5a6dab5f2983313d745bea84ff302e01,Machines/wxMachines.py,Machines/wxMachines.py,"# -*- coding: utf-8 -*-

# Import

# Import for changing the Python Path for importing Gestalt
import sys
import os

# Change the Python Path
base_dir = os.path.dirname(__file__) or '.'
appdir = os.path.abspath(os.path.join(base_dir, os.pardir))
sys.path.insert(0, appdir)

# Import Gestalt
from gestalt import nodes
from gestalt import interfaces
from gestalt import machines
from gestalt import functions
from gestalt.machines import elements
from gestalt.machines import kinematics
from gestalt.machines import state
from gestalt.utilities import notice
from gestalt.publish import rpc	#remote procedure call dispatcher


# Classes

class Axis():

    def __init__(self):
        self.linear = True
        self.rotary = False

class Machine():

    def __init__(self):
        self.axes = []




if __name__ == '__main__':
    pass
","# -*- coding: utf-8 -*-

# Import

# Import for changing the Python Path for importing Gestalt
import sys
import os

# Change the Python Path
base_dir = os.path.dirname(__file__) or '.'
appdir = os.path.abspath(os.path.join(base_dir, os.pardir))
sys.path.insert(0, appdir)

# Import Gestalt
from gestalt import nodes
from gestalt import interfaces
from gestalt import machines
from gestalt import functions
from gestalt.machines import elements
from gestalt.machines import kinematics
from gestalt.machines import state
from gestalt.utilities import notice
from gestalt.publish import rpc	#remote procedure call dispatcher


# Classes

# A class for each Node / Axis
class wxNode():

    def __init__(self):
        self.linear = True
        self.rotary = False


# A basic class for each Machine
class wxMachine():

    def __init__(self):
        self.axes = []


# Solo/Independent Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 35
class wxSolo_Independent():

    def __init__(self):
        pass


# Solo/Gestalt Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 36
class wxSolo_Gestalt():

    def __init__(self):
        pass


# Networked/Gestalt Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 36
class wxNetworked_Gestalt(wxSolo_Gestalt):

    def __init__(self):
        pass


# Managed/Gestalt Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 37
class wxManaged_Gestalt():

    def __init__(self):
        pass


# Compound Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 38
class wxCompound_Nodes():

    def __init__(self):
        pass


if __name__ == '__main__':
    pass
",Improve general classes with input from Ilan Ellison Moyer's thesis,"Improve general classes with input from Ilan Ellison Moyer's thesis
",Python,mit,openp2pdesign/wxGestalt,90,"```python
# -*- coding: utf-8 -*-

# Import

# Import for changing the Python Path for importing Gestalt
import sys
import os

# Change the Python Path
base_dir = os.path.dirname(__file__) or '.'
appdir = os.path.abspath(os.path.join(base_dir, os.pardir))
sys.path.insert(0, appdir)

# Import Gestalt
from gestalt import nodes
from gestalt import interfaces
from gestalt import machines
from gestalt import functions
from gestalt.machines import elements
from gestalt.machines import kinematics
from gestalt.machines import state
from gestalt.utilities import notice
from gestalt.publish import rpc	#remote procedure call dispatcher


# Classes

# A class for each Node / Axis
class wxNode():

    def __init__(self):
        self.linear = True
        self.rotary = False


# A basic class for each Machine
class wxMachine():

    def __init__(self):
        self.axes = []


# Solo/Independent Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 35
class wxSolo_Independent():

    def __init__(self):
        pass


# Solo/Gestalt Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 36
class wxSolo_Gestalt():

    def __init__(self):
        pass


# Networked/Gestalt Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 36
class wxNetworked_Gestalt(wxSolo_Gestalt):

    def __init__(self):
        pass


# Managed/Gestalt Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 37
class wxManaged_Gestalt():

    def __init__(self):
        pass


# Compound Nodes
# http://pygestalt.org/VMC_IEM.pdf
# p. 38
class wxCompound_Nodes():

    def __init__(self):
        pass


if __name__ == '__main__':
    pass

```"
25e6fd9b6a17def2f3a07b83ace1e65e6f1fa40d,test/functional/abc_rpc_buildavalancheproof.py,test/functional/abc_rpc_buildavalancheproof.py,,"#!/usr/bin/env python3
# Copyright (c) 2021 The Bitcoin developers
# Distributed under the MIT software license, see the accompanying
# file COPYING or http://www.opensource.org/licenses/mit-license.php.
""""""Test the buildavalancheproof RPC""""""

from test_framework.avatools import create_coinbase_stakes
from test_framework.key import ECKey
from test_framework.test_framework import BitcoinTestFramework
from test_framework.util import (
    assert_raises_rpc_error,
)


class BuildAvalancheProofTest(BitcoinTestFramework):
    def set_test_params(self):
        self.num_nodes = 1
        self.extra_args = [['-enableavalanche=1', '-avacooldown=0']]

    def run_test(self):
        node = self.nodes[0]

        addrkey0 = node.get_deterministic_priv_key()
        blockhashes = node.generatetoaddress(2, addrkey0.address)
        stakes = create_coinbase_stakes(node, [blockhashes[0]], addrkey0.key)

        privkey = ECKey()
        privkey.generate()

        proof_master = privkey.get_pubkey().get_bytes().hex()

        def check_buildavalancheproof_error(
                error_code, error_message, stakes):
            assert_raises_rpc_error(
                error_code,
                error_message,
                node.buildavalancheproof,
                # Sequence
                0,
                # Expiration
                0,
                # Master
                proof_master,
                stakes,
            )

        good_stake = stakes[0]

        self.log.info(""Error cases"")

        negative_vout = good_stake.copy()
        negative_vout['vout'] = -1
        check_buildavalancheproof_error(-22,
                                        ""vout must be positive"",
                                        [negative_vout],
                                        )

        zero_height = good_stake.copy()
        zero_height['height'] = 0
        check_buildavalancheproof_error(-22,
                                        ""height must be positive"",
                                        [zero_height],
                                        )
        negative_height = good_stake.copy()
        negative_height['height'] = -1
        check_buildavalancheproof_error(-22,
                                        ""height must be positive"",
                                        [negative_height],
                                        )

        missing_amount = good_stake.copy()
        del missing_amount['amount']
        check_buildavalancheproof_error(-8,
                                        ""Missing amount"",
                                        [missing_amount],
                                        )

        invalid_privkey = good_stake.copy()
        invalid_privkey['privatekey'] = 'foobar'
        check_buildavalancheproof_error(-8,
                                        ""Invalid private key"",
                                        [invalid_privkey],
                                        )

        self.log.info(""Happy path"")
        assert node.buildavalancheproof(0, 0, proof_master, [good_stake])


if __name__ == '__main__':
    BuildAvalancheProofTest().main()
",Add some test coverage for the buildavalancheproof RPC,"[avalanche] Add some test coverage for the buildavalancheproof RPC

Summary:
The RPC is used everywhere in the tests but is not tested itself.
This diff does not intend to extend the safety checks but only check for the existing errors to prevent regressions.

Test Plan:
  ./test/functional/test_runner.py abc_rpc_buildavalancheproof

Reviewers: #bitcoin_abc, PiRK

Reviewed By: #bitcoin_abc, PiRK

Differential Revision: https://reviews.bitcoinabc.org/D10084
",Python,mit,"Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc,Bitcoin-ABC/bitcoin-abc",91,"```python
#!/usr/bin/env python3
# Copyright (c) 2021 The Bitcoin developers
# Distributed under the MIT software license, see the accompanying
# file COPYING or http://www.opensource.org/licenses/mit-license.php.
""""""Test the buildavalancheproof RPC""""""

from test_framework.avatools import create_coinbase_stakes
from test_framework.key import ECKey
from test_framework.test_framework import BitcoinTestFramework
from test_framework.util import (
    assert_raises_rpc_error,
)


class BuildAvalancheProofTest(BitcoinTestFramework):
    def set_test_params(self):
        self.num_nodes = 1
        self.extra_args = [['-enableavalanche=1', '-avacooldown=0']]

    def run_test(self):
        node = self.nodes[0]

        addrkey0 = node.get_deterministic_priv_key()
        blockhashes = node.generatetoaddress(2, addrkey0.address)
        stakes = create_coinbase_stakes(node, [blockhashes[0]], addrkey0.key)

        privkey = ECKey()
        privkey.generate()

        proof_master = privkey.get_pubkey().get_bytes().hex()

        def check_buildavalancheproof_error(
                error_code, error_message, stakes):
            assert_raises_rpc_error(
                error_code,
                error_message,
                node.buildavalancheproof,
                # Sequence
                0,
                # Expiration
                0,
                # Master
                proof_master,
                stakes,
            )

        good_stake = stakes[0]

        self.log.info(""Error cases"")

        negative_vout = good_stake.copy()
        negative_vout['vout'] = -1
        check_buildavalancheproof_error(-22,
                                        ""vout must be positive"",
                                        [negative_vout],
                                        )

        zero_height = good_stake.copy()
        zero_height['height'] = 0
        check_buildavalancheproof_error(-22,
                                        ""height must be positive"",
                                        [zero_height],
                                        )
        negative_height = good_stake.copy()
        negative_height['height'] = -1
        check_buildavalancheproof_error(-22,
                                        ""height must be positive"",
                                        [negative_height],
                                        )

        missing_amount = good_stake.copy()
        del missing_amount['amount']
        check_buildavalancheproof_error(-8,
                                        ""Missing amount"",
                                        [missing_amount],
                                        )

        invalid_privkey = good_stake.copy()
        invalid_privkey['privatekey'] = 'foobar'
        check_buildavalancheproof_error(-8,
                                        ""Invalid private key"",
                                        [invalid_privkey],
                                        )

        self.log.info(""Happy path"")
        assert node.buildavalancheproof(0, 0, proof_master, [good_stake])


if __name__ == '__main__':
    BuildAvalancheProofTest().main()

```"
7ceadc54b271d0534229c2f2feed617e97331671,tests/test_testlevelsplit_output_task_order.py,tests/test_testlevelsplit_output_task_order.py,,"import shutil
import subprocess
import sys
import tempfile
import textwrap
import unittest

from robot.api import ExecutionResult


class PabotTestlevelsplitOutputTaskOrderTest(unittest.TestCase):
    def setUp(self):
        self.tmpdir = tempfile.mkdtemp()

    def tearDown(self):
        shutil.rmtree(self.tmpdir)

    def _run_tests_with(self, testfile):
        robot_file = open(""{}/test.robot"".format(self.tmpdir), ""w"")
        robot_file.write(textwrap.dedent(testfile))
        robot_file.close()
        process = subprocess.Popen(
            [
                sys.executable,
                ""-m"" ""pabot.pabot"",
                ""--testlevelsplit"",
                ""{}/test.robot"".format(self.tmpdir),
            ],
            cwd=self.tmpdir,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        process.wait()

    def test_testlevelsplit_output_task_order(self):
        self._run_tests_with(
            """"""
                *** Test Cases ***
                Test 1
                    Log    Executing test

                Test 2
                    Log    Executing test

                Test 3
                    Log    Executing test

                Test 4
                    Log    Executing test

                Test 5
                    Log    Executing test

                Test 6
                    Log    Executing test

                Test 7
                    Log    Executing test

                Test 8
                    Log    Executing test

                Test 9
                    Log    Executing test

                Test 10
                    Log    Executing test

                Test 11
                    Log    Executing test
            """"""
        )
        result = ExecutionResult(""{}/output.xml"".format(self.tmpdir))
        test_names = [test.name for test in result.suite.tests]
        self.assertEqual(
            [
                ""Test 1"",
                ""Test 2"",
                ""Test 3"",
                ""Test 4"",
                ""Test 5"",
                ""Test 6"",
                ""Test 7"",
                ""Test 8"",
                ""Test 9"",
                ""Test 10"",
                ""Test 11"",
            ],
            test_names
        )
",Create test for testcase order in output.,"Create test for testcase order in output.
",Python,apache-2.0,"mkorpela/pabot,mkorpela/pabot",91,"```python
import shutil
import subprocess
import sys
import tempfile
import textwrap
import unittest

from robot.api import ExecutionResult


class PabotTestlevelsplitOutputTaskOrderTest(unittest.TestCase):
    def setUp(self):
        self.tmpdir = tempfile.mkdtemp()

    def tearDown(self):
        shutil.rmtree(self.tmpdir)

    def _run_tests_with(self, testfile):
        robot_file = open(""{}/test.robot"".format(self.tmpdir), ""w"")
        robot_file.write(textwrap.dedent(testfile))
        robot_file.close()
        process = subprocess.Popen(
            [
                sys.executable,
                ""-m"" ""pabot.pabot"",
                ""--testlevelsplit"",
                ""{}/test.robot"".format(self.tmpdir),
            ],
            cwd=self.tmpdir,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        process.wait()

    def test_testlevelsplit_output_task_order(self):
        self._run_tests_with(
            """"""
                *** Test Cases ***
                Test 1
                    Log    Executing test

                Test 2
                    Log    Executing test

                Test 3
                    Log    Executing test

                Test 4
                    Log    Executing test

                Test 5
                    Log    Executing test

                Test 6
                    Log    Executing test

                Test 7
                    Log    Executing test

                Test 8
                    Log    Executing test

                Test 9
                    Log    Executing test

                Test 10
                    Log    Executing test

                Test 11
                    Log    Executing test
            """"""
        )
        result = ExecutionResult(""{}/output.xml"".format(self.tmpdir))
        test_names = [test.name for test in result.suite.tests]
        self.assertEqual(
            [
                ""Test 1"",
                ""Test 2"",
                ""Test 3"",
                ""Test 4"",
                ""Test 5"",
                ""Test 6"",
                ""Test 7"",
                ""Test 8"",
                ""Test 9"",
                ""Test 10"",
                ""Test 11"",
            ],
            test_names
        )

```"
3a074a43f6e979854de3e2f80dfc5dd733b7d64c,double_link_list.py,double_link_list.py,,"from __future__ import unicode_literals


class Node(object):

    def __init__(self, val, prev=None, next_=None):
        self.val = val
        self.prev = prev
        self.next = next_

    def __repr__(self):
        """"""Print representation of node.""""""
        return ""{val}"".format(val=self.val)


class DoubleLinkList(object):
    """"""Class for a doubly-linked list.""""""
    def __init__(self, iterable=()):
        self._current = None
        self.head = None
        self.length = 0
        for val in reversed(iterable):
            self.insert(val)

    def __repr__(self):
        """"""Print representation of DoubleLinkList.""""""
        node = self.head
        output = """"
        for node in self:
            output += ""{!r}, "".format(node.val)
        return ""({})"".format(output.rstrip(' ,'))

    def __len__(self):
        return self.length

    def __iter__(self):
        if self.head is not None:
            self._current = self.head
        return self

    def next(self):
        if self._current is None:
            raise StopIteration
        node = self._current
        self._current = self._current.next
        return node

    def insert(self, val):
        """"""Insert value at head of DoubleLinkList.

        args:
            val: the value to add
        """"""
        current_head = self.head
        self.head = Node(val, prev=None, next_=current_head)
        current_head.prev = self.head
        self.length += 1
        return None

    def pop(self):
        """"""Pop the first val off the head and return it.""""""
        if self.head is None:
            raise IndexError
        else:
            to_return = self.head
            self.head = to_return.next
            self.head.prev = None
            self.length -= 1
            return to_return.val

    def size(self):
        """"""Return current length of DoubleLinkList.""""""
        return len(self)

    def search(self, search_val):
        """"""Return the node containing val if present, else None.

        args:
            search_val: the value to search by

        returns: a node object or None
        """"""
        for node in self:
            if node.val == search_val:
                return node
        else:
            return None

    def remove(self, search_node):
        """"""Remove given node from list, return None.

        args:
            search_node: the node to be removed
        """"""
        for node in self:
            if node == search_node:
                node.prev.next = node.next
                node.next.prev = node.prev
                return None

    def display(self):
        """"""Shows representation of DoubleLinkList.""""""
        return repr(self)
",Add first pass for double link list,"Add first pass for double link list
",Python,mit,"jonathanstallings/data-structures,jay-tyler/data-structures",104,"```python
from __future__ import unicode_literals


class Node(object):

    def __init__(self, val, prev=None, next_=None):
        self.val = val
        self.prev = prev
        self.next = next_

    def __repr__(self):
        """"""Print representation of node.""""""
        return ""{val}"".format(val=self.val)


class DoubleLinkList(object):
    """"""Class for a doubly-linked list.""""""
    def __init__(self, iterable=()):
        self._current = None
        self.head = None
        self.length = 0
        for val in reversed(iterable):
            self.insert(val)

    def __repr__(self):
        """"""Print representation of DoubleLinkList.""""""
        node = self.head
        output = """"
        for node in self:
            output += ""{!r}, "".format(node.val)
        return ""({})"".format(output.rstrip(' ,'))

    def __len__(self):
        return self.length

    def __iter__(self):
        if self.head is not None:
            self._current = self.head
        return self

    def next(self):
        if self._current is None:
            raise StopIteration
        node = self._current
        self._current = self._current.next
        return node

    def insert(self, val):
        """"""Insert value at head of DoubleLinkList.

        args:
            val: the value to add
        """"""
        current_head = self.head
        self.head = Node(val, prev=None, next_=current_head)
        current_head.prev = self.head
        self.length += 1
        return None

    def pop(self):
        """"""Pop the first val off the head and return it.""""""
        if self.head is None:
            raise IndexError
        else:
            to_return = self.head
            self.head = to_return.next
            self.head.prev = None
            self.length -= 1
            return to_return.val

    def size(self):
        """"""Return current length of DoubleLinkList.""""""
        return len(self)

    def search(self, search_val):
        """"""Return the node containing val if present, else None.

        args:
            search_val: the value to search by

        returns: a node object or None
        """"""
        for node in self:
            if node.val == search_val:
                return node
        else:
            return None

    def remove(self, search_node):
        """"""Remove given node from list, return None.

        args:
            search_node: the node to be removed
        """"""
        for node in self:
            if node == search_node:
                node.prev.next = node.next
                node.next.prev = node.prev
                return None

    def display(self):
        """"""Shows representation of DoubleLinkList.""""""
        return repr(self)

```"
526560178f93894f19ed6d99557254eea78653ea,services/migrations/0002_add_initial_servicetypes.py,services/migrations/0002_add_initial_servicetypes.py,,"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    def add_servicetypes(apps, schema_editor):
        ServiceType = apps.get_model('services', 'ServiceType')

        for st in SERVICE_TYPES:
            ServiceType.objects.get_or_create(
                pk=st['pk'],
                servicetype=st['fields']['servicetype']
            )

    def del_servicetypes(apps, schema_editor):
        ServiceType = apps.get_model('services', 'ServiceType')

        for st in SERVICE_TYPES:
            ServiceType.objects.get(pk=st['pk'], servicetype=st['fields']['servicetype']).delete()

    dependencies = [
        ('services', '0001_initial'),
    ]

    operations = [
        migrations.RunPython(add_servicetypes, del_servicetypes)
    ]


SERVICE_TYPES = [
  {
    ""pk"": 1,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""UNIX account""
    }
  },
  {
    ""pk"": 2,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""Email alias""
    }
  },
  {
    ""pk"": 3,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""WWW vhost""
    }
  },
  {
    ""pk"": 4,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""MySQL database""
    }
  },
  {
    ""pk"": 5,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""PostgreSQL database""
    }
  },
  {
    ""pk"": 6,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""DNS domain""
    }
  },
  {
    ""pk"": 7,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""IRC vhost""
    }
  },
  {
    ""pk"": 8,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""SVN repository""
    }
  },
  {
    ""pk"": 9,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""Mailbox account""
    }
  },
  {
    ""pk"": 10,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""Firewall port""
    }
  }
]
",Add ServiceTypes as a data migration instead of fixtures,"Add ServiceTypes as a data migration instead of fixtures
",Python,mit,"AriMartti/sikteeri,kapsiry/sikteeri,kapsiry/sikteeri,joneskoo/sikteeri,joneskoo/sikteeri,kapsiry/sikteeri,annttu/sikteeri,annttu/sikteeri,annttu/sikteeri,joneskoo/sikteeri,annttu/sikteeri,AriMartti/sikteeri,AriMartti/sikteeri,joneskoo/sikteeri,AriMartti/sikteeri,kapsiry/sikteeri",105,"```python
# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    def add_servicetypes(apps, schema_editor):
        ServiceType = apps.get_model('services', 'ServiceType')

        for st in SERVICE_TYPES:
            ServiceType.objects.get_or_create(
                pk=st['pk'],
                servicetype=st['fields']['servicetype']
            )

    def del_servicetypes(apps, schema_editor):
        ServiceType = apps.get_model('services', 'ServiceType')

        for st in SERVICE_TYPES:
            ServiceType.objects.get(pk=st['pk'], servicetype=st['fields']['servicetype']).delete()

    dependencies = [
        ('services', '0001_initial'),
    ]

    operations = [
        migrations.RunPython(add_servicetypes, del_servicetypes)
    ]


SERVICE_TYPES = [
  {
    ""pk"": 1,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""UNIX account""
    }
  },
  {
    ""pk"": 2,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""Email alias""
    }
  },
  {
    ""pk"": 3,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""WWW vhost""
    }
  },
  {
    ""pk"": 4,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""MySQL database""
    }
  },
  {
    ""pk"": 5,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""PostgreSQL database""
    }
  },
  {
    ""pk"": 6,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""DNS domain""
    }
  },
  {
    ""pk"": 7,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""IRC vhost""
    }
  },
  {
    ""pk"": 8,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""SVN repository""
    }
  },
  {
    ""pk"": 9,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""Mailbox account""
    }
  },
  {
    ""pk"": 10,
    ""model"": ""services.servicetype"",
    ""fields"": {
      ""servicetype"": ""Firewall port""
    }
  }
]

```"
dbd1708c562c698dd6ea53fb8276ceaf9820045b,test_project/test_app/tests/test_models.py,test_project/test_app/tests/test_models.py,,"from django.contrib.gis.geos import Point
from django.test import TestCase

from cities import models


class SlugModelTest(object):
    """"""
    Common tests for SlugModel subclasses.
    """"""

    def instantiate(self):
        """"""
        Implement this to return a valid instance of the model under test.
        """"""
        raise NotImplementedError

    def test_save(self):
        instance = self.instantiate()
        instance.save()

    def test_save_force_insert(self):
        """"""
        Regression test: save() with force_insert=True should work.
        """"""
        instance = self.instantiate()
        instance.save(force_insert=True)


class ContinentTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        return models.Continent()


class CountryTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        return models.Country(
            population=0,
        )


class RegionTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        return models.Region(
            country=country,
        )


class SubregionTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        region = models.Region(
            country=country,
        )
        region.save()
        return models.Subregion(
            region=region,
        )


class CityTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        return models.City(
            country=country,
            location=Point(0, 0),
            population=0,
        )


class DistrictTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        city = models.City(
            country=country,
            location=Point(0, 0),
            population=0,
        )
        city.save()
        return models.District(
            location=Point(0, 0),
            population=0,
            city=city,
        )


class AlternativeNameTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        return models.AlternativeName()


class PostalCodeTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        return models.PostalCode(
            location=Point(0, 0),
            country=country,
        )
",Add regression tests for saving with force_insert,"Add regression tests for saving with force_insert
",Python,mit,"coderholic/django-cities,coderholic/django-cities,coderholic/django-cities",123,"```python
from django.contrib.gis.geos import Point
from django.test import TestCase

from cities import models


class SlugModelTest(object):
    """"""
    Common tests for SlugModel subclasses.
    """"""

    def instantiate(self):
        """"""
        Implement this to return a valid instance of the model under test.
        """"""
        raise NotImplementedError

    def test_save(self):
        instance = self.instantiate()
        instance.save()

    def test_save_force_insert(self):
        """"""
        Regression test: save() with force_insert=True should work.
        """"""
        instance = self.instantiate()
        instance.save(force_insert=True)


class ContinentTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        return models.Continent()


class CountryTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        return models.Country(
            population=0,
        )


class RegionTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        return models.Region(
            country=country,
        )


class SubregionTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        region = models.Region(
            country=country,
        )
        region.save()
        return models.Subregion(
            region=region,
        )


class CityTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        return models.City(
            country=country,
            location=Point(0, 0),
            population=0,
        )


class DistrictTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        city = models.City(
            country=country,
            location=Point(0, 0),
            population=0,
        )
        city.save()
        return models.District(
            location=Point(0, 0),
            population=0,
            city=city,
        )


class AlternativeNameTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        return models.AlternativeName()


class PostalCodeTestCase(SlugModelTest, TestCase):

    def instantiate(self):
        country = models.Country(
            population=0
        )
        country.save()
        return models.PostalCode(
            location=Point(0, 0),
            country=country,
        )

```"
f78109b5afb8972b2e5b3115e7d892c50f775040,others/HideAndSeek/solve.py,others/HideAndSeek/solve.py,,"#! /usr/bin/env python

# -*- coding: utf-8 -*-

from itertools import permutations, chain

# Board

Empty = 1
Elephant = 2
Lion = 3
Zebra = 5
Gazelle = 7
Rhino = 11

# E   L | G   L
# Z G Z |   Z E
# R L E | L R G
# -------------
# R E Z |
# G   G |   R L
# L   R | G E Z 

board = [Elephant, Empty, Lion, Zebra, Gazelle, Zebra, Rhino, Lion, Elephant,
         Gazelle, Empty, Lion, Empty, Zebra, Elephant, Lion, Rhino, Gazelle,
         Rhino, Elephant, Zebra, Gazelle, Empty, Gazelle, Lion, Empty, Rhino,
         Empty, Empty, Empty, Empty, Rhino, Lion, Gazelle, Elephant, Zebra]

# Masks (distinct with rotations)
# ████
#   ████
#   ████
#
# ██████
#   ██
# ██████
#
# ██  ██
# ██  ██
# ██████
#
# ████
# ██████
# ██  ██


def rotate(mask):
    return [mask[2], mask[5], mask[8],
            mask[1], mask[4], mask[7],
            mask[0], mask[3], mask[6]]

basic_masks = [
               [
                [0, 0, 1, 1, 0, 0, 1, 0, 0]
               ],
               [
                [0, 0, 0, 1, 0, 1, 0, 0, 0]
               ],
               [
                [0, 1, 0, 0, 1, 0, 0, 0, 0]
               ],
               [
                [0, 0, 1, 0, 0, 0, 0, 1, 0]
               ]
              ]


length = len(basic_masks)

for i in range(0, length):
    rotated_mask = rotate(basic_masks[i][0])
    while rotated_mask not in basic_masks[i]:
        basic_masks[i].append(rotated_mask)
        rotated_mask = rotate(rotated_mask)

mask_group_permutations = list(permutations(basic_masks))
mask_permutations = []
for mask_group in mask_group_permutations:
    for masks in mask_group:
        mask_permutations.append(masks)

target = Gazelle ** 5

for m in mask_permutations:
    masks = list(chain.from_iterable(m))
    if len(masks) == len(board):
        result = 1
        for i in range(0, len(board)):
            result *= board[i] ** masks[i]
        if result == target:
            print(masks)
",Add a Python solution for Hide&Seek game,"Add a Python solution for Hide&Seek game
",Python,cc0-1.0,boltomli/PicatEuler,92,"```python
#! /usr/bin/env python

# -*- coding: utf-8 -*-

from itertools import permutations, chain

# Board

Empty = 1
Elephant = 2
Lion = 3
Zebra = 5
Gazelle = 7
Rhino = 11

# E   L | G   L
# Z G Z |   Z E
# R L E | L R G
# -------------
# R E Z |
# G   G |   R L
# L   R | G E Z 

board = [Elephant, Empty, Lion, Zebra, Gazelle, Zebra, Rhino, Lion, Elephant,
         Gazelle, Empty, Lion, Empty, Zebra, Elephant, Lion, Rhino, Gazelle,
         Rhino, Elephant, Zebra, Gazelle, Empty, Gazelle, Lion, Empty, Rhino,
         Empty, Empty, Empty, Empty, Rhino, Lion, Gazelle, Elephant, Zebra]

# Masks (distinct with rotations)
# ████
#   ████
#   ████
#
# ██████
#   ██
# ██████
#
# ██  ██
# ██  ██
# ██████
#
# ████
# ██████
# ██  ██


def rotate(mask):
    return [mask[2], mask[5], mask[8],
            mask[1], mask[4], mask[7],
            mask[0], mask[3], mask[6]]

basic_masks = [
               [
                [0, 0, 1, 1, 0, 0, 1, 0, 0]
               ],
               [
                [0, 0, 0, 1, 0, 1, 0, 0, 0]
               ],
               [
                [0, 1, 0, 0, 1, 0, 0, 0, 0]
               ],
               [
                [0, 0, 1, 0, 0, 0, 0, 1, 0]
               ]
              ]


length = len(basic_masks)

for i in range(0, length):
    rotated_mask = rotate(basic_masks[i][0])
    while rotated_mask not in basic_masks[i]:
        basic_masks[i].append(rotated_mask)
        rotated_mask = rotate(rotated_mask)

mask_group_permutations = list(permutations(basic_masks))
mask_permutations = []
for mask_group in mask_group_permutations:
    for masks in mask_group:
        mask_permutations.append(masks)

target = Gazelle ** 5

for m in mask_permutations:
    masks = list(chain.from_iterable(m))
    if len(masks) == len(board):
        result = 1
        for i in range(0, len(board)):
            result *= board[i] ** masks[i]
        if result == target:
            print(masks)

```"
908e901f7e5b737b5a363b0f817dbf46b45267a0,SessionTools/feature_usage_jsons_shuffler.py,SessionTools/feature_usage_jsons_shuffler.py,,"# Condenses all the feature files into a single location,
# Split by the names of the features


import sys
from os.path import isfile
import os
import json

path = sys.argv[1]
out_path = sys.argv[2]
paths = []

i = 0
skipped = 0

pretty_print_json_output = False

feature_versions_map = {}
known_files = set()


def flush():
    # Create one file per feature version
    for k in feature_versions_map.keys():
        out_full_path = out_path + ""."" + k + '.jsons'
        
        # Ensure we created all of the data files
        if out_full_path not in known_files:
            known_files.add(out_full_path)
            if os.path.exists(out_full_path):
                print (""Removing existing file: "" + out_full_path)
                os.remove(out_full_path)
        
        sessions = feature_versions_map[k]
        feature_version = k

        with open(out_full_path, 'wa') as f:
            for session_id in sessions.keys():
                data_to_dump = {
                    'feature_version' : feature_version,
                    'session_id' : session_id,
                    'features' : sessions[session_id]
                }

                if pretty_print_json_output:
                    f.write(json.dumps(data_to_dump, sort_keys=True, indent=2) + ""\n"")
                else:
                    f.write(json.dumps(data_to_dump) + ""\n"")
            f.flush()
    feature_versions_map.clear()


# Main function

print ('Enumerating feature files')

for root, subdirs, files in os.walk(path):
    for ff in files:
        i = i + 1
        if i % 1000 == 0:
            print (i, skipped)
            flush()

        path = os.path.join(root,ff)
        if (path.find('.sorted.gz.features.') == -1 ):
            continue
        path_split = path.split ('.sorted.gz.features.')

        feature_version = path_split[-1]
        if not feature_versions_map.has_key(feature_version):
            feature_versions_map[feature_version] = {}

        session_id = path_split[0].split('/')[-1]


        
        if feature_versions_map[feature_version].has_key(session_id):
            # We've already added this session
            # This can be used in a version that loads a partially complete file
            print (""Session: "") + session_id + "" skipped, features already added for: "" + feature_version
            skipped += 1
            continue
        feature_versions_map[feature_version][session_id] = []

        paths.append(path)
        print (feature_version, session_id, path)

        with open(path, 'r') as f:
            lines = f.readlines()
            for ln in lines:
                feature_versions_map[feature_version][session_id].append(json.loads(ln))

flush()
    
                ",Add out persistent storage backed shuffler,"Add out persistent storage backed shuffler
",Python,mit,"DynamoDS/Coulomb,DynamoDS/Coulomb,DynamoDS/Coulomb",96,"```python
# Condenses all the feature files into a single location,
# Split by the names of the features


import sys
from os.path import isfile
import os
import json

path = sys.argv[1]
out_path = sys.argv[2]
paths = []

i = 0
skipped = 0

pretty_print_json_output = False

feature_versions_map = {}
known_files = set()


def flush():
    # Create one file per feature version
    for k in feature_versions_map.keys():
        out_full_path = out_path + ""."" + k + '.jsons'
        
        # Ensure we created all of the data files
        if out_full_path not in known_files:
            known_files.add(out_full_path)
            if os.path.exists(out_full_path):
                print (""Removing existing file: "" + out_full_path)
                os.remove(out_full_path)
        
        sessions = feature_versions_map[k]
        feature_version = k

        with open(out_full_path, 'wa') as f:
            for session_id in sessions.keys():
                data_to_dump = {
                    'feature_version' : feature_version,
                    'session_id' : session_id,
                    'features' : sessions[session_id]
                }

                if pretty_print_json_output:
                    f.write(json.dumps(data_to_dump, sort_keys=True, indent=2) + ""\n"")
                else:
                    f.write(json.dumps(data_to_dump) + ""\n"")
            f.flush()
    feature_versions_map.clear()


# Main function

print ('Enumerating feature files')

for root, subdirs, files in os.walk(path):
    for ff in files:
        i = i + 1
        if i % 1000 == 0:
            print (i, skipped)
            flush()

        path = os.path.join(root,ff)
        if (path.find('.sorted.gz.features.') == -1 ):
            continue
        path_split = path.split ('.sorted.gz.features.')

        feature_version = path_split[-1]
        if not feature_versions_map.has_key(feature_version):
            feature_versions_map[feature_version] = {}

        session_id = path_split[0].split('/')[-1]


        
        if feature_versions_map[feature_version].has_key(session_id):
            # We've already added this session
            # This can be used in a version that loads a partially complete file
            print (""Session: "") + session_id + "" skipped, features already added for: "" + feature_version
            skipped += 1
            continue
        feature_versions_map[feature_version][session_id] = []

        paths.append(path)
        print (feature_version, session_id, path)

        with open(path, 'r') as f:
            lines = f.readlines()
            for ln in lines:
                feature_versions_map[feature_version][session_id].append(json.loads(ln))

flush()
    
                
```"
92fb1bd323e69a625e38d48bb293e72787d84808,test/test_commands.py,test/test_commands.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""
This test check the commands
""""""

from __future__ import print_function
import os
import time
import shlex
import subprocess
import copy
import requests
import unittest2


class TestGroups(unittest2.TestCase):
    """"""
    This class test hostgroups and tree feature
    """"""

    @classmethod
    def setUpClass(cls):
        """"""
        This method:
          * deletes mongodb database
          * starts the backend with uwsgi
          * logs in the backend and get the token
          * gets the hostgroup

        :return: None
        """"""
        # Set test mode for Alignak backend
        os.environ['TEST_ALIGNAK_BACKEND'] = '1'
        os.environ['ALIGNAK_BACKEND_MONGO_DBNAME'] = 'alignak-backend-test'

        # Delete used mongo DBs
        exit_code = subprocess.call(
            shlex.split(
                'mongo %s --eval ""db.dropDatabase()""' % os.environ['ALIGNAK_BACKEND_MONGO_DBNAME'])
        )
        assert exit_code == 0

        cls.p = subprocess.Popen(['uwsgi', '--plugin', 'python', '-w', 'alignakbackend:app',
                                  '--socket', '0.0.0.0:5000',
                                  '--protocol=http', '--enable-threads', '--pidfile',
                                  '/tmp/uwsgi.pid'])
        time.sleep(3)

        cls.endpoint = 'http://127.0.0.1:5000'

        headers = {'Content-Type': 'application/json'}
        params = {'username': 'admin', 'password': 'admin', 'action': 'generate'}
        # get token
        response = requests.post(cls.endpoint + '/login', json=params, headers=headers)
        resp = response.json()
        cls.token = resp['token']
        cls.auth = requests.auth.HTTPBasicAuth(cls.token, '')

    @classmethod
    def tearDownClass(cls):
        """"""
        Kill uwsgi

        :return: None
        """"""
        subprocess.call(['uwsgi', '--stop', '/tmp/uwsgi.pid'])
        time.sleep(2)

    @classmethod
    def tearDown(cls):
        """"""
        Delete resources in backend

        :return: None
        """"""
        for resource in ['host', 'service', 'command', 'livestate', 'livesynthesis']:
            requests.delete(cls.endpoint + '/' + resource, auth=cls.auth)

    def test_default_commands(self):
        """"""
        Default cmomands exist ..
        :return:
        """"""

        # get commands
        response = requests.get(self.endpoint + '/command', auth=self.auth)
        resp = response.json()
        self.assertEqual(len(resp['_items']), 2)
        for item in resp['_items']:
            self.assertIn(item['name'], ['_echo', '_internal_host_up'])
",Add test for default commands,"Add test for default commands
",Python,agpl-3.0,"Alignak-monitoring-contrib/alignak-backend,Alignak-monitoring-contrib/alignak-backend,Alignak-monitoring-contrib/alignak-backend,Alignak-monitoring-contrib/alignak-backend",92,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""
This test check the commands
""""""

from __future__ import print_function
import os
import time
import shlex
import subprocess
import copy
import requests
import unittest2


class TestGroups(unittest2.TestCase):
    """"""
    This class test hostgroups and tree feature
    """"""

    @classmethod
    def setUpClass(cls):
        """"""
        This method:
          * deletes mongodb database
          * starts the backend with uwsgi
          * logs in the backend and get the token
          * gets the hostgroup

        :return: None
        """"""
        # Set test mode for Alignak backend
        os.environ['TEST_ALIGNAK_BACKEND'] = '1'
        os.environ['ALIGNAK_BACKEND_MONGO_DBNAME'] = 'alignak-backend-test'

        # Delete used mongo DBs
        exit_code = subprocess.call(
            shlex.split(
                'mongo %s --eval ""db.dropDatabase()""' % os.environ['ALIGNAK_BACKEND_MONGO_DBNAME'])
        )
        assert exit_code == 0

        cls.p = subprocess.Popen(['uwsgi', '--plugin', 'python', '-w', 'alignakbackend:app',
                                  '--socket', '0.0.0.0:5000',
                                  '--protocol=http', '--enable-threads', '--pidfile',
                                  '/tmp/uwsgi.pid'])
        time.sleep(3)

        cls.endpoint = 'http://127.0.0.1:5000'

        headers = {'Content-Type': 'application/json'}
        params = {'username': 'admin', 'password': 'admin', 'action': 'generate'}
        # get token
        response = requests.post(cls.endpoint + '/login', json=params, headers=headers)
        resp = response.json()
        cls.token = resp['token']
        cls.auth = requests.auth.HTTPBasicAuth(cls.token, '')

    @classmethod
    def tearDownClass(cls):
        """"""
        Kill uwsgi

        :return: None
        """"""
        subprocess.call(['uwsgi', '--stop', '/tmp/uwsgi.pid'])
        time.sleep(2)

    @classmethod
    def tearDown(cls):
        """"""
        Delete resources in backend

        :return: None
        """"""
        for resource in ['host', 'service', 'command', 'livestate', 'livesynthesis']:
            requests.delete(cls.endpoint + '/' + resource, auth=cls.auth)

    def test_default_commands(self):
        """"""
        Default cmomands exist ..
        :return:
        """"""

        # get commands
        response = requests.get(self.endpoint + '/command', auth=self.auth)
        resp = response.json()
        self.assertEqual(len(resp['_items']), 2)
        for item in resp['_items']:
            self.assertIn(item['name'], ['_echo', '_internal_host_up'])

```"
82cc7fa29747672abe6c27d3540c272b576c1d4b,listfriendships.py,listfriendships.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""This script lists all the followers of a Twitter user.

Examples:
  You must specify the user's screen_name you want to show::

    $ python listfriendships.py list-friends screen_name
    $ python listfriendships.py list-followers screen_name
""""""

from secret import twitter_instance
from argparse import ArgumentParser

__version__ = '1.0.0'

COMMAND_LIST_FRIENDS = 'list-friends'
COMMAND_LIST_FOLLOWERS = 'list-followers'

def configure():
    """"""Parse the command line parameters.

    Returns:
        An instance of argparse.ArgumentParser that stores the command line
        parameters.
    """"""

    parser = ArgumentParser(description='Twitter Followers Viewer')
    parser.add_argument('--version', action='version', version=__version__)

    # Positional arguments
    parser.add_argument(
        'command',
        choices=[COMMAND_LIST_FRIENDS, COMMAND_LIST_FOLLOWERS,],
        help='Either ""{0}"" or ""{1}"".'.format(COMMAND_LIST_FRIENDS, COMMAND_LIST_FOLLOWERS))

    parser.add_argument(
        'screen_name',
        help='The screen name of the target user.')

    return parser

def format_user(user):
    """"""Return a string that shows user information.

    Args:
        user: An instance of the Twitter API users response object.

    Returns:
        A colon-separated value string.
    """"""
    return '{screen_name}:{name}:{description}:{url}'.format(**user).replace('\r', '').replace('\n', '')

def main(args):
    """"""The main function.

    Args:
        args: An instance of argparse.ArgumentParser parsed in the configure
            function.

    Returns:
        None.
    """"""

    tw = twitter_instance()
    next_cursor = -1

    cmd = None
    if args.command == COMMAND_LIST_FRIENDS:
        cmd = tw.friends.list
    elif args.command == COMMAND_LIST_FOLLOWERS:
        cmd = tw.followers.list

    while next_cursor != 0:
        friends = cmd(
            screen_name=args.screen_name,
            cursor=next_cursor,
            count=200,
            skip_status=True,
            include_user_entities=False,)

        for user in friends['users']:
            print(format_user(user))

        next_cursor = friends['next_cursor']

if __name__ == '__main__':
    parser = configure()
    main(parser.parse_args())
",Add a Python script which lists the friendships of a Twitter user.,"Add a Python script which lists the friendships of a Twitter user.
",Python,mit,"showa-yojyo/bin,showa-yojyo/bin",90,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
""""""This script lists all the followers of a Twitter user.

Examples:
  You must specify the user's screen_name you want to show::

    $ python listfriendships.py list-friends screen_name
    $ python listfriendships.py list-followers screen_name
""""""

from secret import twitter_instance
from argparse import ArgumentParser

__version__ = '1.0.0'

COMMAND_LIST_FRIENDS = 'list-friends'
COMMAND_LIST_FOLLOWERS = 'list-followers'

def configure():
    """"""Parse the command line parameters.

    Returns:
        An instance of argparse.ArgumentParser that stores the command line
        parameters.
    """"""

    parser = ArgumentParser(description='Twitter Followers Viewer')
    parser.add_argument('--version', action='version', version=__version__)

    # Positional arguments
    parser.add_argument(
        'command',
        choices=[COMMAND_LIST_FRIENDS, COMMAND_LIST_FOLLOWERS,],
        help='Either ""{0}"" or ""{1}"".'.format(COMMAND_LIST_FRIENDS, COMMAND_LIST_FOLLOWERS))

    parser.add_argument(
        'screen_name',
        help='The screen name of the target user.')

    return parser

def format_user(user):
    """"""Return a string that shows user information.

    Args:
        user: An instance of the Twitter API users response object.

    Returns:
        A colon-separated value string.
    """"""
    return '{screen_name}:{name}:{description}:{url}'.format(**user).replace('\r', '').replace('\n', '')

def main(args):
    """"""The main function.

    Args:
        args: An instance of argparse.ArgumentParser parsed in the configure
            function.

    Returns:
        None.
    """"""

    tw = twitter_instance()
    next_cursor = -1

    cmd = None
    if args.command == COMMAND_LIST_FRIENDS:
        cmd = tw.friends.list
    elif args.command == COMMAND_LIST_FOLLOWERS:
        cmd = tw.followers.list

    while next_cursor != 0:
        friends = cmd(
            screen_name=args.screen_name,
            cursor=next_cursor,
            count=200,
            skip_status=True,
            include_user_entities=False,)

        for user in friends['users']:
            print(format_user(user))

        next_cursor = friends['next_cursor']

if __name__ == '__main__':
    parser = configure()
    main(parser.parse_args())

```"
bae587f2d6d9b9ea3023934ad5796ff6a15fe765,nstl/passes/nameresolve.py,nstl/passes/nameresolve.py,,"from .. import ast

import sys
from itertools import chain


class Scope(dict):
    def __init__(self, parent=None, name=None, *args, **kwargs):
        assert isinstance(parent, Scope) or parent is None
        
        super().__init__(*args, **kwargs)
        self.name = name
        self.parent = parent
        self.scopes = dict()
        if parent is not None and self.name is not None:
            self.parent.scopes.update({self.name:self})
    
    
    def get_outer_scope(self, name):
        """"""Return the nearest reachable scope with the corresponding name.
        """"""
        try:
            return self.scopes[name]
        except KeyError:
            if self.parent is None:
                raise NameError(""scope {} is not reachable"".format(name))
            return self.parent.get_outer_scope(name)
    
    
    def __contains__(self, name):
        """"""Return whether a name is reachable from the current scope.
        """"""
        return (super().__contains__(name) or
                                any(name in scope for scope in self.parents()))
    
    
    def __getitem__(self, name):
        """"""Return the object binding to a name, if the name is in scope.
        """"""
        try:
            return super().__getitem__(name)
        except KeyError:
            if self.parent is None:
                raise NameError(""name {} is not in scope"".format(name))
            return self.parent.__getitem__(name)
    
    
    def __setitem__(self, *args, **kwargs):
        """"""Bind a name to an object inside the current scope.
        """"""
        return super().__setitem__(*args, **kwargs)
    
    
    def parents(self):
        parent = self.parent
        while parent is not None:
            yield parent
            parent = parent.parent
    
    
    def show(self, buf=sys.stdout):
        lead = ''
        for scope in reversed(list(chain([self], self.parents()))):
            for name, binding in scope.items():
                buf.write(lead + str(name) + "" : "" + str(binding) + ""\n"")
            lead = lead + ' ' * 4



class NameResolver(ast.NodeVisitor):
    def visit_Namespace(self, node, current_scope=Scope()):
        current_scope[node.name.value] = node
        node.scope = Scope(current_scope, node.name.value)
        self.generic_visit(node, node.scope)
    
    
    def visit_Template(self, node, current_scope=Scope()):
        current_scope[node.name.value] = node
        node.scope = Scope(current_scope)
        self.generic_visit(node, node.scope)
    
    
    def visit_Identifier(self, node, current_scope):
        if node.value not in current_scope:
            raise NameError(""unresolved reference {}"".format(node.value))
        node.resolved = current_scope[node.value]
    
    
    def visit_QualifiedIdentifier(self, node, current_scope):
        outer, *rest = node.quals
        scope = current_scope.get_outer_scope(outer.value)
        for qual in rest:
            scope = getattr(scope, qual.value)
        
        self.visit(node.name, scope)
        node.resolved = node.name.resolved

",Rewrite the name resolving pass.,"Rewrite the name resolving pass.

This pass takes care of resolving which object are identifiers bound to.
It then stores the result by adding a 'resolved' attribute to each identifier
and qualified identifier node in the tree. These 'resolved' attributes can then
be accessed to refer to the object bound to the identifier, which is
invaluable during code generation.
",Python,bsd-3-clause,"ldionne/nstl-lang,ldionne/nstl-lang",98,"```python
from .. import ast

import sys
from itertools import chain


class Scope(dict):
    def __init__(self, parent=None, name=None, *args, **kwargs):
        assert isinstance(parent, Scope) or parent is None
        
        super().__init__(*args, **kwargs)
        self.name = name
        self.parent = parent
        self.scopes = dict()
        if parent is not None and self.name is not None:
            self.parent.scopes.update({self.name:self})
    
    
    def get_outer_scope(self, name):
        """"""Return the nearest reachable scope with the corresponding name.
        """"""
        try:
            return self.scopes[name]
        except KeyError:
            if self.parent is None:
                raise NameError(""scope {} is not reachable"".format(name))
            return self.parent.get_outer_scope(name)
    
    
    def __contains__(self, name):
        """"""Return whether a name is reachable from the current scope.
        """"""
        return (super().__contains__(name) or
                                any(name in scope for scope in self.parents()))
    
    
    def __getitem__(self, name):
        """"""Return the object binding to a name, if the name is in scope.
        """"""
        try:
            return super().__getitem__(name)
        except KeyError:
            if self.parent is None:
                raise NameError(""name {} is not in scope"".format(name))
            return self.parent.__getitem__(name)
    
    
    def __setitem__(self, *args, **kwargs):
        """"""Bind a name to an object inside the current scope.
        """"""
        return super().__setitem__(*args, **kwargs)
    
    
    def parents(self):
        parent = self.parent
        while parent is not None:
            yield parent
            parent = parent.parent
    
    
    def show(self, buf=sys.stdout):
        lead = ''
        for scope in reversed(list(chain([self], self.parents()))):
            for name, binding in scope.items():
                buf.write(lead + str(name) + "" : "" + str(binding) + ""\n"")
            lead = lead + ' ' * 4



class NameResolver(ast.NodeVisitor):
    def visit_Namespace(self, node, current_scope=Scope()):
        current_scope[node.name.value] = node
        node.scope = Scope(current_scope, node.name.value)
        self.generic_visit(node, node.scope)
    
    
    def visit_Template(self, node, current_scope=Scope()):
        current_scope[node.name.value] = node
        node.scope = Scope(current_scope)
        self.generic_visit(node, node.scope)
    
    
    def visit_Identifier(self, node, current_scope):
        if node.value not in current_scope:
            raise NameError(""unresolved reference {}"".format(node.value))
        node.resolved = current_scope[node.value]
    
    
    def visit_QualifiedIdentifier(self, node, current_scope):
        outer, *rest = node.quals
        scope = current_scope.get_outer_scope(outer.value)
        for qual in rest:
            scope = getattr(scope, qual.value)
        
        self.visit(node.name, scope)
        node.resolved = node.name.resolved


```"
50586ffe08473e5c7dc11b51f9b20923900e4f1c,tests/integration/cloudformation/test_connection.py,tests/integration/cloudformation/test_connection.py,,"#!/usr/bin/env python
import time
import json

from tests.unit import  unittest
from boto.cloudformation.connection import CloudFormationConnection


BASIC_EC2_TEMPLATE = {
    ""AWSTemplateFormatVersion"": ""2010-09-09"",
    ""Description"": ""AWS CloudFormation Sample Template EC2InstanceSample"",
    ""Parameters"": {
    },
    ""Mappings"": {
        ""RegionMap"": {
            ""us-east-1"": {
                ""AMI"": ""ami-7f418316""
            }
        }
    },
    ""Resources"": {
        ""Ec2Instance"": {
            ""Type"": ""AWS::EC2::Instance"",
            ""Properties"": {
                ""ImageId"": {
                    ""Fn::FindInMap"": [
                        ""RegionMap"",
                        {
                            ""Ref"": ""AWS::Region""
                        },
                        ""AMI""
                    ]
                },
                ""UserData"": {
                    ""Fn::Base64"": ""a"" * 15000
                }
            }
        }
    },
    ""Outputs"": {
        ""InstanceId"": {
            ""Description"": ""InstanceId of the newly created EC2 instance"",
            ""Value"": {
                ""Ref"": ""Ec2Instance""
            }
        },
        ""AZ"": {
            ""Description"": ""Availability Zone of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""AvailabilityZone""
                ]
            }
        },
        ""PublicIP"": {
            ""Description"": ""Public IP address of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PublicIp""
                ]
            }
        },
        ""PrivateIP"": {
            ""Description"": ""Private IP address of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PrivateIp""
                ]
            }
        },
        ""PublicDNS"": {
            ""Description"": ""Public DNSName of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PublicDnsName""
                ]
            }
        },
        ""PrivateDNS"": {
            ""Description"": ""Private DNSName of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PrivateDnsName""
                ]
            }
        }
    }
}


class TestCloudformationConnection(unittest.TestCase):
    def setUp(self):
        self.connection = CloudFormationConnection()
        self.stack_name = 'testcfnstack' + str(int(time.time()))

    def test_large_template_stack_size(self):
        # See https://github.com/boto/boto/issues/1037
        body = self.connection.create_stack(
            self.stack_name,
            template_body=json.dumps(BASIC_EC2_TEMPLATE))
        self.addCleanup(self.connection.delete_stack, self.stack_name)


if __name__ == '__main__':
    unittest.main()
",Add failing integration test for sigv4 failure,"Add failing integration test for sigv4 failure

This repros #1037.
",Python,mit,"jotes/boto,revmischa/boto,trademob/boto,shaunbrady/boto,awatts/boto,darjus-amzn/boto,alex/boto,jindongh/boto,zzzirk/boto,jameslegg/boto,campenberger/boto,nexusz99/boto,acourtney2015/boto,garnaat/boto,s0enke/boto,lra/boto,ocadotechnology/boto,pfhayes/boto,nikhilraog/boto,clouddocx/boto,ryansb/boto,ekalosak/boto,vijaylbais/boto,israelbenatar/boto,kouk/boto,disruptek/boto,tpodowd/boto,kouk/boto,FATruden/boto,ddzialak/boto,Asana/boto,dablak/boto,lochiiconnectivity/boto,weka-io/boto,nishigori/boto,dablak/boto,rjschwei/boto,khagler/boto,Timus1712/boto,stevenbrichards/boto,vishnugonela/boto,bleib1dj/boto,varunarya10/boto,alex/boto,ric03uec/boto,alfredodeza/boto,drbild/boto,SaranyaKarthikeyan/boto,cyclecomputing/boto,rjschwei/boto,ramitsurana/boto,appneta/boto,shipci/boto,appneta/boto,jamesls/boto,rayluo/boto,podhmo/boto,andresriancho/boto,serviceagility/boto,felix-d/boto,drbild/boto,dimdung/boto,Pretio/boto,lochiiconnectivity/boto,rosmo/boto,bryx-inc/boto,janslow/boto,zachmullen/boto,weebygames/boto,jamesls/boto,abridgett/boto,tpodowd/boto,TiVoMaker/boto,jameslegg/boto,j-carl/boto,disruptek/boto,elainexmas/boto,andresriancho/boto,yangchaogit/boto",111,"```python
#!/usr/bin/env python
import time
import json

from tests.unit import  unittest
from boto.cloudformation.connection import CloudFormationConnection


BASIC_EC2_TEMPLATE = {
    ""AWSTemplateFormatVersion"": ""2010-09-09"",
    ""Description"": ""AWS CloudFormation Sample Template EC2InstanceSample"",
    ""Parameters"": {
    },
    ""Mappings"": {
        ""RegionMap"": {
            ""us-east-1"": {
                ""AMI"": ""ami-7f418316""
            }
        }
    },
    ""Resources"": {
        ""Ec2Instance"": {
            ""Type"": ""AWS::EC2::Instance"",
            ""Properties"": {
                ""ImageId"": {
                    ""Fn::FindInMap"": [
                        ""RegionMap"",
                        {
                            ""Ref"": ""AWS::Region""
                        },
                        ""AMI""
                    ]
                },
                ""UserData"": {
                    ""Fn::Base64"": ""a"" * 15000
                }
            }
        }
    },
    ""Outputs"": {
        ""InstanceId"": {
            ""Description"": ""InstanceId of the newly created EC2 instance"",
            ""Value"": {
                ""Ref"": ""Ec2Instance""
            }
        },
        ""AZ"": {
            ""Description"": ""Availability Zone of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""AvailabilityZone""
                ]
            }
        },
        ""PublicIP"": {
            ""Description"": ""Public IP address of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PublicIp""
                ]
            }
        },
        ""PrivateIP"": {
            ""Description"": ""Private IP address of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PrivateIp""
                ]
            }
        },
        ""PublicDNS"": {
            ""Description"": ""Public DNSName of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PublicDnsName""
                ]
            }
        },
        ""PrivateDNS"": {
            ""Description"": ""Private DNSName of the newly created EC2 instance"",
            ""Value"": {
                ""Fn::GetAtt"": [
                    ""Ec2Instance"",
                    ""PrivateDnsName""
                ]
            }
        }
    }
}


class TestCloudformationConnection(unittest.TestCase):
    def setUp(self):
        self.connection = CloudFormationConnection()
        self.stack_name = 'testcfnstack' + str(int(time.time()))

    def test_large_template_stack_size(self):
        # See https://github.com/boto/boto/issues/1037
        body = self.connection.create_stack(
            self.stack_name,
            template_body=json.dumps(BASIC_EC2_TEMPLATE))
        self.addCleanup(self.connection.delete_stack, self.stack_name)


if __name__ == '__main__':
    unittest.main()

```"
c99929e2aaa119104c20b0d865b8e42c0b15a03e,morse/morse_radio.py,morse/morse_radio.py,,"# Import modules
from microbit import *
import radio

# define morse code dictionary
morse = {
    ""a"": "".-"",
    ""b"": ""-..."",
    ""c"": ""-.-."",
    ""d"": ""-.."",
    ""e"": ""."",
    ""f"": ""..-."",
    ""g"": ""--."",
    ""h"": ""...."",
    ""i"": "".."",
    ""j"": "".---"",
    ""k"": ""-.-"",
    ""l"": "".-.."",
    ""m"": ""--"",
    ""n"": ""-."",
    ""o"": ""---"",
    ""p"": "".--."",
    ""q"": ""--.-"",
    ""r"": "".-."",
    ""s"": ""..."",
    ""t"": ""-"",
    ""u"": ""..-"",
    ""v"": ""...-"",
    ""w"": "".--"",
    ""x"": ""-..-"",
    ""y"": ""-.--"",
    ""z"": ""--.."",
    ""1"": "".----"",
    ""2"": ""..---"",
    ""3"": ""...--"",
    ""4"": ""....-"",
    ""5"": ""....."",
    ""6"": ""-...."",
    ""7"": ""--..."",
    ""8"": ""---.."",
    ""9"": ""----."",
    ""0"": ""-----""
}

current_letter = """"
morse_string = """"
pressed = 0
paused = 0
letters = []

radio.on()

def detect_dot_dash(time_pressed):
    return ""."" if time_pressed <= 50 else ""-""

def get_letter(code):
    global morse
    for key,value in morse.items():
        if code == value:
            return key

    return """"

while True:
    sleep(1) # do not use all the cpu power

    #check for incoming messages
    incoming = radio.receive()
    if incoming is not None:
        print(incoming.split(""|""))
        sent_letters = []

        for letter in incoming.split(""|""):
            sent_letters.append(get_letter(letter) if letter != "" "" else "" "")

        display.scroll("""".join(sent_letters))

    # make a loop to test for the button being pressed
    if button_a.is_pressed():
        if paused >= 100:
            letters.append(get_letter(current_letter))
            morse_string += current_letter + ""|""
            current_letter = """"

            if paused >= 200:
                letters.append("" "")
                morse_string += ""| |""

            paused = 0

        pressed = 1
        while button_a.is_pressed():
            # wait until the button is not pressed any more
            sleep(1) # do not use all the cpu power
            pressed += 1
        # measure the time
        current_letter += detect_dot_dash(pressed)
        paused = 1
    else:
        if paused > 0:
            paused +=1

    if button_b.is_pressed() or accelerometer.current_gesture() == ""shake"":
        letters.append(get_letter(current_letter))
        morse_string += current_letter
        display.scroll("""".join(letters))
        paused = 0
        pressed = 0
        print(morse_string)
        radio.send(morse_string)
        current_letter = """"
        morse_string = """"
        letters = []
",Add ability to send morse code via radio,"Add ability to send morse code via radio
",Python,mit,OiNutter/microbit-scripts,114,"```python
# Import modules
from microbit import *
import radio

# define morse code dictionary
morse = {
    ""a"": "".-"",
    ""b"": ""-..."",
    ""c"": ""-.-."",
    ""d"": ""-.."",
    ""e"": ""."",
    ""f"": ""..-."",
    ""g"": ""--."",
    ""h"": ""...."",
    ""i"": "".."",
    ""j"": "".---"",
    ""k"": ""-.-"",
    ""l"": "".-.."",
    ""m"": ""--"",
    ""n"": ""-."",
    ""o"": ""---"",
    ""p"": "".--."",
    ""q"": ""--.-"",
    ""r"": "".-."",
    ""s"": ""..."",
    ""t"": ""-"",
    ""u"": ""..-"",
    ""v"": ""...-"",
    ""w"": "".--"",
    ""x"": ""-..-"",
    ""y"": ""-.--"",
    ""z"": ""--.."",
    ""1"": "".----"",
    ""2"": ""..---"",
    ""3"": ""...--"",
    ""4"": ""....-"",
    ""5"": ""....."",
    ""6"": ""-...."",
    ""7"": ""--..."",
    ""8"": ""---.."",
    ""9"": ""----."",
    ""0"": ""-----""
}

current_letter = """"
morse_string = """"
pressed = 0
paused = 0
letters = []

radio.on()

def detect_dot_dash(time_pressed):
    return ""."" if time_pressed <= 50 else ""-""

def get_letter(code):
    global morse
    for key,value in morse.items():
        if code == value:
            return key

    return """"

while True:
    sleep(1) # do not use all the cpu power

    #check for incoming messages
    incoming = radio.receive()
    if incoming is not None:
        print(incoming.split(""|""))
        sent_letters = []

        for letter in incoming.split(""|""):
            sent_letters.append(get_letter(letter) if letter != "" "" else "" "")

        display.scroll("""".join(sent_letters))

    # make a loop to test for the button being pressed
    if button_a.is_pressed():
        if paused >= 100:
            letters.append(get_letter(current_letter))
            morse_string += current_letter + ""|""
            current_letter = """"

            if paused >= 200:
                letters.append("" "")
                morse_string += ""| |""

            paused = 0

        pressed = 1
        while button_a.is_pressed():
            # wait until the button is not pressed any more
            sleep(1) # do not use all the cpu power
            pressed += 1
        # measure the time
        current_letter += detect_dot_dash(pressed)
        paused = 1
    else:
        if paused > 0:
            paused +=1

    if button_b.is_pressed() or accelerometer.current_gesture() == ""shake"":
        letters.append(get_letter(current_letter))
        morse_string += current_letter
        display.scroll("""".join(letters))
        paused = 0
        pressed = 0
        print(morse_string)
        radio.send(morse_string)
        current_letter = """"
        morse_string = """"
        letters = []

```"
2311673478616bf16b4411772511e301e82b9860,misc/cairo-group-pdf-bug.py,misc/cairo-group-pdf-bug.py,,"import cairo
import math

def fill_background(cr):
    cr.set_source_rgb(0, 0, 1)
    cr.paint()

def paint_from_image(cr, src):
    cr.set_operator(cairo.OPERATOR_SATURATE)

    cr.set_source_surface(src, 0, 0)
    cr.paint()

    cr.set_source_surface(src, 100, 0)
    cr.paint()

    cr.set_source_surface(src, 200, 0)
    cr.paint()

    cr.set_source_surface(src, 300, 0)
    cr.paint()

def clip(cr):
    cr.rectangle(0, 0, 400, 400)
    cr.clip()

def clear(cr):
    cr.set_operator(cairo.OPERATOR_CLEAR)
    cr.paint()


# init image
src = cairo.ImageSurface(cairo.FORMAT_ARGB32, 100, 100)
cr = cairo.Context(src)
cr.set_source_rgb(1, 1, 1)
cr.paint()

# init pdf
pdf = cairo.PDFSurface(""out.pdf"", 400, 400)
cr = cairo.Context(pdf)
cr.rotate(math.pi / 4)

# page 1, push paint pop paint
fill_background(cr)

cr.push_group()
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()

# page 2, push clear paint pop paint
fill_background(cr)

cr.push_group()
clear(cr)
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()

# page 3, push clip paint pop paint
fill_background(cr)

cr.push_group()
clip(cr)
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()

# page 4, push clip clear paint pop paint
fill_background(cr)

cr.push_group()
clip(cr)
clear(cr)
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()


# done
pdf.finish()
",Add python program to illustrate strange cairo bug with group and pdf,"Add python program to illustrate strange cairo bug with group and pdf
",Python,lgpl-2.1,"openslide/openslide,openslide/openslide,openslide/openslide,openslide/openslide",90,"```python
import cairo
import math

def fill_background(cr):
    cr.set_source_rgb(0, 0, 1)
    cr.paint()

def paint_from_image(cr, src):
    cr.set_operator(cairo.OPERATOR_SATURATE)

    cr.set_source_surface(src, 0, 0)
    cr.paint()

    cr.set_source_surface(src, 100, 0)
    cr.paint()

    cr.set_source_surface(src, 200, 0)
    cr.paint()

    cr.set_source_surface(src, 300, 0)
    cr.paint()

def clip(cr):
    cr.rectangle(0, 0, 400, 400)
    cr.clip()

def clear(cr):
    cr.set_operator(cairo.OPERATOR_CLEAR)
    cr.paint()


# init image
src = cairo.ImageSurface(cairo.FORMAT_ARGB32, 100, 100)
cr = cairo.Context(src)
cr.set_source_rgb(1, 1, 1)
cr.paint()

# init pdf
pdf = cairo.PDFSurface(""out.pdf"", 400, 400)
cr = cairo.Context(pdf)
cr.rotate(math.pi / 4)

# page 1, push paint pop paint
fill_background(cr)

cr.push_group()
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()

# page 2, push clear paint pop paint
fill_background(cr)

cr.push_group()
clear(cr)
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()

# page 3, push clip paint pop paint
fill_background(cr)

cr.push_group()
clip(cr)
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()

# page 4, push clip clear paint pop paint
fill_background(cr)

cr.push_group()
clip(cr)
clear(cr)
paint_from_image(cr, src)
cr.pop_group_to_source()
cr.paint()

cr.show_page()


# done
pdf.finish()

```"
dc181cd45adba6433c2422b9550e398c6385bca5,shop/serializers/catalog.py,shop/serializers/catalog.py,,"# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from cms.utils import get_current_site
from cms.utils.page import get_page_from_path
from django.core.urlresolvers import reverse
from filer.models.imagemodels import Image
from rest_framework import serializers


class CMSPagesField(serializers.Field):
    """"""
    A serializer field used to create the many-to-many relations for models inheriting from the
    unmanaged :class:`shop.models.related.BaseProductPage`.

    Usage in serializers to import/export product model data:

    class MyProductSerializer():
        ...
        cms_pages = CMSPagesField()
        ...
    """"""
    def to_representation(self, value):
        urls = {page.get_absolute_url() for page in value.all()}
        return list(urls)

    def to_internal_value(self, data):
        site = get_current_site()
        pages_root = reverse('pages-root')
        ret = []
        for path in data:
            if path.startswith(pages_root):
                path = path[len(pages_root):]
            # strip any final slash
            if path.endswith('/'):
                path = path[:-1]
            page = get_page_from_path(site, path)
            if page:
                ret.append(page)
        return ret


class ImagesField(serializers.Field):
    """"""
    A serializer field used to create the many-to-many relations for models inheriting from the
    unmanaged :class:`shop.models.related.BaseProductImage`.

    Usage in serializers to import/export product model data:

    class MyProductSerializer():
        ...
        images = ImagesField()
        ...
    """"""
    def to_representation(self, value):
        return list(value.values_list('pk', flat=True))

    def to_internal_value(self, data):
        return list(Image.objects.filter(pk__in=data))


class ValueRelatedField(serializers.RelatedField):
    """"""
    A serializer field used to access a single value from a related model.
    Usage:

        myfield = ValueRelatedField(model=MyModel)
        myfield = ValueRelatedField(model=MyModel, field_name='myfield')

    This serializes objects of type ``MyModel`` so that that the return data is a simple scalar.

    On deserialization it creates an object of type ``MyModel``, if none could be found with the
    given field name.
    """"""
    def __init__(self, *args, **kwargs):
        self.model = kwargs.pop('model')
        self.related_field_name = kwargs.pop('field_name', 'name')
        super(ValueRelatedField, self).__init__(*args, **kwargs)

    def get_queryset(self):
        return self.model.objects.all()

    def to_representation(self, value):
        return getattr(value, self.related_field_name)

    def to_internal_value(self, value):
        data = {self.related_field_name: value}
        instance, _ = self.model.objects.get_or_create(**data)
        return instance
",Create M2M serializers for export/import of products,"Create M2M serializers for export/import of products
",Python,bsd-3-clause,"awesto/django-shop,divio/django-shop,divio/django-shop,awesto/django-shop,awesto/django-shop,divio/django-shop",90,"```python
# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from cms.utils import get_current_site
from cms.utils.page import get_page_from_path
from django.core.urlresolvers import reverse
from filer.models.imagemodels import Image
from rest_framework import serializers


class CMSPagesField(serializers.Field):
    """"""
    A serializer field used to create the many-to-many relations for models inheriting from the
    unmanaged :class:`shop.models.related.BaseProductPage`.

    Usage in serializers to import/export product model data:

    class MyProductSerializer():
        ...
        cms_pages = CMSPagesField()
        ...
    """"""
    def to_representation(self, value):
        urls = {page.get_absolute_url() for page in value.all()}
        return list(urls)

    def to_internal_value(self, data):
        site = get_current_site()
        pages_root = reverse('pages-root')
        ret = []
        for path in data:
            if path.startswith(pages_root):
                path = path[len(pages_root):]
            # strip any final slash
            if path.endswith('/'):
                path = path[:-1]
            page = get_page_from_path(site, path)
            if page:
                ret.append(page)
        return ret


class ImagesField(serializers.Field):
    """"""
    A serializer field used to create the many-to-many relations for models inheriting from the
    unmanaged :class:`shop.models.related.BaseProductImage`.

    Usage in serializers to import/export product model data:

    class MyProductSerializer():
        ...
        images = ImagesField()
        ...
    """"""
    def to_representation(self, value):
        return list(value.values_list('pk', flat=True))

    def to_internal_value(self, data):
        return list(Image.objects.filter(pk__in=data))


class ValueRelatedField(serializers.RelatedField):
    """"""
    A serializer field used to access a single value from a related model.
    Usage:

        myfield = ValueRelatedField(model=MyModel)
        myfield = ValueRelatedField(model=MyModel, field_name='myfield')

    This serializes objects of type ``MyModel`` so that that the return data is a simple scalar.

    On deserialization it creates an object of type ``MyModel``, if none could be found with the
    given field name.
    """"""
    def __init__(self, *args, **kwargs):
        self.model = kwargs.pop('model')
        self.related_field_name = kwargs.pop('field_name', 'name')
        super(ValueRelatedField, self).__init__(*args, **kwargs)

    def get_queryset(self):
        return self.model.objects.all()

    def to_representation(self, value):
        return getattr(value, self.related_field_name)

    def to_internal_value(self, value):
        data = {self.related_field_name: value}
        instance, _ = self.model.objects.get_or_create(**data)
        return instance

```"
096424ea7809e5512a932c79eff6676695c1d27e,telethon/network/connection/connection.py,telethon/network/connection/connection.py,,"import abc
import asyncio


class Connection(abc.ABC):
    """"""
    The `Connection` class is a wrapper around ``asyncio.open_connection``.

    Subclasses are meant to communicate with this class through a queue.

    This class provides a reliable interface that will stay connected
    under any conditions for as long as the user doesn't disconnect or
    the input parameters to auto-reconnect dictate otherwise.
    """"""
    # TODO Support proxy. Support timeout?
    def __init__(self, ip, port, *, loop):
        self._ip = ip
        self._port = port
        self._loop = loop
        self._reader = None
        self._writer = None
        self._disconnected = asyncio.Event(loop=loop)
        self._disconnected.set()
        self._send_task = None
        self._recv_task = None
        self._send_queue = asyncio.Queue(1)
        self._recv_queue = asyncio.Queue(1)

    async def connect(self):
        """"""
        Establishes a connection with the server.
        """"""
        self._reader, self._writer = await asyncio.open_connection(
            self._ip, self._port, loop=self._loop)

        self._disconnected.clear()
        self._send_task = self._loop.create_task(self._send_loop())
        self._recv_task = self._loop.create_task(self._send_loop())

    def disconnect(self):
        """"""
        Disconnects from the server.
        """"""
        self._disconnected.set()
        self._writer.close()

    def send(self, data):
        """"""
        Sends a packet of data through this connection mode.

        This method returns a coroutine.
        """"""
        return self._send_queue.put(data)

    def recv(self):
        """"""
        Receives a packet of data through this connection mode.

        This method returns a coroutine.
        """"""
        return self._recv_queue.get()

    # TODO Get/put to the queue with cancellation
    async def _send_loop(self):
        """"""
        This loop is constantly popping items off the queue to send them.
        """"""
        while not self._disconnected.is_set():
            self._send(await self._send_queue.get())
            await self._writer.drain()

    async def _recv_loop(self):
        """"""
        This loop is constantly putting items on the queue as they're read.
        """"""
        while not self._disconnected.is_set():
            data = await self._recv()
            await self._recv_queue.put(data)

    @abc.abstractmethod
    def _send(self, data):
        """"""
        This method should be implemented differently under each
        connection mode and serialize the data into the packet
        the way it should be sent through `self._writer`.
        """"""
        raise NotImplementedError

    @abc.abstractmethod
    async def _recv(self):
        """"""
        This method should be implemented differently under each
        connection mode and deserialize the data from the packet
        the way it should be read from `self._reader`.
        """"""
        raise NotImplementedError
",Create a new Connection class to work through queues,"Create a new Connection class to work through queues
",Python,mit,"LonamiWebs/Telethon,expectocode/Telethon,LonamiWebs/Telethon,LonamiWebs/Telethon,LonamiWebs/Telethon",97,"```python
import abc
import asyncio


class Connection(abc.ABC):
    """"""
    The `Connection` class is a wrapper around ``asyncio.open_connection``.

    Subclasses are meant to communicate with this class through a queue.

    This class provides a reliable interface that will stay connected
    under any conditions for as long as the user doesn't disconnect or
    the input parameters to auto-reconnect dictate otherwise.
    """"""
    # TODO Support proxy. Support timeout?
    def __init__(self, ip, port, *, loop):
        self._ip = ip
        self._port = port
        self._loop = loop
        self._reader = None
        self._writer = None
        self._disconnected = asyncio.Event(loop=loop)
        self._disconnected.set()
        self._send_task = None
        self._recv_task = None
        self._send_queue = asyncio.Queue(1)
        self._recv_queue = asyncio.Queue(1)

    async def connect(self):
        """"""
        Establishes a connection with the server.
        """"""
        self._reader, self._writer = await asyncio.open_connection(
            self._ip, self._port, loop=self._loop)

        self._disconnected.clear()
        self._send_task = self._loop.create_task(self._send_loop())
        self._recv_task = self._loop.create_task(self._send_loop())

    def disconnect(self):
        """"""
        Disconnects from the server.
        """"""
        self._disconnected.set()
        self._writer.close()

    def send(self, data):
        """"""
        Sends a packet of data through this connection mode.

        This method returns a coroutine.
        """"""
        return self._send_queue.put(data)

    def recv(self):
        """"""
        Receives a packet of data through this connection mode.

        This method returns a coroutine.
        """"""
        return self._recv_queue.get()

    # TODO Get/put to the queue with cancellation
    async def _send_loop(self):
        """"""
        This loop is constantly popping items off the queue to send them.
        """"""
        while not self._disconnected.is_set():
            self._send(await self._send_queue.get())
            await self._writer.drain()

    async def _recv_loop(self):
        """"""
        This loop is constantly putting items on the queue as they're read.
        """"""
        while not self._disconnected.is_set():
            data = await self._recv()
            await self._recv_queue.put(data)

    @abc.abstractmethod
    def _send(self, data):
        """"""
        This method should be implemented differently under each
        connection mode and serialize the data into the packet
        the way it should be sent through `self._writer`.
        """"""
        raise NotImplementedError

    @abc.abstractmethod
    async def _recv(self):
        """"""
        This method should be implemented differently under each
        connection mode and deserialize the data from the packet
        the way it should be read from `self._reader`.
        """"""
        raise NotImplementedError

```"
64216fcc89b251a64a4ceda5da02ccff5285f548,octohatrack_graphql.py,octohatrack_graphql.py,,"#!/usr/bin/env python
""""""
Quick implementation of octhatrack with GraphQL


USAGE

./octohatrack_graphql.py user/repo


LIMITATIONS

Limitations in the github graphql api means that this will only return the: 
    - last 100 issues
        - last 100 comments per issue
    - last 100 pull requests
        - last 100 comments per pull request
    - last 100 commit comments
""""""

import requests
import json
import os
import click

GITHUB_API = ""https://api.github.com/graphql""
GITHUB_TOKEN = os.environ.get(""GITHUB_TOKEN"")
HEADERS = {""Authorization"": ""token %s"" % GITHUB_TOKEN}

graphql_query = """"""
query ($owner: String!, $name: String!, $history: Int!) {
  repository(owner: $owner, name: $name) {
    issues(last:$history) {
      nodes {
        author { login avatarUrl } 
        comments (last:$history){ nodes {author {login avatarUrl}}}
      }
    }
    pullRequests(last: $history) {
      edges { node {
          author { avatarUrl login }
          comments (last:$history){ nodes {author {login avatarUrl}}}
      }}
    }
    commitComments(last: $history) {
      edges { node { author { login  avatarUrl }}}
    }
  }
}
""""""

def reducejson(j):
    """""" 
    Not sure if there's a better way to walk the ... interesting result
    """"""

    authors = []

    for key in j[""data""][""repository""][""commitComments""][""edges""]:
            authors.append(key[""node""][""author""])

    for key in j[""data""][""repository""][""issues""][""nodes""]:
            authors.append(key[""author""])
            for c in key[""comments""][""nodes""]:
                    authors.append(c[""author""])
            
    for key in j[""data""][""repository""][""pullRequests""][""edges""]:
            authors.append(key[""node""][""author""])
            for c in key[""node""][""comments""][""nodes""]:
                    authors.append(c[""author""])

    unique = list({v['login']:v for v in authors}.values())
    return unique


@click.command()
@click.argument('repo')
def main(repo):
    owner, name = repo.split(""/"")
    variables = { ""owner"": owner, ""name"": name, ""history"":100}
    result = requests.post(GITHUB_API, json.dumps({""query"": graphql_query, ""variables"": variables}), headers=HEADERS)

    authors = reducejson(result.json())
    for a in authors:
            print(a)

    print(len(authors))


if __name__ == '__main__':
    main()
",Add proof of concept graphql implementation,"Add proof of concept graphql implementation
",Python,bsd-3-clause,"glasnt/octohat,LABHR/octohatrack",92,"```python
#!/usr/bin/env python
""""""
Quick implementation of octhatrack with GraphQL


USAGE

./octohatrack_graphql.py user/repo


LIMITATIONS

Limitations in the github graphql api means that this will only return the: 
    - last 100 issues
        - last 100 comments per issue
    - last 100 pull requests
        - last 100 comments per pull request
    - last 100 commit comments
""""""

import requests
import json
import os
import click

GITHUB_API = ""https://api.github.com/graphql""
GITHUB_TOKEN = os.environ.get(""GITHUB_TOKEN"")
HEADERS = {""Authorization"": ""token %s"" % GITHUB_TOKEN}

graphql_query = """"""
query ($owner: String!, $name: String!, $history: Int!) {
  repository(owner: $owner, name: $name) {
    issues(last:$history) {
      nodes {
        author { login avatarUrl } 
        comments (last:$history){ nodes {author {login avatarUrl}}}
      }
    }
    pullRequests(last: $history) {
      edges { node {
          author { avatarUrl login }
          comments (last:$history){ nodes {author {login avatarUrl}}}
      }}
    }
    commitComments(last: $history) {
      edges { node { author { login  avatarUrl }}}
    }
  }
}
""""""

def reducejson(j):
    """""" 
    Not sure if there's a better way to walk the ... interesting result
    """"""

    authors = []

    for key in j[""data""][""repository""][""commitComments""][""edges""]:
            authors.append(key[""node""][""author""])

    for key in j[""data""][""repository""][""issues""][""nodes""]:
            authors.append(key[""author""])
            for c in key[""comments""][""nodes""]:
                    authors.append(c[""author""])
            
    for key in j[""data""][""repository""][""pullRequests""][""edges""]:
            authors.append(key[""node""][""author""])
            for c in key[""node""][""comments""][""nodes""]:
                    authors.append(c[""author""])

    unique = list({v['login']:v for v in authors}.values())
    return unique


@click.command()
@click.argument('repo')
def main(repo):
    owner, name = repo.split(""/"")
    variables = { ""owner"": owner, ""name"": name, ""history"":100}
    result = requests.post(GITHUB_API, json.dumps({""query"": graphql_query, ""variables"": variables}), headers=HEADERS)

    authors = reducejson(result.json())
    for a in authors:
            print(a)

    print(len(authors))


if __name__ == '__main__':
    main()

```"
f87b10b6a6639843b68777e5346109acb44c948a,profile_compressible_solver/gaussian.py,profile_compressible_solver/gaussian.py,,"from firedrake import (SpatialCoordinate, dot, cross, sqrt, atan_2,
                       exp, as_vector, Constant, acos)
import numpy as np


class Gaussian(object):

    def __init__(self,
                 mesh,
                 dir_from_center,
                 radial_dist,
                 sigma_theta,
                 sigma_r,
                 amplitude=1):

        self._mesh = mesh
        self._n0 = dir_from_center
        self._r0 = radial_dist
        self._sigma_theta = sigma_theta
        self._sigma_r = sigma_r
        self._amp = amplitude

        self.x = SpatialCoordinate(mesh)

    @property
    def r(self):
        x = self.x
        return sqrt(x[0]**2 + x[1]**2 + x[2]**2)

    @property
    def theta(self):
        x = self.x
        n0 = self._n0
        return acos(dot(x, n0) / abs(dot(x, n0)))

    @property
    def r_expr(self):
        r = self.r
        r0 = self._r0
        return r - r0

    @property
    def expression(self):
        A = self._amp
        theta = self.theta
        R = self.r_expr
        sigma_theta = self._sigma_theta
        sigma_r = self._sigma_r
        return A*exp(-0.5*((theta/sigma_theta)**2 + (R/sigma_r)**2))


class MultipleGaussians(object):

    def __init__(self, n_gaussians, r_earth, thickness, seed=2097152):

        self._N = n_gaussians
        self._R = r_earth
        self._H = thickness
        self._seed = seed
        self._generate_random_vars()

    def _generate_random_vars(self):
        np.random.rand(self._seed)
        ns = []
        rs = []
        for i in range(self._N):
            nrm = 0.0

            while (nrm < 0.5) or (nrm > 1.0):
                n = 2*np.random.rand(3) - 1.0
                nrm = np.linalg.norm(n)

            ns.append(as_vector(list(n)))
            rs.append(Constant(self._R + self._H * np.random.rand()))

        self._random_Ns = ns
        self._random_Rs = rs

    def expression(self, mesh):

        gs = []
        for i, (n, r0) in enumerate(zip(self._random_Ns, self._random_Rs)):
            sigma_theta = 1.0 - 0.5 * (i / self._N)
            sigma_r = (1.0 - 0.5 * (i / self._N)) * self._H
            amplitude = 1.0
            g = Gaussian(mesh, n, r0, sigma_theta, sigma_r, amplitude)
            gs.append(g.expression)

        return sum(gs)
",Set up object to create random pressure field,"Set up object to create random pressure field
",Python,mit,thomasgibson/firedrake-hybridization,90,"```python
from firedrake import (SpatialCoordinate, dot, cross, sqrt, atan_2,
                       exp, as_vector, Constant, acos)
import numpy as np


class Gaussian(object):

    def __init__(self,
                 mesh,
                 dir_from_center,
                 radial_dist,
                 sigma_theta,
                 sigma_r,
                 amplitude=1):

        self._mesh = mesh
        self._n0 = dir_from_center
        self._r0 = radial_dist
        self._sigma_theta = sigma_theta
        self._sigma_r = sigma_r
        self._amp = amplitude

        self.x = SpatialCoordinate(mesh)

    @property
    def r(self):
        x = self.x
        return sqrt(x[0]**2 + x[1]**2 + x[2]**2)

    @property
    def theta(self):
        x = self.x
        n0 = self._n0
        return acos(dot(x, n0) / abs(dot(x, n0)))

    @property
    def r_expr(self):
        r = self.r
        r0 = self._r0
        return r - r0

    @property
    def expression(self):
        A = self._amp
        theta = self.theta
        R = self.r_expr
        sigma_theta = self._sigma_theta
        sigma_r = self._sigma_r
        return A*exp(-0.5*((theta/sigma_theta)**2 + (R/sigma_r)**2))


class MultipleGaussians(object):

    def __init__(self, n_gaussians, r_earth, thickness, seed=2097152):

        self._N = n_gaussians
        self._R = r_earth
        self._H = thickness
        self._seed = seed
        self._generate_random_vars()

    def _generate_random_vars(self):
        np.random.rand(self._seed)
        ns = []
        rs = []
        for i in range(self._N):
            nrm = 0.0

            while (nrm < 0.5) or (nrm > 1.0):
                n = 2*np.random.rand(3) - 1.0
                nrm = np.linalg.norm(n)

            ns.append(as_vector(list(n)))
            rs.append(Constant(self._R + self._H * np.random.rand()))

        self._random_Ns = ns
        self._random_Rs = rs

    def expression(self, mesh):

        gs = []
        for i, (n, r0) in enumerate(zip(self._random_Ns, self._random_Rs)):
            sigma_theta = 1.0 - 0.5 * (i / self._N)
            sigma_r = (1.0 - 0.5 * (i / self._N)) * self._H
            amplitude = 1.0
            g = Gaussian(mesh, n, r0, sigma_theta, sigma_r, amplitude)
            gs.append(g.expression)

        return sum(gs)

```"
c23c9d562fc7f3fb99b1f57832db2efd2441d992,new_equation.py,new_equation.py,,"#! /usr/bin/env python
from __future__ import print_function

import datetime
import os
import sys
import json
import uuid

if len(sys.argv) > 1:
    sys.stderr.write(""Usage: python ""+sys.argv[0]+"" 'command-invocation'""+'\n')
    sys.exit(1)

def get_year():
    now = datetime.datetime.now()
    return now.year

def get_username():
    try:
        # POSIX only
        import pwd
        gecos_field =  pwd.getpwuid(os.getuid()).pw_gecos
        full_name = gecos_field.split(',')[0]
        return full_name
    except ImportError:
        import getpass
        return getpass.getuser()

new_uuid = str(uuid.uuid4())

equajson = \
{
    ""copying"": {
        ""authors"": [
            get_username()
        ],
        ""license-name"": ""MIT (Expat) License"",
        ""license-url"": ""http://opensource.org/licenses/MIT"",
        ""year"": get_year(),
    },
    ""description"": {
        ""verbose"": ""<FIXME>"",
        ""terse"": ""<FIXME>"",
    },
    ""markup-languages"": {
        ""LaTeX"": [
            {
                ""markup"": ""<FIXME>""
            }
        ]
    },
    ""relevant-urls"": [
        {
            ""url"": ""<FIXME>""
        }
    ],
    ""unicode-pretty-print"": {
        ""multiline"": [
            ""<FIXME>"",
            ""<FIXME>"",
            ""<FIXME>"",
        ],
        ""one-line"": ""<FIXME>"",
        ""parameters"": {
            ""<FIXME>"" : {
                ""label"": ""<FIXME>"",
                ""dimensionless"": ""<FIXME>"",
                ""fixed-constant"": ""<FIXME>"",
                ""urls"": [
                    ""<FIXME>""
                ]
            },
            ""<FIXME>"" : {
                ""label"": ""<FIXME>"",
                ""dimensionless"": ""<FIXME>"",
                ""fixed-constant"": ""<FIXME>"",
                ""urls"": [
                    ""<FIXME>""
                ]
            }
        }
    },
    ""uuid"": new_uuid
}

root = sys.path[0]
new_filename = new_uuid + '.json'
new_filepath = os.path.join(root, 'equajson', new_filename)
with open(new_filepath, 'w') as new_file:
    json.dump(equajson, new_file, indent=4, separators=(',', ': '), sort_keys=True)
    print('Created new equation:\n{}'.format(new_filepath))
",Add an equation generator script.,"Add an equation generator script.
",Python,mit,nbeaver/equajson,92,"```python
#! /usr/bin/env python
from __future__ import print_function

import datetime
import os
import sys
import json
import uuid

if len(sys.argv) > 1:
    sys.stderr.write(""Usage: python ""+sys.argv[0]+"" 'command-invocation'""+'\n')
    sys.exit(1)

def get_year():
    now = datetime.datetime.now()
    return now.year

def get_username():
    try:
        # POSIX only
        import pwd
        gecos_field =  pwd.getpwuid(os.getuid()).pw_gecos
        full_name = gecos_field.split(',')[0]
        return full_name
    except ImportError:
        import getpass
        return getpass.getuser()

new_uuid = str(uuid.uuid4())

equajson = \
{
    ""copying"": {
        ""authors"": [
            get_username()
        ],
        ""license-name"": ""MIT (Expat) License"",
        ""license-url"": ""http://opensource.org/licenses/MIT"",
        ""year"": get_year(),
    },
    ""description"": {
        ""verbose"": ""<FIXME>"",
        ""terse"": ""<FIXME>"",
    },
    ""markup-languages"": {
        ""LaTeX"": [
            {
                ""markup"": ""<FIXME>""
            }
        ]
    },
    ""relevant-urls"": [
        {
            ""url"": ""<FIXME>""
        }
    ],
    ""unicode-pretty-print"": {
        ""multiline"": [
            ""<FIXME>"",
            ""<FIXME>"",
            ""<FIXME>"",
        ],
        ""one-line"": ""<FIXME>"",
        ""parameters"": {
            ""<FIXME>"" : {
                ""label"": ""<FIXME>"",
                ""dimensionless"": ""<FIXME>"",
                ""fixed-constant"": ""<FIXME>"",
                ""urls"": [
                    ""<FIXME>""
                ]
            },
            ""<FIXME>"" : {
                ""label"": ""<FIXME>"",
                ""dimensionless"": ""<FIXME>"",
                ""fixed-constant"": ""<FIXME>"",
                ""urls"": [
                    ""<FIXME>""
                ]
            }
        }
    },
    ""uuid"": new_uuid
}

root = sys.path[0]
new_filename = new_uuid + '.json'
new_filepath = os.path.join(root, 'equajson', new_filename)
with open(new_filepath, 'w') as new_file:
    json.dump(equajson, new_file, indent=4, separators=(',', ': '), sort_keys=True)
    print('Created new equation:\n{}'.format(new_filepath))

```"
386781f1b125f6983a6aa44795dee87d86b68b56,pombola/south_africa/management/commands/south_africa_import_scraped_photos.py,pombola/south_africa/management/commands/south_africa_import_scraped_photos.py,,""""""" Loop through images in a directory and attempt to match them to a person.""""""

import re
import os

from django.core.exceptions import ObjectDoesNotExist
from django.core.exceptions import MultipleObjectsReturned
from django.core.management.base import LabelCommand

from django.core.files import File

from django.utils.text import slugify

from pombola.core.models import (
    Person, ContentType, Image
)

from haystack.query import SearchQuerySet

# Pretty colours to make it easier to spot things.
BRIGHT = '\033[95m'
ENDC = '\033[0m'

def match_person(name):
    """""" Match up a person by name with their database entry. """"""

    slug = slugify(name)

    # Try match on the name first
    try:
        person = Person.objects.get(legal_name__iexact=name)
    except ObjectDoesNotExist:
        try:
            person = Person.objects.get(slug=slug)
        except ObjectDoesNotExist:
            search = SearchQuerySet().models(Person).filter(text=name)
            if len(search) == 1 and search[0]:
                person = search[0].object
            else:
                return None

    except MultipleObjectsReturned:
        print 'Multiple people returned for ' + name + ' (' + slug + '). Cannot continue.'
        exit(1)

    return person

class Command(LabelCommand):

    help = 'Imports scraped photos from the ZA parliament site, and matches them to people.'

    def __init__(self, *args, **kwargs):
        super(Command, self).__init__(*args, **kwargs)

        self.content_type_person = ContentType.objects.get_for_model(Person)

    def handle_label(self, path, **options):

        matched = 0
        unmatched = 0

        for filename in os.listdir(path):

            # Strip out the .jpg
            name = re.sub('\.jpg', '', filename)

            # Strip any non-alpha trailing characters
            name = re.sub('[^a-zA-Z]*$', '', name)

            # Strip any more trailing whitespace that may have snuck in
            name = name.strip()

            # Make the name unicode so we can actually work with it in the DB
            name = unicode(name)

            # Slice the name into two
            name = name.split('_')

            if len(name) == 2:

                # Match up the person
                person = match_person(name[1] + ' ' + name[0])

                if person is None:
                    print BRIGHT + 'Unable to match ""' + filename + '"" to a person!'+ ENDC
                    unmatched += 1
                else:
                    print 'Matched ' + person.name.encode('utf-8')

                    Image.objects.create(
                        object_id=person.id,
                        content_type=self.content_type_person,
                        is_primary=True,
                        source='http://www.parliament.gov.za',
                        image=File(open(path + filename, 'r'))
                    )

                    matched += 1

            else:

                # This name doesn't have two bits, complain.
                print BRIGHT + '""' + filename + '"" does not parse to a first and last name.'+ ENDC
                unmatched += 1

        print 'Done! Matched ' + str(matched) + ', failed to match ' + str(unmatched)
",Add script to import a folder of named images and attach to people.,"Add script to import a folder of named images and attach to people.
",Python,agpl-3.0,"hzj123/56th,geoffkilpin/pombola,hzj123/56th,patricmutwiri/pombola,patricmutwiri/pombola,patricmutwiri/pombola,mysociety/pombola,mysociety/pombola,ken-muturi/pombola,mysociety/pombola,geoffkilpin/pombola,ken-muturi/pombola,geoffkilpin/pombola,mysociety/pombola,ken-muturi/pombola,ken-muturi/pombola,patricmutwiri/pombola,ken-muturi/pombola,geoffkilpin/pombola,geoffkilpin/pombola,geoffkilpin/pombola,hzj123/56th,hzj123/56th,ken-muturi/pombola,hzj123/56th,patricmutwiri/pombola,mysociety/pombola,hzj123/56th,patricmutwiri/pombola,mysociety/pombola",107,"```python
"""""" Loop through images in a directory and attempt to match them to a person.""""""

import re
import os

from django.core.exceptions import ObjectDoesNotExist
from django.core.exceptions import MultipleObjectsReturned
from django.core.management.base import LabelCommand

from django.core.files import File

from django.utils.text import slugify

from pombola.core.models import (
    Person, ContentType, Image
)

from haystack.query import SearchQuerySet

# Pretty colours to make it easier to spot things.
BRIGHT = '\033[95m'
ENDC = '\033[0m'

def match_person(name):
    """""" Match up a person by name with their database entry. """"""

    slug = slugify(name)

    # Try match on the name first
    try:
        person = Person.objects.get(legal_name__iexact=name)
    except ObjectDoesNotExist:
        try:
            person = Person.objects.get(slug=slug)
        except ObjectDoesNotExist:
            search = SearchQuerySet().models(Person).filter(text=name)
            if len(search) == 1 and search[0]:
                person = search[0].object
            else:
                return None

    except MultipleObjectsReturned:
        print 'Multiple people returned for ' + name + ' (' + slug + '). Cannot continue.'
        exit(1)

    return person

class Command(LabelCommand):

    help = 'Imports scraped photos from the ZA parliament site, and matches them to people.'

    def __init__(self, *args, **kwargs):
        super(Command, self).__init__(*args, **kwargs)

        self.content_type_person = ContentType.objects.get_for_model(Person)

    def handle_label(self, path, **options):

        matched = 0
        unmatched = 0

        for filename in os.listdir(path):

            # Strip out the .jpg
            name = re.sub('\.jpg', '', filename)

            # Strip any non-alpha trailing characters
            name = re.sub('[^a-zA-Z]*$', '', name)

            # Strip any more trailing whitespace that may have snuck in
            name = name.strip()

            # Make the name unicode so we can actually work with it in the DB
            name = unicode(name)

            # Slice the name into two
            name = name.split('_')

            if len(name) == 2:

                # Match up the person
                person = match_person(name[1] + ' ' + name[0])

                if person is None:
                    print BRIGHT + 'Unable to match ""' + filename + '"" to a person!'+ ENDC
                    unmatched += 1
                else:
                    print 'Matched ' + person.name.encode('utf-8')

                    Image.objects.create(
                        object_id=person.id,
                        content_type=self.content_type_person,
                        is_primary=True,
                        source='http://www.parliament.gov.za',
                        image=File(open(path + filename, 'r'))
                    )

                    matched += 1

            else:

                # This name doesn't have two bits, complain.
                print BRIGHT + '""' + filename + '"" does not parse to a first and last name.'+ ENDC
                unmatched += 1

        print 'Done! Matched ' + str(matched) + ', failed to match ' + str(unmatched)

```"
d95804836fd5f693bd70ee4d8480e0e63d98cdb7,tests/test_utils.py,tests/test_utils.py,,"from fudge.patcher import with_patched_object
from functools import wraps
from nose.tools import eq_

from nose.tools import raises
from fabric.state import output
from fabric.utils import warn, indent, abort
import sys
from StringIO import StringIO

#
# Setup/teardown helpers and decorators
#

def mock_streams(*which):
    """"""
    Replaces ``sys.stderr`` with a ``StringIO`` during the test, then restores
    after.

    Must specify which stream via string args, e.g.::

        @mock_streams('stdout')
        def func():
            pass

        @mock_streams('stderr')
        def func():
            pass

        @mock_streams('stdout', 'stderr')
        def func()
            pass
    """"""
    def mocked_streams_decorator(func):
        @wraps(func)
        def inner_wrapper(*args, **kwargs):
            if 'stdout' in which:
                my_stdout, sys.stdout = sys.stdout, StringIO()
            if 'stderr' in which:
                my_stderr, sys.stderr = sys.stderr, StringIO()
            result = func(*args, **kwargs)
            if 'stderr' in which:
                sys.stderr = my_stderr
            if 'stdout' in which:
                sys.stdout = my_stdout
            return result
        return inner_wrapper
    return mocked_streams_decorator

@mock_streams('stderr')
@with_patched_object(output, 'warnings', True)
def test_warn():
    warn(""Test"")
    result = sys.stderr.getvalue()
    assert ""\nWarning: Test\n\n"" == result

def test_indent():
    for description, input, output in (
        (""Sanity check: 1 line string"",
            'Test', '    Test'),
        (""List of strings turns in to strings joined by \n"",
            [""Test"", ""Test""], '    Test\n    Test'),
    ):
        eq_.description = description
        yield eq_, indent(input), output
        del eq_.description

def test_indent_with_strip():
    for description, input, output in (
        (""Sanity check: 1 line string"",
            indent('Test', strip=True), '    Test'),
        (""Check list of strings"",
            indent([""Test"", ""Test""], strip=True), '    Test\n    Test'),
        (""Check list of strings"",
            indent([""        Test"", ""        Test""], strip=True), '    Test\n    Test'),
    ):
        eq_.description = description
        yield eq_, input, output
        del eq_.description

@mock_streams('stderr')
@raises(SystemExit)
def test_abort():
    """"""
    Abort should raise SystemExit
    """"""
    abort(""Test"")

@mock_streams('stderr')
@with_patched_object(output, 'aborts', True)
def test_abort_message():
    try:
        abort(""Test"")
    except SystemExit:
        pass
    result = sys.stderr.getvalue()
    eq_(""\nFatal error: Test\n\nAborting.\n"", result)
   
",Add basic tests for utils,"Add basic tests for utils

Signed-off-by: Jeff Forcier <f3e731dfa293c7a83119d8aacfa41b5d2d780be9@bitprophet.org>
",Python,bsd-2-clause,"amaniak/fabric,SamuelMarks/fabric,simon-engledew/fabric,likesxuqiang/fabric,kmonsoor/fabric,rane-hs/fabric-py3,ploxiln/fabric,StackStorm/fabric,bitprophet/fabric,bitmonk/fabric,itoed/fabric,haridsv/fabric,xLegoz/fabric,pashinin/fabric,cgvarela/fabric,cmattoon/fabric,tolbkni/fabric,raimon49/fabric,kxxoling/fabric,rbramwell/fabric,felix-d/fabric,fernandezcuesta/fabric,mathiasertl/fabric,askulkarni2/fabric,qinrong/fabric,bspink/fabric,TarasRudnyk/fabric,akaariai/fabric,opavader/fabric,elijah513/fabric,jaraco/fabric,ericholscher/fabric,hrubi/fabric,sdelements/fabric,rodrigc/fabric,tekapo/fabric,MjAbuz/fabric,getsentry/fabric,pgroudas/fabric",99,"```python
from fudge.patcher import with_patched_object
from functools import wraps
from nose.tools import eq_

from nose.tools import raises
from fabric.state import output
from fabric.utils import warn, indent, abort
import sys
from StringIO import StringIO

#
# Setup/teardown helpers and decorators
#

def mock_streams(*which):
    """"""
    Replaces ``sys.stderr`` with a ``StringIO`` during the test, then restores
    after.

    Must specify which stream via string args, e.g.::

        @mock_streams('stdout')
        def func():
            pass

        @mock_streams('stderr')
        def func():
            pass

        @mock_streams('stdout', 'stderr')
        def func()
            pass
    """"""
    def mocked_streams_decorator(func):
        @wraps(func)
        def inner_wrapper(*args, **kwargs):
            if 'stdout' in which:
                my_stdout, sys.stdout = sys.stdout, StringIO()
            if 'stderr' in which:
                my_stderr, sys.stderr = sys.stderr, StringIO()
            result = func(*args, **kwargs)
            if 'stderr' in which:
                sys.stderr = my_stderr
            if 'stdout' in which:
                sys.stdout = my_stdout
            return result
        return inner_wrapper
    return mocked_streams_decorator

@mock_streams('stderr')
@with_patched_object(output, 'warnings', True)
def test_warn():
    warn(""Test"")
    result = sys.stderr.getvalue()
    assert ""\nWarning: Test\n\n"" == result

def test_indent():
    for description, input, output in (
        (""Sanity check: 1 line string"",
            'Test', '    Test'),
        (""List of strings turns in to strings joined by \n"",
            [""Test"", ""Test""], '    Test\n    Test'),
    ):
        eq_.description = description
        yield eq_, indent(input), output
        del eq_.description

def test_indent_with_strip():
    for description, input, output in (
        (""Sanity check: 1 line string"",
            indent('Test', strip=True), '    Test'),
        (""Check list of strings"",
            indent([""Test"", ""Test""], strip=True), '    Test\n    Test'),
        (""Check list of strings"",
            indent([""        Test"", ""        Test""], strip=True), '    Test\n    Test'),
    ):
        eq_.description = description
        yield eq_, input, output
        del eq_.description

@mock_streams('stderr')
@raises(SystemExit)
def test_abort():
    """"""
    Abort should raise SystemExit
    """"""
    abort(""Test"")

@mock_streams('stderr')
@with_patched_object(output, 'aborts', True)
def test_abort_message():
    try:
        abort(""Test"")
    except SystemExit:
        pass
    result = sys.stderr.getvalue()
    eq_(""\nFatal error: Test\n\nAborting.\n"", result)
   

```"
8c0689decb6953f04b059f9b7838c69f6d41c8b0,drudge/drs.py,drudge/drs.py,,"""""""Support for drudge scripts.""""""

import collections

from sympy import Symbol, Indexed, IndexedBase


#
# Special classes for SymPy objects
# ---------------------------------
#

class DrsSymbol(Symbol):
    """"""Symbols used in drudge scripts.

    The drudge symbol needs to behave as similar to the actual symbol as
    possible, because it is possible that they are used for keys in
    dictionaries.
    """"""

    __slots__ = [
        '_drudge',
        '_orig'
    ]

    def __new__(cls, drudge, name):
        """"""Create a symbol object.""""""
        symb = super().__new__(cls, name)
        return symb

    def __init__(self, drudge, name):
        """"""Initialize the symbol object.""""""
        self._drudge = drudge
        self._orig = Symbol(name)

    def __eq__(self, other):
        """"""Make equality comparison.""""""
        return self._orig == other

    def __hash__(self):
        """"""Compute the hash.""""""
        return hash(self._orig)

    def _hashable_content(self):
        """"""Hashable content for SymPy usages.""""""
        return self._orig._hashable_content()

    @classmethod
    def class_key(cls):
        return Symbol.class_key()

    def __getitem__(self, indices):
        """"""Index the given symbol.

        In drudge scripts, all symbols are by itself indexed bases.
        """"""
        base = IndexedBase(self._orig)
        if isinstance(indices, collections.Sequence):
            return DrsIndexed(self._drudge, base, *indices)
        else:
            return DrsIndexed(self._drudge, base, indices)

    def __iter__(self):
        """"""Disable iterability of the symbol.

        Or a default implementation from ``__getitem__`` will be used,
        which makes the symbols unable to be used as subscripts for indexed
        objects.
        """"""
        raise TypeError('Drudge script symbol cannot be iterated over.')


class DrsIndexed(Indexed):
    """"""Indexed objects for drudge scripts.""""""

    __slots__ = [
        '_drudge',
        '_orig'
    ]

    def __new__(cls, drudge, base, *args, **kwargs):
        """"""Create an indexed object for drudge scripts.""""""
        indexed = super().__new__(cls, base, *args, **kwargs)
        return indexed

    def __init__(self, drudge, base, *args, **kwargs):
        """"""Initialize the indexed object.""""""
        self._drudge = drudge
        self._orig = Indexed(base, *args, **kwargs)

    def __eq__(self, other):
        """"""Make equality comparison.""""""
        return self._orig == other

    def __hash__(self):
        """"""Compute the hash.""""""
        return hash(self._orig)

    def _hashable_content(self):
        """"""Hashable content for SymPy usages.""""""
        return self._orig._hashable_content()

    @classmethod
    def class_key(cls):
        return Indexed.class_key()
",Add utility SymPy subclasses for drudge scripts,"Add utility SymPy subclasses for drudge scripts

These two subclasses are going to be used for SymPy objects in drudge
scripts, especially used as the objects for unresolved names.
",Python,mit,"tschijnmo/drudge,tschijnmo/drudge,tschijnmo/drudge",106,"```python
""""""Support for drudge scripts.""""""

import collections

from sympy import Symbol, Indexed, IndexedBase


#
# Special classes for SymPy objects
# ---------------------------------
#

class DrsSymbol(Symbol):
    """"""Symbols used in drudge scripts.

    The drudge symbol needs to behave as similar to the actual symbol as
    possible, because it is possible that they are used for keys in
    dictionaries.
    """"""

    __slots__ = [
        '_drudge',
        '_orig'
    ]

    def __new__(cls, drudge, name):
        """"""Create a symbol object.""""""
        symb = super().__new__(cls, name)
        return symb

    def __init__(self, drudge, name):
        """"""Initialize the symbol object.""""""
        self._drudge = drudge
        self._orig = Symbol(name)

    def __eq__(self, other):
        """"""Make equality comparison.""""""
        return self._orig == other

    def __hash__(self):
        """"""Compute the hash.""""""
        return hash(self._orig)

    def _hashable_content(self):
        """"""Hashable content for SymPy usages.""""""
        return self._orig._hashable_content()

    @classmethod
    def class_key(cls):
        return Symbol.class_key()

    def __getitem__(self, indices):
        """"""Index the given symbol.

        In drudge scripts, all symbols are by itself indexed bases.
        """"""
        base = IndexedBase(self._orig)
        if isinstance(indices, collections.Sequence):
            return DrsIndexed(self._drudge, base, *indices)
        else:
            return DrsIndexed(self._drudge, base, indices)

    def __iter__(self):
        """"""Disable iterability of the symbol.

        Or a default implementation from ``__getitem__`` will be used,
        which makes the symbols unable to be used as subscripts for indexed
        objects.
        """"""
        raise TypeError('Drudge script symbol cannot be iterated over.')


class DrsIndexed(Indexed):
    """"""Indexed objects for drudge scripts.""""""

    __slots__ = [
        '_drudge',
        '_orig'
    ]

    def __new__(cls, drudge, base, *args, **kwargs):
        """"""Create an indexed object for drudge scripts.""""""
        indexed = super().__new__(cls, base, *args, **kwargs)
        return indexed

    def __init__(self, drudge, base, *args, **kwargs):
        """"""Initialize the indexed object.""""""
        self._drudge = drudge
        self._orig = Indexed(base, *args, **kwargs)

    def __eq__(self, other):
        """"""Make equality comparison.""""""
        return self._orig == other

    def __hash__(self):
        """"""Compute the hash.""""""
        return hash(self._orig)

    def _hashable_content(self):
        """"""Hashable content for SymPy usages.""""""
        return self._orig._hashable_content()

    @classmethod
    def class_key(cls):
        return Indexed.class_key()

```"
5a46337647436b1f39562d4f5664a7833b87c269,__init__.py,__init__.py,,"
import requests

# Yummly API: https://developer.yummly.com

# API URLs
URL_BASE    = 'http://api.yummly.com/v1'
URL_GET     = URL_BASE + '/api/recipe/'
URL_SEARCH  = URL_BASE + '/api/recipes'

# API auth properties which should be set externally
api_id  = None
api_key = None

# basic request config options
# @note: have found that Yummly's API ""hangs"" so it might be a good idea to have some reasonable timeout and handle appropriately
timeout = 5.0

class YummlyError( Exception ):
    '''Exception class for Yummly errors'''
    pass

### Methods for API get recipe and search recipes

def recipe( recipe_id ):
    url = URL_GET + recipe_id
    response    = _request( url )
    result      = _extract_response( response )
    return result

def search( q, limit=40, offset=0 ):
    '''
    Prepares yummly search API request

    :param q: search string
    :param limit: max results
    :param offset: pagination offset in # of records (e.g. offset=5 means skip first 5 results)

    '''

    url     = URL_SEARCH
    params  = {
        'q':            q,
        'maxResult':    limit,
        'start':        offset
    }

    response    = _request( url, params=params )
    results     = _extract_response( response )
    return results


### Helper functions

def handle_errors( fn ):
    '''Decorator for handling Yummly errors'''
    def handle( *args, **kargs ):
        response = fn( *args, **kargs )

        status = response.status_code

        if status != 200:
            if status == 409:
                raise YummlyError( 'API id and/or key are invalid or API rate limit exceeded' )
            else:
                # error ocurred which may not be documented
                response.raise_for_status()

        return response

    return handle

@handle_errors
def _request( url, params=None ):
    '''
    Generic yummly request which attaches meta info (e.g. auth)

    :param url: URL of endpoint
    :param params: GET params of request

    '''

    # auth headers
    headers = {
        'X-Yummly-App-ID':  api_id,
        'X-Yummly-App-Key': api_key,
    }

    response = requests.get( url, params=params, headers=headers, timeout=timeout )

    return response

def _extract_response( response ):
    '''Extract data from api resposne'''
    return response.json()

",Add yummly.recipe and yummly.search functions.,"Add yummly.recipe and yummly.search functions.
",Python,mit,dgilland/yummly.py,97,"```python

import requests

# Yummly API: https://developer.yummly.com

# API URLs
URL_BASE    = 'http://api.yummly.com/v1'
URL_GET     = URL_BASE + '/api/recipe/'
URL_SEARCH  = URL_BASE + '/api/recipes'

# API auth properties which should be set externally
api_id  = None
api_key = None

# basic request config options
# @note: have found that Yummly's API ""hangs"" so it might be a good idea to have some reasonable timeout and handle appropriately
timeout = 5.0

class YummlyError( Exception ):
    '''Exception class for Yummly errors'''
    pass

### Methods for API get recipe and search recipes

def recipe( recipe_id ):
    url = URL_GET + recipe_id
    response    = _request( url )
    result      = _extract_response( response )
    return result

def search( q, limit=40, offset=0 ):
    '''
    Prepares yummly search API request

    :param q: search string
    :param limit: max results
    :param offset: pagination offset in # of records (e.g. offset=5 means skip first 5 results)

    '''

    url     = URL_SEARCH
    params  = {
        'q':            q,
        'maxResult':    limit,
        'start':        offset
    }

    response    = _request( url, params=params )
    results     = _extract_response( response )
    return results


### Helper functions

def handle_errors( fn ):
    '''Decorator for handling Yummly errors'''
    def handle( *args, **kargs ):
        response = fn( *args, **kargs )

        status = response.status_code

        if status != 200:
            if status == 409:
                raise YummlyError( 'API id and/or key are invalid or API rate limit exceeded' )
            else:
                # error ocurred which may not be documented
                response.raise_for_status()

        return response

    return handle

@handle_errors
def _request( url, params=None ):
    '''
    Generic yummly request which attaches meta info (e.g. auth)

    :param url: URL of endpoint
    :param params: GET params of request

    '''

    # auth headers
    headers = {
        'X-Yummly-App-ID':  api_id,
        'X-Yummly-App-Key': api_key,
    }

    response = requests.get( url, params=params, headers=headers, timeout=timeout )

    return response

def _extract_response( response ):
    '''Extract data from api resposne'''
    return response.json()


```"
0f3dbed232e73dfa63c219402fff5c74a0f107bc,django_graphene_utils/pager.py,django_graphene_utils/pager.py,,"import graphene
from graphene.utils.str_converters import to_camel_case
from django.utils import six
from django.utils.functional import cached_property

__all__ = ['Pager']


class BasePager(object):
    def __init__(self, data, queryset, default_size=20):
        # process data
        self.qs = self._process_data(data or {}, queryset, default_size)

    def _process_data(self, data, queryset, default_size):
        # we may have to handle sorting fields
        if self._sort_fields:
            # check if we've some
            sort_fields = data.get('sort', None)

            if sort_fields:
                # so order the queryset
                queryset = queryset.order_by(*sort_fields)

        # we maye have to get all items
        if data.get('all', False):
            # no need to paginate it
            return queryset

        # get the offset (0 by default)
        offset = data.get('offset', 0)
        # final queryset
        return queryset[offset:offset + data.get('size', default_size)]

    @classmethod
    def to_input(cls, input_name, graphql_type=graphene.InputObjectType, enum_name=None):
        # start with basic attributes
        attrs = {
            'offset': graphene.Int(),
            'size': graphene.Int(),
            'all': graphene.Boolean()
        }

        # we might have to support sorting
        if cls._sort_fields:
            # first ensure we have a name
            if enum_name is None:
                # made it from the input name
                enum_name = '%SortField' % input_name

            # then build the enum for this input
            sort_enum = graphene.Enum(enum_name, list(cls._sort_fields.items()))
            # and the field
            attrs['sort'] = graphene.List(sort_enum)

        # build the final type
        return type(input_name, (graphql_type,), attrs)


class PagerMeta(type):
    def __new__(mcs, name, bases, attrs):
        # build the new class
        new_method = super(PagerMeta, mcs).__new__

        if bases == (BasePager,):
            return new_method(mcs, name, bases, attrs)

        # start with an empty list of fields
        fields = {}

        # loop on attributes
        for key, field in list(attrs.items()):
            # only attributes with an upper name and a string as value
            # will be considered as sorting field
            if key.isupper() and isinstance(field, str):
                # remove it from attribute
                attrs.pop(key)
                # turn the key into camel case
                key = to_camel_case(key.lower())
                # push it into valid fields
                fields[key] = field
                fields['%s_Desc' % key] = '-%s' % field

        # create the new class
        new_cls = new_method(mcs, name, bases, attrs)

        # then add fields
        new_cls._sort_fields = fields

        return new_cls


class Pager(six.with_metaclass(PagerMeta, BasePager)):
    pass
",Add a way to handle sorts & pagination,"Add a way to handle sorts & pagination
",Python,mit,amille44420/django-graphene-utils,94,"```python
import graphene
from graphene.utils.str_converters import to_camel_case
from django.utils import six
from django.utils.functional import cached_property

__all__ = ['Pager']


class BasePager(object):
    def __init__(self, data, queryset, default_size=20):
        # process data
        self.qs = self._process_data(data or {}, queryset, default_size)

    def _process_data(self, data, queryset, default_size):
        # we may have to handle sorting fields
        if self._sort_fields:
            # check if we've some
            sort_fields = data.get('sort', None)

            if sort_fields:
                # so order the queryset
                queryset = queryset.order_by(*sort_fields)

        # we maye have to get all items
        if data.get('all', False):
            # no need to paginate it
            return queryset

        # get the offset (0 by default)
        offset = data.get('offset', 0)
        # final queryset
        return queryset[offset:offset + data.get('size', default_size)]

    @classmethod
    def to_input(cls, input_name, graphql_type=graphene.InputObjectType, enum_name=None):
        # start with basic attributes
        attrs = {
            'offset': graphene.Int(),
            'size': graphene.Int(),
            'all': graphene.Boolean()
        }

        # we might have to support sorting
        if cls._sort_fields:
            # first ensure we have a name
            if enum_name is None:
                # made it from the input name
                enum_name = '%SortField' % input_name

            # then build the enum for this input
            sort_enum = graphene.Enum(enum_name, list(cls._sort_fields.items()))
            # and the field
            attrs['sort'] = graphene.List(sort_enum)

        # build the final type
        return type(input_name, (graphql_type,), attrs)


class PagerMeta(type):
    def __new__(mcs, name, bases, attrs):
        # build the new class
        new_method = super(PagerMeta, mcs).__new__

        if bases == (BasePager,):
            return new_method(mcs, name, bases, attrs)

        # start with an empty list of fields
        fields = {}

        # loop on attributes
        for key, field in list(attrs.items()):
            # only attributes with an upper name and a string as value
            # will be considered as sorting field
            if key.isupper() and isinstance(field, str):
                # remove it from attribute
                attrs.pop(key)
                # turn the key into camel case
                key = to_camel_case(key.lower())
                # push it into valid fields
                fields[key] = field
                fields['%s_Desc' % key] = '-%s' % field

        # create the new class
        new_cls = new_method(mcs, name, bases, attrs)

        # then add fields
        new_cls._sort_fields = fields

        return new_cls


class Pager(six.with_metaclass(PagerMeta, BasePager)):
    pass

```"
8e744f67457222a143505f303c0254e43706d3f8,tests/aggregate/test_join_table_inheritance.py,tests/aggregate/test_join_table_inheritance.py,,"from decimal import Decimal
import sqlalchemy as sa
from sqlalchemy_utils.aggregates import aggregated
from tests import TestCase


class TestLazyEvaluatedSelectExpressionsForAggregates(TestCase):
    dns = 'postgres://postgres@localhost/sqlalchemy_utils_test'

    def create_models(self):
        class Catalog(self.Base):
            __tablename__ = 'catalog'
            id = sa.Column(sa.Integer, primary_key=True)
            name = sa.Column(sa.Unicode(255))
            type = sa.Column(sa.Unicode(255))

            __mapper_args__ = {
                'polymorphic_on': type
            }

            @aggregated('products', sa.Column(sa.Numeric, default=0))
            def net_worth(self):
                return sa.func.sum(Product.price)

            products = sa.orm.relationship('Product', backref='catalog')

        class CostumeCatalog(Catalog):
            __tablename__ = 'costume_catalog'
            id = sa.Column(
                sa.Integer, sa.ForeignKey(Catalog.id), primary_key=True
            )

            __mapper_args__ = {
                'polymorphic_identity': 'costumes',
            }

        class CarCatalog(Catalog):
            __tablename__ = 'car_catalog'
            id = sa.Column(
                sa.Integer, sa.ForeignKey(Catalog.id), primary_key=True
            )

            __mapper_args__ = {
                'polymorphic_identity': 'cars',
            }

        class Product(self.Base):
            __tablename__ = 'product'
            id = sa.Column(sa.Integer, primary_key=True)
            name = sa.Column(sa.Unicode(255))
            price = sa.Column(sa.Numeric)

            catalog_id = sa.Column(sa.Integer, sa.ForeignKey('catalog.id'))

        self.Catalog = Catalog
        self.Product = Product

    def test_assigns_aggregates_on_insert(self):
        catalog = self.Catalog(
            name=u'Some catalog'
        )
        self.session.add(catalog)
        self.session.commit()
        product = self.Product(
            name=u'Some product',
            price=Decimal('1000'),
            catalog=catalog
        )
        self.session.add(product)
        self.session.commit()
        self.session.refresh(catalog)
        assert catalog.net_worth == Decimal('1000')

    def test_assigns_aggregates_on_update(self):
        catalog = self.Catalog(
            name=u'Some catalog'
        )
        self.session.add(catalog)
        self.session.commit()
        product = self.Product(
            name=u'Some product',
            price=Decimal('1000'),
            catalog=catalog
        )
        self.session.add(product)
        self.session.commit()
        product.price = Decimal('500')
        self.session.commit()
        self.session.refresh(catalog)
        assert catalog.net_worth == Decimal('500')
",Add tests for aggregates with inheritance,"Add tests for aggregates with inheritance
",Python,bsd-3-clause,"joshfriend/sqlalchemy-utils,marrybird/sqlalchemy-utils,konstantinoskostis/sqlalchemy-utils,rmoorman/sqlalchemy-utils,tonyseek/sqlalchemy-utils,joshfriend/sqlalchemy-utils,tonyseek/sqlalchemy-utils,cheungpat/sqlalchemy-utils,spoqa/sqlalchemy-utils,JackWink/sqlalchemy-utils",91,"```python
from decimal import Decimal
import sqlalchemy as sa
from sqlalchemy_utils.aggregates import aggregated
from tests import TestCase


class TestLazyEvaluatedSelectExpressionsForAggregates(TestCase):
    dns = 'postgres://postgres@localhost/sqlalchemy_utils_test'

    def create_models(self):
        class Catalog(self.Base):
            __tablename__ = 'catalog'
            id = sa.Column(sa.Integer, primary_key=True)
            name = sa.Column(sa.Unicode(255))
            type = sa.Column(sa.Unicode(255))

            __mapper_args__ = {
                'polymorphic_on': type
            }

            @aggregated('products', sa.Column(sa.Numeric, default=0))
            def net_worth(self):
                return sa.func.sum(Product.price)

            products = sa.orm.relationship('Product', backref='catalog')

        class CostumeCatalog(Catalog):
            __tablename__ = 'costume_catalog'
            id = sa.Column(
                sa.Integer, sa.ForeignKey(Catalog.id), primary_key=True
            )

            __mapper_args__ = {
                'polymorphic_identity': 'costumes',
            }

        class CarCatalog(Catalog):
            __tablename__ = 'car_catalog'
            id = sa.Column(
                sa.Integer, sa.ForeignKey(Catalog.id), primary_key=True
            )

            __mapper_args__ = {
                'polymorphic_identity': 'cars',
            }

        class Product(self.Base):
            __tablename__ = 'product'
            id = sa.Column(sa.Integer, primary_key=True)
            name = sa.Column(sa.Unicode(255))
            price = sa.Column(sa.Numeric)

            catalog_id = sa.Column(sa.Integer, sa.ForeignKey('catalog.id'))

        self.Catalog = Catalog
        self.Product = Product

    def test_assigns_aggregates_on_insert(self):
        catalog = self.Catalog(
            name=u'Some catalog'
        )
        self.session.add(catalog)
        self.session.commit()
        product = self.Product(
            name=u'Some product',
            price=Decimal('1000'),
            catalog=catalog
        )
        self.session.add(product)
        self.session.commit()
        self.session.refresh(catalog)
        assert catalog.net_worth == Decimal('1000')

    def test_assigns_aggregates_on_update(self):
        catalog = self.Catalog(
            name=u'Some catalog'
        )
        self.session.add(catalog)
        self.session.commit()
        product = self.Product(
            name=u'Some product',
            price=Decimal('1000'),
            catalog=catalog
        )
        self.session.add(product)
        self.session.commit()
        product.price = Decimal('500')
        self.session.commit()
        self.session.refresh(catalog)
        assert catalog.net_worth == Decimal('500')

```"
edf2c0d777672568b2223fdbd6858f9c9a34ee44,DataWrangling/scraping_web2.py,DataWrangling/scraping_web2.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Script to get information from:
http://www.transtats.bts.gov/Data_Elements.aspx?Data=2
about carrier and airports
""""""

from bs4 import BeautifulSoup
import requests
import urllib2



def extract_data(url, s):
    # Extract data from a html source from a URL
    r = s.get(url)
    soup = BeautifulSoup(r.text)
    data = {""eventvalidation"": """",
            ""viewstate"": """"}

    eventvalidation_element = soup.find(id=""__EVENTVALIDATION"")
    data[""eventvalidation""] = eventvalidation_element[""value""]

    viewstate_element = soup.find(id=""__VIEWSTATE"")
    data[""viewstate""] = viewstate_element[""value""]

    return data




def make_request(data, s):
    # Make request to get data
    eventvalidation = data[""eventvalidation""]
    viewstate = data[""viewstate""]

    r = s.post(""http://www.transtats.bts.gov/Data_Elements.aspx?Data=2"",
               data={'AirportList': ""BOS"",
                     'CarrierList': ""VX"",
                     'Submit': 'Submit',
                     ""__EVENTTARGET"": """",
                     ""__EVENTARGUMENT"": """",
                     ""__EVENTVALIDATION"": eventvalidation,
                     ""__VIEWSTATE"": viewstate
                     })

    return r.text


def make_file(html):
    # Make file with the result data
    f = open(""text.html"", ""w"")
    f.write(html)


def options(soup, id):
    # Get data about options: airport and carriers
    option_values = []
    carrier_list = soup.find(id=id)
    for option in carrier_list.find_all('option'):
        option_values.append(option['value'])
    return option_values


def print_list(label, codes):
    # Print data
    print ""\n%s:"" % label
    for c in codes:
        print c


def get_web(url):
    # Get url
    page = urllib2.urlopen(url)
    page_source = page.read()
    return page_source

def main():
    # setup the location files
    URL = 'http://www.transtats.bts.gov/Data_Elements.aspx?Data=2'

    page_source = get_web(URL)
    soup = BeautifulSoup(page_source)

    codes = options(soup, ""CarrierList"")
    print_list(""Carriers"", codes)

    codes = options(soup, ""AirportList"")
    print_list(""Airports"", codes)

    s = requests.Session()
    data = extract_data(URL, s)

    html = make_request(data, s)

    make_file(html)

if __name__ == '__main__':
    main()
",Create file to get information from a particular URL to get data about airplanes and carriers.,"feat: Create file to get information from a particular URL to get data about airplanes and carriers.
",Python,mit,aguijarro/DataSciencePython,102,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

""""""
Script to get information from:
http://www.transtats.bts.gov/Data_Elements.aspx?Data=2
about carrier and airports
""""""

from bs4 import BeautifulSoup
import requests
import urllib2



def extract_data(url, s):
    # Extract data from a html source from a URL
    r = s.get(url)
    soup = BeautifulSoup(r.text)
    data = {""eventvalidation"": """",
            ""viewstate"": """"}

    eventvalidation_element = soup.find(id=""__EVENTVALIDATION"")
    data[""eventvalidation""] = eventvalidation_element[""value""]

    viewstate_element = soup.find(id=""__VIEWSTATE"")
    data[""viewstate""] = viewstate_element[""value""]

    return data




def make_request(data, s):
    # Make request to get data
    eventvalidation = data[""eventvalidation""]
    viewstate = data[""viewstate""]

    r = s.post(""http://www.transtats.bts.gov/Data_Elements.aspx?Data=2"",
               data={'AirportList': ""BOS"",
                     'CarrierList': ""VX"",
                     'Submit': 'Submit',
                     ""__EVENTTARGET"": """",
                     ""__EVENTARGUMENT"": """",
                     ""__EVENTVALIDATION"": eventvalidation,
                     ""__VIEWSTATE"": viewstate
                     })

    return r.text


def make_file(html):
    # Make file with the result data
    f = open(""text.html"", ""w"")
    f.write(html)


def options(soup, id):
    # Get data about options: airport and carriers
    option_values = []
    carrier_list = soup.find(id=id)
    for option in carrier_list.find_all('option'):
        option_values.append(option['value'])
    return option_values


def print_list(label, codes):
    # Print data
    print ""\n%s:"" % label
    for c in codes:
        print c


def get_web(url):
    # Get url
    page = urllib2.urlopen(url)
    page_source = page.read()
    return page_source

def main():
    # setup the location files
    URL = 'http://www.transtats.bts.gov/Data_Elements.aspx?Data=2'

    page_source = get_web(URL)
    soup = BeautifulSoup(page_source)

    codes = options(soup, ""CarrierList"")
    print_list(""Carriers"", codes)

    codes = options(soup, ""AirportList"")
    print_list(""Airports"", codes)

    s = requests.Session()
    data = extract_data(URL, s)

    html = make_request(data, s)

    make_file(html)

if __name__ == '__main__':
    main()

```"
f306b2145d5bff7a3d399e14b60274c58c3bf098,scripts/tests/test_box_migrate_to_external_account.py,scripts/tests/test_box_migrate_to_external_account.py,,"from nose.tools import *

from scripts.box.migrate_to_external_account import do_migration, get_targets

from framework.auth import Auth

from tests.base import OsfTestCase
from tests.factories import ProjectFactory, UserFactory

from website.addons.box.model import BoxUserSettings
from website.addons.box.tests.factories import BoxOAuthSettingsFactory


class TestBoxMigration(OsfTestCase):
    # Note: BoxUserSettings.user_settings has to be changed to foreign_user_settings (model and mongo). See migration instructions

    def test_migration_no_project(self):

        user = UserFactory()

        user.add_addon('box')
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        do_migration([user_addon])
        user_addon.reload()

        assert_is_none(user_addon.oauth_settings)
        assert_equal(len(user.external_accounts), 1)

        account = user.external_accounts[0]
        assert_equal(account.provider, 'box')
        assert_equal(account.oauth_key, 'abcdef1')

    def test_migration_removes_targets(self):
        BoxUserSettings.remove()

        user = UserFactory()
        project = ProjectFactory(creator=user)

        user.add_addon('box', auth=Auth(user))
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        project.add_addon('box', auth=Auth(user))
        node_addon = project.get_addon('box')
        node_addon.foreign_user_settings = user_addon
        node_addon.save()

        assert_equal(get_targets().count(), 1)

        do_migration([user_addon])
        user_addon.reload()

        assert_equal(get_targets().count(), 0)

    def test_migration_multiple_users(self):
        user1 = UserFactory()
        user2 = UserFactory()
        oauth_settings = BoxOAuthSettingsFactory()

        user1.add_addon('box')
        user1_addon = user1.get_addon('box')
        user1_addon.oauth_settings = oauth_settings
        user1_addon.save()

        user2.add_addon('box')
        user2_addon = user2.get_addon('box')
        user2_addon.oauth_settings = oauth_settings
        user2_addon.save()

        do_migration([user1_addon, user2_addon])
        user1_addon.reload()
        user2_addon.reload()

        assert_equal(
            user1.external_accounts[0],
            user2.external_accounts[0],
        )

    def test_get_targets(self):
        BoxUserSettings.remove()
        addons = [
            BoxUserSettings(),
            BoxUserSettings(oauth_settings=BoxOAuthSettingsFactory()),
        ]
        for addon in addons:
            addon.save()
        targets = get_targets()
        assert_equal(targets.count(), 1)
        assert_equal(targets[0]._id, addons[-1]._id)
",Add test for box migration script,"Add test for box migration script
",Python,apache-2.0,"KAsante95/osf.io,aaxelb/osf.io,Nesiehr/osf.io,emetsger/osf.io,Nesiehr/osf.io,chrisseto/osf.io,Nesiehr/osf.io,saradbowman/osf.io,SSJohns/osf.io,binoculars/osf.io,mluo613/osf.io,leb2dg/osf.io,wearpants/osf.io,HalcyonChimera/osf.io,amyshi188/osf.io,kch8qx/osf.io,acshi/osf.io,saradbowman/osf.io,haoyuchen1992/osf.io,CenterForOpenScience/osf.io,pattisdr/osf.io,billyhunt/osf.io,mluo613/osf.io,caseyrygt/osf.io,cslzchen/osf.io,aaxelb/osf.io,mattclark/osf.io,GageGaskins/osf.io,leb2dg/osf.io,acshi/osf.io,njantrania/osf.io,GageGaskins/osf.io,samchrisinger/osf.io,Johnetordoff/osf.io,samanehsan/osf.io,baylee-d/osf.io,TomBaxter/osf.io,doublebits/osf.io,RomanZWang/osf.io,Johnetordoff/osf.io,doublebits/osf.io,mluke93/osf.io,kwierman/osf.io,cwisecarver/osf.io,aaxelb/osf.io,cosenal/osf.io,adlius/osf.io,samanehsan/osf.io,CenterForOpenScience/osf.io,kwierman/osf.io,kwierman/osf.io,alexschiller/osf.io,ticklemepierce/osf.io,acshi/osf.io,brandonPurvis/osf.io,brandonPurvis/osf.io,mfraezz/osf.io,haoyuchen1992/osf.io,mluo613/osf.io,cosenal/osf.io,cslzchen/osf.io,kch8qx/osf.io,mluke93/osf.io,alexschiller/osf.io,brianjgeiger/osf.io,njantrania/osf.io,TomHeatwole/osf.io,billyhunt/osf.io,RomanZWang/osf.io,DanielSBrown/osf.io,amyshi188/osf.io,chrisseto/osf.io,laurenrevere/osf.io,caseyrollins/osf.io,brandonPurvis/osf.io,hmoco/osf.io,danielneis/osf.io,felliott/osf.io,laurenrevere/osf.io,njantrania/osf.io,caseyrygt/osf.io,cslzchen/osf.io,asanfilippo7/osf.io,erinspace/osf.io,amyshi188/osf.io,TomBaxter/osf.io,samanehsan/osf.io,asanfilippo7/osf.io,hmoco/osf.io,jnayak1/osf.io,zamattiac/osf.io,baylee-d/osf.io,hmoco/osf.io,pattisdr/osf.io,billyhunt/osf.io,brianjgeiger/osf.io,caseyrygt/osf.io,leb2dg/osf.io,samchrisinger/osf.io,monikagrabowska/osf.io,crcresearch/osf.io,abought/osf.io,samchrisinger/osf.io,binoculars/osf.io,felliott/osf.io,TomHeatwole/osf.io,samchrisinger/osf.io,brandonPurvis/osf.io,emetsger/osf.io,mluke93/osf.io,Johnetordoff/osf.io,aaxelb/osf.io,billyhunt/osf.io,amyshi188/osf.io,RomanZWang/osf.io,doublebits/osf.io,adlius/osf.io,Nesiehr/osf.io,zachjanicki/osf.io,DanielSBrown/osf.io,zamattiac/osf.io,DanielSBrown/osf.io,ticklemepierce/osf.io,Ghalko/osf.io,HalcyonChimera/osf.io,CenterForOpenScience/osf.io,cwisecarver/osf.io,cosenal/osf.io,KAsante95/osf.io,ticklemepierce/osf.io,haoyuchen1992/osf.io,SSJohns/osf.io,monikagrabowska/osf.io,jnayak1/osf.io,alexschiller/osf.io,njantrania/osf.io,chennan47/osf.io,mluke93/osf.io,Ghalko/osf.io,kch8qx/osf.io,chennan47/osf.io,cosenal/osf.io,abought/osf.io,emetsger/osf.io,GageGaskins/osf.io,chrisseto/osf.io,CenterForOpenScience/osf.io,jnayak1/osf.io,icereval/osf.io,jnayak1/osf.io,danielneis/osf.io,RomanZWang/osf.io,caseyrygt/osf.io,zamattiac/osf.io,Ghalko/osf.io,RomanZWang/osf.io,KAsante95/osf.io,icereval/osf.io,TomHeatwole/osf.io,chrisseto/osf.io,kwierman/osf.io,rdhyee/osf.io,danielneis/osf.io,mattclark/osf.io,wearpants/osf.io,KAsante95/osf.io,billyhunt/osf.io,abought/osf.io,TomBaxter/osf.io,abought/osf.io,mfraezz/osf.io,brianjgeiger/osf.io,SSJohns/osf.io,ZobairAlijan/osf.io,binoculars/osf.io,felliott/osf.io,monikagrabowska/osf.io,rdhyee/osf.io,erinspace/osf.io,asanfilippo7/osf.io,caneruguz/osf.io,zachjanicki/osf.io,TomHeatwole/osf.io,ZobairAlijan/osf.io,felliott/osf.io,monikagrabowska/osf.io,Ghalko/osf.io,caseyrollins/osf.io,zachjanicki/osf.io,brianjgeiger/osf.io,adlius/osf.io,samanehsan/osf.io,SSJohns/osf.io,zamattiac/osf.io,wearpants/osf.io,KAsante95/osf.io,cwisecarver/osf.io,mluo613/osf.io,adlius/osf.io,danielneis/osf.io,icereval/osf.io,ticklemepierce/osf.io,Johnetordoff/osf.io,alexschiller/osf.io,caseyrollins/osf.io,GageGaskins/osf.io,mfraezz/osf.io,caneruguz/osf.io,hmoco/osf.io,chennan47/osf.io,baylee-d/osf.io,crcresearch/osf.io,HalcyonChimera/osf.io,zachjanicki/osf.io,cslzchen/osf.io,kch8qx/osf.io,ZobairAlijan/osf.io,asanfilippo7/osf.io,emetsger/osf.io,cwisecarver/osf.io,sloria/osf.io,doublebits/osf.io,rdhyee/osf.io,HalcyonChimera/osf.io,pattisdr/osf.io,wearpants/osf.io,erinspace/osf.io,ZobairAlijan/osf.io,mfraezz/osf.io,brandonPurvis/osf.io,haoyuchen1992/osf.io,alexschiller/osf.io,monikagrabowska/osf.io,doublebits/osf.io,kch8qx/osf.io,rdhyee/osf.io,sloria/osf.io,laurenrevere/osf.io,crcresearch/osf.io,caneruguz/osf.io,DanielSBrown/osf.io,acshi/osf.io,acshi/osf.io,sloria/osf.io,GageGaskins/osf.io,mattclark/osf.io,caneruguz/osf.io,mluo613/osf.io,leb2dg/osf.io",94,"```python
from nose.tools import *

from scripts.box.migrate_to_external_account import do_migration, get_targets

from framework.auth import Auth

from tests.base import OsfTestCase
from tests.factories import ProjectFactory, UserFactory

from website.addons.box.model import BoxUserSettings
from website.addons.box.tests.factories import BoxOAuthSettingsFactory


class TestBoxMigration(OsfTestCase):
    # Note: BoxUserSettings.user_settings has to be changed to foreign_user_settings (model and mongo). See migration instructions

    def test_migration_no_project(self):

        user = UserFactory()

        user.add_addon('box')
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        do_migration([user_addon])
        user_addon.reload()

        assert_is_none(user_addon.oauth_settings)
        assert_equal(len(user.external_accounts), 1)

        account = user.external_accounts[0]
        assert_equal(account.provider, 'box')
        assert_equal(account.oauth_key, 'abcdef1')

    def test_migration_removes_targets(self):
        BoxUserSettings.remove()

        user = UserFactory()
        project = ProjectFactory(creator=user)

        user.add_addon('box', auth=Auth(user))
        user_addon = user.get_addon('box')
        user_addon.oauth_settings = BoxOAuthSettingsFactory()
        user_addon.save()

        project.add_addon('box', auth=Auth(user))
        node_addon = project.get_addon('box')
        node_addon.foreign_user_settings = user_addon
        node_addon.save()

        assert_equal(get_targets().count(), 1)

        do_migration([user_addon])
        user_addon.reload()

        assert_equal(get_targets().count(), 0)

    def test_migration_multiple_users(self):
        user1 = UserFactory()
        user2 = UserFactory()
        oauth_settings = BoxOAuthSettingsFactory()

        user1.add_addon('box')
        user1_addon = user1.get_addon('box')
        user1_addon.oauth_settings = oauth_settings
        user1_addon.save()

        user2.add_addon('box')
        user2_addon = user2.get_addon('box')
        user2_addon.oauth_settings = oauth_settings
        user2_addon.save()

        do_migration([user1_addon, user2_addon])
        user1_addon.reload()
        user2_addon.reload()

        assert_equal(
            user1.external_accounts[0],
            user2.external_accounts[0],
        )

    def test_get_targets(self):
        BoxUserSettings.remove()
        addons = [
            BoxUserSettings(),
            BoxUserSettings(oauth_settings=BoxOAuthSettingsFactory()),
        ]
        for addon in addons:
            addon.save()
        targets = get_targets()
        assert_equal(targets.count(), 1)
        assert_equal(targets[0]._id, addons[-1]._id)

```"
5dd8284d9f8b3891de74f22685270b058051c3f0,tests/test_inherit.py,tests/test_inherit.py,,"# -*- coding: utf-8 -*-
""""""
    Tests for inheritance in RegexLexer
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    :copyright: Copyright 2006-2014 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

import unittest

from pygments.lexer import RegexLexer, inherit
from pygments.token import Text


class InheritTest(unittest.TestCase):
    def test_single_inheritance_position(self):
        t = Two()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['x', 'a', 'b', 'y'], pats)
    def test_multi_inheritance_beginning(self):
        t = Beginning()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['x', 'a', 'b', 'y', 'm'], pats)
    def test_multi_inheritance_end(self):
        t = End()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['m', 'x', 'a', 'b', 'y'], pats)

    def test_multi_inheritance_position(self):
        t = Three()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['i', 'x', 'a', 'b', 'y', 'j'], pats)

    def test_single_inheritance_with_skip(self):
        t = Skipped()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['x', 'a', 'b', 'y'], pats)


class One(RegexLexer):
    tokens = {
        'root': [
            ('a', Text),
            ('b', Text),
        ],
    }

class Two(One):
    tokens = {
        'root': [
            ('x', Text),
            inherit,
            ('y', Text),
        ],
    }

class Three(Two):
    tokens = {
        'root': [
            ('i', Text),
            inherit,
            ('j', Text),
        ],
    }

class Beginning(Two):
    tokens = {
        'root': [
            inherit,
            ('m', Text),
        ],
    }

class End(Two):
    tokens = {
        'root': [
            ('m', Text),
            inherit,
        ],
    }

class Empty(One):
    tokens = {}

class Skipped(Empty):
    tokens = {
        'root': [
            ('x', Text),
            inherit,
            ('y', Text),
        ],
    }

",Add test for RegexLexer inheritance (fails with current code).,"Add test for RegexLexer inheritance (fails with current code).
",Python,bsd-2-clause,"aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments,aswinpj/Pygments",95,"```python
# -*- coding: utf-8 -*-
""""""
    Tests for inheritance in RegexLexer
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    :copyright: Copyright 2006-2014 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""

import unittest

from pygments.lexer import RegexLexer, inherit
from pygments.token import Text


class InheritTest(unittest.TestCase):
    def test_single_inheritance_position(self):
        t = Two()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['x', 'a', 'b', 'y'], pats)
    def test_multi_inheritance_beginning(self):
        t = Beginning()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['x', 'a', 'b', 'y', 'm'], pats)
    def test_multi_inheritance_end(self):
        t = End()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['m', 'x', 'a', 'b', 'y'], pats)

    def test_multi_inheritance_position(self):
        t = Three()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['i', 'x', 'a', 'b', 'y', 'j'], pats)

    def test_single_inheritance_with_skip(self):
        t = Skipped()
        pats = [x[0].__self__.pattern for x in t._tokens['root']]
        self.assertEqual(['x', 'a', 'b', 'y'], pats)


class One(RegexLexer):
    tokens = {
        'root': [
            ('a', Text),
            ('b', Text),
        ],
    }

class Two(One):
    tokens = {
        'root': [
            ('x', Text),
            inherit,
            ('y', Text),
        ],
    }

class Three(Two):
    tokens = {
        'root': [
            ('i', Text),
            inherit,
            ('j', Text),
        ],
    }

class Beginning(Two):
    tokens = {
        'root': [
            inherit,
            ('m', Text),
        ],
    }

class End(Two):
    tokens = {
        'root': [
            ('m', Text),
            inherit,
        ],
    }

class Empty(One):
    tokens = {}

class Skipped(Empty):
    tokens = {
        'root': [
            ('x', Text),
            inherit,
            ('y', Text),
        ],
    }


```"
3228bf3dd1a32694b42f4d08a5c6f0e63bf5128a,all_reports_smell_search_final.py,all_reports_smell_search_final.py,,"
from map import mapping
# walk through the os and get all files
# read each file in tern and go through line by line
# print lines that contain smell and the report name
from os import listdir
import nltk.data
import json

SMELL_WORDS = ['smell', 'stench', 'stink', 'odour', 'sniff', 'effluvium']
REPORTS_DIR = '/Users/deborah/Documents/scripts/python_work/project2016/Full Text Online'

global finalResult
finalResult = {}

def addToDic(d, report, rDate, val):
    d.setDefault(report, []).append(val)
    return d


def getFileNames():
    '''Retrieve file names'''
    fileNames = [f for f in listdir(REPORTS_DIR) if f.endswith('txt')]
    return fileNames


def processFile(fileName):
    path = REPORTS_DIR + '/' + fileName
    references = []
    with open(path) as f:
        for line in f:
            report_tokenized = tokenize(line)
            for scentence in report_tokenized:
                for word in SMELL_WORDS:
                    if word in scentence.lower():
                        references.append(scentence)
    return references


def tokenize(sentence):
    parser = nltk.data.load('tokenizers/punkt/english.pickle')
    result = parser.tokenize(sentence.strip())
    return result


def saveObject(results):
    '''Save results dictionary as file'''
    with open('processed_results.txt', 'w') as outfile:
        json.dump(results, outfile)


def performAnalysis(fileName, references):
    '''Create the resuts output'''
    # splits a fileName into :['Acton', '1900', 'b19783358', 'txt']
    splitReport = fileName.split('.')
    bID = splitReport[2]
    year = splitReport[1]

    try:
        region = mapping[bID]
    except:
        return
        # print bID

    if region in finalResult:
        nestedDic = finalResult[region]
    else:
        nestedDic = {}
    
    nestedDic[year] = references
    finalResult[region] = nestedDic

    # if nestedDic[splitReport[1]]:
    #     val = nestedDic[splitReport[1]]
    #     nestedDic[splitReport[1]] = len(references) + val
    # else:
    #     if len(references):
    #         nestedDic[splitReport[1]] = len(references)
    # # nestedDic.setDefault(splitReport[1], 0).__add__(len(references))
    # result[region] = nestedDic

# print(result)
# for k,v in result.iteritems():



def main():
    # tokenize(s)
    fileNames = getFileNames()
    # f1 = fileNames[0]
    # processFile(f1)
    fileNames = fileNames[:100]
    for f in fileNames:
        references = processFile(f)
        if references:
            performAnalysis(f, references)
    saveObject(finalResult)


if __name__ == '__main__':
    main()",Add script to datamine the reports via NLTK,"Add script to datamine the reports via NLTK
",Python,apache-2.0,"Smelly-London/Smelly-London,Smelly-London/Smelly-London,Smelly-London/datavisualisation,Smelly-London/Smelly-London,Smelly-London/datavisualisation,Smelly-London/Smelly-London",101,"```python

from map import mapping
# walk through the os and get all files
# read each file in tern and go through line by line
# print lines that contain smell and the report name
from os import listdir
import nltk.data
import json

SMELL_WORDS = ['smell', 'stench', 'stink', 'odour', 'sniff', 'effluvium']
REPORTS_DIR = '/Users/deborah/Documents/scripts/python_work/project2016/Full Text Online'

global finalResult
finalResult = {}

def addToDic(d, report, rDate, val):
    d.setDefault(report, []).append(val)
    return d


def getFileNames():
    '''Retrieve file names'''
    fileNames = [f for f in listdir(REPORTS_DIR) if f.endswith('txt')]
    return fileNames


def processFile(fileName):
    path = REPORTS_DIR + '/' + fileName
    references = []
    with open(path) as f:
        for line in f:
            report_tokenized = tokenize(line)
            for scentence in report_tokenized:
                for word in SMELL_WORDS:
                    if word in scentence.lower():
                        references.append(scentence)
    return references


def tokenize(sentence):
    parser = nltk.data.load('tokenizers/punkt/english.pickle')
    result = parser.tokenize(sentence.strip())
    return result


def saveObject(results):
    '''Save results dictionary as file'''
    with open('processed_results.txt', 'w') as outfile:
        json.dump(results, outfile)


def performAnalysis(fileName, references):
    '''Create the resuts output'''
    # splits a fileName into :['Acton', '1900', 'b19783358', 'txt']
    splitReport = fileName.split('.')
    bID = splitReport[2]
    year = splitReport[1]

    try:
        region = mapping[bID]
    except:
        return
        # print bID

    if region in finalResult:
        nestedDic = finalResult[region]
    else:
        nestedDic = {}
    
    nestedDic[year] = references
    finalResult[region] = nestedDic

    # if nestedDic[splitReport[1]]:
    #     val = nestedDic[splitReport[1]]
    #     nestedDic[splitReport[1]] = len(references) + val
    # else:
    #     if len(references):
    #         nestedDic[splitReport[1]] = len(references)
    # # nestedDic.setDefault(splitReport[1], 0).__add__(len(references))
    # result[region] = nestedDic

# print(result)
# for k,v in result.iteritems():



def main():
    # tokenize(s)
    fileNames = getFileNames()
    # f1 = fileNames[0]
    # processFile(f1)
    fileNames = fileNames[:100]
    for f in fileNames:
        references = processFile(f)
        if references:
            performAnalysis(f, references)
    saveObject(finalResult)


if __name__ == '__main__':
    main()
```"
16d6dd8f3f2359a3a5d83eac9d2812560ab2d6bf,microcosm_pubsub/handlers.py,microcosm_pubsub/handlers.py,,"""""""
Handler base classes.

""""""
from abc import ABCMeta
from inflection import humanize

from requests import get


class URIHandler(object):
    """"""
    Base handler for URI-driven events.

    As a general rule, we want PubSub events to convey the URI of a resource that was created
    (because resources are ideally immutable state). In this case, we want asynchronous workers
    to query the existing URI to get more information (and to handle race conditions where the
    message was delivered before the resource was committed.)

    There are five expected outcomes for this handler:
      - Raising an error (e.g. a bug)
      - Skipping the handlers (because the pubsub message carried enough information to bypass processing)
      - Handling the message after fetching the resource by URI
      - Ignore the message after fetching the resource by URI
      - Raising a nack (e.g. because the resource was not committed yet)

    The middle three cases are all handled here with the expectation that we produce *one* INFO-level
    log per message processed (unless an error/nack is raised).

    """"""
    __metaclass__ = ABCMeta

    @property
    def name(self):
        return humanize(self.__class__.__name__)

    def __call__(self, message):
        uri = message[""uri""]
        self.on_call(message, uri)

        skip_reason = self.get_reason_to_skip(message, uri)
        if skip_reason is not None:
            self.on_skip(message, uri, skip_reason)
            return False

        resource = self.get_resource(uri=uri)

        if self.handle(message, uri, resource):
            self.on_handle(message, uri, resource)
            return True
        else:
            self.on_ignore(message, uri, resource)
            return False

    def on_call(self, message, uri):
        self.logger.debug(
            ""Starting {handler}"",
            extra=dict(
                handler=self.name,
                uri=uri,
            ),
        )

    def on_skip(self, message, uri, reason):
        self.logger.info(
            ""Skipping {handler} because {reason}"",
            extra=dict(
                handler=self.name,
                reason=reason,
                uri=uri,
            ),
        )

    def on_handle(self, message, uri, resource):
        self.logger.info(
            ""Handled {handler}"",
            extra=dict(
                handler=self.name,
                uri=uri,
            ),
        )

    def on_ignore(self, message, uri, resource):
        self.logger.info(
            ""Ignored {handler}"",
            extra=dict(
                handler=self.name,
                uri=uri,
            ),
        )

    def get_reason_to_skip(self, message, uri):
        """"""
        Some messages carry enough context that we can avoid resolving the URI entirely.

        """"""
        return None

    def get_resource(self, uri):
        """"""
        Mock-friendly URI getter.

        """"""
        response = get(uri)
        response.raise_for_status()
        return response.json()

    def handle(self, message, resource):
        return True
",Add a base handler for URI-oriented pubsub messages,"Add a base handler for URI-oriented pubsub messages
",Python,apache-2.0,"globality-corp/microcosm-pubsub,globality-corp/microcosm-pubsub",110,"```python
""""""
Handler base classes.

""""""
from abc import ABCMeta
from inflection import humanize

from requests import get


class URIHandler(object):
    """"""
    Base handler for URI-driven events.

    As a general rule, we want PubSub events to convey the URI of a resource that was created
    (because resources are ideally immutable state). In this case, we want asynchronous workers
    to query the existing URI to get more information (and to handle race conditions where the
    message was delivered before the resource was committed.)

    There are five expected outcomes for this handler:
      - Raising an error (e.g. a bug)
      - Skipping the handlers (because the pubsub message carried enough information to bypass processing)
      - Handling the message after fetching the resource by URI
      - Ignore the message after fetching the resource by URI
      - Raising a nack (e.g. because the resource was not committed yet)

    The middle three cases are all handled here with the expectation that we produce *one* INFO-level
    log per message processed (unless an error/nack is raised).

    """"""
    __metaclass__ = ABCMeta

    @property
    def name(self):
        return humanize(self.__class__.__name__)

    def __call__(self, message):
        uri = message[""uri""]
        self.on_call(message, uri)

        skip_reason = self.get_reason_to_skip(message, uri)
        if skip_reason is not None:
            self.on_skip(message, uri, skip_reason)
            return False

        resource = self.get_resource(uri=uri)

        if self.handle(message, uri, resource):
            self.on_handle(message, uri, resource)
            return True
        else:
            self.on_ignore(message, uri, resource)
            return False

    def on_call(self, message, uri):
        self.logger.debug(
            ""Starting {handler}"",
            extra=dict(
                handler=self.name,
                uri=uri,
            ),
        )

    def on_skip(self, message, uri, reason):
        self.logger.info(
            ""Skipping {handler} because {reason}"",
            extra=dict(
                handler=self.name,
                reason=reason,
                uri=uri,
            ),
        )

    def on_handle(self, message, uri, resource):
        self.logger.info(
            ""Handled {handler}"",
            extra=dict(
                handler=self.name,
                uri=uri,
            ),
        )

    def on_ignore(self, message, uri, resource):
        self.logger.info(
            ""Ignored {handler}"",
            extra=dict(
                handler=self.name,
                uri=uri,
            ),
        )

    def get_reason_to_skip(self, message, uri):
        """"""
        Some messages carry enough context that we can avoid resolving the URI entirely.

        """"""
        return None

    def get_resource(self, uri):
        """"""
        Mock-friendly URI getter.

        """"""
        response = get(uri)
        response.raise_for_status()
        return response.json()

    def handle(self, message, resource):
        return True

```"
df4601af8ce70e48ffd4362556c2b07e4a6f53db,nettests/experimental/script.py,nettests/experimental/script.py,,"from ooni import nettest
from ooni.utils import log
from twisted.internet import defer, protocol, reactor
from twisted.python import usage

import os


def which(program):
    def is_exe(fpath):
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

    fpath, fname = os.path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for path in os.environ[""PATH""].split(os.pathsep):
            path = path.strip('""')
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                return exe_file
    return None


class UsageOptions(usage.Options):
    optParameters = [
        ['interpreter', 'i', '', 'The interpreter to use'],
        ['script', 's', '', 'The script to run']
    ]


class ScriptProcessProtocol(protocol.ProcessProtocol):
    def __init__(self, test_case):
        self.test_case = test_case
        self.deferred = defer.Deferred()

    def connectionMade(self):
        log.debug(""connectionMade"")
        self.transport.closeStdin()
        self.test_case.report['lua_output'] = """"

    def outReceived(self, data):
        log.debug('outReceived: %s' % data)
        self.test_case.report['lua_output'] += data

    def errReceived(self, data):
        log.err('Script error: %s' % data)
        self.transport.signalProcess('KILL')

    def processEnded(self, status):
        rc = status.value.exitCode
        log.debug('processEnded: %s, %s' % \
                  (rc, self.test_case.report['lua_output']))
        if rc == 0:
            self.deferred.callback(self)
        else:
            self.deferred.errback(rc)


# TODO: Maybe the script requires a back-end.
class Script(nettest.NetTestCase):
    name = ""Script test""
    version = ""0.1""
    authors = ""Dominic Hamon""

    usageOptions = UsageOptions
    requiredOptions = ['interpreter', 'script']

    def test_run_script(self):
        """"""
        We run the script specified in the usage options and take whatever
        is printed to stdout as the results of the test.
        """"""
        processProtocol = ScriptProcessProtocol(self)

        interpreter = self.localOptions['interpreter']
        if not which(interpreter):
            log.err('Unable to find %s executable in PATH.' % interpreter)
            return

        reactor.spawnProcess(processProtocol,
                             interpreter,
                             args=[interpreter, self.localOptions['script']],
                             env={'HOME': os.environ['HOME']},
                             usePTY=True)

        if not reactor.running:
            reactor.run()
        return processProtocol.deferred
",Add Dominic Hamon's nettest for running tests written with other interpreters.,"Add Dominic Hamon's nettest for running tests written with other interpreters.

 * Fixes #8011.
",Python,bsd-2-clause,"kdmurray91/ooni-probe,Karthikeyan-kkk/ooni-probe,Karthikeyan-kkk/ooni-probe,kdmurray91/ooni-probe,lordappsec/ooni-probe,lordappsec/ooni-probe,lordappsec/ooni-probe,0xPoly/ooni-probe,juga0/ooni-probe,Karthikeyan-kkk/ooni-probe,0xPoly/ooni-probe,0xPoly/ooni-probe,juga0/ooni-probe,juga0/ooni-probe,kdmurray91/ooni-probe,kdmurray91/ooni-probe,Karthikeyan-kkk/ooni-probe,juga0/ooni-probe,0xPoly/ooni-probe,lordappsec/ooni-probe",91,"```python
from ooni import nettest
from ooni.utils import log
from twisted.internet import defer, protocol, reactor
from twisted.python import usage

import os


def which(program):
    def is_exe(fpath):
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

    fpath, fname = os.path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for path in os.environ[""PATH""].split(os.pathsep):
            path = path.strip('""')
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                return exe_file
    return None


class UsageOptions(usage.Options):
    optParameters = [
        ['interpreter', 'i', '', 'The interpreter to use'],
        ['script', 's', '', 'The script to run']
    ]


class ScriptProcessProtocol(protocol.ProcessProtocol):
    def __init__(self, test_case):
        self.test_case = test_case
        self.deferred = defer.Deferred()

    def connectionMade(self):
        log.debug(""connectionMade"")
        self.transport.closeStdin()
        self.test_case.report['lua_output'] = """"

    def outReceived(self, data):
        log.debug('outReceived: %s' % data)
        self.test_case.report['lua_output'] += data

    def errReceived(self, data):
        log.err('Script error: %s' % data)
        self.transport.signalProcess('KILL')

    def processEnded(self, status):
        rc = status.value.exitCode
        log.debug('processEnded: %s, %s' % \
                  (rc, self.test_case.report['lua_output']))
        if rc == 0:
            self.deferred.callback(self)
        else:
            self.deferred.errback(rc)


# TODO: Maybe the script requires a back-end.
class Script(nettest.NetTestCase):
    name = ""Script test""
    version = ""0.1""
    authors = ""Dominic Hamon""

    usageOptions = UsageOptions
    requiredOptions = ['interpreter', 'script']

    def test_run_script(self):
        """"""
        We run the script specified in the usage options and take whatever
        is printed to stdout as the results of the test.
        """"""
        processProtocol = ScriptProcessProtocol(self)

        interpreter = self.localOptions['interpreter']
        if not which(interpreter):
            log.err('Unable to find %s executable in PATH.' % interpreter)
            return

        reactor.spawnProcess(processProtocol,
                             interpreter,
                             args=[interpreter, self.localOptions['script']],
                             env={'HOME': os.environ['HOME']},
                             usePTY=True)

        if not reactor.running:
            reactor.run()
        return processProtocol.deferred

```"
567e12bfb8d0f4e2a4f6fddf0fab9ffbcbf6d49f,requests/_bug.py,requests/_bug.py,,"""""""Module containing bug report helper(s).""""""
from __future__ import print_function

import json
import platform
import sys
import ssl

from . import __version__ as requests_version

try:
    from .packages.urllib3.contrib import pyopenssl
except ImportError:
    pyopenssl = None
    OpenSSL = None
    cryptography = None
else:
    import OpenSSL
    import cryptography


def _implementation():
    """"""Return a dict with the Python implementation and verison.

    Provide both the name and the version of the Python implementation
    currently running. For example, on CPython 2.7.5 it will return
    {'name': 'CPython', 'version': '2.7.5'}.

    This function works best on CPython and PyPy: in particular, it probably
    doesn't work for Jython or IronPython. Future investigation should be done
    to work out the correct shape of the code for those platforms.
    """"""
    implementation = platform.python_implementation()

    if implementation == 'CPython':
        implementation_version = platform.python_version()
    elif implementation == 'PyPy':
        implementation_version = '%s.%s.%s' % (sys.pypy_version_info.major,
                                               sys.pypy_version_info.minor,
                                               sys.pypy_version_info.micro)
        if sys.pypy_version_info.releaselevel != 'final':
            implementation_version = ''.join([
                implementation_version, sys.pypy_version_info.releaselevel
                ])
    elif implementation == 'Jython':
        implementation_version = platform.python_version()  # Complete Guess
    elif implementation == 'IronPython':
        implementation_version = platform.python_version()  # Complete Guess
    else:
        implementation_version = 'Unknown'

    return {'name': implementation, 'version': implementation_version}


def information():
    """"""Generate information for a bug report.""""""
    try:
        platform_info = {
            'system': platform.system(),
            'release': platform.release(),
        }
    except IOError:
        platform_info = {
            'system': 'Unknown',
            'release': 'Unknown',
        }

    implementation_info = _implementation()
    pyopenssl_info = {
        'version': None,
        'openssl_version': '',
    }
    if OpenSSL:
        pyopenssl_info = {
            'version': OpenSSL.__version__,
            'openssl_version': '%x' % OpenSSL.SSL.OPENSSL_VERSION_NUMBER,
        }
    cryptography_info = {
        'version': getattr(cryptography, '__version__', ''),
    }

    return {
        'platform': platform_info,
        'implementation': implementation_info,
        'system_ssl': {
            'version': '%x' % ssl.OPENSSL_VERSION_NUMBER,
        },
        'using_pyopenssl': pyopenssl is not None,
        'pyOpenSSL': pyopenssl_info,
        'cryptography': cryptography_info,
        'requests': {
            'version': requests_version,
        },
    }


def print_information():
    """"""Pretty-print the bug information as JSON.""""""
    print(json.dumps(information(), sort_keys=True, indent=2))
",Add debugging submodule for bug reporters,"Add debugging submodule for bug reporters

The suggested usage in a bug report would be

    python -c 'from requests import _bug; _bug.print_information()'

This should generate most of the information we tend to ask for
repeatedly from bug reporters.
",Python,apache-2.0,psf/requests,100,"```python
""""""Module containing bug report helper(s).""""""
from __future__ import print_function

import json
import platform
import sys
import ssl

from . import __version__ as requests_version

try:
    from .packages.urllib3.contrib import pyopenssl
except ImportError:
    pyopenssl = None
    OpenSSL = None
    cryptography = None
else:
    import OpenSSL
    import cryptography


def _implementation():
    """"""Return a dict with the Python implementation and verison.

    Provide both the name and the version of the Python implementation
    currently running. For example, on CPython 2.7.5 it will return
    {'name': 'CPython', 'version': '2.7.5'}.

    This function works best on CPython and PyPy: in particular, it probably
    doesn't work for Jython or IronPython. Future investigation should be done
    to work out the correct shape of the code for those platforms.
    """"""
    implementation = platform.python_implementation()

    if implementation == 'CPython':
        implementation_version = platform.python_version()
    elif implementation == 'PyPy':
        implementation_version = '%s.%s.%s' % (sys.pypy_version_info.major,
                                               sys.pypy_version_info.minor,
                                               sys.pypy_version_info.micro)
        if sys.pypy_version_info.releaselevel != 'final':
            implementation_version = ''.join([
                implementation_version, sys.pypy_version_info.releaselevel
                ])
    elif implementation == 'Jython':
        implementation_version = platform.python_version()  # Complete Guess
    elif implementation == 'IronPython':
        implementation_version = platform.python_version()  # Complete Guess
    else:
        implementation_version = 'Unknown'

    return {'name': implementation, 'version': implementation_version}


def information():
    """"""Generate information for a bug report.""""""
    try:
        platform_info = {
            'system': platform.system(),
            'release': platform.release(),
        }
    except IOError:
        platform_info = {
            'system': 'Unknown',
            'release': 'Unknown',
        }

    implementation_info = _implementation()
    pyopenssl_info = {
        'version': None,
        'openssl_version': '',
    }
    if OpenSSL:
        pyopenssl_info = {
            'version': OpenSSL.__version__,
            'openssl_version': '%x' % OpenSSL.SSL.OPENSSL_VERSION_NUMBER,
        }
    cryptography_info = {
        'version': getattr(cryptography, '__version__', ''),
    }

    return {
        'platform': platform_info,
        'implementation': implementation_info,
        'system_ssl': {
            'version': '%x' % ssl.OPENSSL_VERSION_NUMBER,
        },
        'using_pyopenssl': pyopenssl is not None,
        'pyOpenSSL': pyopenssl_info,
        'cryptography': cryptography_info,
        'requests': {
            'version': requests_version,
        },
    }


def print_information():
    """"""Pretty-print the bug information as JSON.""""""
    print(json.dumps(information(), sort_keys=True, indent=2))

```"
d5fbbc55286d249c320ed5b54460b2091a023419,concurren-futures.py,concurren-futures.py,,"from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing
from multiprocessing.pool import ThreadPool
import threading
import time


def bar(i=0):
    if i == 0:
        raise ValueError(""bar raise"")
    return i ** 2


def main_Thread():
    thread = threading.Thread(target=bar)
    thread.start()
    thread.join()
    raise RuntimeError(""Exception not caught"")


def main_ThreadPool():
    p = ThreadPool(4)
    for i in p.map(bar, xrange(4)):
        print i
    raise RuntimeError(""Exception not caught"")


def main_ThreadPoolExecutorMap():
    with ThreadPoolExecutor(4) as ex:
        for i in ex.map(bar, xrange(4)):
            print i
    raise RuntimeError(""Exception not caught"")


def main_ThreadPoolExecutorSubmit():
    with ThreadPoolExecutor(4) as ex:
        s = ex.submit(bar)
        print s.result()
    raise RuntimeError(""Exception not caught"")


def main_Process():
    thread = multiprocessing.Process(target=bar)
    thread.start()
    thread.join()
    raise RuntimeError(""Exception not caught"")


def main_ProcessPool():
    p = multiprocessing.Pool(4)
    for i in p.map(bar, xrange(4)):
        print i
    raise RuntimeError(""Exception not caught"")


def main_ProcessPoolExecutorMap():
    with ProcessPoolExecutor(4) as ex:
        for i in ex.map(bar, xrange(4)):
            print i
    raise RuntimeError(""Exception not caught"")


def main_ProcessPoolExecutorSubmit():
    with ProcessPoolExecutor(4) as ex:
        s = ex.submit(bar, 0)
        print s.result()
    raise RuntimeError(""Exception not caught"")


def run(fun):
    ac = threading.active_count()
    try:
        fun()
    except RuntimeError:
        print fun.__name__, ""[NOT raised]""
    except ValueError:
        print fun.__name__, ""[RAISED]""
    time.sleep(1)
    print ""Zombie thread:"", threading.active_count() - ac

if __name__ == '__main__':
    run(main_Thread)
    run(main_ThreadPool)
    run(main_ThreadPoolExecutorMap)
    run(main_ThreadPoolExecutorSubmit)
    run(main_Process)
    run(main_ProcessPool)
    run(main_ProcessPoolExecutorMap)
    run(main_ProcessPoolExecutorSubmit)
",Add concurrent futures benchmark script,Add concurrent futures benchmark script,Python,mit,"voidabhi/python-scripts,voidabhi/python-scripts,voidabhi/python-scripts,voidabhi/python-scripts,voidabhi/python-scripts",90,"```python
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
import multiprocessing
from multiprocessing.pool import ThreadPool
import threading
import time


def bar(i=0):
    if i == 0:
        raise ValueError(""bar raise"")
    return i ** 2


def main_Thread():
    thread = threading.Thread(target=bar)
    thread.start()
    thread.join()
    raise RuntimeError(""Exception not caught"")


def main_ThreadPool():
    p = ThreadPool(4)
    for i in p.map(bar, xrange(4)):
        print i
    raise RuntimeError(""Exception not caught"")


def main_ThreadPoolExecutorMap():
    with ThreadPoolExecutor(4) as ex:
        for i in ex.map(bar, xrange(4)):
            print i
    raise RuntimeError(""Exception not caught"")


def main_ThreadPoolExecutorSubmit():
    with ThreadPoolExecutor(4) as ex:
        s = ex.submit(bar)
        print s.result()
    raise RuntimeError(""Exception not caught"")


def main_Process():
    thread = multiprocessing.Process(target=bar)
    thread.start()
    thread.join()
    raise RuntimeError(""Exception not caught"")


def main_ProcessPool():
    p = multiprocessing.Pool(4)
    for i in p.map(bar, xrange(4)):
        print i
    raise RuntimeError(""Exception not caught"")


def main_ProcessPoolExecutorMap():
    with ProcessPoolExecutor(4) as ex:
        for i in ex.map(bar, xrange(4)):
            print i
    raise RuntimeError(""Exception not caught"")


def main_ProcessPoolExecutorSubmit():
    with ProcessPoolExecutor(4) as ex:
        s = ex.submit(bar, 0)
        print s.result()
    raise RuntimeError(""Exception not caught"")


def run(fun):
    ac = threading.active_count()
    try:
        fun()
    except RuntimeError:
        print fun.__name__, ""[NOT raised]""
    except ValueError:
        print fun.__name__, ""[RAISED]""
    time.sleep(1)
    print ""Zombie thread:"", threading.active_count() - ac

if __name__ == '__main__':
    run(main_Thread)
    run(main_ThreadPool)
    run(main_ThreadPoolExecutorMap)
    run(main_ThreadPoolExecutorSubmit)
    run(main_Process)
    run(main_ProcessPool)
    run(main_ProcessPoolExecutorMap)
    run(main_ProcessPoolExecutorSubmit)

```"
5752df3cf5e77e76836376846db6c3cbcbfe2ef7,troposphere/ecs.py,troposphere/ecs.py,,"from . import AWSObject, AWSProperty
from .validators import boolean, network_port, integer


class Cluster(AWSObject):
    resource_type = ""AWS::ECS::Cluster""

    props = {}


class LoadBalancer(AWSProperty):
    props = {
        'ContainerName': (basestring, False),
        'ContainerPort': (network_port, False),
        'LoadBalancerName': (basestring, False),
    }


class Service(AWSObject):
    resource_type = ""AWS::ECS::Service""

    props = {
        'Cluster': (basestring, False),
        'DesiredCount': (integer, False),
        'LoadBalancers': ([LoadBalancer], False),
        'Role': (basestring, False),
        'TaskDefinition': (basestring, False),
    }


class Environment(AWSProperty):
    props = {
        'Name': (basestring, True),
        'Value': (basestring, True),
    }


class MountPoint(AWSProperty):
    props = {
        'ContainerPath': (basestring, True),
        'SourceVolume': (basestring, True),
        'ReadOnly': (boolean, False),
    }


class PortMapping(AWSProperty):
    props = {
        'ContainerPort': (network_port, True),
        'HostPort': (network_port, False),
    }


class VolumesFrom(AWSProperty):
    props = {
        'SourceContainer': (basestring, True),
        'ReadOnly': (boolean, False),
    }


class ContainerDefinition(AWSProperty):
    props = {
        'Command': ([basestring], False),
        'Cpu': (integer, False),
        'EntryPoint': ([basestring], False),
        'Environment': ([Environment], False),
        'Essential': (boolean, False),
        'Image': (basestring, True),
        'Links': ([basestring], False),
        'Memory': (integer, True),
        'MountPoints': ([MountPoint], False),
        'Name': (basestring, True),
        'PortMappings': ([PortMapping], False),
        'VolumesFrom': ([VolumesFrom], False),
    }


class Host(AWSProperty):
    props = {
        'SourcePath': (basestring, False),
    }


class Volume(AWSProperty):
    props = {
        'Name': (basestring, True),
        'Host': (Host, False),
    }


class TaskDefinition(AWSObject):
    resource_type = ""AWS::ECS::TaskDefinition""

    props = {
        'ContainerDefinitions': ([ContainerDefinition], True),
        'Volumes': ([Volume], True),
    }
",Add EC2 Container Service (ECS),Add EC2 Container Service (ECS),Python,bsd-2-clause,"Yipit/troposphere,xxxVxxx/troposphere,samcrang/troposphere,ptoraskar/troposphere,horacio3/troposphere,jdc0589/troposphere,amosshapira/troposphere,7digital/troposphere,micahhausler/troposphere,LouTheBrew/troposphere,ikben/troposphere,cloudtools/troposphere,johnctitus/troposphere,cryptickp/troposphere,cloudtools/troposphere,dmm92/troposphere,inetCatapult/troposphere,johnctitus/troposphere,pas256/troposphere,yxd-hde/troposphere,ikben/troposphere,garnaat/troposphere,mannytoledo/troposphere,kid/troposphere,mhahn/troposphere,7digital/troposphere,nicolaka/troposphere,horacio3/troposphere,craigbruce/troposphere,wangqiang8511/troposphere,alonsodomin/troposphere,pas256/troposphere,ccortezb/troposphere,dmm92/troposphere,WeAreCloudar/troposphere,alonsodomin/troposphere,unravelin/troposphere",97,"```python
from . import AWSObject, AWSProperty
from .validators import boolean, network_port, integer


class Cluster(AWSObject):
    resource_type = ""AWS::ECS::Cluster""

    props = {}


class LoadBalancer(AWSProperty):
    props = {
        'ContainerName': (basestring, False),
        'ContainerPort': (network_port, False),
        'LoadBalancerName': (basestring, False),
    }


class Service(AWSObject):
    resource_type = ""AWS::ECS::Service""

    props = {
        'Cluster': (basestring, False),
        'DesiredCount': (integer, False),
        'LoadBalancers': ([LoadBalancer], False),
        'Role': (basestring, False),
        'TaskDefinition': (basestring, False),
    }


class Environment(AWSProperty):
    props = {
        'Name': (basestring, True),
        'Value': (basestring, True),
    }


class MountPoint(AWSProperty):
    props = {
        'ContainerPath': (basestring, True),
        'SourceVolume': (basestring, True),
        'ReadOnly': (boolean, False),
    }


class PortMapping(AWSProperty):
    props = {
        'ContainerPort': (network_port, True),
        'HostPort': (network_port, False),
    }


class VolumesFrom(AWSProperty):
    props = {
        'SourceContainer': (basestring, True),
        'ReadOnly': (boolean, False),
    }


class ContainerDefinition(AWSProperty):
    props = {
        'Command': ([basestring], False),
        'Cpu': (integer, False),
        'EntryPoint': ([basestring], False),
        'Environment': ([Environment], False),
        'Essential': (boolean, False),
        'Image': (basestring, True),
        'Links': ([basestring], False),
        'Memory': (integer, True),
        'MountPoints': ([MountPoint], False),
        'Name': (basestring, True),
        'PortMappings': ([PortMapping], False),
        'VolumesFrom': ([VolumesFrom], False),
    }


class Host(AWSProperty):
    props = {
        'SourcePath': (basestring, False),
    }


class Volume(AWSProperty):
    props = {
        'Name': (basestring, True),
        'Host': (Host, False),
    }


class TaskDefinition(AWSObject):
    resource_type = ""AWS::ECS::TaskDefinition""

    props = {
        'ContainerDefinitions': ([ContainerDefinition], True),
        'Volumes': ([Volume], True),
    }

```"
7d26f961a4e6eff6f9dad7b42a04a8648efdefb8,.travis-output.py,.travis-output.py,,"#!/usr/bin/python3

import io
import pexpect
import string
import sys
import time

sys.stdin = io.TextIOWrapper(sys.stdin.detach(), newline='')

output_to=sys.stdout

args = list(sys.argv[1:])
logfile = open(args.pop(0), ""w"")
child = pexpect.spawn(' '.join(args))

def output_line(line_bits, last_skip):
  line = """".join(line_bits)
  sline = line.strip()

  skip = True
  if line.startswith("" ""):
    skip = False

  if len(sline) > 0:
    if sline[0] in string.ascii_uppercase:
      skip = False
    if sline[0] in ('[', '=', '!', '+'):
      skip = False

  if skip != last_skip:
    output_to.write('\n')

  if skip:
    output_to.write('.')
  else:
    output_to.write(line)

  output_to.flush()
  line_bits.clear()
  return skip


def find_newline(data):
  fulldata = b"""".join(data)
  newlinechar = fulldata.find(b'\n')
  retlinechar = fulldata.find(b'\r')

  if newlinechar == -1:
    newlinechar = len(fulldata)+1
  if retlinechar == -1:
    retlinechar = len(fulldata)+1

  if retlinechar+1 == newlinechar:
    splitpos = newlinechar
  else:
    splitpos = min(newlinechar, retlinechar)

  if splitpos > len(fulldata):
    return

  newline = fulldata[:splitpos+1]
  leftover = fulldata[splitpos+1:]

  data.clear()
  data.append(leftover)
  return newline


last_skip = False
cont = []
data = [b'']
while True:
  line = None
  while len(data) > 1 or len(data[0]) > 0 or child.isalive():
    line = find_newline(data)
    if line is not None:
      break
    try:
      data.append(child.read_nonblocking(100))
    except pexpect.TIMEOUT:
      pass
    except pexpect.EOF as e:
      data.append(b'\n')

  if not line:
    break

  line = line.decode('utf-8')
  logfile.write(line)
  logfile.flush()

  if line.endswith('\r'):
    cont.append(line[:-1])
    last_skip = output_line(cont, last_skip)
    cont.append('\r')
    continue

  sline = line.strip('\n\r')
  cont.append(sline)
  if sline.endswith('\\'):
    continue

  cont.append('\n')
  last_skip = output_line(cont, last_skip)
",Clean up the travis log output.,"Clean up the travis log output.
",Python,apache-2.0,"litex-hub/litex-conda-ci,litex-hub/litex-conda-ci",106,"```python
#!/usr/bin/python3

import io
import pexpect
import string
import sys
import time

sys.stdin = io.TextIOWrapper(sys.stdin.detach(), newline='')

output_to=sys.stdout

args = list(sys.argv[1:])
logfile = open(args.pop(0), ""w"")
child = pexpect.spawn(' '.join(args))

def output_line(line_bits, last_skip):
  line = """".join(line_bits)
  sline = line.strip()

  skip = True
  if line.startswith("" ""):
    skip = False

  if len(sline) > 0:
    if sline[0] in string.ascii_uppercase:
      skip = False
    if sline[0] in ('[', '=', '!', '+'):
      skip = False

  if skip != last_skip:
    output_to.write('\n')

  if skip:
    output_to.write('.')
  else:
    output_to.write(line)

  output_to.flush()
  line_bits.clear()
  return skip


def find_newline(data):
  fulldata = b"""".join(data)
  newlinechar = fulldata.find(b'\n')
  retlinechar = fulldata.find(b'\r')

  if newlinechar == -1:
    newlinechar = len(fulldata)+1
  if retlinechar == -1:
    retlinechar = len(fulldata)+1

  if retlinechar+1 == newlinechar:
    splitpos = newlinechar
  else:
    splitpos = min(newlinechar, retlinechar)

  if splitpos > len(fulldata):
    return

  newline = fulldata[:splitpos+1]
  leftover = fulldata[splitpos+1:]

  data.clear()
  data.append(leftover)
  return newline


last_skip = False
cont = []
data = [b'']
while True:
  line = None
  while len(data) > 1 or len(data[0]) > 0 or child.isalive():
    line = find_newline(data)
    if line is not None:
      break
    try:
      data.append(child.read_nonblocking(100))
    except pexpect.TIMEOUT:
      pass
    except pexpect.EOF as e:
      data.append(b'\n')

  if not line:
    break

  line = line.decode('utf-8')
  logfile.write(line)
  logfile.flush()

  if line.endswith('\r'):
    cont.append(line[:-1])
    last_skip = output_line(cont, last_skip)
    cont.append('\r')
    continue

  sline = line.strip('\n\r')
  cont.append(sline)
  if sline.endswith('\\'):
    continue

  cont.append('\n')
  last_skip = output_line(cont, last_skip)

```"
0f1b0dec702e314c4e891115b7b72adac7896402,src/ggrc/converters/__init__.py,src/ggrc/converters/__init__.py,,"# Copyright (C) 2015 Google Inc., authors, and contributors <see AUTHORS file>
# Licensed under http://www.apache.org/licenses/LICENSE-2.0 <see LICENSE file>
# Created By: miha@reciprocitylabs.com
# Maintained By: miha@reciprocitylabs.com

from ggrc.models import (
    Audit, Control, ControlAssessment, DataAsset, Directive, Contract,
    Policy, Regulation, Standard, Facility, Market, Objective, Option,
    OrgGroup, Vendor, Person, Product, Program, Project, Request, Response,
    Section, Clause, System, Process, Issue,
)
from ggrc.utils import get_mapping_rules


def get_allowed_mappings():
  """""" get all mapping rules with lowercase names

  import export is case insensitive so we use lower case names for all
  comparisons.
  """"""
  mapping_rules = get_mapping_rules()
  for object_mappings in mapping_rules.values():
    map(str.lower, object_mappings)
  return mapping_rules


IMPORTABLE = {
    ""audit"": Audit,
    ""control"": Control,
    ""control assessment"": ControlAssessment,
    ""control_assessment"": ControlAssessment,
    ""data asset"": DataAsset,
    ""data_asset"": DataAsset,
    ""directive"": Directive,
    ""contract"": Contract,
    ""policy"": Policy,
    ""regulation"": Regulation,
    ""standard"": Standard,
    ""facility"": Facility,
    ""market"": Market,
    ""objective"": Objective,
    ""option"": Option,
    ""org group"": OrgGroup,
    ""org_group"": OrgGroup,
    ""vendor"": Vendor,
    ""person"": Person,
    ""product"": Product,
    ""program"": Program,
    ""project"": Project,
    ""request"": Request,
    ""response"": Response,
    ""section"": Section,
    ""clause"": Clause,
    ""system"": System,
    ""process"": Process,
    ""issue"": Issue,
}

COLUMN_ORDER = (
    ""slug"",
    ""title"",
    ""description"",
    ""test_plan"",
    ""notes"",
    ""owners"",
    ""start_date"",
    ""end_date"",
    ""report_end_date"",
    ""report_start_date"",
    ""assertions"",
    ""audit"",
    ""categories"",
    ""contact"",
    ""control"",
    ""design"",
    ""directive_id"",
    ""fraud_related"",
    ""key_control"",
    ""kind"",
    ""link"",
    ""means"",
    ""network_zone"",
    ""operationally"",
    ""principal_assessor"",
    ""private"",
    ""program_id"",
    ""secondary_assessor"",
    ""secondary_contact"",
    ""status"",
    ""url"",
    ""reference_url"",
    ""_user_role_auditor"",
    ""verify_frequency"",
    ""name"",
    ""email"",
    ""is_enabled"",
    ""company"",
    ""_custom_attributes"",
)

COLUMN_HANDLERS = {}

ALLOWED_MAPPINGS = get_allowed_mappings()
",Add convertes folder with init python file,"Add convertes folder with init python file

Add the folder that will contain all files related to import export. The
init file contains:

- Column order for csv files.
- Mapping rules that will be used for setting mapping columns
- List of all objects that are/should be importable.
",Python,apache-2.0,"edofic/ggrc-core,prasannav7/ggrc-core,jmakov/ggrc-core,prasannav7/ggrc-core,plamut/ggrc-core,prasannav7/ggrc-core,prasannav7/ggrc-core,plamut/ggrc-core,hyperNURb/ggrc-core,VinnieJohns/ggrc-core,jmakov/ggrc-core,hasanalom/ggrc-core,andrei-karalionak/ggrc-core,j0gurt/ggrc-core,josthkko/ggrc-core,hyperNURb/ggrc-core,jmakov/ggrc-core,josthkko/ggrc-core,jmakov/ggrc-core,selahssea/ggrc-core,VinnieJohns/ggrc-core,j0gurt/ggrc-core,j0gurt/ggrc-core,kr41/ggrc-core,hyperNURb/ggrc-core,hasanalom/ggrc-core,NejcZupec/ggrc-core,NejcZupec/ggrc-core,AleksNeStu/ggrc-core,VinnieJohns/ggrc-core,plamut/ggrc-core,edofic/ggrc-core,edofic/ggrc-core,hasanalom/ggrc-core,kr41/ggrc-core,AleksNeStu/ggrc-core,plamut/ggrc-core,hyperNURb/ggrc-core,kr41/ggrc-core,NejcZupec/ggrc-core,selahssea/ggrc-core,selahssea/ggrc-core,andrei-karalionak/ggrc-core,andrei-karalionak/ggrc-core,andrei-karalionak/ggrc-core,AleksNeStu/ggrc-core,selahssea/ggrc-core,josthkko/ggrc-core,hasanalom/ggrc-core,AleksNeStu/ggrc-core,kr41/ggrc-core,hyperNURb/ggrc-core,jmakov/ggrc-core,josthkko/ggrc-core,edofic/ggrc-core,hasanalom/ggrc-core,NejcZupec/ggrc-core,VinnieJohns/ggrc-core,j0gurt/ggrc-core",104,"```python
# Copyright (C) 2015 Google Inc., authors, and contributors <see AUTHORS file>
# Licensed under http://www.apache.org/licenses/LICENSE-2.0 <see LICENSE file>
# Created By: miha@reciprocitylabs.com
# Maintained By: miha@reciprocitylabs.com

from ggrc.models import (
    Audit, Control, ControlAssessment, DataAsset, Directive, Contract,
    Policy, Regulation, Standard, Facility, Market, Objective, Option,
    OrgGroup, Vendor, Person, Product, Program, Project, Request, Response,
    Section, Clause, System, Process, Issue,
)
from ggrc.utils import get_mapping_rules


def get_allowed_mappings():
  """""" get all mapping rules with lowercase names

  import export is case insensitive so we use lower case names for all
  comparisons.
  """"""
  mapping_rules = get_mapping_rules()
  for object_mappings in mapping_rules.values():
    map(str.lower, object_mappings)
  return mapping_rules


IMPORTABLE = {
    ""audit"": Audit,
    ""control"": Control,
    ""control assessment"": ControlAssessment,
    ""control_assessment"": ControlAssessment,
    ""data asset"": DataAsset,
    ""data_asset"": DataAsset,
    ""directive"": Directive,
    ""contract"": Contract,
    ""policy"": Policy,
    ""regulation"": Regulation,
    ""standard"": Standard,
    ""facility"": Facility,
    ""market"": Market,
    ""objective"": Objective,
    ""option"": Option,
    ""org group"": OrgGroup,
    ""org_group"": OrgGroup,
    ""vendor"": Vendor,
    ""person"": Person,
    ""product"": Product,
    ""program"": Program,
    ""project"": Project,
    ""request"": Request,
    ""response"": Response,
    ""section"": Section,
    ""clause"": Clause,
    ""system"": System,
    ""process"": Process,
    ""issue"": Issue,
}

COLUMN_ORDER = (
    ""slug"",
    ""title"",
    ""description"",
    ""test_plan"",
    ""notes"",
    ""owners"",
    ""start_date"",
    ""end_date"",
    ""report_end_date"",
    ""report_start_date"",
    ""assertions"",
    ""audit"",
    ""categories"",
    ""contact"",
    ""control"",
    ""design"",
    ""directive_id"",
    ""fraud_related"",
    ""key_control"",
    ""kind"",
    ""link"",
    ""means"",
    ""network_zone"",
    ""operationally"",
    ""principal_assessor"",
    ""private"",
    ""program_id"",
    ""secondary_assessor"",
    ""secondary_contact"",
    ""status"",
    ""url"",
    ""reference_url"",
    ""_user_role_auditor"",
    ""verify_frequency"",
    ""name"",
    ""email"",
    ""is_enabled"",
    ""company"",
    ""_custom_attributes"",
)

COLUMN_HANDLERS = {}

ALLOWED_MAPPINGS = get_allowed_mappings()

```"
c5bfe8550c50977750561bd5759db5da8bcab48d,problem1/gml_read.py,problem1/gml_read.py,,"import networkx as nx

# from gml_read import read_gml2
# g = read_gml2(""steiner-small.gml"")

def read_gml2(path):

    # Read file lines
    f = open(path)
    lines = f.readlines()
    f.close()

    # Split lines into symbols
    syms = []
    for line in lines:
        line = line.strip().split(' ')
        if len(line) != 0 and line[0] == '#': # skip comments
            continue
        for sym in line:
            syms.append(sym)
    n_syms = len(syms)

    # Find node labeled 'graph'
    has_graph = False
    i_sym = 0
    while not has_graph and i_sym < n_syms:
        sym = syms[i_sym]
        i_sym += 1
        if sym == 'graph':
            has_graph = True
            break

    if not has_graph:
        print (""Couldn't find a graph"")
        return

    G = nx.Graph()

    # Recurse the graph structure
    level = 0
    expect_value = True
    attribs = {1:G.graph}
    current_node = None
    current_edge = None
    while level >= 0 and i_sym < n_syms:
        sym = syms[i_sym]
        if expect_value:
            value = None
            if sym == '[':       # list
                level += 1
            elif sym[0] == '\""': # string
                value = sym
                while syms[i_sym][-1] != '\""':
                    i_sym += 1
                    value += ' ' + syms[i_sym]
                value = value[1:-1]
            elif '.' in sym:
                value = float(sym)
            else:
                value = int(sym)
            if value != None:
                #if level not in attribs:
                #    attribs[level] = {}
                attribs[level][current_key] = value
            expect_value = False
        else:
            if sym == ']':       # list end
                level -= 1
                if level == 1:
                    if current_node != None:
                        id = current_node['id']
                        current_node.pop('id', None)   # don't need id in attribs
                        G.add_node(id, current_node)
                        current_node = None
                    elif current_edge != None:
                        source = current_edge['source']
                        target = current_edge['target']
                        current_edge.pop('source', None)
                        current_edge.pop('target', None)
                        G.add_edge(source, target, current_edge)
                        current_edge = None
            else:
                if level == 1:
                    if sym == 'node':
                        current_node = {}
                        attribs[level + 1] = current_node
                    elif sym == 'edge':
                        current_edge = {}
                        attribs[level + 1] = current_edge
                current_key = sym
                expect_value = True
        i_sym += 1

    return G
",Add a fast GML reader (reads in seconds).,"Add a fast GML reader (reads in seconds).
",Python,mit,karulont/combopt,95,"```python
import networkx as nx

# from gml_read import read_gml2
# g = read_gml2(""steiner-small.gml"")

def read_gml2(path):

    # Read file lines
    f = open(path)
    lines = f.readlines()
    f.close()

    # Split lines into symbols
    syms = []
    for line in lines:
        line = line.strip().split(' ')
        if len(line) != 0 and line[0] == '#': # skip comments
            continue
        for sym in line:
            syms.append(sym)
    n_syms = len(syms)

    # Find node labeled 'graph'
    has_graph = False
    i_sym = 0
    while not has_graph and i_sym < n_syms:
        sym = syms[i_sym]
        i_sym += 1
        if sym == 'graph':
            has_graph = True
            break

    if not has_graph:
        print (""Couldn't find a graph"")
        return

    G = nx.Graph()

    # Recurse the graph structure
    level = 0
    expect_value = True
    attribs = {1:G.graph}
    current_node = None
    current_edge = None
    while level >= 0 and i_sym < n_syms:
        sym = syms[i_sym]
        if expect_value:
            value = None
            if sym == '[':       # list
                level += 1
            elif sym[0] == '\""': # string
                value = sym
                while syms[i_sym][-1] != '\""':
                    i_sym += 1
                    value += ' ' + syms[i_sym]
                value = value[1:-1]
            elif '.' in sym:
                value = float(sym)
            else:
                value = int(sym)
            if value != None:
                #if level not in attribs:
                #    attribs[level] = {}
                attribs[level][current_key] = value
            expect_value = False
        else:
            if sym == ']':       # list end
                level -= 1
                if level == 1:
                    if current_node != None:
                        id = current_node['id']
                        current_node.pop('id', None)   # don't need id in attribs
                        G.add_node(id, current_node)
                        current_node = None
                    elif current_edge != None:
                        source = current_edge['source']
                        target = current_edge['target']
                        current_edge.pop('source', None)
                        current_edge.pop('target', None)
                        G.add_edge(source, target, current_edge)
                        current_edge = None
            else:
                if level == 1:
                    if sym == 'node':
                        current_node = {}
                        attribs[level + 1] = current_node
                    elif sym == 'edge':
                        current_edge = {}
                        attribs[level + 1] = current_edge
                current_key = sym
                expect_value = True
        i_sym += 1

    return G

```"
c1099e9410b8ad35e69d59e1d27f36903495cd67,scripts/migrate_categories.py,scripts/migrate_categories.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-
import logging

from tests.base import OsfTestCase
from tests.factories import NodeFactory

from website.project.model import Node

logger = logging.getLogger(__name__)

# legacy => new category
MIGRATE_MAP = {
    'category': '',
    'measure': 'methods and measures',
}


def migrate_category(node):
    """"""Migrate legacy, invalid category to new, valid category. Return whether
    the node was changed.
    """"""
    if node.category not in Node.CATEGORY_MAP.keys():  # invalid category
        node.category = MIGRATE_MAP.get(node.category, 'other')
        return True
    return False


def migrate_nodes():
    migrated_count = 0
    for node in Node.find():
        was_migrated = migrate_category(node)
        if was_migrated:
            node.save()
            logger.info('Migrated {0}'.format(node._id))
            migrated_count += 1
    logger.info('Finished migrating {0} nodes.'.format(migrated_count))


class TestMigratingCategories(OsfTestCase):

    def test_migrate_category(self):
        node = NodeFactory(category='category')
        was_migrated = migrate_category(node)
        assert was_migrated is True
        node.save()
        assert node.category == ''

    def test_migrate_measure(self):
        node = NodeFactory(category='measure')
        migrate_category(node)
        node.save()
        assert node.category == 'methods and measures'

    def test_everything_else_is_migrated_to_other(self):
        node1 = NodeFactory(category='background')
        migrate_category(node1)
        node1.save()
        assert node1.category == 'other'

        node2 = NodeFactory(category=u'プロジェクト')
        migrate_category(node2)
        node2.save()
        assert node2.category == 'other'

    def test_valid_categories_not_migrated(self):
        node1 = NodeFactory(category='project')
        node2 = NodeFactory(category='hypothesis')

        was_migrated1 = migrate_category(node1)
        was_migrated2 = migrate_category(node2)

        node1.save()
        node2.save()

        assert was_migrated1 is False
        assert was_migrated2 is False
        assert node1.category == 'project'
        assert node2.category == 'hypothesis'

class TestMigrateAll(OsfTestCase):

    def test_migrate_categories_all(self):
        n1 = NodeFactory(category='hypothesis')
        n2 = NodeFactory(category='category')

        migrate_nodes()

        assert n1.category == 'hypothesis'
        assert n2.category == ''

if __name__ == '__main__':
    main()
",Add script for migrating categories,"Add script for migrating categories
",Python,apache-2.0,"rdhyee/osf.io,mluo613/osf.io,cldershem/osf.io,sbt9uc/osf.io,KAsante95/osf.io,zachjanicki/osf.io,danielneis/osf.io,caseyrollins/osf.io,jeffreyliu3230/osf.io,CenterForOpenScience/osf.io,danielneis/osf.io,caneruguz/osf.io,himanshuo/osf.io,Ghalko/osf.io,icereval/osf.io,ckc6cz/osf.io,arpitar/osf.io,MerlinZhang/osf.io,reinaH/osf.io,acshi/osf.io,sloria/osf.io,ckc6cz/osf.io,wearpants/osf.io,samchrisinger/osf.io,ZobairAlijan/osf.io,SSJohns/osf.io,emetsger/osf.io,felliott/osf.io,asanfilippo7/osf.io,baylee-d/osf.io,SSJohns/osf.io,sbt9uc/osf.io,mfraezz/osf.io,kushG/osf.io,kch8qx/osf.io,saradbowman/osf.io,kwierman/osf.io,adlius/osf.io,cosenal/osf.io,monikagrabowska/osf.io,KAsante95/osf.io,alexschiller/osf.io,felliott/osf.io,sbt9uc/osf.io,Nesiehr/osf.io,alexschiller/osf.io,adlius/osf.io,mluo613/osf.io,jeffreyliu3230/osf.io,brianjgeiger/osf.io,hmoco/osf.io,MerlinZhang/osf.io,cwisecarver/osf.io,crcresearch/osf.io,Johnetordoff/osf.io,mluke93/osf.io,hmoco/osf.io,fabianvf/osf.io,emetsger/osf.io,reinaH/osf.io,rdhyee/osf.io,leb2dg/osf.io,acshi/osf.io,cosenal/osf.io,himanshuo/osf.io,HalcyonChimera/osf.io,baylee-d/osf.io,petermalcolm/osf.io,binoculars/osf.io,kwierman/osf.io,doublebits/osf.io,jinluyuan/osf.io,brandonPurvis/osf.io,petermalcolm/osf.io,jeffreyliu3230/osf.io,billyhunt/osf.io,doublebits/osf.io,acshi/osf.io,jolene-esposito/osf.io,HalcyonChimera/osf.io,dplorimer/osf,GaryKriebel/osf.io,GaryKriebel/osf.io,HarryRybacki/osf.io,njantrania/osf.io,TomBaxter/osf.io,abought/osf.io,samanehsan/osf.io,chrisseto/osf.io,zkraime/osf.io,danielneis/osf.io,ZobairAlijan/osf.io,mluo613/osf.io,cosenal/osf.io,zachjanicki/osf.io,aaxelb/osf.io,jnayak1/osf.io,mluke93/osf.io,caneruguz/osf.io,brandonPurvis/osf.io,abought/osf.io,CenterForOpenScience/osf.io,njantrania/osf.io,zamattiac/osf.io,GaryKriebel/osf.io,amyshi188/osf.io,samchrisinger/osf.io,alexschiller/osf.io,zkraime/osf.io,monikagrabowska/osf.io,lyndsysimon/osf.io,billyhunt/osf.io,asanfilippo7/osf.io,Ghalko/osf.io,billyhunt/osf.io,petermalcolm/osf.io,abought/osf.io,mattclark/osf.io,leb2dg/osf.io,caseyrygt/osf.io,laurenrevere/osf.io,adlius/osf.io,GageGaskins/osf.io,aaxelb/osf.io,Ghalko/osf.io,arpitar/osf.io,njantrania/osf.io,brianjgeiger/osf.io,zkraime/osf.io,felliott/osf.io,kch8qx/osf.io,brandonPurvis/osf.io,acshi/osf.io,mfraezz/osf.io,revanthkolli/osf.io,CenterForOpenScience/osf.io,cslzchen/osf.io,arpitar/osf.io,TomBaxter/osf.io,sbt9uc/osf.io,jinluyuan/osf.io,chrisseto/osf.io,fabianvf/osf.io,laurenrevere/osf.io,aaxelb/osf.io,erinspace/osf.io,lyndsysimon/osf.io,emetsger/osf.io,arpitar/osf.io,laurenrevere/osf.io,rdhyee/osf.io,alexschiller/osf.io,ZobairAlijan/osf.io,TomHeatwole/osf.io,mluke93/osf.io,ticklemepierce/osf.io,AndrewSallans/osf.io,billyhunt/osf.io,icereval/osf.io,brianjgeiger/osf.io,bdyetton/prettychart,hmoco/osf.io,adlius/osf.io,pattisdr/osf.io,TomBaxter/osf.io,cldershem/osf.io,kushG/osf.io,njantrania/osf.io,samanehsan/osf.io,RomanZWang/osf.io,jinluyuan/osf.io,ckc6cz/osf.io,RomanZWang/osf.io,fabianvf/osf.io,cldershem/osf.io,bdyetton/prettychart,GageGaskins/osf.io,Johnetordoff/osf.io,haoyuchen1992/osf.io,felliott/osf.io,SSJohns/osf.io,KAsante95/osf.io,KAsante95/osf.io,MerlinZhang/osf.io,dplorimer/osf,crcresearch/osf.io,HalcyonChimera/osf.io,CenterForOpenScience/osf.io,danielneis/osf.io,barbour-em/osf.io,HarryRybacki/osf.io,Ghalko/osf.io,baylee-d/osf.io,doublebits/osf.io,aaxelb/osf.io,himanshuo/osf.io,cwisecarver/osf.io,fabianvf/osf.io,doublebits/osf.io,lamdnhan/osf.io,dplorimer/osf,mfraezz/osf.io,caseyrygt/osf.io,mattclark/osf.io,amyshi188/osf.io,TomHeatwole/osf.io,wearpants/osf.io,acshi/osf.io,ZobairAlijan/osf.io,zachjanicki/osf.io,cslzchen/osf.io,samchrisinger/osf.io,wearpants/osf.io,monikagrabowska/osf.io,barbour-em/osf.io,KAsante95/osf.io,GageGaskins/osf.io,caneruguz/osf.io,zamattiac/osf.io,sloria/osf.io,revanthkolli/osf.io,barbour-em/osf.io,RomanZWang/osf.io,jmcarp/osf.io,kushG/osf.io,billyhunt/osf.io,HarryRybacki/osf.io,cslzchen/osf.io,binoculars/osf.io,zachjanicki/osf.io,lyndsysimon/osf.io,himanshuo/osf.io,pattisdr/osf.io,jmcarp/osf.io,lamdnhan/osf.io,wearpants/osf.io,GageGaskins/osf.io,mluo613/osf.io,MerlinZhang/osf.io,asanfilippo7/osf.io,brandonPurvis/osf.io,samanehsan/osf.io,rdhyee/osf.io,samanehsan/osf.io,cwisecarver/osf.io,kch8qx/osf.io,haoyuchen1992/osf.io,ticklemepierce/osf.io,caseyrollins/osf.io,revanthkolli/osf.io,saradbowman/osf.io,ticklemepierce/osf.io,lyndsysimon/osf.io,jolene-esposito/osf.io,zamattiac/osf.io,chrisseto/osf.io,caseyrygt/osf.io,jnayak1/osf.io,GaryKriebel/osf.io,cldershem/osf.io,DanielSBrown/osf.io,ckc6cz/osf.io,icereval/osf.io,DanielSBrown/osf.io,barbour-em/osf.io,alexschiller/osf.io,kch8qx/osf.io,abought/osf.io,Nesiehr/osf.io,caneruguz/osf.io,mluo613/osf.io,erinspace/osf.io,amyshi188/osf.io,Johnetordoff/osf.io,cslzchen/osf.io,GageGaskins/osf.io,caseyrygt/osf.io,kch8qx/osf.io,chennan47/osf.io,jmcarp/osf.io,jeffreyliu3230/osf.io,chennan47/osf.io,jinluyuan/osf.io,RomanZWang/osf.io,DanielSBrown/osf.io,Nesiehr/osf.io,jnayak1/osf.io,leb2dg/osf.io,kushG/osf.io,dplorimer/osf,monikagrabowska/osf.io,lamdnhan/osf.io,samchrisinger/osf.io,amyshi188/osf.io,mfraezz/osf.io,HalcyonChimera/osf.io,erinspace/osf.io,TomHeatwole/osf.io,SSJohns/osf.io,cosenal/osf.io,brandonPurvis/osf.io,mattclark/osf.io,lamdnhan/osf.io,caseyrollins/osf.io,RomanZWang/osf.io,TomHeatwole/osf.io,emetsger/osf.io,monikagrabowska/osf.io,asanfilippo7/osf.io,bdyetton/prettychart,bdyetton/prettychart,AndrewSallans/osf.io,pattisdr/osf.io,chennan47/osf.io,doublebits/osf.io,DanielSBrown/osf.io,sloria/osf.io,Johnetordoff/osf.io,reinaH/osf.io,petermalcolm/osf.io,binoculars/osf.io,leb2dg/osf.io,chrisseto/osf.io,jolene-esposito/osf.io,haoyuchen1992/osf.io,kwierman/osf.io,mluke93/osf.io,jolene-esposito/osf.io,jmcarp/osf.io,haoyuchen1992/osf.io,HarryRybacki/osf.io,cwisecarver/osf.io,crcresearch/osf.io,hmoco/osf.io,Nesiehr/osf.io,ticklemepierce/osf.io,zamattiac/osf.io,reinaH/osf.io,kwierman/osf.io,zkraime/osf.io,brianjgeiger/osf.io,revanthkolli/osf.io,jnayak1/osf.io",94,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import logging

from tests.base import OsfTestCase
from tests.factories import NodeFactory

from website.project.model import Node

logger = logging.getLogger(__name__)

# legacy => new category
MIGRATE_MAP = {
    'category': '',
    'measure': 'methods and measures',
}


def migrate_category(node):
    """"""Migrate legacy, invalid category to new, valid category. Return whether
    the node was changed.
    """"""
    if node.category not in Node.CATEGORY_MAP.keys():  # invalid category
        node.category = MIGRATE_MAP.get(node.category, 'other')
        return True
    return False


def migrate_nodes():
    migrated_count = 0
    for node in Node.find():
        was_migrated = migrate_category(node)
        if was_migrated:
            node.save()
            logger.info('Migrated {0}'.format(node._id))
            migrated_count += 1
    logger.info('Finished migrating {0} nodes.'.format(migrated_count))


class TestMigratingCategories(OsfTestCase):

    def test_migrate_category(self):
        node = NodeFactory(category='category')
        was_migrated = migrate_category(node)
        assert was_migrated is True
        node.save()
        assert node.category == ''

    def test_migrate_measure(self):
        node = NodeFactory(category='measure')
        migrate_category(node)
        node.save()
        assert node.category == 'methods and measures'

    def test_everything_else_is_migrated_to_other(self):
        node1 = NodeFactory(category='background')
        migrate_category(node1)
        node1.save()
        assert node1.category == 'other'

        node2 = NodeFactory(category=u'プロジェクト')
        migrate_category(node2)
        node2.save()
        assert node2.category == 'other'

    def test_valid_categories_not_migrated(self):
        node1 = NodeFactory(category='project')
        node2 = NodeFactory(category='hypothesis')

        was_migrated1 = migrate_category(node1)
        was_migrated2 = migrate_category(node2)

        node1.save()
        node2.save()

        assert was_migrated1 is False
        assert was_migrated2 is False
        assert node1.category == 'project'
        assert node2.category == 'hypothesis'

class TestMigrateAll(OsfTestCase):

    def test_migrate_categories_all(self):
        n1 = NodeFactory(category='hypothesis')
        n2 = NodeFactory(category='category')

        migrate_nodes()

        assert n1.category == 'hypothesis'
        assert n2.category == ''

if __name__ == '__main__':
    main()

```"
ceea7edd9162e9a834be8888fea18dcd0da43561,test/test_rendering_dot_files.py,test/test_rendering_dot_files.py,,"from __future__ import division
from __future__ import print_function

import glob
import os
import subprocess
import sys
from hashlib import sha256

import pydot_ng as pydot

PY3 = not sys.version_info < (3, 0, 0)

if PY3:
    NULL_SEP = b''
    xrange = range
else:
    NULL_SEP = ''
    bytes = str

DOT_BINARY_PATH = pydot.find_graphviz()['dot']


TEST_DIR = os.path.dirname(__file__)
REGRESSION_TESTS_DIR = os.path.join(TEST_DIR, ""graphs"")
MY_REGRESSION_TESTS_DIR = os.path.join(TEST_DIR, ""my_tests"")

TESTS_DIRS = (
    (""regressions"", REGRESSION_TESTS_DIR),
    (""my_regressions"", MY_REGRESSION_TESTS_DIR),
)


def list_dots(path):
    searchpath = os.path.join(path, ""*.dot"")
    return [f for f in glob.glob(searchpath)]


def pytest_generate_tests(metafunc):
    idlist = []
    argvalues = []
    for name, dir in TESTS_DIRS:
        for filepath in list_dots(dir):
            filename = os.path.basename(filepath)
            idlist.append(""{}-{}"".format(name, filename))
            argvalues.append((filepath,))
    metafunc.parametrize(
        argnames=[""filepath""],
        argvalues=argvalues,
        ids=idlist,
        scope=""function"",
    )


def test_render_and_compare_dot_files(filepath):
    def _render_with_graphviz(filename):
        p = subprocess.Popen(
            (DOT_BINARY_PATH, '-Tjpe'),
            cwd=os.path.dirname(filename),
            stdin=open(filename, 'rt'),
            stderr=subprocess.PIPE, stdout=subprocess.PIPE)

        stdout = p.stdout

        stdout_output = list()
        while True:
            data = stdout.read()
            if not data:
                break
            stdout_output.append(data)
        stdout.close()

        if stdout_output:
            stdout_output = NULL_SEP.join(stdout_output)

        # this returns a status code we should check
        p.wait()

        return sha256(stdout_output).hexdigest()

    def _render_with_pydot(filename):
        g = pydot.graph_from_dot_file(filename)
        if not isinstance(g, list):
            g = [g]
        jpe_data = NULL_SEP.join([_g.create(format='jpe') for _g in g])
        return sha256(jpe_data).hexdigest()
    parsed_data_hexdigest = _render_with_pydot(filepath)
    original_data_hexdigest = _render_with_graphviz(filepath)

    assert original_data_hexdigest == parsed_data_hexdigest
",Add testing regressions of pydot vs graphviz,"Add testing regressions of pydot vs graphviz
",Python,mit,pydot/pydot-ng,91,"```python
from __future__ import division
from __future__ import print_function

import glob
import os
import subprocess
import sys
from hashlib import sha256

import pydot_ng as pydot

PY3 = not sys.version_info < (3, 0, 0)

if PY3:
    NULL_SEP = b''
    xrange = range
else:
    NULL_SEP = ''
    bytes = str

DOT_BINARY_PATH = pydot.find_graphviz()['dot']


TEST_DIR = os.path.dirname(__file__)
REGRESSION_TESTS_DIR = os.path.join(TEST_DIR, ""graphs"")
MY_REGRESSION_TESTS_DIR = os.path.join(TEST_DIR, ""my_tests"")

TESTS_DIRS = (
    (""regressions"", REGRESSION_TESTS_DIR),
    (""my_regressions"", MY_REGRESSION_TESTS_DIR),
)


def list_dots(path):
    searchpath = os.path.join(path, ""*.dot"")
    return [f for f in glob.glob(searchpath)]


def pytest_generate_tests(metafunc):
    idlist = []
    argvalues = []
    for name, dir in TESTS_DIRS:
        for filepath in list_dots(dir):
            filename = os.path.basename(filepath)
            idlist.append(""{}-{}"".format(name, filename))
            argvalues.append((filepath,))
    metafunc.parametrize(
        argnames=[""filepath""],
        argvalues=argvalues,
        ids=idlist,
        scope=""function"",
    )


def test_render_and_compare_dot_files(filepath):
    def _render_with_graphviz(filename):
        p = subprocess.Popen(
            (DOT_BINARY_PATH, '-Tjpe'),
            cwd=os.path.dirname(filename),
            stdin=open(filename, 'rt'),
            stderr=subprocess.PIPE, stdout=subprocess.PIPE)

        stdout = p.stdout

        stdout_output = list()
        while True:
            data = stdout.read()
            if not data:
                break
            stdout_output.append(data)
        stdout.close()

        if stdout_output:
            stdout_output = NULL_SEP.join(stdout_output)

        # this returns a status code we should check
        p.wait()

        return sha256(stdout_output).hexdigest()

    def _render_with_pydot(filename):
        g = pydot.graph_from_dot_file(filename)
        if not isinstance(g, list):
            g = [g]
        jpe_data = NULL_SEP.join([_g.create(format='jpe') for _g in g])
        return sha256(jpe_data).hexdigest()
    parsed_data_hexdigest = _render_with_pydot(filepath)
    original_data_hexdigest = _render_with_graphviz(filepath)

    assert original_data_hexdigest == parsed_data_hexdigest

```"
39c28f76540da294d9dd4adf4f84ae266922498d,test/test_clientagent.py,test/test_clientagent.py,,"#!/usr/bin/env python2
import unittest
from socket import *

from common import *
from testdc import *

CONFIG = """"""\
messagedirector:
    bind: 127.0.0.1:57123

general:
    dc_files:
        - %r

roles:
    - type: clientagent
      bind: 127.0.0.1:57128
      version: ""Sword Art Online v5.1""
"""""" % test_dc
VERSION = 'Sword Art Online v5.1'

class TestClientAgent(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.daemon = Daemon(CONFIG)
        cls.daemon.start()

        s = socket(AF_INET, SOCK_STREAM)
        s.connect(('127.0.0.1', 57123))
        cls.server = MDConnection(s)

    @classmethod
    def tearDownClass(cls):
        cls.server.close()
        cls.daemon.stop()

    def assertDisconnect(self, s, reason_code):
        while True:
            dg = s.recv()
            dgi = DatagramIterator(dg)
            if dgi.read_uint16() == CLIENT_GO_GET_LOST:
                self.assertEqual(dgi.read_uint16(), reason_code)
                s.close()
                return

    def connect(self):
        s = socket(AF_INET, SOCK_STREAM)
        s.connect(('127.0.0.1', 57128))
        client = ClientConnection(s)

        return client

    def test_hello(self):
        # First, see if the CA ensures that the first datagram is a HELLO.
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(5) # invalid msgtype
        client.send(dg)
        self.assertDisconnect(client, CLIENT_DISCONNECT_NO_HELLO)

        # Next, see if the version gets validated:
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO)
        dg.add_uint32(DC_HASH)
        dg.add_string('Equestria Online v5.7')
        client.send(dg)
        self.assertDisconnect(client, CLIENT_DISCONNECT_BAD_VERSION)

        # Now dchash validation:
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO)
        dg.add_uint32(0x12345678)
        dg.add_string(VERSION)
        client.send(dg)
        self.assertDisconnect(client, CLIENT_DISCONNECT_BAD_DCHASH)

        # If everything is correct, it should simply allow us in:
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO)
        dg.add_uint32(DC_HASH)
        dg.add_string(VERSION)
        client.send(dg)
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO_RESP)
        self.assertTrue(client.expect(dg))

        client.close()

if __name__ == '__main__':
    unittest.main()
","Create clientagent unittest file, with test for CLIENT_HELLO behavior.","tests: Create clientagent unittest file, with test for CLIENT_HELLO behavior.
",Python,bsd-3-clause,"pizcogirl/Astron,blindsighttf2/Astron,ketoo/Astron,pizcogirl/Astron,ketoo/Astron,blindsighttf2/Astron,blindsighttf2/Astron,ketoo/Astron,blindsighttf2/Astron,pizcogirl/Astron,ketoo/Astron,pizcogirl/Astron",95,"```python
#!/usr/bin/env python2
import unittest
from socket import *

from common import *
from testdc import *

CONFIG = """"""\
messagedirector:
    bind: 127.0.0.1:57123

general:
    dc_files:
        - %r

roles:
    - type: clientagent
      bind: 127.0.0.1:57128
      version: ""Sword Art Online v5.1""
"""""" % test_dc
VERSION = 'Sword Art Online v5.1'

class TestClientAgent(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        cls.daemon = Daemon(CONFIG)
        cls.daemon.start()

        s = socket(AF_INET, SOCK_STREAM)
        s.connect(('127.0.0.1', 57123))
        cls.server = MDConnection(s)

    @classmethod
    def tearDownClass(cls):
        cls.server.close()
        cls.daemon.stop()

    def assertDisconnect(self, s, reason_code):
        while True:
            dg = s.recv()
            dgi = DatagramIterator(dg)
            if dgi.read_uint16() == CLIENT_GO_GET_LOST:
                self.assertEqual(dgi.read_uint16(), reason_code)
                s.close()
                return

    def connect(self):
        s = socket(AF_INET, SOCK_STREAM)
        s.connect(('127.0.0.1', 57128))
        client = ClientConnection(s)

        return client

    def test_hello(self):
        # First, see if the CA ensures that the first datagram is a HELLO.
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(5) # invalid msgtype
        client.send(dg)
        self.assertDisconnect(client, CLIENT_DISCONNECT_NO_HELLO)

        # Next, see if the version gets validated:
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO)
        dg.add_uint32(DC_HASH)
        dg.add_string('Equestria Online v5.7')
        client.send(dg)
        self.assertDisconnect(client, CLIENT_DISCONNECT_BAD_VERSION)

        # Now dchash validation:
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO)
        dg.add_uint32(0x12345678)
        dg.add_string(VERSION)
        client.send(dg)
        self.assertDisconnect(client, CLIENT_DISCONNECT_BAD_DCHASH)

        # If everything is correct, it should simply allow us in:
        client = self.connect()
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO)
        dg.add_uint32(DC_HASH)
        dg.add_string(VERSION)
        client.send(dg)
        dg = Datagram()
        dg.add_uint16(CLIENT_HELLO_RESP)
        self.assertTrue(client.expect(dg))

        client.close()

if __name__ == '__main__':
    unittest.main()

```"
a20255d2a2531eff982b78c15f5fc4d5cc1ec621,tests/integration/suite/test_istio.py,tests/integration/suite/test_istio.py,,"import os
import pytest
import subprocess
from .common import random_str
from .conftest import cluster_and_client, ClusterContext

kube_fname = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                          ""k8s_kube_config"")
istio_crd_url = ""https://raw.githubusercontent.com/istio/istio/1.1.5"" \
                ""/install/kubernetes/helm/istio-init/files/crd-10.yaml""


def test_virtual_service(admin_pc):
    client = admin_pc.client
    ns = admin_pc.cluster.client.create_namespace(
        name=random_str(),
        projectId=admin_pc.project.id)
    name = random_str()
    client.create_virtualService(
        name=name,
        namespaceId=ns.id,
        hosts=[""test""],
        http=[{
            ""route"": [
                {
                    ""destination"": {
                        ""host"": ""test"",
                        ""subset"": ""v1""
                    }
                }
            ]
        }],
    )
    virtualServices = client.list_virtualService(
        namespaceId=ns.id
    )
    assert len(virtualServices) == 1
    client.delete(virtualServices.data[0])
    client.delete(ns)


def test_destination_rule(admin_pc):
    client = admin_pc.client
    ns = admin_pc.cluster.client.create_namespace(
        name=random_str(),
        projectId=admin_pc.project.id)
    name = random_str()
    client.create_destinationRule(
        name=name,
        namespaceId=ns.id,
        host=""test"",
        subsets=[{
            ""name"": ""v1"",
            ""labels"": {
                ""version"": ""v1"",
            }
        }],
    )
    destinationRules = client.list_destinationRule(
        namespaceId=ns.id
    )
    assert len(destinationRules) == 1
    client.delete(destinationRules.data[0])
    client.delete(ns)


@pytest.fixture(scope='module', autouse=""True"")
def install_crd(admin_mc):
    cluster, client = cluster_and_client('local', admin_mc.client)
    cc = ClusterContext(admin_mc, cluster, client)
    create_kubeconfig(cc.cluster)
    try:
        return subprocess.check_output(
            'kubectl apply ' +
            ' --kubeconfig ' + kube_fname +
            ' -f ' + istio_crd_url,
            stderr=subprocess.STDOUT, shell=True,
        )
    except subprocess.CalledProcessError as err:
        print('kubectl error: ' + str(err.output))
        raise err


def teardown_module(module):
    try:
        return subprocess.check_output(
            'kubectl delete ' +
            ' --kubeconfig ' + kube_fname +
            ' -f ' + istio_crd_url,
            stderr=subprocess.STDOUT, shell=True,
        )
    except subprocess.CalledProcessError as err:
        print('kubectl error: ' + str(err.output))
        raise err


def create_kubeconfig(cluster):
    generateKubeConfigOutput = cluster.generateKubeconfig()
    print(generateKubeConfigOutput.config)
    file = open(kube_fname, ""w"")
    file.write(generateKubeConfigOutput.config)
    file.close()
",Add integration tests for VirtualService and DestinationRule,"Add integration tests for VirtualService and DestinationRule
",Python,apache-2.0,"rancherio/rancher,rancherio/rancher,cjellick/rancher,rancher/rancher,rancher/rancher,cjellick/rancher,rancher/rancher,cjellick/rancher,rancher/rancher",103,"```python
import os
import pytest
import subprocess
from .common import random_str
from .conftest import cluster_and_client, ClusterContext

kube_fname = os.path.join(os.path.dirname(os.path.realpath(__file__)),
                          ""k8s_kube_config"")
istio_crd_url = ""https://raw.githubusercontent.com/istio/istio/1.1.5"" \
                ""/install/kubernetes/helm/istio-init/files/crd-10.yaml""


def test_virtual_service(admin_pc):
    client = admin_pc.client
    ns = admin_pc.cluster.client.create_namespace(
        name=random_str(),
        projectId=admin_pc.project.id)
    name = random_str()
    client.create_virtualService(
        name=name,
        namespaceId=ns.id,
        hosts=[""test""],
        http=[{
            ""route"": [
                {
                    ""destination"": {
                        ""host"": ""test"",
                        ""subset"": ""v1""
                    }
                }
            ]
        }],
    )
    virtualServices = client.list_virtualService(
        namespaceId=ns.id
    )
    assert len(virtualServices) == 1
    client.delete(virtualServices.data[0])
    client.delete(ns)


def test_destination_rule(admin_pc):
    client = admin_pc.client
    ns = admin_pc.cluster.client.create_namespace(
        name=random_str(),
        projectId=admin_pc.project.id)
    name = random_str()
    client.create_destinationRule(
        name=name,
        namespaceId=ns.id,
        host=""test"",
        subsets=[{
            ""name"": ""v1"",
            ""labels"": {
                ""version"": ""v1"",
            }
        }],
    )
    destinationRules = client.list_destinationRule(
        namespaceId=ns.id
    )
    assert len(destinationRules) == 1
    client.delete(destinationRules.data[0])
    client.delete(ns)


@pytest.fixture(scope='module', autouse=""True"")
def install_crd(admin_mc):
    cluster, client = cluster_and_client('local', admin_mc.client)
    cc = ClusterContext(admin_mc, cluster, client)
    create_kubeconfig(cc.cluster)
    try:
        return subprocess.check_output(
            'kubectl apply ' +
            ' --kubeconfig ' + kube_fname +
            ' -f ' + istio_crd_url,
            stderr=subprocess.STDOUT, shell=True,
        )
    except subprocess.CalledProcessError as err:
        print('kubectl error: ' + str(err.output))
        raise err


def teardown_module(module):
    try:
        return subprocess.check_output(
            'kubectl delete ' +
            ' --kubeconfig ' + kube_fname +
            ' -f ' + istio_crd_url,
            stderr=subprocess.STDOUT, shell=True,
        )
    except subprocess.CalledProcessError as err:
        print('kubectl error: ' + str(err.output))
        raise err


def create_kubeconfig(cluster):
    generateKubeConfigOutput = cluster.generateKubeconfig()
    print(generateKubeConfigOutput.config)
    file = open(kube_fname, ""w"")
    file.write(generateKubeConfigOutput.config)
    file.close()

```"
e567bdb0ed315f6e00be7d541e5d034fe926eeb6,scripts/migration/migrate_citation_addons_list_id.py,scripts/migration/migrate_citation_addons_list_id.py,,"import logging
import sys

from modularodm import Q

from framework.mongo import database
from framework.transactions.context import TokuTransaction

from website.app import init_app
from scripts import utils as script_utils

logger = logging.getLogger(__name__)

PROVIDERS = ['mendeley', 'zotero']

def migrate_list_id_field(document, provider):
    try:
        database['{}nodesettings'.format(provider)].find_and_modify(
            {'_id': document['_id']},
            {
                '$set': {
                    'list_id': document['{}_list_id'.format(provider)]
                }
            }
        )
        database['{}nodesettings'.format(provider)].find_and_modify(
            {'_id': document['_id']},
            {
                '$unset': {
                    '{}_list_id'.format(provider): ''
                }
            }
        )
    except Exception:
        return False
    return True

def verify_node_settings_document(document, provider):
    try:
        assert('_id' in document)
        assert('{}_list_id'.format(provider) in document)
    except AssertionError:
        return False
    return True

def migrate(dry_run=True):
    documents_no_list_id = {}
    documents_migration_failed = {}
    documents_migrated = {}

    for provider in PROVIDERS:
        documents_migrated[provider] = []
        documents_migration_failed[provider] = []
        documents_no_list_id[provider] = []

        for document in database['{}nodesettings'.format(provider)].find():
            if verify_node_settings_document(document, provider):
                if migrate_list_id_field(document, provider):
                    documents_migrated[provider].append(document)
                else:
                    documents_migration_failed[provider].append(document)
            else:
                documents_no_list_id[provider].append(document)

    for provider in PROVIDERS:
        if documents_migrated[provider]:
            logger.info('Successfully migrated {0} {1} node settings documents:\n{2}'.format(
                len(documents_migrated[provider]), provider, [e['_id'] for e in documents_migrated[provider]]
            ))

        if documents_no_list_id[provider]:
            logger.error('Failed to migrate {0} {1} node settings documents due to no {1}_list_id field:\n{2}'.format(
                len(documents_no_list_id[provider]), provider, [e['_id'] for e in documents_no_list_id[provider]]
            ))

        if documents_migration_failed[provider]:
            logger.error('Failed to migrate {0} {1} node settings documents for unknown reason:\n{2}'.format(
                len(documents_migration_failed[provider]), provider, [e['_id'] for e in documents_migration_failed[provider]]
            ))

    if dry_run:
        raise RuntimeError('Dry run, transaction rolled back.')

def main():
    dry_run = False
    if '--dry' in sys.argv:
        dry_run = True
    if not dry_run:
        script_utils.add_file_logger(logger, __file__)
    init_app(set_backends=True, routes=False)
    with TokuTransaction():
        migrate(dry_run=dry_run)

if __name__ == ""__main__"":
    main()",Add migration script for [citations]nodesettings <provider>_list_id --> list_id,"Add migration script for [citations]nodesettings
    <provider>_list_id --> list_id
",Python,apache-2.0,"mfraezz/osf.io,leb2dg/osf.io,asanfilippo7/osf.io,chennan47/osf.io,kch8qx/osf.io,brianjgeiger/osf.io,hmoco/osf.io,monikagrabowska/osf.io,caseyrollins/osf.io,SSJohns/osf.io,alexschiller/osf.io,mattclark/osf.io,doublebits/osf.io,HalcyonChimera/osf.io,acshi/osf.io,zamattiac/osf.io,felliott/osf.io,chrisseto/osf.io,CenterForOpenScience/osf.io,doublebits/osf.io,samchrisinger/osf.io,Nesiehr/osf.io,samchrisinger/osf.io,amyshi188/osf.io,hmoco/osf.io,CenterForOpenScience/osf.io,RomanZWang/osf.io,Johnetordoff/osf.io,mfraezz/osf.io,wearpants/osf.io,leb2dg/osf.io,abought/osf.io,cwisecarver/osf.io,cwisecarver/osf.io,amyshi188/osf.io,samchrisinger/osf.io,SSJohns/osf.io,emetsger/osf.io,asanfilippo7/osf.io,amyshi188/osf.io,kch8qx/osf.io,mluo613/osf.io,cslzchen/osf.io,laurenrevere/osf.io,hmoco/osf.io,leb2dg/osf.io,HalcyonChimera/osf.io,adlius/osf.io,chennan47/osf.io,baylee-d/osf.io,rdhyee/osf.io,DanielSBrown/osf.io,kwierman/osf.io,monikagrabowska/osf.io,icereval/osf.io,emetsger/osf.io,HalcyonChimera/osf.io,DanielSBrown/osf.io,monikagrabowska/osf.io,Nesiehr/osf.io,felliott/osf.io,caneruguz/osf.io,brianjgeiger/osf.io,kwierman/osf.io,pattisdr/osf.io,adlius/osf.io,DanielSBrown/osf.io,laurenrevere/osf.io,binoculars/osf.io,RomanZWang/osf.io,RomanZWang/osf.io,caneruguz/osf.io,emetsger/osf.io,saradbowman/osf.io,SSJohns/osf.io,wearpants/osf.io,HalcyonChimera/osf.io,RomanZWang/osf.io,chrisseto/osf.io,aaxelb/osf.io,crcresearch/osf.io,TomBaxter/osf.io,chrisseto/osf.io,baylee-d/osf.io,abought/osf.io,erinspace/osf.io,amyshi188/osf.io,mluo613/osf.io,zamattiac/osf.io,TomHeatwole/osf.io,felliott/osf.io,mluke93/osf.io,cwisecarver/osf.io,chennan47/osf.io,rdhyee/osf.io,acshi/osf.io,TomHeatwole/osf.io,cwisecarver/osf.io,icereval/osf.io,monikagrabowska/osf.io,jnayak1/osf.io,crcresearch/osf.io,kwierman/osf.io,RomanZWang/osf.io,binoculars/osf.io,doublebits/osf.io,saradbowman/osf.io,mluo613/osf.io,cslzchen/osf.io,CenterForOpenScience/osf.io,felliott/osf.io,abought/osf.io,alexschiller/osf.io,kch8qx/osf.io,binoculars/osf.io,mluke93/osf.io,zachjanicki/osf.io,kch8qx/osf.io,Nesiehr/osf.io,rdhyee/osf.io,jnayak1/osf.io,aaxelb/osf.io,crcresearch/osf.io,Johnetordoff/osf.io,brianjgeiger/osf.io,hmoco/osf.io,mfraezz/osf.io,asanfilippo7/osf.io,mattclark/osf.io,doublebits/osf.io,acshi/osf.io,baylee-d/osf.io,zamattiac/osf.io,TomBaxter/osf.io,CenterForOpenScience/osf.io,wearpants/osf.io,monikagrabowska/osf.io,zachjanicki/osf.io,wearpants/osf.io,mluke93/osf.io,erinspace/osf.io,asanfilippo7/osf.io,alexschiller/osf.io,Johnetordoff/osf.io,laurenrevere/osf.io,rdhyee/osf.io,alexschiller/osf.io,alexschiller/osf.io,Nesiehr/osf.io,caneruguz/osf.io,mfraezz/osf.io,aaxelb/osf.io,acshi/osf.io,mluke93/osf.io,doublebits/osf.io,caneruguz/osf.io,aaxelb/osf.io,TomHeatwole/osf.io,leb2dg/osf.io,mluo613/osf.io,chrisseto/osf.io,sloria/osf.io,kwierman/osf.io,mattclark/osf.io,pattisdr/osf.io,samchrisinger/osf.io,SSJohns/osf.io,DanielSBrown/osf.io,Johnetordoff/osf.io,adlius/osf.io,sloria/osf.io,jnayak1/osf.io,sloria/osf.io,mluo613/osf.io,pattisdr/osf.io,abought/osf.io,TomHeatwole/osf.io,zachjanicki/osf.io,zachjanicki/osf.io,emetsger/osf.io,caseyrollins/osf.io,acshi/osf.io,jnayak1/osf.io,cslzchen/osf.io,erinspace/osf.io,icereval/osf.io,brianjgeiger/osf.io,kch8qx/osf.io,cslzchen/osf.io,TomBaxter/osf.io,zamattiac/osf.io,caseyrollins/osf.io,adlius/osf.io",95,"```python
import logging
import sys

from modularodm import Q

from framework.mongo import database
from framework.transactions.context import TokuTransaction

from website.app import init_app
from scripts import utils as script_utils

logger = logging.getLogger(__name__)

PROVIDERS = ['mendeley', 'zotero']

def migrate_list_id_field(document, provider):
    try:
        database['{}nodesettings'.format(provider)].find_and_modify(
            {'_id': document['_id']},
            {
                '$set': {
                    'list_id': document['{}_list_id'.format(provider)]
                }
            }
        )
        database['{}nodesettings'.format(provider)].find_and_modify(
            {'_id': document['_id']},
            {
                '$unset': {
                    '{}_list_id'.format(provider): ''
                }
            }
        )
    except Exception:
        return False
    return True

def verify_node_settings_document(document, provider):
    try:
        assert('_id' in document)
        assert('{}_list_id'.format(provider) in document)
    except AssertionError:
        return False
    return True

def migrate(dry_run=True):
    documents_no_list_id = {}
    documents_migration_failed = {}
    documents_migrated = {}

    for provider in PROVIDERS:
        documents_migrated[provider] = []
        documents_migration_failed[provider] = []
        documents_no_list_id[provider] = []

        for document in database['{}nodesettings'.format(provider)].find():
            if verify_node_settings_document(document, provider):
                if migrate_list_id_field(document, provider):
                    documents_migrated[provider].append(document)
                else:
                    documents_migration_failed[provider].append(document)
            else:
                documents_no_list_id[provider].append(document)

    for provider in PROVIDERS:
        if documents_migrated[provider]:
            logger.info('Successfully migrated {0} {1} node settings documents:\n{2}'.format(
                len(documents_migrated[provider]), provider, [e['_id'] for e in documents_migrated[provider]]
            ))

        if documents_no_list_id[provider]:
            logger.error('Failed to migrate {0} {1} node settings documents due to no {1}_list_id field:\n{2}'.format(
                len(documents_no_list_id[provider]), provider, [e['_id'] for e in documents_no_list_id[provider]]
            ))

        if documents_migration_failed[provider]:
            logger.error('Failed to migrate {0} {1} node settings documents for unknown reason:\n{2}'.format(
                len(documents_migration_failed[provider]), provider, [e['_id'] for e in documents_migration_failed[provider]]
            ))

    if dry_run:
        raise RuntimeError('Dry run, transaction rolled back.')

def main():
    dry_run = False
    if '--dry' in sys.argv:
        dry_run = True
    if not dry_run:
        script_utils.add_file_logger(logger, __file__)
    init_app(set_backends=True, routes=False)
    with TokuTransaction():
        migrate(dry_run=dry_run)

if __name__ == ""__main__"":
    main()
```"
b0648969f03dfa1cd55cf1f201883ec82afd97be,test/test_command_parser.py,test/test_command_parser.py,,"from string import ascii_letters

import pytest

from nex.codes import CatCode
from nex.instructions import Instructions
from nex.instructioner import (Instructioner,
                               make_unexpanded_control_sequence_instruction,
                               char_cat_instr_tok)
from nex.utils import ascii_characters
from nex.banisher import Banisher
from nex.parsing.command_parser import command_parser


from common import ITok


test_char_to_cat = {}
for c in ascii_characters:
    test_char_to_cat[c] = CatCode.other
for c in ascii_letters:
    test_char_to_cat[c] = CatCode.letter
test_char_to_cat.update({
    '$': CatCode.escape,
    ' ': CatCode.space,
    '[': CatCode.begin_group,
    ']': CatCode.end_group,
    '\n': CatCode.end_of_line,
})


class DummyCodes:
    def __init__(self, char_to_cat):
        if char_to_cat is None:
            self.char_to_cat = test_char_to_cat.copy()
        else:
            self.char_to_cat = char_to_cat

    def get_cat_code(self, char):
        return self.char_to_cat[char]

    def get_lower_case_code(self, c):
        return c.lower()

    def get_upper_case_code(self, c):
        return c.upper()


class DummyRouter:

    def __init__(self, cs_map):
        self.cs_map = cs_map

    def lookup_control_sequence(self, name, *args, **kwargs):
        canon_token = self.cs_map[name]
        return canon_token.copy(*args, **kwargs)


class DummyParameters:

    def __init__(self, param_map):
        self.param_map = param_map

    def get(self, name, *args, **kwargs):
        return self.param_map[name]


class DummyState:

    def __init__(self, char_to_cat, cs_map, param_map=None):
        self.router = DummyRouter(cs_map)
        self.parameters = DummyParameters(param_map)
        self.codes = DummyCodes(char_to_cat)


def string_to_banisher(s, cs_map, char_to_cat=None, param_map=None):
    state = DummyState(cs_map=cs_map,
                       param_map=param_map, char_to_cat=char_to_cat)
    instrs = Instructioner.from_string(s, get_cat_code_func=state.codes.get_cat_code)
    return Banisher(instrs, state, instrs.lexer.reader)


def test_h_rule():
    cs_map = {
        'hRule': ITok(Instructions.h_rule),
    }
    b = string_to_banisher('$hRule height 20pt width 10pt depth 30pt', cs_map)
    command = command_parser.parse(b.advance_to_end())
    print(command)
","Add test for command parsing, without executing","Add test for command parsing, without executing
",Python,mit,eddiejessup/nex,90,"```python
from string import ascii_letters

import pytest

from nex.codes import CatCode
from nex.instructions import Instructions
from nex.instructioner import (Instructioner,
                               make_unexpanded_control_sequence_instruction,
                               char_cat_instr_tok)
from nex.utils import ascii_characters
from nex.banisher import Banisher
from nex.parsing.command_parser import command_parser


from common import ITok


test_char_to_cat = {}
for c in ascii_characters:
    test_char_to_cat[c] = CatCode.other
for c in ascii_letters:
    test_char_to_cat[c] = CatCode.letter
test_char_to_cat.update({
    '$': CatCode.escape,
    ' ': CatCode.space,
    '[': CatCode.begin_group,
    ']': CatCode.end_group,
    '\n': CatCode.end_of_line,
})


class DummyCodes:
    def __init__(self, char_to_cat):
        if char_to_cat is None:
            self.char_to_cat = test_char_to_cat.copy()
        else:
            self.char_to_cat = char_to_cat

    def get_cat_code(self, char):
        return self.char_to_cat[char]

    def get_lower_case_code(self, c):
        return c.lower()

    def get_upper_case_code(self, c):
        return c.upper()


class DummyRouter:

    def __init__(self, cs_map):
        self.cs_map = cs_map

    def lookup_control_sequence(self, name, *args, **kwargs):
        canon_token = self.cs_map[name]
        return canon_token.copy(*args, **kwargs)


class DummyParameters:

    def __init__(self, param_map):
        self.param_map = param_map

    def get(self, name, *args, **kwargs):
        return self.param_map[name]


class DummyState:

    def __init__(self, char_to_cat, cs_map, param_map=None):
        self.router = DummyRouter(cs_map)
        self.parameters = DummyParameters(param_map)
        self.codes = DummyCodes(char_to_cat)


def string_to_banisher(s, cs_map, char_to_cat=None, param_map=None):
    state = DummyState(cs_map=cs_map,
                       param_map=param_map, char_to_cat=char_to_cat)
    instrs = Instructioner.from_string(s, get_cat_code_func=state.codes.get_cat_code)
    return Banisher(instrs, state, instrs.lexer.reader)


def test_h_rule():
    cs_map = {
        'hRule': ITok(Instructions.h_rule),
    }
    b = string_to_banisher('$hRule height 20pt width 10pt depth 30pt', cs_map)
    command = command_parser.parse(b.advance_to_end())
    print(command)

```"
bc781453ac14c58fa64f51b5ab91e78dd300621a,tests/devices_test/size_test.py,tests/devices_test/size_test.py,,"
import unittest

from blivet.devices import StorageDevice
from blivet import errors
from blivet.formats import getFormat
from blivet.size import Size

class StorageDeviceSizeTest(unittest.TestCase):
    def _getDevice(self, *args, **kwargs):
        return StorageDevice(*args, **kwargs)

    def testSizeSetter(self):
        initial_size = Size('10 GiB')
        new_size = Size('2 GiB')

        ##
        ## setter sets the size
        ##
        dev = self._getDevice('sizetest', size=initial_size)
        self.assertEqual(dev.size, initial_size)

        dev.size = new_size
        self.assertEqual(dev.size, new_size)

        ##
        ## setter raises exn if size outside of format limits
        ##
        dev.format._maxSize = Size(""5 GiB"")
        with self.assertRaises(errors.DeviceError):
            dev.size = Size(""6 GiB"")

        ##
        ## new formats' min size is checked against device size
        ##
        fmt = getFormat(None)
        fmt._minSize = Size(""10 GiB"")
        with self.assertRaises(errors.DeviceError):
            dev.format = fmt

        # the format assignment should succeed without the min size conflict
        fmt._minSize = Size(0)
        dev.format = fmt

        ##
        ## new formats' max size is checked against device size
        ##
        fmt = getFormat(None)
        fmt._maxSize = Size(""10 MiB"")
        with self.assertRaises(errors.DeviceError):
            dev.format = fmt

        # the format assignment should succeed without the min size conflict
        fmt._maxSize = Size(0)
        dev.format = fmt

    def testSizeGetter(self):
        initial_size = Size(""10 GiB"")
        new_size = Size(""5 GiB"")
        dev = self._getDevice('sizetest', size=initial_size)

        ##
        ## getter returns the size in the basic case for non-existing devices
        ##
        self.assertEqual(dev.size, initial_size)

        # create a new device that exists
        dev = self._getDevice('sizetest', size=initial_size, exists=True)

        ##
        ## getter returns the size in the basic case for existing devices
        ##
        self.assertEqual(dev.size, initial_size)

        ##
        ## size does not reflect target size for non-resizable devices
        ##
        # bypass the setter since the min/max will be the current size for a
        # non-resizable device
        dev._targetSize = new_size
        self.assertEqual(dev.size, initial_size)

        ##
        ## getter returns target size when device is resizable and target size
        ## is non-zero
        ##
        dev._resizable = True
        dev.targetSize = new_size # verify that the target size setter works
        self.assertEqual(dev.size, new_size)
        self.assertEqual(dev.size, dev.targetSize)
        self.assertNotEqual(dev._size, dev.targetSize)

        ##
        ## getter returns current size when device is resizable and target size
        ## is zero
        ##
        dev.targetSize = Size(0)
        self.assertEqual(dev.size, initial_size)
        self.assertEqual(dev.size, dev.currentSize)
",Add unit tests for device size setters and getters.,"Add unit tests for device size setters and getters.
",Python,lgpl-2.1,"rvykydal/blivet,rvykydal/blivet,vojtechtrefny/blivet,vpodzime/blivet,vojtechtrefny/blivet,jkonecny12/blivet,AdamWill/blivet,rhinstaller/blivet,vpodzime/blivet,AdamWill/blivet,jkonecny12/blivet,rhinstaller/blivet",100,"```python

import unittest

from blivet.devices import StorageDevice
from blivet import errors
from blivet.formats import getFormat
from blivet.size import Size

class StorageDeviceSizeTest(unittest.TestCase):
    def _getDevice(self, *args, **kwargs):
        return StorageDevice(*args, **kwargs)

    def testSizeSetter(self):
        initial_size = Size('10 GiB')
        new_size = Size('2 GiB')

        ##
        ## setter sets the size
        ##
        dev = self._getDevice('sizetest', size=initial_size)
        self.assertEqual(dev.size, initial_size)

        dev.size = new_size
        self.assertEqual(dev.size, new_size)

        ##
        ## setter raises exn if size outside of format limits
        ##
        dev.format._maxSize = Size(""5 GiB"")
        with self.assertRaises(errors.DeviceError):
            dev.size = Size(""6 GiB"")

        ##
        ## new formats' min size is checked against device size
        ##
        fmt = getFormat(None)
        fmt._minSize = Size(""10 GiB"")
        with self.assertRaises(errors.DeviceError):
            dev.format = fmt

        # the format assignment should succeed without the min size conflict
        fmt._minSize = Size(0)
        dev.format = fmt

        ##
        ## new formats' max size is checked against device size
        ##
        fmt = getFormat(None)
        fmt._maxSize = Size(""10 MiB"")
        with self.assertRaises(errors.DeviceError):
            dev.format = fmt

        # the format assignment should succeed without the min size conflict
        fmt._maxSize = Size(0)
        dev.format = fmt

    def testSizeGetter(self):
        initial_size = Size(""10 GiB"")
        new_size = Size(""5 GiB"")
        dev = self._getDevice('sizetest', size=initial_size)

        ##
        ## getter returns the size in the basic case for non-existing devices
        ##
        self.assertEqual(dev.size, initial_size)

        # create a new device that exists
        dev = self._getDevice('sizetest', size=initial_size, exists=True)

        ##
        ## getter returns the size in the basic case for existing devices
        ##
        self.assertEqual(dev.size, initial_size)

        ##
        ## size does not reflect target size for non-resizable devices
        ##
        # bypass the setter since the min/max will be the current size for a
        # non-resizable device
        dev._targetSize = new_size
        self.assertEqual(dev.size, initial_size)

        ##
        ## getter returns target size when device is resizable and target size
        ## is non-zero
        ##
        dev._resizable = True
        dev.targetSize = new_size # verify that the target size setter works
        self.assertEqual(dev.size, new_size)
        self.assertEqual(dev.size, dev.targetSize)
        self.assertNotEqual(dev._size, dev.targetSize)

        ##
        ## getter returns current size when device is resizable and target size
        ## is zero
        ##
        dev.targetSize = Size(0)
        self.assertEqual(dev.size, initial_size)
        self.assertEqual(dev.size, dev.currentSize)

```"
352fa259d439a9be5c299470868e2fa82ecd835f,client/python/tests/unittest_runner.py,client/python/tests/unittest_runner.py,,"#!/usr/bin/env python
import os
import re
import sys
import shutil
import subprocess


__author__ = ""Christopher Choi <chutsu@gmail.com>""


# SETTINGS
keep_unittest_logs = False
unittests_bin_dir = ""tests""
unittests_log_dir = ""unittests_log""
unittests_file_pattern = ""^[a-zA-Z0-9_]*_tests.*$""

class TC:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'


def print_stdout(unittest_output_fp):
    # open unittest stdout log file
    unittest_output = open(unittest_output_fp, 'r')
    output_content = unittest_output.read()
    unittest_output.close()

    # print unittest stdout
    print(""-"" * 79)
    print(output_content)
    print(""-"" * 79)


def get_files(path, pattern):
    file_list = []

    for root, directory, files in os.walk(path):
        for f in files:
            if re.match(pattern, f):
                file_list.append(os.path.join(root, f))

    return file_list


if __name__ == ""__main__"":
    orig_cwd = os.getcwd()

    # make log dir if not already exist
    if not os.path.exists(unittests_log_dir):
        os.mkdir(unittests_log_dir)

    # gather all unittests
    file_list = os.listdir(unittests_bin_dir)
    unittests = get_files(unittests_bin_dir, unittests_file_pattern)

    # execute all unittests
    error = False
    return_val = 0
    for unittest in unittests:
        # execute unittest
        try:
            print ""UNITTEST [{0}] {1}Starting{2}"".format(unittest, TC.OKBLUE, TC.ENDC)
            print(""UNITTEST [{0}] "".format(unittest)),
            unittest_output_fp = os.path.join(
                orig_cwd,
                unittests_log_dir,
                os.path.basename(unittest) + "".log""
            )
            unittest_output = open(unittest_output_fp, 'w')

            return_val = subprocess.check_call(
                [""./{0}"".format(unittest)],
                stdout=unittest_output,
                stderr=unittest_output
            )
            unittest_output.close()
            print(""{0}PASSED!{1}"".format(TC.OKGREEN, TC.ENDC))

        except:
            unittest_output.close()
            print(""{0}FAILED!{1}"".format(TC.FAIL, TC.ENDC))
            print_stdout(unittest_output_fp)
            error = True

    os.chdir(orig_cwd)
    # keep unittest stdout dir?
    if keep_unittest_logs is False:
        shutil.rmtree(unittests_log_dir)

    if error is True:
        sys.exit(-1)
    else:
        sys.exit(0)
",Add test folder for python,"Add test folder for python
",Python,apache-2.0,"wallarelvo/backfire,wallarelvo/backfire",99,"```python
#!/usr/bin/env python
import os
import re
import sys
import shutil
import subprocess


__author__ = ""Christopher Choi <chutsu@gmail.com>""


# SETTINGS
keep_unittest_logs = False
unittests_bin_dir = ""tests""
unittests_log_dir = ""unittests_log""
unittests_file_pattern = ""^[a-zA-Z0-9_]*_tests.*$""

class TC:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'


def print_stdout(unittest_output_fp):
    # open unittest stdout log file
    unittest_output = open(unittest_output_fp, 'r')
    output_content = unittest_output.read()
    unittest_output.close()

    # print unittest stdout
    print(""-"" * 79)
    print(output_content)
    print(""-"" * 79)


def get_files(path, pattern):
    file_list = []

    for root, directory, files in os.walk(path):
        for f in files:
            if re.match(pattern, f):
                file_list.append(os.path.join(root, f))

    return file_list


if __name__ == ""__main__"":
    orig_cwd = os.getcwd()

    # make log dir if not already exist
    if not os.path.exists(unittests_log_dir):
        os.mkdir(unittests_log_dir)

    # gather all unittests
    file_list = os.listdir(unittests_bin_dir)
    unittests = get_files(unittests_bin_dir, unittests_file_pattern)

    # execute all unittests
    error = False
    return_val = 0
    for unittest in unittests:
        # execute unittest
        try:
            print ""UNITTEST [{0}] {1}Starting{2}"".format(unittest, TC.OKBLUE, TC.ENDC)
            print(""UNITTEST [{0}] "".format(unittest)),
            unittest_output_fp = os.path.join(
                orig_cwd,
                unittests_log_dir,
                os.path.basename(unittest) + "".log""
            )
            unittest_output = open(unittest_output_fp, 'w')

            return_val = subprocess.check_call(
                [""./{0}"".format(unittest)],
                stdout=unittest_output,
                stderr=unittest_output
            )
            unittest_output.close()
            print(""{0}PASSED!{1}"".format(TC.OKGREEN, TC.ENDC))

        except:
            unittest_output.close()
            print(""{0}FAILED!{1}"".format(TC.FAIL, TC.ENDC))
            print_stdout(unittest_output_fp)
            error = True

    os.chdir(orig_cwd)
    # keep unittest stdout dir?
    if keep_unittest_logs is False:
        shutil.rmtree(unittests_log_dir)

    if error is True:
        sys.exit(-1)
    else:
        sys.exit(0)

```"
abd97f71e54515c057e94f7d21aa953faba3f5fc,taskflow/examples/delayed_return.py,taskflow/examples/delayed_return.py,,"# -*- coding: utf-8 -*-

#    Copyright (C) 2014 Yahoo! Inc. All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging
import os
import sys

from concurrent import futures

logging.basicConfig(level=logging.ERROR)

self_dir = os.path.abspath(os.path.dirname(__file__))
top_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                       os.pardir,
                                       os.pardir))
sys.path.insert(0, top_dir)
sys.path.insert(0, self_dir)

# INTRO: in this example linear_flow we will attach a listener to an engine
# and delay the return from a function until after the result of a task has
# occured in that engine. The engine will continue running (in the background)
# while the function will have returned.

import taskflow.engines

from taskflow.listeners import base
from taskflow.patterns import linear_flow as lf
from taskflow import states
from taskflow import task
from taskflow.utils import misc


class PokeFutureListener(base.ListenerBase):
    def __init__(self, engine, future, task_name):
        super(PokeFutureListener, self).__init__(
            engine,
            task_listen_for=(misc.Notifier.ANY,),
            flow_listen_for=[])
        self._future = future
        self._task_name = task_name

    def _task_receiver(self, state, details):
        if state in (states.SUCCESS, states.FAILURE):
            if details.get('task_name') == self._task_name:
                if state == states.SUCCESS:
                    self._future.set_result(details['result'])
                else:
                    failure = details['result']
                    self._future.set_exception(failure.exception)


class Hi(task.Task):
    def execute(self):
        # raise IOError(""I broken"")
        return 'hi'


class Bye(task.Task):
    def execute(self):
        return 'bye'


def return_from_flow(pool):
    wf = lf.Flow(""root"").add(Hi(""hi""), Bye(""bye""))
    eng = taskflow.engines.load(wf, engine_conf='serial')
    f = futures.Future()
    watcher = PokeFutureListener(eng, f, 'hi')
    watcher.register()
    pool.submit(eng.run)
    return (eng, f.result())


with futures.ThreadPoolExecutor(1) as pool:
    engine, hi_result = return_from_flow(pool)
    print(hi_result)

print(engine.storage.get_flow_state())
",Add a example that activates a future when a result is ready,"Add a example that activates a future when a result is ready

To allow for an engine to continue to run while at the same time
returning from a function when a component of that engine finishes
a pattern can be used that ties and engines listeners to the function
return, allowing for both to be used simulatenously.

Change-Id: Iab49e0c7b233138bc2d02247ab7aa3d99a82cd67
",Python,apache-2.0,"openstack/taskflow,junneyang/taskflow,openstack/taskflow,jimbobhickville/taskflow,pombredanne/taskflow-1,junneyang/taskflow,pombredanne/taskflow-1,jimbobhickville/taskflow",91,"```python
# -*- coding: utf-8 -*-

#    Copyright (C) 2014 Yahoo! Inc. All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the ""License""); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging
import os
import sys

from concurrent import futures

logging.basicConfig(level=logging.ERROR)

self_dir = os.path.abspath(os.path.dirname(__file__))
top_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),
                                       os.pardir,
                                       os.pardir))
sys.path.insert(0, top_dir)
sys.path.insert(0, self_dir)

# INTRO: in this example linear_flow we will attach a listener to an engine
# and delay the return from a function until after the result of a task has
# occured in that engine. The engine will continue running (in the background)
# while the function will have returned.

import taskflow.engines

from taskflow.listeners import base
from taskflow.patterns import linear_flow as lf
from taskflow import states
from taskflow import task
from taskflow.utils import misc


class PokeFutureListener(base.ListenerBase):
    def __init__(self, engine, future, task_name):
        super(PokeFutureListener, self).__init__(
            engine,
            task_listen_for=(misc.Notifier.ANY,),
            flow_listen_for=[])
        self._future = future
        self._task_name = task_name

    def _task_receiver(self, state, details):
        if state in (states.SUCCESS, states.FAILURE):
            if details.get('task_name') == self._task_name:
                if state == states.SUCCESS:
                    self._future.set_result(details['result'])
                else:
                    failure = details['result']
                    self._future.set_exception(failure.exception)


class Hi(task.Task):
    def execute(self):
        # raise IOError(""I broken"")
        return 'hi'


class Bye(task.Task):
    def execute(self):
        return 'bye'


def return_from_flow(pool):
    wf = lf.Flow(""root"").add(Hi(""hi""), Bye(""bye""))
    eng = taskflow.engines.load(wf, engine_conf='serial')
    f = futures.Future()
    watcher = PokeFutureListener(eng, f, 'hi')
    watcher.register()
    pool.submit(eng.run)
    return (eng, f.result())


with futures.ThreadPoolExecutor(1) as pool:
    engine, hi_result = return_from_flow(pool)
    print(hi_result)

print(engine.storage.get_flow_state())

```"
8cf2a16ff98cc831734c82116c4a91ddb1094865,tests/integration/modules/cmdmod.py,tests/integration/modules/cmdmod.py,,"# Import python libs
import os

# Import salt libs
import integration

class CMDModuleTest(integration.ModuleCase):
    '''
    Validate the cmd module
    '''
    def test_run(self):
        '''
        cmd.run
        '''
        self.assertTrue(self.run_function('cmd.run', ['echo $SHELL']))
        self.assertEqual(
                self.run_function('cmd.run',
                    ['echo $SHELL', 'shell=/bin/bash']),
                '/bin/bash')

    def test_stdout(self):
        '''
        cmd.run_stdout
        '''
        self.assertEqual(
                self.run_function('cmd.run_stdout',
                    ['echo ""cheese""']),
                'cheese')

    def test_stderr(self):
        '''
        cmd.run_stderr
        '''
        self.assertEqual(
                self.run_function('cmd.run_stderr',
                    ['echo ""cheese"" 1>&2']),
                'cheese')

    def test_run_all(self):
        '''
        cmd.run_all
        '''
        ret = self.run_function('cmd.run_all', ['echo ""cheese"" 1>&2'])
        self.assertTrue('pid' in ret)
        self.assertTrue('retcode' in ret)
        self.assertTrue('stdout' in ret)
        self.assertTrue('stderr' in ret)
        self.assertTrue(isinstance(ret.get('pid'), int))
        self.assertTrue(isinstance(ret.get('retcode'), int))
        self.assertTrue(isinstance(ret.get('stdout'), basestring))
        self.assertTrue(isinstance(ret.get('stderr'), basestring))
        self.assertEqual(ret.get('stderr'), 'cheese')

    def test_retcode(self):
        '''
        cmd.retcode
        '''
        self.assertEqual(self.run_function('cmd.retcode', ['true']), 0)
        self.assertEqual(self.run_function('cmd.retcode', ['false']), 1)

    def test_which(self):
        '''
        cmd.which
        '''
        self.assertEqual(
                self.run_function('cmd.which', ['echo']),
                self.run_function('cmd.run', ['which echo']))

    def test_has_exec(self):
        '''
        cmd.has_exec
        '''
        self.assertTrue(self.run_function('cmd.has_exec', ['python']))
        self.assertFalse(self.run_function(
            'cmd.has_exec',
            ['alllfsdfnwieulrrh9123857ygf']
            ))

    def test_exec_code(self):
        '''
        cmd.exec_code
        '''
        code = '''
import sys
sys.stdout.write('cheese')
        '''
        self.assertEqual(
                self.run_function('cmd.exec_code', ['python', code]),
                'cheese'
                )
",Add tests for the cmd module,"Add tests for the cmd module
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",91,"```python
# Import python libs
import os

# Import salt libs
import integration

class CMDModuleTest(integration.ModuleCase):
    '''
    Validate the cmd module
    '''
    def test_run(self):
        '''
        cmd.run
        '''
        self.assertTrue(self.run_function('cmd.run', ['echo $SHELL']))
        self.assertEqual(
                self.run_function('cmd.run',
                    ['echo $SHELL', 'shell=/bin/bash']),
                '/bin/bash')

    def test_stdout(self):
        '''
        cmd.run_stdout
        '''
        self.assertEqual(
                self.run_function('cmd.run_stdout',
                    ['echo ""cheese""']),
                'cheese')

    def test_stderr(self):
        '''
        cmd.run_stderr
        '''
        self.assertEqual(
                self.run_function('cmd.run_stderr',
                    ['echo ""cheese"" 1>&2']),
                'cheese')

    def test_run_all(self):
        '''
        cmd.run_all
        '''
        ret = self.run_function('cmd.run_all', ['echo ""cheese"" 1>&2'])
        self.assertTrue('pid' in ret)
        self.assertTrue('retcode' in ret)
        self.assertTrue('stdout' in ret)
        self.assertTrue('stderr' in ret)
        self.assertTrue(isinstance(ret.get('pid'), int))
        self.assertTrue(isinstance(ret.get('retcode'), int))
        self.assertTrue(isinstance(ret.get('stdout'), basestring))
        self.assertTrue(isinstance(ret.get('stderr'), basestring))
        self.assertEqual(ret.get('stderr'), 'cheese')

    def test_retcode(self):
        '''
        cmd.retcode
        '''
        self.assertEqual(self.run_function('cmd.retcode', ['true']), 0)
        self.assertEqual(self.run_function('cmd.retcode', ['false']), 1)

    def test_which(self):
        '''
        cmd.which
        '''
        self.assertEqual(
                self.run_function('cmd.which', ['echo']),
                self.run_function('cmd.run', ['which echo']))

    def test_has_exec(self):
        '''
        cmd.has_exec
        '''
        self.assertTrue(self.run_function('cmd.has_exec', ['python']))
        self.assertFalse(self.run_function(
            'cmd.has_exec',
            ['alllfsdfnwieulrrh9123857ygf']
            ))

    def test_exec_code(self):
        '''
        cmd.exec_code
        '''
        code = '''
import sys
sys.stdout.write('cheese')
        '''
        self.assertEqual(
                self.run_function('cmd.exec_code', ['python', code]),
                'cheese'
                )

```"
43f4c62d45d41b9dae5582e2d7f10b187731f2b7,utils/test/__init__.py,utils/test/__init__.py,,"import os
import unittest

from google.appengine.ext import testbed
from google.appengine.datastore import datastore_stub_util

import settings


HRD_CONSISTENCY = 1


class DatastoreTestCase(unittest.TestCase):
    """"""Test case with stubbed high-replication datastore API. The datastore
    stub uses an optimistic, always-consistent policy, meaning writes will
    always apply.
    """"""

    def setUp(self):
        super(DatastoreTestCase, self).setUp()

        self.original_environ = dict(os.environ)

        os.environ['TZ'] = 'UTC'

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.setup_env(app_id=settings.APP_ID)

        self.policy = datastore_stub_util.PseudoRandomHRConsistencyPolicy(
            probability=HRD_CONSISTENCY)
        self.testbed.init_datastore_v3_stub(consistency_policy=self.policy)

    def tearDown(self):
        super(DatastoreTestCase, self).tearDown()
        self.testbed.deactivate()

        os.environ = self.original_environ


class MemcacheTestCase(unittest.TestCase):
    """"""Test case with stubbed memcache API.""""""

    def setUp(self):
        super(MemcacheTestCase, self).setUp()

        self.original_environ = dict(os.environ)

        os.environ['TZ'] = 'UTC'

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_memcache_stub()

    def tearDown(self):
        super(MemcacheTestCase, self).tearDown()
        self.testbed.deactivate()

        os.environ = self.original_environ


class DatastoreMemcacheTestCase(unittest.TestCase):
    """"""Test case with stubbed datastore and memcache APIs. The datastore
    stub uses an optimistic, always-consistent policy, meaning writes will
    always apply.
    """"""

    def setUp(self):
        super(DatastoreMemcacheTestCase, self).setUp()

        self.original_environ = dict(os.environ)

        os.environ['TZ'] = 'UTC'

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.setup_env(app_id=settings.APP_ID)

        self.policy = datastore_stub_util.PseudoRandomHRConsistencyPolicy(
            probability=HRD_CONSISTENCY)
        self.testbed.init_datastore_v3_stub(consistency_policy=self.policy)

        self.testbed.init_memcache_stub()

    def tearDown(self):
        super(DatastoreMemcacheTestCase, self).tearDown()
        self.testbed.deactivate()

        os.environ = self.original_environ

",Add datastore and memcache stub test cases,"Add datastore and memcache stub test cases
",Python,apache-2.0,tylertreat/gaeutils,91,"```python
import os
import unittest

from google.appengine.ext import testbed
from google.appengine.datastore import datastore_stub_util

import settings


HRD_CONSISTENCY = 1


class DatastoreTestCase(unittest.TestCase):
    """"""Test case with stubbed high-replication datastore API. The datastore
    stub uses an optimistic, always-consistent policy, meaning writes will
    always apply.
    """"""

    def setUp(self):
        super(DatastoreTestCase, self).setUp()

        self.original_environ = dict(os.environ)

        os.environ['TZ'] = 'UTC'

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.setup_env(app_id=settings.APP_ID)

        self.policy = datastore_stub_util.PseudoRandomHRConsistencyPolicy(
            probability=HRD_CONSISTENCY)
        self.testbed.init_datastore_v3_stub(consistency_policy=self.policy)

    def tearDown(self):
        super(DatastoreTestCase, self).tearDown()
        self.testbed.deactivate()

        os.environ = self.original_environ


class MemcacheTestCase(unittest.TestCase):
    """"""Test case with stubbed memcache API.""""""

    def setUp(self):
        super(MemcacheTestCase, self).setUp()

        self.original_environ = dict(os.environ)

        os.environ['TZ'] = 'UTC'

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.init_memcache_stub()

    def tearDown(self):
        super(MemcacheTestCase, self).tearDown()
        self.testbed.deactivate()

        os.environ = self.original_environ


class DatastoreMemcacheTestCase(unittest.TestCase):
    """"""Test case with stubbed datastore and memcache APIs. The datastore
    stub uses an optimistic, always-consistent policy, meaning writes will
    always apply.
    """"""

    def setUp(self):
        super(DatastoreMemcacheTestCase, self).setUp()

        self.original_environ = dict(os.environ)

        os.environ['TZ'] = 'UTC'

        self.testbed = testbed.Testbed()
        self.testbed.activate()
        self.testbed.setup_env(app_id=settings.APP_ID)

        self.policy = datastore_stub_util.PseudoRandomHRConsistencyPolicy(
            probability=HRD_CONSISTENCY)
        self.testbed.init_datastore_v3_stub(consistency_policy=self.policy)

        self.testbed.init_memcache_stub()

    def tearDown(self):
        super(DatastoreMemcacheTestCase, self).tearDown()
        self.testbed.deactivate()

        os.environ = self.original_environ


```"
5096f4432978ef1e5d1f3d449fd3f54050f3c287,test/widgets/test_wlan.py,test/widgets/test_wlan.py,,"# Copyright (c) 2021 elParaguayo
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Widget specific tests

import sys
from importlib import reload
from types import ModuleType

import pytest

import libqtile.config
from libqtile.bar import Bar


def no_op(*args, **kwargs):
    pass


class MockIwlib(ModuleType):
    DATA = {
        'wlan0': {
            'NWID': b'Auto',
            'Frequency': b'5.18 GHz',
            'Access Point': b'12:34:56:78:90:AB',
            'BitRate': b'650 Mb/s',
            'ESSID': b'QtileNet',
            'Mode': b'Managed',
            'stats': {
                'quality': 49,
                'level': 190,
                'noise': 0,
                'updated': 75
            }
        }
    }

    @classmethod
    def get_iwconfig(cls, interface):
        return cls.DATA.get(interface, dict())


# Patch the widget with our mock iwlib module.
@pytest.fixture
def patched_wlan(monkeypatch):
    monkeypatch.setitem(sys.modules, ""iwlib"", MockIwlib(""iwlib""))
    from libqtile.widget import wlan

    # Reload fixes cases where psutil may have been imported previously
    reload(wlan)

    yield wlan


@pytest.mark.parametrize(
    ""kwargs,expected"", [
        ({}, ""QtileNet 49/70""),
        ({""format"": ""{essid} {percent:2.0%}""}, ""QtileNet 70%""),
        ({""interface"": ""wlan1""}, ""Disconnected"")
    ]
)
def test_wlan_display(minimal_conf_noscreen, manager_nospawn, patched_wlan, kwargs, expected):
    widget = patched_wlan.Wlan(**kwargs)
    config = minimal_conf_noscreen
    config.screens = [
        libqtile.config.Screen(
            top=Bar([widget], 10)
        )
    ]
    manager_nospawn.start(config)

    text = manager_nospawn.c.bar[""top""].info()[""widgets""][0][""text""]
    assert text == expected
",Add test for wlan widget,"Add test for wlan widget
",Python,mit,"ramnes/qtile,qtile/qtile,ramnes/qtile,qtile/qtile",91,"```python
# Copyright (c) 2021 elParaguayo
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the ""Software""), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# Widget specific tests

import sys
from importlib import reload
from types import ModuleType

import pytest

import libqtile.config
from libqtile.bar import Bar


def no_op(*args, **kwargs):
    pass


class MockIwlib(ModuleType):
    DATA = {
        'wlan0': {
            'NWID': b'Auto',
            'Frequency': b'5.18 GHz',
            'Access Point': b'12:34:56:78:90:AB',
            'BitRate': b'650 Mb/s',
            'ESSID': b'QtileNet',
            'Mode': b'Managed',
            'stats': {
                'quality': 49,
                'level': 190,
                'noise': 0,
                'updated': 75
            }
        }
    }

    @classmethod
    def get_iwconfig(cls, interface):
        return cls.DATA.get(interface, dict())


# Patch the widget with our mock iwlib module.
@pytest.fixture
def patched_wlan(monkeypatch):
    monkeypatch.setitem(sys.modules, ""iwlib"", MockIwlib(""iwlib""))
    from libqtile.widget import wlan

    # Reload fixes cases where psutil may have been imported previously
    reload(wlan)

    yield wlan


@pytest.mark.parametrize(
    ""kwargs,expected"", [
        ({}, ""QtileNet 49/70""),
        ({""format"": ""{essid} {percent:2.0%}""}, ""QtileNet 70%""),
        ({""interface"": ""wlan1""}, ""Disconnected"")
    ]
)
def test_wlan_display(minimal_conf_noscreen, manager_nospawn, patched_wlan, kwargs, expected):
    widget = patched_wlan.Wlan(**kwargs)
    config = minimal_conf_noscreen
    config.screens = [
        libqtile.config.Screen(
            top=Bar([widget], 10)
        )
    ]
    manager_nospawn.start(config)

    text = manager_nospawn.c.bar[""top""].info()[""widgets""][0][""text""]
    assert text == expected

```"
8f18bc32e84de1ce25bfcb972c6b9f1b24e0f6af,runners/test/python/sparse_codegen_test_util.py,runners/test/python/sparse_codegen_test_util.py,,"""""""Testing error handling in the common test utilities.

The module tests the error handling in the utilities we use for writing
exhaustive tests for the sparse codegen.
""""""

import inspect
import sys

# Import MLIR related modules.
from mlir.dialects import sparse_tensor as st
from mlir.dialects.linalg.opdsl import lang as dsl

# Import common test tools.
import sparse_codegen_test_common as tc

# A test returns 1 when it fails to indicate the number of failing test. This is
# to help accumulating the total number of failing tests.
_PASS = 0
_FAIL = 1


def _pass_test(name: str) -> int:
  print(f""{name} passed."")
  return _PASS


def _fail_test(name: str) -> int:
  print(f""{name} failed."")
  return _FAIL


def _test_mismatching_ordering_sparsity() -> int:
  """"""Test for inconsistent input descriptor parameters.

  The dimension ordering and the sparsities in this test don't have the same
  length.
  """"""
  name = inspect.currentframe().f_code.co_name
  try:
    _ = tc.InputDesc([0, 1, 2], [st.DimLevelType.dense, st.DimLevelType.dense],
                     0, 0)
  except ValueError:
    num_failed = _pass_test(name)
  else:
    num_failed = _fail_test(name)
  return num_failed


def _test_invalid_ordering() -> int:
  """"""Test for invalid dimension orderings.

  The dimension ordering in this test is not a permutation of 0..n-1, where
  n is the length of the dimension ordering.
  """"""
  name = inspect.currentframe().f_code.co_name
  try:
    _ = tc.InputDesc([0, 2], [st.DimLevelType.dense, st.DimLevelType.dense], 0,
                     0)
  except ValueError:
    num_failed = _pass_test(name)
  else:
    num_failed = _fail_test(name)
  return num_failed


def _test_invalid_affine_expression() -> int:
  """"""Test for invalid affine expressions.

  The affine expression in the first input here is not defined in the iteration
  space.
  """"""
  name = inspect.currentframe().f_code.co_name
  try:
    _ = tc.TestDesc([dsl.S.M, dsl.S.N], [8, 8], [dsl.S.M, dsl.S.X])
  except ValueError:
    num_failed = _pass_test(name)
  else:
    num_failed = _fail_test(name)
  return num_failed


def run_test():
  num_failed = (
      _test_mismatching_ordering_sparsity() + _test_invalid_ordering() +
      _test_invalid_affine_expression())

  if num_failed == 0:
    print(""All test passed."")
  else:
    print(f""{num_failed} tests failed."")
    sys.exit(""FAILURE"")


run_test()
",Add tests for the exhaustive test utilities.,"[mlir][python[[sparse] Add tests for the exhaustive test utilities.

Add tests for the common utilities that we use for writing exhaustive tests for
the MLIR sparse codegen.

Simplify the relevant build rules by adding a new filegroup and moving a shared
library to an existing filegroup.

PiperOrigin-RevId: 385571670
",Python,apache-2.0,"iree-org/iree-llvm-sandbox,iree-org/iree-llvm-sandbox,iree-org/iree-llvm-sandbox,iree-org/iree-llvm-sandbox",96,"```python
""""""Testing error handling in the common test utilities.

The module tests the error handling in the utilities we use for writing
exhaustive tests for the sparse codegen.
""""""

import inspect
import sys

# Import MLIR related modules.
from mlir.dialects import sparse_tensor as st
from mlir.dialects.linalg.opdsl import lang as dsl

# Import common test tools.
import sparse_codegen_test_common as tc

# A test returns 1 when it fails to indicate the number of failing test. This is
# to help accumulating the total number of failing tests.
_PASS = 0
_FAIL = 1


def _pass_test(name: str) -> int:
  print(f""{name} passed."")
  return _PASS


def _fail_test(name: str) -> int:
  print(f""{name} failed."")
  return _FAIL


def _test_mismatching_ordering_sparsity() -> int:
  """"""Test for inconsistent input descriptor parameters.

  The dimension ordering and the sparsities in this test don't have the same
  length.
  """"""
  name = inspect.currentframe().f_code.co_name
  try:
    _ = tc.InputDesc([0, 1, 2], [st.DimLevelType.dense, st.DimLevelType.dense],
                     0, 0)
  except ValueError:
    num_failed = _pass_test(name)
  else:
    num_failed = _fail_test(name)
  return num_failed


def _test_invalid_ordering() -> int:
  """"""Test for invalid dimension orderings.

  The dimension ordering in this test is not a permutation of 0..n-1, where
  n is the length of the dimension ordering.
  """"""
  name = inspect.currentframe().f_code.co_name
  try:
    _ = tc.InputDesc([0, 2], [st.DimLevelType.dense, st.DimLevelType.dense], 0,
                     0)
  except ValueError:
    num_failed = _pass_test(name)
  else:
    num_failed = _fail_test(name)
  return num_failed


def _test_invalid_affine_expression() -> int:
  """"""Test for invalid affine expressions.

  The affine expression in the first input here is not defined in the iteration
  space.
  """"""
  name = inspect.currentframe().f_code.co_name
  try:
    _ = tc.TestDesc([dsl.S.M, dsl.S.N], [8, 8], [dsl.S.M, dsl.S.X])
  except ValueError:
    num_failed = _pass_test(name)
  else:
    num_failed = _fail_test(name)
  return num_failed


def run_test():
  num_failed = (
      _test_mismatching_ordering_sparsity() + _test_invalid_ordering() +
      _test_invalid_affine_expression())

  if num_failed == 0:
    print(""All test passed."")
  else:
    print(f""{num_failed} tests failed."")
    sys.exit(""FAILURE"")


run_test()

```"
2b51cbe6204abf12d1eca023a0784a1c903c244d,data/scripts/load_chuls.py,data/scripts/load_chuls.py,,"import os
import csv
import json
from django.conf import settings


chul_file = os.path.join(
    settings.BASE_DIR, 'data/csvs/chul.csv')

table_columns = [
    ""CommUnitId"", ""Cu_code"", ""CommUnitName"", ""Date_CU_Established"",
    ""Date_CU_Operational"", ""CuLocation"", ""Link_Facility_Code"",
    ""CU_OfficialMobile"", ""CU_OfficialEmail"", ""Chew_Facility"", ""Chew_In_Charge"",
    ""NumHouseholds"", ""CUStatus"", ""Approved"", ""ApprovedDate"", ""ApprovedBy"",
    ""ApprovalComments"", ""isEdit"", ""UnitRecordArchived"", ""Date_added"",
    ""Date_modified"", ""Delete_comments""]


def create_chuls_file():
    chul = []
    chews = []
    chu_contacts = []
    with open(chul_file, 'r') as csv_file:
        chul_reader = csv.reader(csv_file)

        for row in chul_reader:
            code = row[1]
            name = row[2]
            date_established = row[3]
            facility_code = row[7]
            households_monitored = row[11]
            status = row[12]
            #  approval = row[13]

            #  contacts
            mobile = row[7]
            mobile_dict = {
                ""contact"": mobile,
                ""contact_type"": {
                    ""PHONE""
                }
            }
            email = row[8]
            email_dict = {
                ""contact"": email,
                ""contact_type"": {
                    ""EMAIL""
                }
            }
            if email_dict not in chu_contacts:
                chu_contacts.append(email_dict)
            if mobile_dict not in chu_contacts:
                chu_contacts.append(email_dict)

            # chew
            first_name = row[10]

            chu = {
                ""code"": code,
                ""name"": name,
                ""date_established"": date_established,
                ""facility"": {
                    ""code"": facility_code
                },
                ""households_monitored"": households_monitored,
                ""status"": status,
            }
            if chu not in chul:
                chul.append(chu)

            chew = {
                ""first_name"": first_name,
                ""heath_unit"": {
                    ""code"": code
                }
            }
            if chew not in chews:
                chews.append(chew)

    return chul, chews


def write_file(file_name, data):
    if os.path.exists(file_name):
        os.remove(file_name)
    fac_file = open(file_name, 'w+')
    del data[0]
    dumped_data = """"
    try:
        dumped_data = json.dumps(data)
    except:
        print data
        raise

    fac_file.write(dumped_data)


def write_chuls_and_chews():
    chus, chews = create_chuls_file()
    write_file('chul.txt', chus)
    write_file('chew.txt', chews)
",Add script to read and load chus and chews,"Add script to read and load chus and chews
",Python,mit,"MasterFacilityList/mfl_api,MasterFacilityList/mfl_api,MasterFacilityList/mfl_api,urandu/mfl_api,MasterFacilityList/mfl_api,urandu/mfl_api,urandu/mfl_api,MasterFacilityList/mfl_api,urandu/mfl_api",102,"```python
import os
import csv
import json
from django.conf import settings


chul_file = os.path.join(
    settings.BASE_DIR, 'data/csvs/chul.csv')

table_columns = [
    ""CommUnitId"", ""Cu_code"", ""CommUnitName"", ""Date_CU_Established"",
    ""Date_CU_Operational"", ""CuLocation"", ""Link_Facility_Code"",
    ""CU_OfficialMobile"", ""CU_OfficialEmail"", ""Chew_Facility"", ""Chew_In_Charge"",
    ""NumHouseholds"", ""CUStatus"", ""Approved"", ""ApprovedDate"", ""ApprovedBy"",
    ""ApprovalComments"", ""isEdit"", ""UnitRecordArchived"", ""Date_added"",
    ""Date_modified"", ""Delete_comments""]


def create_chuls_file():
    chul = []
    chews = []
    chu_contacts = []
    with open(chul_file, 'r') as csv_file:
        chul_reader = csv.reader(csv_file)

        for row in chul_reader:
            code = row[1]
            name = row[2]
            date_established = row[3]
            facility_code = row[7]
            households_monitored = row[11]
            status = row[12]
            #  approval = row[13]

            #  contacts
            mobile = row[7]
            mobile_dict = {
                ""contact"": mobile,
                ""contact_type"": {
                    ""PHONE""
                }
            }
            email = row[8]
            email_dict = {
                ""contact"": email,
                ""contact_type"": {
                    ""EMAIL""
                }
            }
            if email_dict not in chu_contacts:
                chu_contacts.append(email_dict)
            if mobile_dict not in chu_contacts:
                chu_contacts.append(email_dict)

            # chew
            first_name = row[10]

            chu = {
                ""code"": code,
                ""name"": name,
                ""date_established"": date_established,
                ""facility"": {
                    ""code"": facility_code
                },
                ""households_monitored"": households_monitored,
                ""status"": status,
            }
            if chu not in chul:
                chul.append(chu)

            chew = {
                ""first_name"": first_name,
                ""heath_unit"": {
                    ""code"": code
                }
            }
            if chew not in chews:
                chews.append(chew)

    return chul, chews


def write_file(file_name, data):
    if os.path.exists(file_name):
        os.remove(file_name)
    fac_file = open(file_name, 'w+')
    del data[0]
    dumped_data = """"
    try:
        dumped_data = json.dumps(data)
    except:
        print data
        raise

    fac_file.write(dumped_data)


def write_chuls_and_chews():
    chus, chews = create_chuls_file()
    write_file('chul.txt', chus)
    write_file('chew.txt', chews)

```"
2718b1accc110be6e85983c6ffc29d8aba0d72cf,analysis/sbx-patch-bug.py,analysis/sbx-patch-bug.py,,"#!/usr/bin/env python
# vim: set sw=2 ts=2 softtabstop=2 expandtab:
""""""
This script is designed to patch old symbooglix
results where hitting speculative paths was incorrect
treated as BOUND_HIT
""""""
import argparse
import os
import logging
import sys
import yaml
from br_util import FinalResultType, classifyResult

try:
  # Try to use libyaml which is faster
  from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
  # fall back on python implementation
  from yaml import Loader, Dumper

def main(args):
  parser = argparse.ArgumentParser()
  parser.add_argument(""-l"",""--log-level"",type=str, default=""info"", dest=""log_level"", choices=['debug','info','warning','error'])
  parser.add_argument('input_yml', type=argparse.FileType('r'))
  parser.add_argument('output_yml', type=str)
  pargs = parser.parse_args(args)

  logLevel = getattr(logging, pargs.log_level.upper(),None)
  logging.basicConfig(level=logLevel)

  if os.path.exists(pargs.output_yml):
    logging.error('Refusing to overwrite ""{}""'.format(pargs.output_yml))
    return 1

  results = yaml.load(pargs.input_yml, Loader=Loader)

  assert isinstance(results, list)

  if len(results) == 0:
    logging.error('Result list is empty')
    return 1

  # Count
  rewriteCount = 0
  rTypeToResultMap = {}
  for rType in FinalResultType:
    rTypeToResultMap[rType] = []

  for r in results:
    rType = classifyResult(r)
    logging.debug('Classified {} as {}'.format(r['program'], rType))

    if rType == FinalResultType.BOUND_HIT:
      logging.info('Classified {} as {}'.format(r['program'], rType))
      logging.info('Doing rewrite')
      rewriteCount += 1

      # Sanity checks
      assert r['failed'] == False
      assert r['bound_hit'] == True
      assert r['speculative_paths_nb'] == True

      # Set new values
      r['failed'] = True
      r['bound_hit'] = False

      assert classifyResult(r) == FinalResultType.UNKNOWN

    rTypeToResultMap[classifyResult(r)].append(r)

  print(""Rewrite count: {}"".format(rewriteCount))

  print(""Total: {}"".format(len(results)))
  for rType in FinalResultType:
    name = rType.name
    resultList = rTypeToResultMap[rType]
    print(""# of {}: {} ({:.2f}%)"".format(name, len(resultList),
      100*float(len(resultList))/len(results)))

  # Write result out
  with open(pargs.output_yml, 'w') as f:
    yamlText = yaml.dump(results,
                         default_flow_style=False,
                         Dumper=Dumper)
    f.write(yamlText)

  return 0

if __name__ == '__main__':
  sys.exit(main(sys.argv[1:]))
",Add script to patch old symbooglix results when boogie-runner classified hitting speculative paths as BOUND_HIT. This was a bad idea so this script changes those results to UNKNOWN but note we have the ``speculative_paths_nb`` attribute so that can be used to find the results where symbooglix hit speculative paths.,"Add script to patch old symbooglix results when boogie-runner classified
hitting speculative paths as BOUND_HIT. This was a bad idea so this
script changes those results to UNKNOWN but note we have the
``speculative_paths_nb`` attribute so that can be used to find the
results where symbooglix hit speculative paths.
",Python,bsd-3-clause,"symbooglix/boogie-runner,symbooglix/boogie-runner",92,"```python
#!/usr/bin/env python
# vim: set sw=2 ts=2 softtabstop=2 expandtab:
""""""
This script is designed to patch old symbooglix
results where hitting speculative paths was incorrect
treated as BOUND_HIT
""""""
import argparse
import os
import logging
import sys
import yaml
from br_util import FinalResultType, classifyResult

try:
  # Try to use libyaml which is faster
  from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
  # fall back on python implementation
  from yaml import Loader, Dumper

def main(args):
  parser = argparse.ArgumentParser()
  parser.add_argument(""-l"",""--log-level"",type=str, default=""info"", dest=""log_level"", choices=['debug','info','warning','error'])
  parser.add_argument('input_yml', type=argparse.FileType('r'))
  parser.add_argument('output_yml', type=str)
  pargs = parser.parse_args(args)

  logLevel = getattr(logging, pargs.log_level.upper(),None)
  logging.basicConfig(level=logLevel)

  if os.path.exists(pargs.output_yml):
    logging.error('Refusing to overwrite ""{}""'.format(pargs.output_yml))
    return 1

  results = yaml.load(pargs.input_yml, Loader=Loader)

  assert isinstance(results, list)

  if len(results) == 0:
    logging.error('Result list is empty')
    return 1

  # Count
  rewriteCount = 0
  rTypeToResultMap = {}
  for rType in FinalResultType:
    rTypeToResultMap[rType] = []

  for r in results:
    rType = classifyResult(r)
    logging.debug('Classified {} as {}'.format(r['program'], rType))

    if rType == FinalResultType.BOUND_HIT:
      logging.info('Classified {} as {}'.format(r['program'], rType))
      logging.info('Doing rewrite')
      rewriteCount += 1

      # Sanity checks
      assert r['failed'] == False
      assert r['bound_hit'] == True
      assert r['speculative_paths_nb'] == True

      # Set new values
      r['failed'] = True
      r['bound_hit'] = False

      assert classifyResult(r) == FinalResultType.UNKNOWN

    rTypeToResultMap[classifyResult(r)].append(r)

  print(""Rewrite count: {}"".format(rewriteCount))

  print(""Total: {}"".format(len(results)))
  for rType in FinalResultType:
    name = rType.name
    resultList = rTypeToResultMap[rType]
    print(""# of {}: {} ({:.2f}%)"".format(name, len(resultList),
      100*float(len(resultList))/len(results)))

  # Write result out
  with open(pargs.output_yml, 'w') as f:
    yamlText = yaml.dump(results,
                         default_flow_style=False,
                         Dumper=Dumper)
    f.write(yamlText)

  return 0

if __name__ == '__main__':
  sys.exit(main(sys.argv[1:]))

```"
1f61ac74ed7a00d642fa94944cbce3ebc4690e9c,scripts/index-g6-in-elasticsearch.py,scripts/index-g6-in-elasticsearch.py,,"#!/usr/bin/python
'''Process G6 JSON files into elasticsearch

This version reads G6 JSON from disk or DM API.

Usage:
    process-g6-into-elastic-search.py <es_endpoint> <dir_or_endpoint> [<token>]

Arguments:
    es_endpoint      Full ES index URL
    dir_or_endpoint  Directory path to import or an API URL if token is given
    token            Digital Marketplace API token

'''

import os
import sys
import json
import urllib2


def post_to_es(es_endpoint, json_data):
    handler = urllib2.HTTPHandler()
    opener = urllib2.build_opener(handler)

    if not es_endpoint.endswith('/'):
        es_endpoint += '/'
    request = urllib2.Request(es_endpoint + str(json_data['id']),
                              data=json.dumps(json_data))
    request.add_header(""Content-Type"", 'application/json')

    print request.get_full_url()
    # print request.get_data()

    try:
        connection = opener.open(request)
    except urllib2.HTTPError, e:
        connection = e
        print connection

    # check. Substitute with appropriate HTTP code.
    if connection.code == 200:
        data = connection.read()
        print str(connection.code) + "" "" + data
    else:
        print ""connection.code = "" + str(connection.code)


def request_services(endpoint, token):
    handler = urllib2.HTTPBasicAuthHandler()
    opener = urllib2.build_opener(handler)

    page_url = endpoint
    while page_url:
        print ""requesting {}"".format(page_url)

        request = urllib2.Request(page_url)
        request.add_header(""Authorization"", ""Bearer {}"".format(token))
        response = opener.open(request).read()

        data = json.loads(response)
        for service in data[""services""]:
            yield service

        page_url = filter(lambda l: l['rel'] == 'next', data['links'])
        if page_url:
            page_url = page_url[0]['href']


def process_json_files_in_directory(dirname):
    for filename in os.listdir(dirname):
        with open(os.path.join(dirname, filename)) as f:
            data = json.loads(f.read())
            print ""doing "" + filename
            yield data


def main():
    if len(sys.argv) == 4:
        es_endpoint, endpoint, token = sys.argv[1:]
        for data in request_services(endpoint, token):
            post_to_es(es_endpoint, data)
    elif len(sys.argv) == 3:
        es_endpoint, listing_dir = sys.argv[1:]
        for data in process_json_files_in_directory(listing_dir):
            post_to_es(es_endpoint, data)
    else:
        print __doc__

if __name__ == '__main__':
    main()
",Move ElasticSearch import script from buyer-frontend into search-api,"Move ElasticSearch import script from buyer-frontend into search-api

This script should have moved when the search-api was split off from the buyer-frontend app.
",Python,mit,"alphagov/digitalmarketplace-search-api,RichardKnop/digitalmarketplace-search-api,alphagov/digitalmarketplace-search-api,RichardKnop/digitalmarketplace-search-api,RichardKnop/digitalmarketplace-search-api,RichardKnop/digitalmarketplace-search-api",92,"```python
#!/usr/bin/python
'''Process G6 JSON files into elasticsearch

This version reads G6 JSON from disk or DM API.

Usage:
    process-g6-into-elastic-search.py <es_endpoint> <dir_or_endpoint> [<token>]

Arguments:
    es_endpoint      Full ES index URL
    dir_or_endpoint  Directory path to import or an API URL if token is given
    token            Digital Marketplace API token

'''

import os
import sys
import json
import urllib2


def post_to_es(es_endpoint, json_data):
    handler = urllib2.HTTPHandler()
    opener = urllib2.build_opener(handler)

    if not es_endpoint.endswith('/'):
        es_endpoint += '/'
    request = urllib2.Request(es_endpoint + str(json_data['id']),
                              data=json.dumps(json_data))
    request.add_header(""Content-Type"", 'application/json')

    print request.get_full_url()
    # print request.get_data()

    try:
        connection = opener.open(request)
    except urllib2.HTTPError, e:
        connection = e
        print connection

    # check. Substitute with appropriate HTTP code.
    if connection.code == 200:
        data = connection.read()
        print str(connection.code) + "" "" + data
    else:
        print ""connection.code = "" + str(connection.code)


def request_services(endpoint, token):
    handler = urllib2.HTTPBasicAuthHandler()
    opener = urllib2.build_opener(handler)

    page_url = endpoint
    while page_url:
        print ""requesting {}"".format(page_url)

        request = urllib2.Request(page_url)
        request.add_header(""Authorization"", ""Bearer {}"".format(token))
        response = opener.open(request).read()

        data = json.loads(response)
        for service in data[""services""]:
            yield service

        page_url = filter(lambda l: l['rel'] == 'next', data['links'])
        if page_url:
            page_url = page_url[0]['href']


def process_json_files_in_directory(dirname):
    for filename in os.listdir(dirname):
        with open(os.path.join(dirname, filename)) as f:
            data = json.loads(f.read())
            print ""doing "" + filename
            yield data


def main():
    if len(sys.argv) == 4:
        es_endpoint, endpoint, token = sys.argv[1:]
        for data in request_services(endpoint, token):
            post_to_es(es_endpoint, data)
    elif len(sys.argv) == 3:
        es_endpoint, listing_dir = sys.argv[1:]
        for data in process_json_files_in_directory(listing_dir):
            post_to_es(es_endpoint, data)
    else:
        print __doc__

if __name__ == '__main__':
    main()

```"
8b0b0fb1e18dae98737a7de65ee014403da71b67,bmi_tester/bmipytest.py,bmi_tester/bmipytest.py,,"#! /usr/bin/env python
import os
import textwrap
import argparse

import pkg_resources
import pytest


def test(package, input_file=None, verbosity=None, bmi_version='1.1'):
    tests = [
        pkg_resources.resource_filename(__name__, os.path.join('tests_pytest'))
    ]
    os.environ['BMITEST_CLASS'] = package
    os.environ['BMITEST_INPUT_FILE'] = input_file
    os.environ['BMI_VERSION_STRING'] = bmi_version

    if verbosity:
        tests += ['-' + 'v' * verbosity]
    pytest.main(tests)


def configure_parser_test(sub_parsers=None):
    help = ""Test a BMI class.""

    example = textwrap.dedent(""""""

    Examples:

    bmi test bmimodule:BmiClass
    """""")

    if sub_parsers is None:
        p = argparse.ArgumentParser(
            description=help,
            fromfile_prefix_chars='@',
            epilog=example,
            formatter_class=argparse.ArgumentDefaultsHelpFormatter
        )
    else:
        p = sub_parsers.add_parser(
            'test',
            help=help,
            description=help,
            epilog=example,
            formatter_class=argparse.ArgumentDefaultsHelpFormatter
        )

    p.add_argument(
        'cls',
        help='Full name of class to test.',
    )
    p.add_argument(
        '--infile',
        default='',
        help='Name of input file for init method.',
    )
    p.add_argument(
        '--bmi-version',
        default='1.1',
        help='BMI version to test against',
    )
    p.add_argument(
        '-v', '--verbose',
        action='count',
        dest='verbose',
        default=1,
        help='increase verbosity',
    )
    p.add_argument(
        '--no-doctests',
        action='store_false',
        dest='doctests',
        default=True,
        help='Do not run doctests in module',
    )
    p.set_defaults(func=execute)

    return p


def execute(args):
    test(args.cls, input_file=args.infile, verbosity=args.verbose,
         bmi_version=args.bmi_version)


def main():
    p = configure_parser_test()

    args = p.parse_args()

    sys.exit(args.func(args))
",Add bmi-test command that uses pytest.,"Add bmi-test command that uses pytest.
",Python,mit,csdms/bmi-tester,93,"```python
#! /usr/bin/env python
import os
import textwrap
import argparse

import pkg_resources
import pytest


def test(package, input_file=None, verbosity=None, bmi_version='1.1'):
    tests = [
        pkg_resources.resource_filename(__name__, os.path.join('tests_pytest'))
    ]
    os.environ['BMITEST_CLASS'] = package
    os.environ['BMITEST_INPUT_FILE'] = input_file
    os.environ['BMI_VERSION_STRING'] = bmi_version

    if verbosity:
        tests += ['-' + 'v' * verbosity]
    pytest.main(tests)


def configure_parser_test(sub_parsers=None):
    help = ""Test a BMI class.""

    example = textwrap.dedent(""""""

    Examples:

    bmi test bmimodule:BmiClass
    """""")

    if sub_parsers is None:
        p = argparse.ArgumentParser(
            description=help,
            fromfile_prefix_chars='@',
            epilog=example,
            formatter_class=argparse.ArgumentDefaultsHelpFormatter
        )
    else:
        p = sub_parsers.add_parser(
            'test',
            help=help,
            description=help,
            epilog=example,
            formatter_class=argparse.ArgumentDefaultsHelpFormatter
        )

    p.add_argument(
        'cls',
        help='Full name of class to test.',
    )
    p.add_argument(
        '--infile',
        default='',
        help='Name of input file for init method.',
    )
    p.add_argument(
        '--bmi-version',
        default='1.1',
        help='BMI version to test against',
    )
    p.add_argument(
        '-v', '--verbose',
        action='count',
        dest='verbose',
        default=1,
        help='increase verbosity',
    )
    p.add_argument(
        '--no-doctests',
        action='store_false',
        dest='doctests',
        default=True,
        help='Do not run doctests in module',
    )
    p.set_defaults(func=execute)

    return p


def execute(args):
    test(args.cls, input_file=args.infile, verbosity=args.verbose,
         bmi_version=args.bmi_version)


def main():
    p = configure_parser_test()

    args = p.parse_args()

    sys.exit(args.func(args))

```"
455806aa0a25f2c632c62f00de21b5d3768135cb,cherrypy/test/test_wsgi_unix_socket.py,cherrypy/test/test_wsgi_unix_socket.py,,"import os
import sys
import socket
import atexit

import cherrypy
from cherrypy.test import helper
from cherrypy._cpcompat import HTTPConnection


USOCKET_PATH = os.path.join(
    os.path.dirname(os.path.realpath(__file__)),
    'cp_test.sock'
)


class USocketHTTPConnection(HTTPConnection):
    """"""
    HTTPConnection over a unix socket.
    """"""

    def __init__(self, path):
        HTTPConnection.__init__(self, 'localhost')
        self.path = path

    def __call__(self, *args, **kwargs):
        """"""
        Catch-all method just to present itself as a constructor for the
        HTTPConnection.
        """"""
        return self

    def connect(self):
        """"""
        Override the connect method and assign a unix socket as a transport.
        """"""
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        sock.connect(self.path)
        self.sock = sock
        atexit.register(lambda: os.remove(self.path))


def skip_on_windows(method):
    """"""
    Decorator to skip the method call if the test is executing on Windows.
    """"""
    def wrapper(self):
        if sys.platform == ""win32"":
            return self.skip(""No UNIX Socket support in Windows."")
        else:
            return method(self)
    wrapper.__doc__ = method.__doc__
    wrapper.__name__ = method.__name__
    return wrapper



class WSGI_UnixSocket_Test(helper.CPWebCase):
    """"""
    Test basic behavior on a cherrypy wsgi server listening
    on a unix socket.

    It exercises the config option `server.socket_file`.
    """"""
    HTTP_CONN = USocketHTTPConnection(USOCKET_PATH)


    @staticmethod
    def setup_server():
        class Root(object):

            @cherrypy.expose
            def index(self):
                return ""Test OK""

            @cherrypy.expose
            def error(self):
                raise Exception(""Invalid page"")

        config = {
            'server.socket_file': USOCKET_PATH
        }
        cherrypy.config.update(config)
        cherrypy.tree.mount(Root())

    def tearDown(self):
        cherrypy.config.update({'server.socket_file': None})

    @skip_on_windows
    def test_simple_request(self):
        self.getPage(""/"")
        self.assertStatus(""200 OK"")
        self.assertInBody(""Test OK"")

    @skip_on_windows
    def test_not_found(self):
        self.getPage(""/invalid_path"")
        self.assertStatus(""404 Not Found"")

    @skip_on_windows
    def test_internal_error(self):
        self.getPage(""/error"")
        self.assertStatus(""500 Internal Server Error"")
        self.assertInBody(""Invalid page"")
",Add a basic test case for the unix socket support in cherrypy.,"Add a basic test case for the unix socket support in cherrypy.
",Python,bsd-3-clause,"Safihre/cherrypy,cherrypy/cherrypy,Safihre/cherrypy,cherrypy/cherrypy,cherrypy/cheroot",105,"```python
import os
import sys
import socket
import atexit

import cherrypy
from cherrypy.test import helper
from cherrypy._cpcompat import HTTPConnection


USOCKET_PATH = os.path.join(
    os.path.dirname(os.path.realpath(__file__)),
    'cp_test.sock'
)


class USocketHTTPConnection(HTTPConnection):
    """"""
    HTTPConnection over a unix socket.
    """"""

    def __init__(self, path):
        HTTPConnection.__init__(self, 'localhost')
        self.path = path

    def __call__(self, *args, **kwargs):
        """"""
        Catch-all method just to present itself as a constructor for the
        HTTPConnection.
        """"""
        return self

    def connect(self):
        """"""
        Override the connect method and assign a unix socket as a transport.
        """"""
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        sock.connect(self.path)
        self.sock = sock
        atexit.register(lambda: os.remove(self.path))


def skip_on_windows(method):
    """"""
    Decorator to skip the method call if the test is executing on Windows.
    """"""
    def wrapper(self):
        if sys.platform == ""win32"":
            return self.skip(""No UNIX Socket support in Windows."")
        else:
            return method(self)
    wrapper.__doc__ = method.__doc__
    wrapper.__name__ = method.__name__
    return wrapper



class WSGI_UnixSocket_Test(helper.CPWebCase):
    """"""
    Test basic behavior on a cherrypy wsgi server listening
    on a unix socket.

    It exercises the config option `server.socket_file`.
    """"""
    HTTP_CONN = USocketHTTPConnection(USOCKET_PATH)


    @staticmethod
    def setup_server():
        class Root(object):

            @cherrypy.expose
            def index(self):
                return ""Test OK""

            @cherrypy.expose
            def error(self):
                raise Exception(""Invalid page"")

        config = {
            'server.socket_file': USOCKET_PATH
        }
        cherrypy.config.update(config)
        cherrypy.tree.mount(Root())

    def tearDown(self):
        cherrypy.config.update({'server.socket_file': None})

    @skip_on_windows
    def test_simple_request(self):
        self.getPage(""/"")
        self.assertStatus(""200 OK"")
        self.assertInBody(""Test OK"")

    @skip_on_windows
    def test_not_found(self):
        self.getPage(""/invalid_path"")
        self.assertStatus(""404 Not Found"")

    @skip_on_windows
    def test_internal_error(self):
        self.getPage(""/error"")
        self.assertStatus(""500 Internal Server Error"")
        self.assertInBody(""Invalid page"")

```"
f9c40170e545e851a8cf6c0f861aa590f1a6078e,tests/unit/modules/inspect_fsdb_test.py,tests/unit/modules/inspect_fsdb_test.py,,"# -*- coding: utf-8 -*-
#
# Copyright 2016 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

'''
    :codeauthor: :email:`Bo Maryniuk <bo@suse.de>`
'''

# Import Python Libs
from __future__ import absolute_import
import os

# Import Salt Testing Libs
from salttesting import TestCase, skipIf
from salttesting.mock import (
    MagicMock,
    patch,
    NO_MOCK,
    NO_MOCK_REASON
)

from salttesting.helpers import ensure_in_syspath
from salt.modules.inspectlib.fsdb import CsvDB
from StringIO import StringIO

ensure_in_syspath('../../')


def mock_open(data=None):
    '''
    Mock ""open"" function in a simple way.

    :param data:
    :return:
    '''
    data = StringIO(data)
    mock = MagicMock(spec=file)
    handle = MagicMock(spec=file)
    handle.write.return_value = None
    handle.__enter__.return_value = data or handle
    mock.return_value = handle

    return mock


@skipIf(NO_MOCK, NO_MOCK_REASON)
class InspectorFSDBTestCase(TestCase):
    '''
    Test case for the FSDB: FileSystem Database.

    FSDB is a very simple object-to-CSV storage with a very inefficient
    update/delete operations (nice to have at some point) and efficient
    storing/reading the objects (what is exactly needed for the functionality).

    Main advantage of FSDB is to store Python objects in just a CSV files,
    and have a very small code base.
    '''

    @patch(""os.makedirs"", MagicMock())
    @patch(""os.listdir"", MagicMock(return_value=['test_db']))
    @patch(""gzip.open"", mock_open(""foo:int,bar:str""))
    def test_open(self):
        '''
        Test opening the database.
        :return:
        '''
        csvdb = CsvDB('/foobar')
        csvdb.open()
        assert csvdb.list_tables() == ['test_db']


    @patch(""os.makedirs"", MagicMock())
    @patch(""os.listdir"", MagicMock(return_value=['test_db']))
    def test_list_databases(self):
        '''
        Test storing object into the database.

        :return:
        '''
        csvdb = CsvDB('/foobar')
        assert csvdb.list() == ['test_db']
",Add initial tests for fsdb,"Add initial tests for fsdb
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",94,"```python
# -*- coding: utf-8 -*-
#
# Copyright 2016 SUSE LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

'''
    :codeauthor: :email:`Bo Maryniuk <bo@suse.de>`
'''

# Import Python Libs
from __future__ import absolute_import
import os

# Import Salt Testing Libs
from salttesting import TestCase, skipIf
from salttesting.mock import (
    MagicMock,
    patch,
    NO_MOCK,
    NO_MOCK_REASON
)

from salttesting.helpers import ensure_in_syspath
from salt.modules.inspectlib.fsdb import CsvDB
from StringIO import StringIO

ensure_in_syspath('../../')


def mock_open(data=None):
    '''
    Mock ""open"" function in a simple way.

    :param data:
    :return:
    '''
    data = StringIO(data)
    mock = MagicMock(spec=file)
    handle = MagicMock(spec=file)
    handle.write.return_value = None
    handle.__enter__.return_value = data or handle
    mock.return_value = handle

    return mock


@skipIf(NO_MOCK, NO_MOCK_REASON)
class InspectorFSDBTestCase(TestCase):
    '''
    Test case for the FSDB: FileSystem Database.

    FSDB is a very simple object-to-CSV storage with a very inefficient
    update/delete operations (nice to have at some point) and efficient
    storing/reading the objects (what is exactly needed for the functionality).

    Main advantage of FSDB is to store Python objects in just a CSV files,
    and have a very small code base.
    '''

    @patch(""os.makedirs"", MagicMock())
    @patch(""os.listdir"", MagicMock(return_value=['test_db']))
    @patch(""gzip.open"", mock_open(""foo:int,bar:str""))
    def test_open(self):
        '''
        Test opening the database.
        :return:
        '''
        csvdb = CsvDB('/foobar')
        csvdb.open()
        assert csvdb.list_tables() == ['test_db']


    @patch(""os.makedirs"", MagicMock())
    @patch(""os.listdir"", MagicMock(return_value=['test_db']))
    def test_list_databases(self):
        '''
        Test storing object into the database.

        :return:
        '''
        csvdb = CsvDB('/foobar')
        assert csvdb.list() == ['test_db']

```"
4e3b01b3f702c60083dd94136263cea9e11fb725,server/daily_backup.py,server/daily_backup.py,,"#!/usr/bin/env python2
# coding: utf-8
'''
A daily backup script
How to use:
0. Init a borg repository using `borg init ...`
1. Fill information in this script
2. Set a daily cron job for this script
3. Sleep. :) You'll be notified once the backup finished or failed.
'''
import os

from requests import post as http_post
from sh import borg, hostname, date

# Please init before using this script
# `borg init ...`
BORG_REPOSITORY = '/your/borg/repo/name'
# Get a push code following this: https://jokerqyou.github.io/ethbot
PUSH_CODE = 'xxxxxx'
# Add the directories you want to backup
DIRECTORIES = (
    '/important/data',
    '/home/user/secret/data',
)
# Add the directories or patterns you want to exclude
EXCLUDES = (
    '*.pyc',
    '*.swp',
    '/i/dont/care/about/this/data',
    '/home/Ben/Music/Justin\ Bieber',
)
HOSTNAME = hostname().strip()
DATE = date('+%Y-%m-%d').strip()


def backup(*directories, **kwargs):
    '''
    Backup a directory using borg
    '''
    directories = [d for d in directories if os.path.exists(d)]
    repository = '{}::{}-{}'.format(BORG_REPOSITORY, HOSTNAME, DATE)
    excludes = kwargs.pop('excludes', [])
    excludes = [excludes, ]\
        if not isinstance(excludes, (list, tuple, ))\
        else excludes
    arguments = ['--stats', '--compression', 'zlib,5', repository, ]
    arguments.extend(directories)
    [arguments.extend(['--exclude', ex]) for ex in excludes]
    borg.create(arguments)


def push_notification(s):
    '''
    Push a notification via Telegram bot
    '''
    http_post(
        'https://eth-nookcloud.rhcloud.com/bot/push',
        data={
            'code': PUSH_CODE,
            'text': s,
        }
    )


def prune():
    '''
    Prune backups to maintain 7 daily,
    4 weekly and 6 monthly archives of THIS machine
    '''
    prefix = '{}-'.format(HOSTNAME)
    borg.prune(
        '-v',
        BORG_REPOSITORY,
        '--prefix', prefix,
        '--keep-daily=7',
        '--keep-weekly=4',
        '--keep-monthly=6'
    )


def main():
    try:
        backup_name = '{}-{}'.format(HOSTNAME, DATE)
        backup(*DIRECTORIES, excludes=EXCLUDES)
    except Exception as e:
        push_notification(u'每日备份失败，错误原因：`{}`'.format(e))
    else:
        push_notification(
            u'每日备份成功，存档名称：`{}::{}`'.format(BORG_REPOSITORY, backup_name)
        )


if __name__ == '__main__':
    main()
",Add server daily backup script,"Add server daily backup script
",Python,bsd-2-clause,"JokerQyou/toolset,JokerQyou/toolset",96,"```python
#!/usr/bin/env python2
# coding: utf-8
'''
A daily backup script
How to use:
0. Init a borg repository using `borg init ...`
1. Fill information in this script
2. Set a daily cron job for this script
3. Sleep. :) You'll be notified once the backup finished or failed.
'''
import os

from requests import post as http_post
from sh import borg, hostname, date

# Please init before using this script
# `borg init ...`
BORG_REPOSITORY = '/your/borg/repo/name'
# Get a push code following this: https://jokerqyou.github.io/ethbot
PUSH_CODE = 'xxxxxx'
# Add the directories you want to backup
DIRECTORIES = (
    '/important/data',
    '/home/user/secret/data',
)
# Add the directories or patterns you want to exclude
EXCLUDES = (
    '*.pyc',
    '*.swp',
    '/i/dont/care/about/this/data',
    '/home/Ben/Music/Justin\ Bieber',
)
HOSTNAME = hostname().strip()
DATE = date('+%Y-%m-%d').strip()


def backup(*directories, **kwargs):
    '''
    Backup a directory using borg
    '''
    directories = [d for d in directories if os.path.exists(d)]
    repository = '{}::{}-{}'.format(BORG_REPOSITORY, HOSTNAME, DATE)
    excludes = kwargs.pop('excludes', [])
    excludes = [excludes, ]\
        if not isinstance(excludes, (list, tuple, ))\
        else excludes
    arguments = ['--stats', '--compression', 'zlib,5', repository, ]
    arguments.extend(directories)
    [arguments.extend(['--exclude', ex]) for ex in excludes]
    borg.create(arguments)


def push_notification(s):
    '''
    Push a notification via Telegram bot
    '''
    http_post(
        'https://eth-nookcloud.rhcloud.com/bot/push',
        data={
            'code': PUSH_CODE,
            'text': s,
        }
    )


def prune():
    '''
    Prune backups to maintain 7 daily,
    4 weekly and 6 monthly archives of THIS machine
    '''
    prefix = '{}-'.format(HOSTNAME)
    borg.prune(
        '-v',
        BORG_REPOSITORY,
        '--prefix', prefix,
        '--keep-daily=7',
        '--keep-weekly=4',
        '--keep-monthly=6'
    )


def main():
    try:
        backup_name = '{}-{}'.format(HOSTNAME, DATE)
        backup(*DIRECTORIES, excludes=EXCLUDES)
    except Exception as e:
        push_notification(u'每日备份失败，错误原因：`{}`'.format(e))
    else:
        push_notification(
            u'每日备份成功，存档名称：`{}::{}`'.format(BORG_REPOSITORY, backup_name)
        )


if __name__ == '__main__':
    main()

```"
173423ee7f43fb7a33db847e2ef3abb34317944e,tests/test_decorators.py,tests/test_decorators.py,,"from dtest import *
from dtest.util import *


@skip
def test_skip():
    pass


@failing
def test_failing():
    pass


@attr(attr1=1, attr2=2)
def test_attr():
    pass


@depends(test_skip, test_failing, test_attr)
def test_depends():
    pass


class DecoratorTestException(Exception):
    pass


@raises(DecoratorTestException)
def test_raises():
    raise DecoratorTestException()


@timed(1)
def test_timed():
    pass


class TestDecorators(DTestCase):
    @istest
    def skip(self):
        # Verify that skip is true...
        assert_true(test_skip.skip)

        # Verify that it's false on something else
        assert_false(test_failing.skip)

    @istest
    def failing(self):
        # Verify that failing is true...
        assert_true(test_failing.failing)

        # Verify that it's false on something else
        assert_false(test_skip.failing)

    @istest
    def attr(self):
        # Verify that the attributes are set as expected
        assert_equal(test_attr.attr1, 1)
        assert_equal(test_attr.attr2, 2)

    @istest
    def depends(self):
        # Part 1: Verify that test_depends() is dependent on
        # test_skip(), test_failing(), and test_attr()
        assert_in(test_skip, test_depends.dependencies)
        assert_in(test_failing, test_depends.dependencies)
        assert_in(test_attr, test_depends.dependencies)

        # Part 2: Verify that test_depends() is in the depedents set
        # of test_skip(), test_failing(), and test_attr()
        assert_in(test_depends, test_skip.dependents)
        assert_in(test_depends, test_failing.dependents)
        assert_in(test_depends, test_attr.dependents)

    @istest
    def raises(self):
        # Verify that the set of expected exceptions is as expected
        assert_set_equal(test_raises.raises, set([DecoratorTestException]))

        # Verify that it's the empty set on something else
        assert_set_equal(test_timed.raises, set())

    @istest
    def timed(self):
        # Verify that the timeout is set properly
        assert_equal(test_timed.timeout, 1)

        # Verify that it's None on something else
        assert_is_none(test_raises.timeout)
",Add tests to verify the proper functioning of decorators,"Add tests to verify the proper functioning of decorators
",Python,apache-2.0,"klmitch/dtest,klmitch/dtest",91,"```python
from dtest import *
from dtest.util import *


@skip
def test_skip():
    pass


@failing
def test_failing():
    pass


@attr(attr1=1, attr2=2)
def test_attr():
    pass


@depends(test_skip, test_failing, test_attr)
def test_depends():
    pass


class DecoratorTestException(Exception):
    pass


@raises(DecoratorTestException)
def test_raises():
    raise DecoratorTestException()


@timed(1)
def test_timed():
    pass


class TestDecorators(DTestCase):
    @istest
    def skip(self):
        # Verify that skip is true...
        assert_true(test_skip.skip)

        # Verify that it's false on something else
        assert_false(test_failing.skip)

    @istest
    def failing(self):
        # Verify that failing is true...
        assert_true(test_failing.failing)

        # Verify that it's false on something else
        assert_false(test_skip.failing)

    @istest
    def attr(self):
        # Verify that the attributes are set as expected
        assert_equal(test_attr.attr1, 1)
        assert_equal(test_attr.attr2, 2)

    @istest
    def depends(self):
        # Part 1: Verify that test_depends() is dependent on
        # test_skip(), test_failing(), and test_attr()
        assert_in(test_skip, test_depends.dependencies)
        assert_in(test_failing, test_depends.dependencies)
        assert_in(test_attr, test_depends.dependencies)

        # Part 2: Verify that test_depends() is in the depedents set
        # of test_skip(), test_failing(), and test_attr()
        assert_in(test_depends, test_skip.dependents)
        assert_in(test_depends, test_failing.dependents)
        assert_in(test_depends, test_attr.dependents)

    @istest
    def raises(self):
        # Verify that the set of expected exceptions is as expected
        assert_set_equal(test_raises.raises, set([DecoratorTestException]))

        # Verify that it's the empty set on something else
        assert_set_equal(test_timed.raises, set())

    @istest
    def timed(self):
        # Verify that the timeout is set properly
        assert_equal(test_timed.timeout, 1)

        # Verify that it's None on something else
        assert_is_none(test_raises.timeout)

```"
d956cfb04a1ae3c84460cec224dd4f5aba4012e1,lintcode/Medium/104_Merge_k_Sorted_Lists.py,lintcode/Medium/104_Merge_k_Sorted_Lists.py,,"""""""
Definition of ListNode
class ListNode(object):

    def __init__(self, val, next=None):
        self.val = val
        self.next = next
""""""
class Solution:
    """"""
    @param lists: a list of ListNode
    @return: The head of one sorted list.
    """"""
    def mergeKLists(self, lists):
        # write your code here

        # Solution 1 (TLE)
        # if (not lists):
        #     return None
        # heads = []
        # dummy = ListNode(0)
        # tmp = dummy
        # for l in lists:
        #     if (l):
        #         heads.append(l)
        # while(tmp):
        #     mini = None
        #     index = None
        #     for i, h in enumerate(heads):
        #         if (h and (mini is None or h.val < mini.val)):
        #             mini = h
        #             index = i
        #     if (index >= 0):
        #         if (heads[index].next is None):
        #             heads.pop(index)
        #         else:
        #             heads[index] = heads[index].next
        #     tmp.next = mini
        #     tmp = tmp.next
        # return dummy.next


        # Solution 2
        # def mergeTwoList(h1, h2):
        #     dummy = ListNode(0)
        #     tmp = dummy
        #     while (h1 and h2):
        #         if (h1.val < h2.val):
        #             tmp.next = h1
        #             h1 = h1.next
        #         else:
        #             tmp.next = h2
        #             h2 = h2.next
        #         tmp = tmp.next
        #     if (not h1):
        #         tmp.next = h2
        #     if (not h2):
        #         tmp.next = h1
        #     return dummy.next

        # if (not lists):
        #     return None
        # end = len(lists) - 1
        # while (end > 0):
        #     start = 0
        #     while (start < end):
        #         lists[start] = mergeTwoList(lists[start], lists[end])
        #         start += 1
        #         end -= 1
        # return lists[0]

        # Solution 3
        q = []
        dummy = ListNode(0)
        tmp = dummy
        def insertNode(n, arr):
            start = 0
            end = len(arr) - 1
            if (len(arr) == 0):
                arr.append(n)
                return True
            while (start < end):
                mid = (start + end) / 2
                if (arr[mid].val == n.val):
                    arr.insert(mid + 1, n)
                    return True
                elif (arr[mid].val > n.val):
                    end = mid - 1
                else:
                    start = mid + 1
            if (arr[start].val < n.val):
                arr.insert(start+1, n)
            else:
                arr.insert(start, n)
        for node in lists:
            if (node):
                insertNode(node, q)
        while (q):
            node = q.pop(0)
            if (node.next):
                insertNode(node.next, q)
            tmp.next = node
            tmp = tmp.next
        return dummy.next
",Add solution to lintcode question 104,"Add solution to lintcode question 104
",Python,mit,"Rhadow/leetcode,Rhadow/leetcode,Rhadow/leetcode,Rhadow/leetcode",105,"```python
""""""
Definition of ListNode
class ListNode(object):

    def __init__(self, val, next=None):
        self.val = val
        self.next = next
""""""
class Solution:
    """"""
    @param lists: a list of ListNode
    @return: The head of one sorted list.
    """"""
    def mergeKLists(self, lists):
        # write your code here

        # Solution 1 (TLE)
        # if (not lists):
        #     return None
        # heads = []
        # dummy = ListNode(0)
        # tmp = dummy
        # for l in lists:
        #     if (l):
        #         heads.append(l)
        # while(tmp):
        #     mini = None
        #     index = None
        #     for i, h in enumerate(heads):
        #         if (h and (mini is None or h.val < mini.val)):
        #             mini = h
        #             index = i
        #     if (index >= 0):
        #         if (heads[index].next is None):
        #             heads.pop(index)
        #         else:
        #             heads[index] = heads[index].next
        #     tmp.next = mini
        #     tmp = tmp.next
        # return dummy.next


        # Solution 2
        # def mergeTwoList(h1, h2):
        #     dummy = ListNode(0)
        #     tmp = dummy
        #     while (h1 and h2):
        #         if (h1.val < h2.val):
        #             tmp.next = h1
        #             h1 = h1.next
        #         else:
        #             tmp.next = h2
        #             h2 = h2.next
        #         tmp = tmp.next
        #     if (not h1):
        #         tmp.next = h2
        #     if (not h2):
        #         tmp.next = h1
        #     return dummy.next

        # if (not lists):
        #     return None
        # end = len(lists) - 1
        # while (end > 0):
        #     start = 0
        #     while (start < end):
        #         lists[start] = mergeTwoList(lists[start], lists[end])
        #         start += 1
        #         end -= 1
        # return lists[0]

        # Solution 3
        q = []
        dummy = ListNode(0)
        tmp = dummy
        def insertNode(n, arr):
            start = 0
            end = len(arr) - 1
            if (len(arr) == 0):
                arr.append(n)
                return True
            while (start < end):
                mid = (start + end) / 2
                if (arr[mid].val == n.val):
                    arr.insert(mid + 1, n)
                    return True
                elif (arr[mid].val > n.val):
                    end = mid - 1
                else:
                    start = mid + 1
            if (arr[start].val < n.val):
                arr.insert(start+1, n)
            else:
                arr.insert(start, n)
        for node in lists:
            if (node):
                insertNode(node, q)
        while (q):
            node = q.pop(0)
            if (node.next):
                insertNode(node.next, q)
            tmp.next = node
            tmp = tmp.next
        return dummy.next

```"
4b6e1b2426efa4f96dc1120718b4acbfbcdbee98,numba/tests/test_caching.py,numba/tests/test_caching.py,,"from __future__ import print_function, absolute_import, division

import sys
import os
import multiprocessing as mp
import traceback

from numba import njit
from .support import (
    TestCase,
    temp_directory,
    override_env_config,
    captured_stdout,
    captured_stderr,
)


def constant_unicode_cache():
    c = ""abcd""
    return hash(c), c


def check_constant_unicode_cache():
    pyfunc = constant_unicode_cache
    cfunc = njit(cache=True)(pyfunc)
    exp_hv, exp_str = pyfunc()
    got_hv, got_str = cfunc()
    assert exp_hv == got_hv
    assert exp_str == got_str


def dict_cache():
    return {'a': 1, 'b': 2}


def check_dict_cache():
    pyfunc = dict_cache
    cfunc = njit(cache=True)(pyfunc)
    exp = pyfunc()
    got = cfunc()
    assert exp == got


class TestCaching(TestCase):
    def run_test(self, func):
        func()
        ctx = mp.get_context('spawn')
        qout = ctx.Queue()
        cache_dir = temp_directory(__name__)
        with override_env_config('NUMBA_CACHE_DIR', cache_dir):
            proc = ctx.Process(target=_remote_runner, args=[func, qout])
            proc.start()
            stdout = qout.get()
            stderr = qout.get()
            if stdout.strip():
                print()
                print('STDOUT'.center(80, '-'))
                print(stdout)
            if stderr.strip():
                print()
                print('STDERR'.center(80, '-'))
                print(stderr)
            proc.join()
            self.assertEqual(proc.exitcode, 0)


    # The following is used to auto populate test methods into this class

    def _make_test(fn):
        def udt(self):
            self.run_test(fn)
        return udt

    for k, v in globals().items():
        prefix = 'check_'
        if k.startswith(prefix):
            locals()['test_' + k[len(prefix):]] = _make_test(v)


def _remote_runner(fn, qout):
    with captured_stderr() as stderr:
        with captured_stdout() as stdout:
            try:
                fn()
            except Exception:
                print(traceback.format_exc(), file=sys.stderr)
                exitcode = 1
            else:
                exitcode = 0
        qout.put(stdout.getvalue())
    qout.put(stderr.getvalue())
    sys.exit(exitcode)


def _remote_wrapper(fn):
    _remote_wrapper()

",Add test for unicode cache,"Add test for unicode cache
",Python,bsd-2-clause,"jriehl/numba,IntelLabs/numba,jriehl/numba,cpcloud/numba,IntelLabs/numba,gmarkall/numba,IntelLabs/numba,gmarkall/numba,jriehl/numba,sklam/numba,sklam/numba,seibert/numba,numba/numba,stuartarchibald/numba,seibert/numba,cpcloud/numba,stonebig/numba,seibert/numba,cpcloud/numba,gmarkall/numba,numba/numba,gmarkall/numba,stuartarchibald/numba,sklam/numba,stonebig/numba,numba/numba,stonebig/numba,IntelLabs/numba,stonebig/numba,stuartarchibald/numba,stuartarchibald/numba,seibert/numba,stuartarchibald/numba,sklam/numba,IntelLabs/numba,gmarkall/numba,jriehl/numba,cpcloud/numba,sklam/numba,cpcloud/numba,numba/numba,jriehl/numba,seibert/numba,numba/numba,stonebig/numba",98,"```python
from __future__ import print_function, absolute_import, division

import sys
import os
import multiprocessing as mp
import traceback

from numba import njit
from .support import (
    TestCase,
    temp_directory,
    override_env_config,
    captured_stdout,
    captured_stderr,
)


def constant_unicode_cache():
    c = ""abcd""
    return hash(c), c


def check_constant_unicode_cache():
    pyfunc = constant_unicode_cache
    cfunc = njit(cache=True)(pyfunc)
    exp_hv, exp_str = pyfunc()
    got_hv, got_str = cfunc()
    assert exp_hv == got_hv
    assert exp_str == got_str


def dict_cache():
    return {'a': 1, 'b': 2}


def check_dict_cache():
    pyfunc = dict_cache
    cfunc = njit(cache=True)(pyfunc)
    exp = pyfunc()
    got = cfunc()
    assert exp == got


class TestCaching(TestCase):
    def run_test(self, func):
        func()
        ctx = mp.get_context('spawn')
        qout = ctx.Queue()
        cache_dir = temp_directory(__name__)
        with override_env_config('NUMBA_CACHE_DIR', cache_dir):
            proc = ctx.Process(target=_remote_runner, args=[func, qout])
            proc.start()
            stdout = qout.get()
            stderr = qout.get()
            if stdout.strip():
                print()
                print('STDOUT'.center(80, '-'))
                print(stdout)
            if stderr.strip():
                print()
                print('STDERR'.center(80, '-'))
                print(stderr)
            proc.join()
            self.assertEqual(proc.exitcode, 0)


    # The following is used to auto populate test methods into this class

    def _make_test(fn):
        def udt(self):
            self.run_test(fn)
        return udt

    for k, v in globals().items():
        prefix = 'check_'
        if k.startswith(prefix):
            locals()['test_' + k[len(prefix):]] = _make_test(v)


def _remote_runner(fn, qout):
    with captured_stderr() as stderr:
        with captured_stdout() as stdout:
            try:
                fn()
            except Exception:
                print(traceback.format_exc(), file=sys.stderr)
                exitcode = 1
            else:
                exitcode = 0
        qout.put(stdout.getvalue())
    qout.put(stderr.getvalue())
    sys.exit(exitcode)


def _remote_wrapper(fn):
    _remote_wrapper()


```"
a206807606130d9eb1d55b9b54ec03b3fa3f1816,test/integrationtests/skills/single_test.py,test/integrationtests/skills/single_test.py,,"# Copyright 2017 Mycroft AI Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""""" Test a single skill

    python single_test.py PATH_TO_SKILL
""""""

import glob
import unittest
import os
from test.integrationtests.skills.skill_tester import MockSkillsLoader
from test.integrationtests.skills.skill_tester import SkillTest

import sys
d = sys.argv.pop() + '/'
HOME_DIR = os.path.dirname(d)


def discover_tests():
    """"""Find skills whith test files

    For all skills with test files, starten from current directory,
    find the test files in subdirectory test/intent.

    :return: skills and corresponding test case files found
    """"""

    tests = {}

    skills = [HOME_DIR]

    for skill in skills:
        test_intent_files = [
            f for f
            in glob.glob(os.path.join(skill, 'test/intent/*.intent.json'))
        ]
        if len(test_intent_files) > 0:
            tests[skill] = test_intent_files

    return tests


class IntentTestSequenceMeta(type):
    def __new__(mcs, name, bases, d):
        def gen_test(a, b):
            def test(self):
                if not SkillTest(a, b, self.emitter).run(self.loader):
                    assert False
            return test

        tests = discover_tests()
        for skill in tests.keys():
            skill_name = os.path.basename(skill)  # Path of the skill
            for example in tests[skill]:
                # Name of the intent
                example_name = os.path.basename(
                    os.path.splitext(os.path.splitext(example)[0])[0])
                test_name = ""test_IntentValidation[%s:%s]"" % (skill_name,
                                                              example_name)
                d[test_name] = gen_test(skill, example)
        return type.__new__(mcs, name, bases, d)


class IntentTestSequence(unittest.TestCase):
    """"""This is the TestCase class that pythons unit tester can execute.
    """"""
    __metaclass__ = IntentTestSequenceMeta
    loader = None

    @classmethod
    def setUpClass(cls):
        cls.loader = MockSkillsLoader(HOME_DIR)
        cls.emitter = cls.loader.load_skills()

    @classmethod
    def tearDownClass(cls):
        cls.loader.unload_skills()


if __name__ == '__main__':
    unittest.main()
",Add simple script for running a single skill,"Add simple script for running a single skill

Based on the skill_developers_testrunner.py, takes the path to a skill
as argument and runs any tests for that skill.
",Python,apache-2.0,"MycroftAI/mycroft-core,linuxipho/mycroft-core,forslund/mycroft-core,Dark5ide/mycroft-core,forslund/mycroft-core,MycroftAI/mycroft-core,Dark5ide/mycroft-core,linuxipho/mycroft-core",94,"```python
# Copyright 2017 Mycroft AI Inc.
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""""" Test a single skill

    python single_test.py PATH_TO_SKILL
""""""

import glob
import unittest
import os
from test.integrationtests.skills.skill_tester import MockSkillsLoader
from test.integrationtests.skills.skill_tester import SkillTest

import sys
d = sys.argv.pop() + '/'
HOME_DIR = os.path.dirname(d)


def discover_tests():
    """"""Find skills whith test files

    For all skills with test files, starten from current directory,
    find the test files in subdirectory test/intent.

    :return: skills and corresponding test case files found
    """"""

    tests = {}

    skills = [HOME_DIR]

    for skill in skills:
        test_intent_files = [
            f for f
            in glob.glob(os.path.join(skill, 'test/intent/*.intent.json'))
        ]
        if len(test_intent_files) > 0:
            tests[skill] = test_intent_files

    return tests


class IntentTestSequenceMeta(type):
    def __new__(mcs, name, bases, d):
        def gen_test(a, b):
            def test(self):
                if not SkillTest(a, b, self.emitter).run(self.loader):
                    assert False
            return test

        tests = discover_tests()
        for skill in tests.keys():
            skill_name = os.path.basename(skill)  # Path of the skill
            for example in tests[skill]:
                # Name of the intent
                example_name = os.path.basename(
                    os.path.splitext(os.path.splitext(example)[0])[0])
                test_name = ""test_IntentValidation[%s:%s]"" % (skill_name,
                                                              example_name)
                d[test_name] = gen_test(skill, example)
        return type.__new__(mcs, name, bases, d)


class IntentTestSequence(unittest.TestCase):
    """"""This is the TestCase class that pythons unit tester can execute.
    """"""
    __metaclass__ = IntentTestSequenceMeta
    loader = None

    @classmethod
    def setUpClass(cls):
        cls.loader = MockSkillsLoader(HOME_DIR)
        cls.emitter = cls.loader.load_skills()

    @classmethod
    def tearDownClass(cls):
        cls.loader.unload_skills()


if __name__ == '__main__':
    unittest.main()

```"
1b8edb32664e63d1dc2ad9af295395f62d08f4bd,crackingcointsolutions/chapter2/exercisesix.py,crackingcointsolutions/chapter2/exercisesix.py,,"'''
Created on 22 Aug 2017

Palindrome: implement a function to check if a linked list is a palindrome

Assumptions: each node of the list contains a single letter.

1) Invert-copy list, gives also back list size
2) compare until middle: size/2

@author: igoroya
'''
from chapter2 import utils # TODO: try relative imports


def invert_list(my_list):
    ""returns inverted list and size of it""
    inv_list = utils.SinglyLinkedList()
    node = my_list.head_node
    entries = 0

    while node is not None:
        inv_list.add_in_front(node.cargo)
        entries += 1
        node = node.next_node
    return inv_list, entries

def are_same_until_middle(list1, list2, size):
    node1 = list1.head_node
    node2 = list2.head_node

    for _ in range(int(1 + size/2)):
        if node1.cargo != node2.cargo:
            return False
        node1 = node1.next_node
        node2 = node2.next_node

    return True

def is_palindrome(orig_list):
    inv_list, size = invert_list(orig_list)

    return are_same_until_middle(orig_list, inv_list, size)


if __name__ == '__main__':
    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('b')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('b')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")


    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('a')
    list1.append('b')
    list1.append('a')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('c')
    list1.append('d')
    list1.append('e')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('c')
    list1.append('b')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")


",Add exercise six of chapter two,Add exercise six of chapter two,Python,mit,igoroya/igor-oya-solutions-cracking-coding-interview,98,"```python
'''
Created on 22 Aug 2017

Palindrome: implement a function to check if a linked list is a palindrome

Assumptions: each node of the list contains a single letter.

1) Invert-copy list, gives also back list size
2) compare until middle: size/2

@author: igoroya
'''
from chapter2 import utils # TODO: try relative imports


def invert_list(my_list):
    ""returns inverted list and size of it""
    inv_list = utils.SinglyLinkedList()
    node = my_list.head_node
    entries = 0

    while node is not None:
        inv_list.add_in_front(node.cargo)
        entries += 1
        node = node.next_node
    return inv_list, entries

def are_same_until_middle(list1, list2, size):
    node1 = list1.head_node
    node2 = list2.head_node

    for _ in range(int(1 + size/2)):
        if node1.cargo != node2.cargo:
            return False
        node1 = node1.next_node
        node2 = node2.next_node

    return True

def is_palindrome(orig_list):
    inv_list, size = invert_list(orig_list)

    return are_same_until_middle(orig_list, inv_list, size)


if __name__ == '__main__':
    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('b')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('b')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")


    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('a')
    list1.append('b')
    list1.append('a')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('c')
    list1.append('d')
    list1.append('e')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")

    list1 = utils.SinglyLinkedList()
    list1.append('a')
    list1.append('b')
    list1.append('c')
    list1.append('b')
    list1.append('a')
    print(""List {} is palindrome? {}"".format(list1, is_palindrome(list1)))
    print(""----"")



```"
2f611cccedfcb75dd23167f27d556f4f9b3e9a70,quantecon/optimize/tests/test_root_finding.py,quantecon/optimize/tests/test_root_finding.py,,"import numpy as np
from numpy.testing import assert_almost_equal, assert_allclose
from numba import njit

from quantecon.optimize import newton, newton_secant

@njit
def func(x):
    """"""
    Function for testing on.
    """"""
    return (x**3 - 1)


@njit
def func_prime(x):
    """"""
    Derivative for func.
    """"""
    return (3*x**2)


@njit
def func_two(x):
    """"""
    Harder function for testing on.
    """"""
    return np.sin(4 * (x - 1/4)) + x + x**20 - 1


@njit
def func_two_prime(x):
    """"""
    Derivative for func_two.
    """"""
    return 4*np.cos(4*(x - 1/4)) + 20*x**19 + 1


def test_newton_basic():
    """"""
    Uses the function f defined above to test the scalar maximization 
    routine.
    """"""
    true_fval = 1.0
    fval = newton(func, 5, func_prime)
    assert_almost_equal(true_fval, fval.root, decimal=4)
    

def test_newton_basic_two():
    """"""
    Uses the function f defined above to test the scalar maximization 
    routine.
    """"""
    true_fval = 1.0
    fval = newton(func, 5, func_prime)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0)
    
    
def test_newton_hard():
    """"""
    Harder test for convergence.
    """"""
    true_fval = 0.408
    fval = newton(func_two, 0.4, func_two_prime)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0.01)
    

def test_secant_basic():
    """"""
    Basic test for secant option.
    """"""
    true_fval = 1.0
    fval = newton_secant(func, 5)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0.001)


def test_secant_hard():
    """"""
    Harder test for convergence for secant function.
    """"""
    true_fval = 0.408
    fval = newton_secant(func_two, 0.4)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0.01)


# executing testcases.

if __name__ == '__main__':
    import sys
    import nose

    argv = sys.argv[:]
    argv.append('--verbose')
    argv.append('--nocapture')
    nose.main(argv=argv, defaultTest=__file__)
",Add test cases for newton methods,"Add test cases for newton methods
",Python,bsd-3-clause,"oyamad/QuantEcon.py,QuantEcon/QuantEcon.py,QuantEcon/QuantEcon.py,oyamad/QuantEcon.py",96,"```python
import numpy as np
from numpy.testing import assert_almost_equal, assert_allclose
from numba import njit

from quantecon.optimize import newton, newton_secant

@njit
def func(x):
    """"""
    Function for testing on.
    """"""
    return (x**3 - 1)


@njit
def func_prime(x):
    """"""
    Derivative for func.
    """"""
    return (3*x**2)


@njit
def func_two(x):
    """"""
    Harder function for testing on.
    """"""
    return np.sin(4 * (x - 1/4)) + x + x**20 - 1


@njit
def func_two_prime(x):
    """"""
    Derivative for func_two.
    """"""
    return 4*np.cos(4*(x - 1/4)) + 20*x**19 + 1


def test_newton_basic():
    """"""
    Uses the function f defined above to test the scalar maximization 
    routine.
    """"""
    true_fval = 1.0
    fval = newton(func, 5, func_prime)
    assert_almost_equal(true_fval, fval.root, decimal=4)
    

def test_newton_basic_two():
    """"""
    Uses the function f defined above to test the scalar maximization 
    routine.
    """"""
    true_fval = 1.0
    fval = newton(func, 5, func_prime)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0)
    
    
def test_newton_hard():
    """"""
    Harder test for convergence.
    """"""
    true_fval = 0.408
    fval = newton(func_two, 0.4, func_two_prime)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0.01)
    

def test_secant_basic():
    """"""
    Basic test for secant option.
    """"""
    true_fval = 1.0
    fval = newton_secant(func, 5)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0.001)


def test_secant_hard():
    """"""
    Harder test for convergence for secant function.
    """"""
    true_fval = 0.408
    fval = newton_secant(func_two, 0.4)
    assert_allclose(true_fval, fval.root, rtol=1e-5, atol=0.01)


# executing testcases.

if __name__ == '__main__':
    import sys
    import nose

    argv = sys.argv[:]
    argv.append('--verbose')
    argv.append('--nocapture')
    nose.main(argv=argv, defaultTest=__file__)

```"
58ab8c317c71aa2c5fc8d47d2bb9e4778290482e,bayespy/inference/vmp/nodes/tests/test_beta.py,bayespy/inference/vmp/nodes/tests/test_beta.py,,"######################################################################
# Copyright (C) 2014 Jaakko Luttinen
#
# This file is licensed under Version 3.0 of the GNU General Public
# License. See LICENSE for a text of the license.
######################################################################

######################################################################
# This file is part of BayesPy.
#
# BayesPy is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License version 3 as
# published by the Free Software Foundation.
#
# BayesPy is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with BayesPy.  If not, see <http://www.gnu.org/licenses/>.
######################################################################

""""""
Unit tests for `beta` module.
""""""

import numpy as np
from scipy import special

from bayespy.nodes import Beta

from bayespy.utils import utils
from bayespy.utils import random

from bayespy.utils.utils import TestCase

class TestBinomial(TestCase):
    """"""
    Unit tests for Binomial node
    """"""

    
    def test_init(self):
        """"""
        Test the creation of binomial nodes.
        """"""

        # Some simple initializations
        p = Beta([1.5, 4.2])

        # Check that plates are correct
        p = Beta([2, 3], plates=(4,3))
        self.assertEqual(p.plates,
                         (4,3))
        p = Beta(np.ones((4,3,2)))
        self.assertEqual(p.plates,
                         (4,3))

        # Parent not a vector
        self.assertRaises(ValueError,
                          Beta,
                          4)
        
        # Parent vector has wrong shape
        self.assertRaises(ValueError,
                          Beta,
                          [4])
        self.assertRaises(ValueError,
                          Beta,
                          [4,4,4])

        # Parent vector has invalid values
        self.assertRaises(ValueError,
                          Beta,
                          [-2,3])

        # Plates inconsistent
        self.assertRaises(ValueError,
                          Beta,
                          np.ones((4,2)),
                          plates=(3,))

        # Explicit plates too small
        self.assertRaises(ValueError,
                          Beta,
                          np.ones((4,2)),
                          plates=(1,))

        pass

    
    def test_moments(self):
        """"""
        Test the moments of binomial nodes.
        """"""

        p = Beta([2, 3])
        u = p._message_to_child()
        self.assertAllClose(u[0],
                            special.psi([2,3]) - special.psi(2+3))
        
        pass

    

",Add unit tests for Beta node,"TST: Add unit tests for Beta node
",Python,mit,"fivejjs/bayespy,SalemAmeen/bayespy,bayespy/bayespy,jluttine/bayespy",107,"```python
######################################################################
# Copyright (C) 2014 Jaakko Luttinen
#
# This file is licensed under Version 3.0 of the GNU General Public
# License. See LICENSE for a text of the license.
######################################################################

######################################################################
# This file is part of BayesPy.
#
# BayesPy is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License version 3 as
# published by the Free Software Foundation.
#
# BayesPy is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with BayesPy.  If not, see <http://www.gnu.org/licenses/>.
######################################################################

""""""
Unit tests for `beta` module.
""""""

import numpy as np
from scipy import special

from bayespy.nodes import Beta

from bayespy.utils import utils
from bayespy.utils import random

from bayespy.utils.utils import TestCase

class TestBinomial(TestCase):
    """"""
    Unit tests for Binomial node
    """"""

    
    def test_init(self):
        """"""
        Test the creation of binomial nodes.
        """"""

        # Some simple initializations
        p = Beta([1.5, 4.2])

        # Check that plates are correct
        p = Beta([2, 3], plates=(4,3))
        self.assertEqual(p.plates,
                         (4,3))
        p = Beta(np.ones((4,3,2)))
        self.assertEqual(p.plates,
                         (4,3))

        # Parent not a vector
        self.assertRaises(ValueError,
                          Beta,
                          4)
        
        # Parent vector has wrong shape
        self.assertRaises(ValueError,
                          Beta,
                          [4])
        self.assertRaises(ValueError,
                          Beta,
                          [4,4,4])

        # Parent vector has invalid values
        self.assertRaises(ValueError,
                          Beta,
                          [-2,3])

        # Plates inconsistent
        self.assertRaises(ValueError,
                          Beta,
                          np.ones((4,2)),
                          plates=(3,))

        # Explicit plates too small
        self.assertRaises(ValueError,
                          Beta,
                          np.ones((4,2)),
                          plates=(1,))

        pass

    
    def test_moments(self):
        """"""
        Test the moments of binomial nodes.
        """"""

        p = Beta([2, 3])
        u = p._message_to_child()
        self.assertAllClose(u[0],
                            special.psi([2,3]) - special.psi(2+3))
        
        pass

    


```"
4f7af6fc11e529ffd2dd81690797a913163bea5f,warehouse/database/types.py,warehouse/database/types.py,,"from __future__ import absolute_import
from __future__ import division
from __future__ import unicode_literals

import re

from sqlalchemy.types import SchemaType, TypeDecorator
from sqlalchemy.types import Enum as SQLAEnum


class EnumSymbol(object):
    """"""
    Define a fixed symbol tied to a parent class.
    """"""

    def __init__(self, cls_, name, value, description):
        self.cls_ = cls_
        self.name = name
        self.value = value
        self.description = description

    def __reduce__(self):
        """"""
        Allow unpickling to return the symbol linked to the Enum class.
        """"""
        return getattr, (self.cls_, self.name)

    def __iter__(self):
        return iter([self.value, self.description])

    def __repr__(self):
        return ""<%s>"" % self.name


class EnumMeta(type):
    """"""
    Generate new Enum classes.
    """"""

    def __init__(cls, classname, bases, dict_):
        cls._reg = reg = cls._reg.copy()
        for k, v in dict_.items():
            if isinstance(v, tuple):
                sym = reg[v[0]] = EnumSymbol(cls, k, *v)
                setattr(cls, k, sym)
        return type.__init__(cls, classname, bases, dict_)

    def __iter__(cls):
        return iter(cls._reg.values())


class EnumType(SchemaType, TypeDecorator):
    def __init__(self, enum):
        self.enum = enum
        self.impl = SQLAEnum(
                        *enum.values(),
                        name=""ck%s"" % re.sub(
                            ""([A-Z])"",
                            lambda m: ""_"" + m.group(1).lower(),
                            enum.__name__))

    def _set_table(self, table, column):
        self.impl._set_table(table, column)

    def copy(self):
        return EnumType(self.enum)

    def process_bind_param(self, value, dialect):
        if value is None:
            return None
        return value.value

    def process_result_value(self, value, dialect):
        if value is None:
            return None
        return self.enum.from_string(value.strip())


class Enum(object):
    """"""
    Declarative enumeration.
    """"""

    __metaclass__ = EnumMeta
    _reg = {}

    @classmethod
    def from_string(cls, value):
        try:
            return cls._reg[value]
        except KeyError:
            raise ValueError(
                    ""Invalid value for %r: %r"" %
                    (cls.__name__, value)
                )

    @classmethod
    def values(cls):
        return cls._reg.keys()

    @classmethod
    def db_type(cls):
        return EnumType(cls)
",Add an Enum type that is in Python as well as SQL,"Add an Enum type that is in Python as well as SQL
",Python,bsd-2-clause,davidfischer/warehouse,104,"```python
from __future__ import absolute_import
from __future__ import division
from __future__ import unicode_literals

import re

from sqlalchemy.types import SchemaType, TypeDecorator
from sqlalchemy.types import Enum as SQLAEnum


class EnumSymbol(object):
    """"""
    Define a fixed symbol tied to a parent class.
    """"""

    def __init__(self, cls_, name, value, description):
        self.cls_ = cls_
        self.name = name
        self.value = value
        self.description = description

    def __reduce__(self):
        """"""
        Allow unpickling to return the symbol linked to the Enum class.
        """"""
        return getattr, (self.cls_, self.name)

    def __iter__(self):
        return iter([self.value, self.description])

    def __repr__(self):
        return ""<%s>"" % self.name


class EnumMeta(type):
    """"""
    Generate new Enum classes.
    """"""

    def __init__(cls, classname, bases, dict_):
        cls._reg = reg = cls._reg.copy()
        for k, v in dict_.items():
            if isinstance(v, tuple):
                sym = reg[v[0]] = EnumSymbol(cls, k, *v)
                setattr(cls, k, sym)
        return type.__init__(cls, classname, bases, dict_)

    def __iter__(cls):
        return iter(cls._reg.values())


class EnumType(SchemaType, TypeDecorator):
    def __init__(self, enum):
        self.enum = enum
        self.impl = SQLAEnum(
                        *enum.values(),
                        name=""ck%s"" % re.sub(
                            ""([A-Z])"",
                            lambda m: ""_"" + m.group(1).lower(),
                            enum.__name__))

    def _set_table(self, table, column):
        self.impl._set_table(table, column)

    def copy(self):
        return EnumType(self.enum)

    def process_bind_param(self, value, dialect):
        if value is None:
            return None
        return value.value

    def process_result_value(self, value, dialect):
        if value is None:
            return None
        return self.enum.from_string(value.strip())


class Enum(object):
    """"""
    Declarative enumeration.
    """"""

    __metaclass__ = EnumMeta
    _reg = {}

    @classmethod
    def from_string(cls, value):
        try:
            return cls._reg[value]
        except KeyError:
            raise ValueError(
                    ""Invalid value for %r: %r"" %
                    (cls.__name__, value)
                )

    @classmethod
    def values(cls):
        return cls._reg.keys()

    @classmethod
    def db_type(cls):
        return EnumType(cls)

```"
c56aef2c927290732dfb8ed65f173f7d8bb58439,git_externals/gitlab_utils.py,git_externals/gitlab_utils.py,,"#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import time

import click
import gitlab


def iter_projects(gl):
    page = 0
    while True:
        projects = gl.projects.list(page=page, per_page=10)
        if len(projects) == 0:
            break
        else:
            page = page + 1
        for project in projects:
            yield project


@click.group()
@click.option('--gitlab-id', default=None)
@click.option('--config', default=None)
@click.pass_context
def cli(ctx, gitlab_id, config):
    if config is not None:
        config = [config]
    gl = gitlab.Gitlab.from_config(gitlab_id=gitlab_id, config_files=config)
    ctx.obj['api'] = gl


@cli.group()
@click.pass_context
def project(ctx):
    pass


def get_project_by_path(gl, path):
    with click.progressbar(iter_projects(gl), label='Searching project...') as projects:
        for prj in projects:
            import q
            q(prj.path_with_namespace)
            if prj.path_with_namespace == path:
                return prj


@project.command()
@click.argument('path')
@click.option('--sync', is_flag=True)
@click.pass_context
def delete(ctx, path, sync):
    gl = ctx.obj['api']
    prj = get_project_by_path(gl, path)
    if prj is None:
        click.echo('Unable to find a matching project for path %r' % path, err=True)
        return
    try:
        if not gl.delete(prj):
            raise click.UsegeError('Unable to delete project for path %r' % path)
    except gitlab.GitlabGetError:
        click.echo('The project %r seems to be already deleted' % path, err=True)
        return
    if sync:
        with click.progressbar(range(12), label='Waiting for deletion...') as waiting:
            for step in waiting:
                try:
                    gl.projects.get(prj.id)
                except gitlab.GitlabGetError:
                    deleted = True
                    break
                time.sleep(10)
            else:
                deleted = False
        if deleted:
            click.echo('Project %r deleted' % path)
        else:
            click.UsegeError('Timeout waiting for %r deletion' % path)
    else:
        click.echo('Project %r submitted for deletion' % path)


def main():
    cli(obj={})


if __name__ == '__main__':
    main()
",Add gittify-gitlab to detete projects by path,"Add gittify-gitlab to detete projects by path
",Python,mit,"develersrl/git-externals,develersrl/git-externals,develersrl/git-externals",90,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import time

import click
import gitlab


def iter_projects(gl):
    page = 0
    while True:
        projects = gl.projects.list(page=page, per_page=10)
        if len(projects) == 0:
            break
        else:
            page = page + 1
        for project in projects:
            yield project


@click.group()
@click.option('--gitlab-id', default=None)
@click.option('--config', default=None)
@click.pass_context
def cli(ctx, gitlab_id, config):
    if config is not None:
        config = [config]
    gl = gitlab.Gitlab.from_config(gitlab_id=gitlab_id, config_files=config)
    ctx.obj['api'] = gl


@cli.group()
@click.pass_context
def project(ctx):
    pass


def get_project_by_path(gl, path):
    with click.progressbar(iter_projects(gl), label='Searching project...') as projects:
        for prj in projects:
            import q
            q(prj.path_with_namespace)
            if prj.path_with_namespace == path:
                return prj


@project.command()
@click.argument('path')
@click.option('--sync', is_flag=True)
@click.pass_context
def delete(ctx, path, sync):
    gl = ctx.obj['api']
    prj = get_project_by_path(gl, path)
    if prj is None:
        click.echo('Unable to find a matching project for path %r' % path, err=True)
        return
    try:
        if not gl.delete(prj):
            raise click.UsegeError('Unable to delete project for path %r' % path)
    except gitlab.GitlabGetError:
        click.echo('The project %r seems to be already deleted' % path, err=True)
        return
    if sync:
        with click.progressbar(range(12), label='Waiting for deletion...') as waiting:
            for step in waiting:
                try:
                    gl.projects.get(prj.id)
                except gitlab.GitlabGetError:
                    deleted = True
                    break
                time.sleep(10)
            else:
                deleted = False
        if deleted:
            click.echo('Project %r deleted' % path)
        else:
            click.UsegeError('Timeout waiting for %r deletion' % path)
    else:
        click.echo('Project %r submitted for deletion' % path)


def main():
    cli(obj={})


if __name__ == '__main__':
    main()

```"
5b298e30fd5251bd9bf2c154c267bd86f1bc03cc,libexec/check_shinken_mem.py,libexec/check_shinken_mem.py,,"#!/usr/bin/env python
#   Autor : David Hannequin <david.hannequin@gmail.com>
#   Date : 29 Nov 2011

#
# Script init
#

import sys
import os
import argparse
import getopt 

#
# Usage 
#

def usage():
    print 'Usage :'
    print sys.argv[0] + ' -w <80> -c <90>'
    print '-p --port : snmp port by default 161' 
    print '   -c (--critical)      Critical tresholds (defaults : 90%)\n';
    print '   -w (--warning)       Warning tresholds (defaults : 80%)\n';
    print '   -h (--help)          Usage help\n';

#
# Main
#

def readLines(filename): 
    f = open(filename, ""r"") 
    lines = f.readlines() 
    return lines 

def MemValues():
    global memTotal, memCached, memFree
    for line in readLines('/proc/meminfo'):
        if line.split()[0] == 'MemTotal:':
            memTotal = line.split()[1]
        if line.split()[0] == 'MemFree:':
            memFree = line.split()[1]
        if line.split()[0] == 'Cached:':
             memCached = line.split()[1]

def percentMem():                                                                                                                                             
    MemValues()
    return (((int(memFree) + int(memCached)) * 100) / int(memTotal))  

def main():

    try:
        opts, args = getopt.getopt(sys.argv[1:], ""hwc:v"", [""help"", ""warning"", ""critical""])
    except getopt.GetoptError, err:
        # print help information and exit:
        print str(err) 
        usage()
        sys.exit(2)
    output = None
    verbose = False

    for o, a in opts:
        if o == ""-v"":
            verbose = True
        elif o in (""-h"", ""--help""):
            usage()
            sys.exit()
        elif o in (""-w"", ""--warning""):
            notification = a
        elif o in (""-c"", ""--critical""):
            notification = a
	else :
	    assert False , ""unknown options""

    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--warning', default = '80')
    parser.add_argument('-c', '--critical', default = '90' )
    args = parser.parse_args()
    critical = args.critical
    warning = args.warning
    cmem = str(critical) 
    wmem = str(warning)
    pmemFree = percentMem()
    pmemUsage = 100 - pmemFree 
    pmemUsage = str(pmemUsage)
 
    if pmemUsage >= cmem : 
       print 'CRITICAL - Memory usage : '+pmemUsage+'% |mem='+pmemUsage   
       sys.exit(2)
    elif pmemUsage >= wmem :
       print 'WARNING - Memory usage : '+pmemUsage+'% |mem='+pmemUsage    
       sys.exit(1)
    else :
       print 'OK - Memory usage : '+pmemUsage+'% |mem='+pmemUsage   
       sys.exit(0)

if __name__ == ""__main__"":
    main()
",Add shinken plugin to check memory usage,"Add shinken plugin to check memory usage
",Python,agpl-3.0,"rledisez/shinken,staute/shinken_deb,peeyush-tm/shinken,h4wkmoon/shinken,fpeyre/shinken,savoirfairelinux/shinken,peeyush-tm/shinken,rledisez/shinken,Simage/shinken,ddurieux/alignak,kaji-project/shinken,rednach/krill,dfranco/shinken,fpeyre/shinken,lets-software/shinken,fpeyre/shinken,tal-nino/shinken,baloo/shinken,geektophe/shinken,xorpaul/shinken,Aimage/shinken,Simage/shinken,claneys/shinken,naparuba/shinken,KerkhoffTechnologies/shinken,h4wkmoon/shinken,fpeyre/shinken,dfranco/shinken,Aimage/shinken,dfranco/shinken,naparuba/shinken,staute/shinken_deb,kaji-project/shinken,titilambert/alignak,kaji-project/shinken,geektophe/shinken,gst/alignak,ddurieux/alignak,Simage/shinken,rednach/krill,KerkhoffTechnologies/shinken,titilambert/alignak,rednach/krill,xorpaul/shinken,savoirfairelinux/shinken,kaji-project/shinken,Aimage/shinken,rednach/krill,rednach/krill,tal-nino/shinken,tal-nino/shinken,mohierf/shinken,lets-software/shinken,xorpaul/shinken,geektophe/shinken,Aimage/shinken,tal-nino/shinken,KerkhoffTechnologies/shinken,dfranco/shinken,KerkhoffTechnologies/shinken,claneys/shinken,lets-software/shinken,Alignak-monitoring/alignak,h4wkmoon/shinken,staute/shinken_package,rledisez/shinken,baloo/shinken,mohierf/shinken,kaji-project/shinken,staute/shinken_deb,xorpaul/shinken,staute/shinken_package,dfranco/shinken,lets-software/shinken,savoirfairelinux/shinken,titilambert/alignak,baloo/shinken,naparuba/shinken,ddurieux/alignak,rednach/krill,savoirfairelinux/shinken,rledisez/shinken,peeyush-tm/shinken,baloo/shinken,xorpaul/shinken,KerkhoffTechnologies/shinken,gst/alignak,mohierf/shinken,fpeyre/shinken,naparuba/shinken,peeyush-tm/shinken,naparuba/shinken,fpeyre/shinken,h4wkmoon/shinken,Aimage/shinken,lets-software/shinken,geektophe/shinken,staute/shinken_package,mohierf/shinken,tal-nino/shinken,claneys/shinken,staute/shinken_package,dfranco/shinken,kaji-project/shinken,h4wkmoon/shinken,h4wkmoon/shinken,xorpaul/shinken,claneys/shinken,baloo/shinken,Aimage/shinken,ddurieux/alignak,mohierf/shinken,Simage/shinken,gst/alignak,Simage/shinken,ddurieux/alignak,mohierf/shinken,xorpaul/shinken,staute/shinken_deb,Alignak-monitoring/alignak,geektophe/shinken,titilambert/alignak,staute/shinken_deb,savoirfairelinux/shinken,claneys/shinken,kaji-project/shinken,gst/alignak,h4wkmoon/shinken,ddurieux/alignak,rledisez/shinken,naparuba/shinken,lets-software/shinken,peeyush-tm/shinken,Simage/shinken,geektophe/shinken,xorpaul/shinken,savoirfairelinux/shinken,KerkhoffTechnologies/shinken,peeyush-tm/shinken,tal-nino/shinken,staute/shinken_deb,claneys/shinken,h4wkmoon/shinken,baloo/shinken,rledisez/shinken,staute/shinken_package,staute/shinken_package",98,"```python
#!/usr/bin/env python
#   Autor : David Hannequin <david.hannequin@gmail.com>
#   Date : 29 Nov 2011

#
# Script init
#

import sys
import os
import argparse
import getopt 

#
# Usage 
#

def usage():
    print 'Usage :'
    print sys.argv[0] + ' -w <80> -c <90>'
    print '-p --port : snmp port by default 161' 
    print '   -c (--critical)      Critical tresholds (defaults : 90%)\n';
    print '   -w (--warning)       Warning tresholds (defaults : 80%)\n';
    print '   -h (--help)          Usage help\n';

#
# Main
#

def readLines(filename): 
    f = open(filename, ""r"") 
    lines = f.readlines() 
    return lines 

def MemValues():
    global memTotal, memCached, memFree
    for line in readLines('/proc/meminfo'):
        if line.split()[0] == 'MemTotal:':
            memTotal = line.split()[1]
        if line.split()[0] == 'MemFree:':
            memFree = line.split()[1]
        if line.split()[0] == 'Cached:':
             memCached = line.split()[1]

def percentMem():                                                                                                                                             
    MemValues()
    return (((int(memFree) + int(memCached)) * 100) / int(memTotal))  

def main():

    try:
        opts, args = getopt.getopt(sys.argv[1:], ""hwc:v"", [""help"", ""warning"", ""critical""])
    except getopt.GetoptError, err:
        # print help information and exit:
        print str(err) 
        usage()
        sys.exit(2)
    output = None
    verbose = False

    for o, a in opts:
        if o == ""-v"":
            verbose = True
        elif o in (""-h"", ""--help""):
            usage()
            sys.exit()
        elif o in (""-w"", ""--warning""):
            notification = a
        elif o in (""-c"", ""--critical""):
            notification = a
	else :
	    assert False , ""unknown options""

    parser = argparse.ArgumentParser()
    parser.add_argument('-w', '--warning', default = '80')
    parser.add_argument('-c', '--critical', default = '90' )
    args = parser.parse_args()
    critical = args.critical
    warning = args.warning
    cmem = str(critical) 
    wmem = str(warning)
    pmemFree = percentMem()
    pmemUsage = 100 - pmemFree 
    pmemUsage = str(pmemUsage)
 
    if pmemUsage >= cmem : 
       print 'CRITICAL - Memory usage : '+pmemUsage+'% |mem='+pmemUsage   
       sys.exit(2)
    elif pmemUsage >= wmem :
       print 'WARNING - Memory usage : '+pmemUsage+'% |mem='+pmemUsage    
       sys.exit(1)
    else :
       print 'OK - Memory usage : '+pmemUsage+'% |mem='+pmemUsage   
       sys.exit(0)

if __name__ == ""__main__"":
    main()

```"
e443527d28d24219c81dc1a2ec1c649de7bb16c7,examples/lstm_stateful_seq.py,examples/lstm_stateful_seq.py,,"'''Example script to predict sequence using stateful rnns.
At least 10 epochs are required before the generated text
starts sounding coherent.
'''

import numpy as np
import matplotlib.pyplot as mpl
from keras.models import Sequential
from keras.layers.core import Dense
from keras.layers.recurrent import LSTM


# since we are using stateful rnn tsteps can be set to 1
tsteps = 1
batch_size = 25
epochs = 25
# number of elements ahead that are used to make the prediction
lahead = 1


def gen_cosine_amp(amp=100, period=25, x0=0, xn=50000, step=1, k=0.0001):
    """"""
    Generates an absolute cosine time series with the amplitude exponentially
    decreasing

    Keyword arguments:
    amp -- amplitude of the cosine function
    period -- period of the cosine function
    x0 -- initial x of the time series
    xn -- final x of the time series
    step -- step of the time series discretization
    k -- exponential rate
    """"""
    cos = np.zeros(((xn - x0) * step, 1, 1))
    for i in range(len(cos)):
        idx = x0 + i * step
        cos[i, 0, 0] = amp * np.cos(idx / (2 * np.pi * period))
        cos[i, 0, 0] = cos[i, 0, 0] * np.exp(-k * idx)
    return cos


print('Creating Data')
cos = gen_cosine_amp()
print('Input shape:')
print(cos.shape)
print('Calculating expected predicted_out')
expected_out = np.zeros((len(cos), 1))
for i in range(len(cos) - lahead):
    expected_out[i, 0] = np.mean(cos[i + 1:i + lahead + 1])

print('Output shape')
print(expected_out.shape)

print('Creating Model')
model = Sequential()
model.add(
    LSTM(
        50,
        batch_input_shape=(
            batch_size,
            tsteps,
            1),
        return_sequences=True,
        stateful=True))
model.add(
    LSTM(
        50,
        batch_input_shape=(
            batch_size,
            tsteps,
            1),
        return_sequences=False,
        stateful=True))
model.add(Dense(1))
model.compile(loss='rmse', optimizer='rmsprop')

print('Training')
for i in range(epochs):
    model.fit(
        cos,
        expected_out,
        batch_size=batch_size,
        verbose=1,
        nb_epoch=1)
    model.reset_states()

print('Predicting')
predicted_out = model.predict(cos, batch_size=batch_size)

print('Ploting Results')
mpl.subplot(2, 1, 1)
mpl.plot(expected_out)
mpl.title('Expected')
mpl.subplot(2, 1, 2)
mpl.plot(predicted_out)
mpl.title('Predicted')
mpl.show()
",Add example of stateful LSTM sequence prediction,"Add example of stateful LSTM sequence prediction
",Python,mit,"relh/keras,keras-team/keras,daviddiazvico/keras,nebw/keras,dolaameng/keras,kuza55/keras,keras-team/keras,DeepGnosis/keras,kemaswill/keras",98,"```python
'''Example script to predict sequence using stateful rnns.
At least 10 epochs are required before the generated text
starts sounding coherent.
'''

import numpy as np
import matplotlib.pyplot as mpl
from keras.models import Sequential
from keras.layers.core import Dense
from keras.layers.recurrent import LSTM


# since we are using stateful rnn tsteps can be set to 1
tsteps = 1
batch_size = 25
epochs = 25
# number of elements ahead that are used to make the prediction
lahead = 1


def gen_cosine_amp(amp=100, period=25, x0=0, xn=50000, step=1, k=0.0001):
    """"""
    Generates an absolute cosine time series with the amplitude exponentially
    decreasing

    Keyword arguments:
    amp -- amplitude of the cosine function
    period -- period of the cosine function
    x0 -- initial x of the time series
    xn -- final x of the time series
    step -- step of the time series discretization
    k -- exponential rate
    """"""
    cos = np.zeros(((xn - x0) * step, 1, 1))
    for i in range(len(cos)):
        idx = x0 + i * step
        cos[i, 0, 0] = amp * np.cos(idx / (2 * np.pi * period))
        cos[i, 0, 0] = cos[i, 0, 0] * np.exp(-k * idx)
    return cos


print('Creating Data')
cos = gen_cosine_amp()
print('Input shape:')
print(cos.shape)
print('Calculating expected predicted_out')
expected_out = np.zeros((len(cos), 1))
for i in range(len(cos) - lahead):
    expected_out[i, 0] = np.mean(cos[i + 1:i + lahead + 1])

print('Output shape')
print(expected_out.shape)

print('Creating Model')
model = Sequential()
model.add(
    LSTM(
        50,
        batch_input_shape=(
            batch_size,
            tsteps,
            1),
        return_sequences=True,
        stateful=True))
model.add(
    LSTM(
        50,
        batch_input_shape=(
            batch_size,
            tsteps,
            1),
        return_sequences=False,
        stateful=True))
model.add(Dense(1))
model.compile(loss='rmse', optimizer='rmsprop')

print('Training')
for i in range(epochs):
    model.fit(
        cos,
        expected_out,
        batch_size=batch_size,
        verbose=1,
        nb_epoch=1)
    model.reset_states()

print('Predicting')
predicted_out = model.predict(cos, batch_size=batch_size)

print('Ploting Results')
mpl.subplot(2, 1, 1)
mpl.plot(expected_out)
mpl.title('Expected')
mpl.subplot(2, 1, 2)
mpl.plot(predicted_out)
mpl.title('Predicted')
mpl.show()

```"
7376dcf2222f24bafee9896c7b6631cbede829c8,bayespy/inference/vmp/nodes/tests/test_dirichlet.py,bayespy/inference/vmp/nodes/tests/test_dirichlet.py,,"######################################################################
# Copyright (C) 2014 Jaakko Luttinen
#
# This file is licensed under Version 3.0 of the GNU General Public
# License. See LICENSE for a text of the license.
######################################################################

######################################################################
# This file is part of BayesPy.
#
# BayesPy is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License version 3 as
# published by the Free Software Foundation.
#
# BayesPy is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with BayesPy.  If not, see <http://www.gnu.org/licenses/>.
######################################################################

""""""
Unit tests for `dirichlet` module.
""""""

import numpy as np
from scipy import special

from bayespy.nodes import Dirichlet

from bayespy.utils import utils
from bayespy.utils import random

from bayespy.utils.utils import TestCase

class TestDirichlet(TestCase):
    """"""
    Unit tests for Dirichlet node
    """"""

    
    def test_init(self):
        """"""
        Test the creation of Dirichlet nodes.
        """"""

        # Some simple initializations
        p = Dirichlet([1.5, 4.2, 3.5])

        # Check that plates are correct
        p = Dirichlet([2, 3, 4], plates=(4,3))
        self.assertEqual(p.plates,
                         (4,3))
        p = Dirichlet(np.ones((4,3,5)))
        self.assertEqual(p.plates,
                         (4,3))

        # Parent not a vector
        self.assertRaises(ValueError,
                          Dirichlet,
                          4)
        
        # Parent vector has invalid values
        self.assertRaises(ValueError,
                          Dirichlet,
                          [-2,3,1])

        # Plates inconsistent
        self.assertRaises(ValueError,
                          Dirichlet,
                          np.ones((4,3)),
                          plates=(3,))

        # Explicit plates too small
        self.assertRaises(ValueError,
                          Dirichlet,
                          np.ones((4,3)),
                          plates=(1,))

        pass

    
    def test_moments(self):
        """"""
        Test the moments of Dirichlet nodes.
        """"""

        p = Dirichlet([2, 3, 4])
        u = p._message_to_child()
        self.assertAllClose(u[0],
                            special.psi([2,3,4]) - special.psi(2+3+4))
        
        pass


    def test_constant(self):
        """"""
        Test the constant moments of Dirichlet nodes.
        """"""

        p = Dirichlet([1, 1, 1])
        p.initialize_from_value([0.5, 0.4, 0.1])
        u = p._message_to_child()
        self.assertAllClose(u[0],
                            np.log([0.5, 0.4, 0.1]))

        pass
",Add unit tests for Dirichlet node,"TST: Add unit tests for Dirichlet node
",Python,mit,"jluttine/bayespy,bayespy/bayespy,SalemAmeen/bayespy,fivejjs/bayespy",110,"```python
######################################################################
# Copyright (C) 2014 Jaakko Luttinen
#
# This file is licensed under Version 3.0 of the GNU General Public
# License. See LICENSE for a text of the license.
######################################################################

######################################################################
# This file is part of BayesPy.
#
# BayesPy is free software: you can redistribute it and/or modify it
# under the terms of the GNU General Public License version 3 as
# published by the Free Software Foundation.
#
# BayesPy is distributed in the hope that it will be useful, but
# WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with BayesPy.  If not, see <http://www.gnu.org/licenses/>.
######################################################################

""""""
Unit tests for `dirichlet` module.
""""""

import numpy as np
from scipy import special

from bayespy.nodes import Dirichlet

from bayespy.utils import utils
from bayespy.utils import random

from bayespy.utils.utils import TestCase

class TestDirichlet(TestCase):
    """"""
    Unit tests for Dirichlet node
    """"""

    
    def test_init(self):
        """"""
        Test the creation of Dirichlet nodes.
        """"""

        # Some simple initializations
        p = Dirichlet([1.5, 4.2, 3.5])

        # Check that plates are correct
        p = Dirichlet([2, 3, 4], plates=(4,3))
        self.assertEqual(p.plates,
                         (4,3))
        p = Dirichlet(np.ones((4,3,5)))
        self.assertEqual(p.plates,
                         (4,3))

        # Parent not a vector
        self.assertRaises(ValueError,
                          Dirichlet,
                          4)
        
        # Parent vector has invalid values
        self.assertRaises(ValueError,
                          Dirichlet,
                          [-2,3,1])

        # Plates inconsistent
        self.assertRaises(ValueError,
                          Dirichlet,
                          np.ones((4,3)),
                          plates=(3,))

        # Explicit plates too small
        self.assertRaises(ValueError,
                          Dirichlet,
                          np.ones((4,3)),
                          plates=(1,))

        pass

    
    def test_moments(self):
        """"""
        Test the moments of Dirichlet nodes.
        """"""

        p = Dirichlet([2, 3, 4])
        u = p._message_to_child()
        self.assertAllClose(u[0],
                            special.psi([2,3,4]) - special.psi(2+3+4))
        
        pass


    def test_constant(self):
        """"""
        Test the constant moments of Dirichlet nodes.
        """"""

        p = Dirichlet([1, 1, 1])
        p.initialize_from_value([0.5, 0.4, 0.1])
        u = p._message_to_child()
        self.assertAllClose(u[0],
                            np.log([0.5, 0.4, 0.1]))

        pass

```"
fefd4db49bf5590a1d95eb59ae6de9a63131f5ef,mutation/mutation_results.py,mutation/mutation_results.py,,"import matplotlib.pyplot as plt
import numpy

class Result:

    def __init__(self, mut_type, schemes, results):
        self.mut_type = mut_type
        self.schemes = schemes
        self.results = results

    def __repr__(self):
        return ""result - "" + str(self.mut_type) + ""-"" + str(self.results)

class MutationResults:

    def __init__(self, sbfl_dir):
        self.sbfl_dir = sbfl_dir

        self.trans_array = [""F2P"", ""RSS"", ""UML2ER"", ""GM"", ""Kiltera""]


    def calc(self):

        for trans in self.trans_array:
            results_dir = ""models_"" + trans + ""/results/""
            results_file = self.sbfl_dir + results_dir + ""results_AC.csv""

            print(""Opening file: "" + results_file)

            results_arr = []
            schemes = []

            first_line = True
            with open(results_file) as f:
                for line in f:

                    if first_line:
                        first_line = False


                        for s in line.split("";""):
                            if not s:
                                continue
                            if ""PCs"" in s:
                                break
                            schemes.append(s)
                        continue

                    tokens = line.split("";"")

                    mut_type = tokens[0].split(""'"")[1]

                    results = {}
                    for i, t in enumerate(tokens):
                        if i == 0:
                            continue
                        if not t.strip():
                            break
                        scheme = schemes[i-1]
                        results[scheme] = t

                    result = Result(mut_type, schemes, results)
                    results_arr.append(result)

            self.parse_results(trans, schemes, results_arr)

    def parse_results(self, trans, schemes, results_arr):

        print(""For trans: "" + trans)
        bins = list(numpy.arange(0, 1, 0.1))

        # order schemes by avg
        schemes_with_avg = []

        for scheme in schemes:
            scores = [float(result.results[scheme]) for result in results_arr]
            schemes_with_avg.append((scheme, numpy.average(scores)))
        schemes_with_avg.sort(key=lambda t: t[1])
        print(schemes_with_avg)

        fig, axs = plt.subplots(len(schemes))
        fig.suptitle(""Scores for "" + trans)

        for i, scheme_and_score in enumerate(schemes_with_avg):
            scheme, avg = scheme_and_score
            scores = [float(result.results[scheme]) for result in results_arr]

            print(""Scheme: "" + scheme + "" - "" + str(avg))
            print(scores)

            axs[i].title.set_text(scheme)
            axs[i].hist(scores, bins=bins)
        plt.show()

if __name__ == ""__main__"":

    sbfl_dir = ""/home/xubuntu/Projects/SBFL/""
    mr = MutationResults(sbfl_dir)

    mr.calc()

",Add script for parsing mutation results.,"Add script for parsing mutation results.
",Python,mit,"levilucio/SyVOLT,levilucio/SyVOLT",102,"```python
import matplotlib.pyplot as plt
import numpy

class Result:

    def __init__(self, mut_type, schemes, results):
        self.mut_type = mut_type
        self.schemes = schemes
        self.results = results

    def __repr__(self):
        return ""result - "" + str(self.mut_type) + ""-"" + str(self.results)

class MutationResults:

    def __init__(self, sbfl_dir):
        self.sbfl_dir = sbfl_dir

        self.trans_array = [""F2P"", ""RSS"", ""UML2ER"", ""GM"", ""Kiltera""]


    def calc(self):

        for trans in self.trans_array:
            results_dir = ""models_"" + trans + ""/results/""
            results_file = self.sbfl_dir + results_dir + ""results_AC.csv""

            print(""Opening file: "" + results_file)

            results_arr = []
            schemes = []

            first_line = True
            with open(results_file) as f:
                for line in f:

                    if first_line:
                        first_line = False


                        for s in line.split("";""):
                            if not s:
                                continue
                            if ""PCs"" in s:
                                break
                            schemes.append(s)
                        continue

                    tokens = line.split("";"")

                    mut_type = tokens[0].split(""'"")[1]

                    results = {}
                    for i, t in enumerate(tokens):
                        if i == 0:
                            continue
                        if not t.strip():
                            break
                        scheme = schemes[i-1]
                        results[scheme] = t

                    result = Result(mut_type, schemes, results)
                    results_arr.append(result)

            self.parse_results(trans, schemes, results_arr)

    def parse_results(self, trans, schemes, results_arr):

        print(""For trans: "" + trans)
        bins = list(numpy.arange(0, 1, 0.1))

        # order schemes by avg
        schemes_with_avg = []

        for scheme in schemes:
            scores = [float(result.results[scheme]) for result in results_arr]
            schemes_with_avg.append((scheme, numpy.average(scores)))
        schemes_with_avg.sort(key=lambda t: t[1])
        print(schemes_with_avg)

        fig, axs = plt.subplots(len(schemes))
        fig.suptitle(""Scores for "" + trans)

        for i, scheme_and_score in enumerate(schemes_with_avg):
            scheme, avg = scheme_and_score
            scores = [float(result.results[scheme]) for result in results_arr]

            print(""Scheme: "" + scheme + "" - "" + str(avg))
            print(scores)

            axs[i].title.set_text(scheme)
            axs[i].hist(scores, bins=bins)
        plt.show()

if __name__ == ""__main__"":

    sbfl_dir = ""/home/xubuntu/Projects/SBFL/""
    mr = MutationResults(sbfl_dir)

    mr.calc()


```"
4f0415f5cb7f8322a0738cb1d55c7102464d3aef,openedx/core/djangoapps/discussions/tests/test_views.py,openedx/core/djangoapps/discussions/tests/test_views.py,,"""""""
Test app view logic
""""""
# pylint: disable=test-inherits-tests
import unittest

from django.conf import settings
from django.urls import reverse
from opaque_keys.edx.keys import CourseKey
from rest_framework import status
from rest_framework.test import APITestCase

from common.djangoapps.student.tests.factories import UserFactory
from lms.djangoapps.courseware.tests.factories import GlobalStaffFactory
from lms.djangoapps.courseware.tests.factories import StaffFactory


@unittest.skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'URLs are only configured in LMS')
class ApiTest(APITestCase):
    """"""
    Test basic API operations
    """"""
    def setUp(self):
        super().setUp()
        self.course_key = CourseKey.from_string('course-v1:Test+Course+Configured')
        self.url = reverse(
            'discussions',
            kwargs={
                'course_key_string': str(self.course_key),
            }
        )
        self.password = 'password'
        self.user_student = UserFactory(username='dummy', password=self.password)
        self.user_staff_course = StaffFactory(course_key=self.course_key, password=self.password)
        self.user_staff_global = GlobalStaffFactory(password=self.password)


class UnauthorizedApiTest(ApiTest):
    """"""
    Logged-out users should _not_ have any access
    """"""

    expected_response_code = status.HTTP_401_UNAUTHORIZED

    def test_access_get(self):
        response = self.client.get(self.url)
        assert response.status_code == self.expected_response_code

    def test_access_patch(self):
        response = self.client.patch(self.url)
        assert response.status_code == self.expected_response_code

    def test_access_post(self):
        response = self.client.post(self.url)
        assert response.status_code == self.expected_response_code

    def test_access_put(self):
        response = self.client.put(self.url)
        assert response.status_code == self.expected_response_code


class AuthenticatedApiTest(UnauthorizedApiTest):
    """"""
    Logged-in users should _not_ have any access
    """"""

    expected_response_code = status.HTTP_403_FORBIDDEN

    def setUp(self):
        super().setUp()
        self._login()

    def _login(self):
        self.client.login(username=self.user_student.username, password=self.password)


class AuthorizedApiTest(AuthenticatedApiTest):
    """"""
    Global Staff should have access to all supported methods
    """"""

    expected_response_code = status.HTTP_200_OK

    def _login(self):
        self.client.login(username=self.user_staff_global.username, password=self.password)

    def test_access_patch(self):
        response = self.client.patch(self.url)
        assert response.status_code == status.HTTP_405_METHOD_NOT_ALLOWED

    def test_access_put(self):
        response = self.client.put(self.url)
        assert response.status_code == status.HTTP_405_METHOD_NOT_ALLOWED
",Add tests for discussions API access,"test: Add tests for discussions API access

This checks for expected API access [1];
data integrity will be checked later [2].

This work exposes that the code currently does _not_ grant access to
_course_ staff, only _global_ staff. This is being addressed next [3].

Fix: TNL-8229 [1]

- [1] https://openedx.atlassian.net/browse/TNL-8229
- [2] https://openedx.atlassian.net/browse/TNL-8230
- [3] https://openedx.atlassian.net/browse/TNL-8231
",Python,agpl-3.0,"edx/edx-platform,angelapper/edx-platform,eduNEXT/edx-platform,arbrandes/edx-platform,eduNEXT/edx-platform,angelapper/edx-platform,arbrandes/edx-platform,angelapper/edx-platform,arbrandes/edx-platform,edx/edx-platform,arbrandes/edx-platform,eduNEXT/edx-platform,eduNEXT/edx-platform,edx/edx-platform,edx/edx-platform,angelapper/edx-platform",94,"```python
""""""
Test app view logic
""""""
# pylint: disable=test-inherits-tests
import unittest

from django.conf import settings
from django.urls import reverse
from opaque_keys.edx.keys import CourseKey
from rest_framework import status
from rest_framework.test import APITestCase

from common.djangoapps.student.tests.factories import UserFactory
from lms.djangoapps.courseware.tests.factories import GlobalStaffFactory
from lms.djangoapps.courseware.tests.factories import StaffFactory


@unittest.skipUnless(settings.ROOT_URLCONF == 'lms.urls', 'URLs are only configured in LMS')
class ApiTest(APITestCase):
    """"""
    Test basic API operations
    """"""
    def setUp(self):
        super().setUp()
        self.course_key = CourseKey.from_string('course-v1:Test+Course+Configured')
        self.url = reverse(
            'discussions',
            kwargs={
                'course_key_string': str(self.course_key),
            }
        )
        self.password = 'password'
        self.user_student = UserFactory(username='dummy', password=self.password)
        self.user_staff_course = StaffFactory(course_key=self.course_key, password=self.password)
        self.user_staff_global = GlobalStaffFactory(password=self.password)


class UnauthorizedApiTest(ApiTest):
    """"""
    Logged-out users should _not_ have any access
    """"""

    expected_response_code = status.HTTP_401_UNAUTHORIZED

    def test_access_get(self):
        response = self.client.get(self.url)
        assert response.status_code == self.expected_response_code

    def test_access_patch(self):
        response = self.client.patch(self.url)
        assert response.status_code == self.expected_response_code

    def test_access_post(self):
        response = self.client.post(self.url)
        assert response.status_code == self.expected_response_code

    def test_access_put(self):
        response = self.client.put(self.url)
        assert response.status_code == self.expected_response_code


class AuthenticatedApiTest(UnauthorizedApiTest):
    """"""
    Logged-in users should _not_ have any access
    """"""

    expected_response_code = status.HTTP_403_FORBIDDEN

    def setUp(self):
        super().setUp()
        self._login()

    def _login(self):
        self.client.login(username=self.user_student.username, password=self.password)


class AuthorizedApiTest(AuthenticatedApiTest):
    """"""
    Global Staff should have access to all supported methods
    """"""

    expected_response_code = status.HTTP_200_OK

    def _login(self):
        self.client.login(username=self.user_staff_global.username, password=self.password)

    def test_access_patch(self):
        response = self.client.patch(self.url)
        assert response.status_code == status.HTTP_405_METHOD_NOT_ALLOWED

    def test_access_put(self):
        response = self.client.put(self.url)
        assert response.status_code == status.HTTP_405_METHOD_NOT_ALLOWED

```"
482a00c0a479a06a0e8d54058bb8ea9a12be0023,api_examples/export_institution_csv.py,api_examples/export_institution_csv.py,,"#!/bin/python2.7
# -*- coding: utf-8 -*-
""""""
A tool to insert institutions from CSV files.

Requirements:
 - requests
 - gusregon
 - unicodecsv
 - jmespath

Example usage:

To run help text use:
$ python insert_institution_csv.py -h
""""""
from __future__ import print_function, unicode_literals

import argparse
import sys

import itertools
from Queue import Queue

import jmespath
import requests
import tqdm
import unicodecsv as csv
import requests_cache

try:
    from urlparse import urljoin
except ImportError:
    from urllib.parse import urljoin


class Client(object):

    def __init__(self, start, s=None):
        self.start = start
        self.s = s or requests.Session()

    def get_page_iter(self):
        q = Queue()
        q.put(self.start)
        while not q.empty():
            resp = self.s.get(url=q.get())
            if resp.status_code != 200:
                return
            data = resp.json()
            if data.get('next'):
                q.put(data['next'])
            for row in data['results']:
                yield row

JMES_DEFAULT = ""{name: name, url:url, pk:pk, email:email, tags:join(',',tags), jst:jst, regon:regon}""


class Command(object):

    def __init__(self, argv):
        self.argv = argv
        self.args = self.get_build_args(argv[1:])
        self.s = requests.Session() if not self.args.cache else requests_cache.CachedSession()

    def get_build_args(self, argv):
        parser = argparse.ArgumentParser()
        parser.add_argument('--output',
                            required=True,
                            type=argparse.FileType('w'),
                            help=""Output CSV-file"")
        parser.add_argument('--start',
                            required=True,
                            help=""Start URL"")
        parser.add_argument('--jmes', type=jmespath.compile,
                            required=False,
                            help='JMESPath to convert values (default: ""{}"")'.format(JMES_DEFAULT),
                            default=jmespath.compile(JMES_DEFAULT))
        parser.add_argument('--cache', action='store_true', help=""Enable cache"")
        return parser.parse_args(argv)

    def run(self):

        client = Client(start=self.args.start, s=self.s)
        data = client.get_page_iter()
        first = next(data)

        fieldnames = self.args.jmes.search(first).keys()

        print(""Identified fields: {}"".format("", "".join(fieldnames)))

        writer = csv.DictWriter(self.args.output, fieldnames=fieldnames)
        writer.writeheader()
        for item in tqdm.tqdm(itertools.chain([first], data)):
            writer.writerow(self.args.jmes.search(item))

if __name__ == ""__main__"":
    sys.exit(Command(sys.argv).run())
",Add examples of API to export institutions,"Add examples of API to export institutions
",Python,mit,"watchdogpolska/feder,watchdogpolska/feder,watchdogpolska/feder,watchdogpolska/feder",99,"```python
#!/bin/python2.7
# -*- coding: utf-8 -*-
""""""
A tool to insert institutions from CSV files.

Requirements:
 - requests
 - gusregon
 - unicodecsv
 - jmespath

Example usage:

To run help text use:
$ python insert_institution_csv.py -h
""""""
from __future__ import print_function, unicode_literals

import argparse
import sys

import itertools
from Queue import Queue

import jmespath
import requests
import tqdm
import unicodecsv as csv
import requests_cache

try:
    from urlparse import urljoin
except ImportError:
    from urllib.parse import urljoin


class Client(object):

    def __init__(self, start, s=None):
        self.start = start
        self.s = s or requests.Session()

    def get_page_iter(self):
        q = Queue()
        q.put(self.start)
        while not q.empty():
            resp = self.s.get(url=q.get())
            if resp.status_code != 200:
                return
            data = resp.json()
            if data.get('next'):
                q.put(data['next'])
            for row in data['results']:
                yield row

JMES_DEFAULT = ""{name: name, url:url, pk:pk, email:email, tags:join(',',tags), jst:jst, regon:regon}""


class Command(object):

    def __init__(self, argv):
        self.argv = argv
        self.args = self.get_build_args(argv[1:])
        self.s = requests.Session() if not self.args.cache else requests_cache.CachedSession()

    def get_build_args(self, argv):
        parser = argparse.ArgumentParser()
        parser.add_argument('--output',
                            required=True,
                            type=argparse.FileType('w'),
                            help=""Output CSV-file"")
        parser.add_argument('--start',
                            required=True,
                            help=""Start URL"")
        parser.add_argument('--jmes', type=jmespath.compile,
                            required=False,
                            help='JMESPath to convert values (default: ""{}"")'.format(JMES_DEFAULT),
                            default=jmespath.compile(JMES_DEFAULT))
        parser.add_argument('--cache', action='store_true', help=""Enable cache"")
        return parser.parse_args(argv)

    def run(self):

        client = Client(start=self.args.start, s=self.s)
        data = client.get_page_iter()
        first = next(data)

        fieldnames = self.args.jmes.search(first).keys()

        print(""Identified fields: {}"".format("", "".join(fieldnames)))

        writer = csv.DictWriter(self.args.output, fieldnames=fieldnames)
        writer.writeheader()
        for item in tqdm.tqdm(itertools.chain([first], data)):
            writer.writerow(self.args.jmes.search(item))

if __name__ == ""__main__"":
    sys.exit(Command(sys.argv).run())

```"
cfcd9d72eba2b2f59586816693a93bdab1f01c70,src/plan_tool.py,src/plan_tool.py,,"#!env python

from argparse import ArgumentParser
import json

import database as db
from database.model import Team, RouteDistance
from geotools import openroute_link
from geotools.routing import MapPoint
from webapp.cfg.config import DB_CONNECTION


db.init_session(connection_string=DB_CONNECTION)


def read_legacy_plan(in_file):
    with open(in_file, ""r"") as in_fn:
        data = json.load(in_fn)

    result = {}
    for entry in data:
        result[entry[""team_id""][0]] = [station[0] for station in entry[""plan""]]
    return result


def read_plan_file(args):
    result = {}
    if args.inform == ""legacy"":
        result = read_legacy_plan(args.in_file)
    elif args.inform == ""dan_marc"":
        print ""to be implemented""
    return result


def cmd_convert_plan(args):
    result = read_plan_file(args)
    with open(args.out_file, ""w+"") as out_fn:
        json.dump(result, out_fn)


def cmd_print_plan(args):
    result = read_plan_file(args)
    teams = {}

    for team in db.session.query(Team).filter_by(deleted=False).filter_by(confirmed=True):
        teams[str(team.id)] = team

    for entry in result:
        team = teams[entry]
        plan = result[entry]
        print ""# %s ::"" % team.name
        station_points = []
        last_station = None
        for station in plan:
            station_team = teams[station]
            dist = """"
            if last_station is not None:
                distance = db.session.query(RouteDistance).filter_by(location_from=last_station.location,
                                                                     location_to=station_team.location).first()
                dist = ""[dist=%d]"" % distance.distance
            print ""+ %s %s"" % (station_team.name, dist)
            station_points.append(MapPoint.from_team(station_team))
            last_station = station_team

        print ""- route: %s"" % openroute_link(station_points)
        print """"


def parse_args():
    args = ArgumentParser()

    subcommands = args.add_subparsers()
    args.add_argument(""--inform"", help=""Specify the input format"", required=True,
                      choices=(""legacy"", ""dan_marc""))
    args.add_argument(""in_file"", help=""The file to convert"")

    convert_parser = subcommands.add_parser(""convert"")
    convert_parser.add_argument(""out_file"", help=""The output file"")
    convert_parser.set_defaults(func=cmd_convert_plan)

    print_parser = subcommands.add_parser(""print"")
    print_parser.add_argument(""--osm"", action=""store_true"", help=""build osm route links"")
    print_parser.set_defaults(func=cmd_print_plan)

    return args.parse_args()


if __name__ == ""__main__"":
    args = parse_args()
    args.func(args)",Add a script to process the team plans.,"Add a script to process the team plans.

It can read the legacy json format and print the old overview.
It can also convert the legacy format to some more readeable.
",Python,bsd-3-clause,"janLo/meet-and-eat-registration-system,janLo/meet-and-eat-registration-system,eXma/meet-and-eat-registration-system,eXma/meet-and-eat-registration-system,eXma/meet-and-eat-registration-system,janLo/meet-and-eat-registration-system,eXma/meet-and-eat-registration-system,janLo/meet-and-eat-registration-system",90,"```python
#!env python

from argparse import ArgumentParser
import json

import database as db
from database.model import Team, RouteDistance
from geotools import openroute_link
from geotools.routing import MapPoint
from webapp.cfg.config import DB_CONNECTION


db.init_session(connection_string=DB_CONNECTION)


def read_legacy_plan(in_file):
    with open(in_file, ""r"") as in_fn:
        data = json.load(in_fn)

    result = {}
    for entry in data:
        result[entry[""team_id""][0]] = [station[0] for station in entry[""plan""]]
    return result


def read_plan_file(args):
    result = {}
    if args.inform == ""legacy"":
        result = read_legacy_plan(args.in_file)
    elif args.inform == ""dan_marc"":
        print ""to be implemented""
    return result


def cmd_convert_plan(args):
    result = read_plan_file(args)
    with open(args.out_file, ""w+"") as out_fn:
        json.dump(result, out_fn)


def cmd_print_plan(args):
    result = read_plan_file(args)
    teams = {}

    for team in db.session.query(Team).filter_by(deleted=False).filter_by(confirmed=True):
        teams[str(team.id)] = team

    for entry in result:
        team = teams[entry]
        plan = result[entry]
        print ""# %s ::"" % team.name
        station_points = []
        last_station = None
        for station in plan:
            station_team = teams[station]
            dist = """"
            if last_station is not None:
                distance = db.session.query(RouteDistance).filter_by(location_from=last_station.location,
                                                                     location_to=station_team.location).first()
                dist = ""[dist=%d]"" % distance.distance
            print ""+ %s %s"" % (station_team.name, dist)
            station_points.append(MapPoint.from_team(station_team))
            last_station = station_team

        print ""- route: %s"" % openroute_link(station_points)
        print """"


def parse_args():
    args = ArgumentParser()

    subcommands = args.add_subparsers()
    args.add_argument(""--inform"", help=""Specify the input format"", required=True,
                      choices=(""legacy"", ""dan_marc""))
    args.add_argument(""in_file"", help=""The file to convert"")

    convert_parser = subcommands.add_parser(""convert"")
    convert_parser.add_argument(""out_file"", help=""The output file"")
    convert_parser.set_defaults(func=cmd_convert_plan)

    print_parser = subcommands.add_parser(""print"")
    print_parser.add_argument(""--osm"", action=""store_true"", help=""build osm route links"")
    print_parser.set_defaults(func=cmd_print_plan)

    return args.parse_args()


if __name__ == ""__main__"":
    args = parse_args()
    args.func(args)
```"
63f9bf61a8edcbd88844281c3d343097a8ca49ef,tests/test_cli.py,tests/test_cli.py,,"import re


def test_simple(pyscript):
    script = pyscript(""""""
        import click
        from daemonocle.cli import DaemonCLI

        @click.command(cls=DaemonCLI,
                       daemon_params={'prog': 'foo', 'pidfile': 'foo.pid'})
        def main():
            \""\""\""My awesome daemon\""\""\""
            pass

        if __name__ == '__main__':
            main()
    """""")
    result = script.run('--help')
    assert result.returncode == 0
    assert b'My awesome daemon' in result.stdout
    assert re.search((
        br'\s*start\s+Start the daemon\.\n'
        br'\s*stop\s+Stop the daemon\.\n'
        br'\s*restart\s+Stop then start the daemon\.\n'
        br'\s*status\s+Get the status of the daemon\.\n'),
        result.stdout)

    result = script.run('start', '--help')
    assert result.returncode == 0
    assert re.search(
        br'\s*--debug\s+Do NOT detach and run in the background\.\n',
        result.stdout)

    assert script.run('stop', '--help').returncode == 0
    assert script.run('restart', '--help').returncode == 0
    assert script.run('status', '--help').returncode == 0


def test_debug(pyscript):
    script = pyscript(""""""
        import click
        from daemonocle.cli import DaemonCLI

        @click.command(cls=DaemonCLI, daemon_params={'prog': 'foo'})
        def main():
            \""\""\""My awesome daemon\""\""\""
            print('hello world')

        if __name__ == '__main__':
            main()
    """""")
    result = script.run('start', '--debug')
    assert result.returncode == 0
    assert result.stdout == (
        b'Starting foo ... OK\n'
        b'hello world\n'
        b'All children are gone. Parent is exiting...\n')
    assert result.stderr == b''


def test_custom_actions(pyscript):
    script = pyscript(""""""
        import time
        import click
        from daemonocle import Daemon, expose_action
        from daemonocle.cli import DaemonCLI

        class BananaDaemon(Daemon):
            @expose_action
            def banana(self):
                \""\""\""Go bananas.\""\""\""
                pass

            def plantain(self):
                pass

        @click.command(cls=DaemonCLI, daemon_class=BananaDaemon,
                       daemon_params={'prog': 'foo', 'pidfile': 'foo.pid'})
        def main():
            \""\""\""The banana daemon\""\""\""
            pass

        if __name__ == '__main__':
            main()
    """""")
    result = script.run('--help')
    assert result.returncode == 0
    assert b'The banana daemon' in result.stdout
    assert re.search(br'\s*banana\s+Go bananas\.\n', result.stdout)

    assert script.run('banana', '--help').returncode == 0

    result = script.run('plantain', '--help')
    assert result.returncode != 0
    assert 'No such command' in result.stderr
",Add tests for Click integration,Add tests for Click integration,Python,mit,jnrbsn/daemonocle,96,"```python
import re


def test_simple(pyscript):
    script = pyscript(""""""
        import click
        from daemonocle.cli import DaemonCLI

        @click.command(cls=DaemonCLI,
                       daemon_params={'prog': 'foo', 'pidfile': 'foo.pid'})
        def main():
            \""\""\""My awesome daemon\""\""\""
            pass

        if __name__ == '__main__':
            main()
    """""")
    result = script.run('--help')
    assert result.returncode == 0
    assert b'My awesome daemon' in result.stdout
    assert re.search((
        br'\s*start\s+Start the daemon\.\n'
        br'\s*stop\s+Stop the daemon\.\n'
        br'\s*restart\s+Stop then start the daemon\.\n'
        br'\s*status\s+Get the status of the daemon\.\n'),
        result.stdout)

    result = script.run('start', '--help')
    assert result.returncode == 0
    assert re.search(
        br'\s*--debug\s+Do NOT detach and run in the background\.\n',
        result.stdout)

    assert script.run('stop', '--help').returncode == 0
    assert script.run('restart', '--help').returncode == 0
    assert script.run('status', '--help').returncode == 0


def test_debug(pyscript):
    script = pyscript(""""""
        import click
        from daemonocle.cli import DaemonCLI

        @click.command(cls=DaemonCLI, daemon_params={'prog': 'foo'})
        def main():
            \""\""\""My awesome daemon\""\""\""
            print('hello world')

        if __name__ == '__main__':
            main()
    """""")
    result = script.run('start', '--debug')
    assert result.returncode == 0
    assert result.stdout == (
        b'Starting foo ... OK\n'
        b'hello world\n'
        b'All children are gone. Parent is exiting...\n')
    assert result.stderr == b''


def test_custom_actions(pyscript):
    script = pyscript(""""""
        import time
        import click
        from daemonocle import Daemon, expose_action
        from daemonocle.cli import DaemonCLI

        class BananaDaemon(Daemon):
            @expose_action
            def banana(self):
                \""\""\""Go bananas.\""\""\""
                pass

            def plantain(self):
                pass

        @click.command(cls=DaemonCLI, daemon_class=BananaDaemon,
                       daemon_params={'prog': 'foo', 'pidfile': 'foo.pid'})
        def main():
            \""\""\""The banana daemon\""\""\""
            pass

        if __name__ == '__main__':
            main()
    """""")
    result = script.run('--help')
    assert result.returncode == 0
    assert b'The banana daemon' in result.stdout
    assert re.search(br'\s*banana\s+Go bananas\.\n', result.stdout)

    assert script.run('banana', '--help').returncode == 0

    result = script.run('plantain', '--help')
    assert result.returncode != 0
    assert 'No such command' in result.stderr

```"
dd4d624ab51b610cfe1a7047368d8c305156916b,scripts/check_repeated_token.py,scripts/check_repeated_token.py,,"#!/usr/bin/env python
""""""
    Checker for repeated tokens
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Helper script to find suspicious lexers which produce the same token
    repeatedly, i.e. for example:

    .. code::
   
      'd'           Text
      'a'           Text
      't'           Text
      'a'           Text
      'b'           Text
      'a'           Text
      's'           Text
      'e'           Text
   
    This script has two test modes: Check for tokens repeating more often than
    a given threshold, and exclude anything but single-character tokens.
    Repeated single-character tokens are quite problematic as they result in
    bloated output and are usually an indication that someone is missing a + or *
    in the regex. 

    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""
import argparse
import os
import sys



def unpack_file(path):
    """"""Unpack a file into text, token pairs.""""""
    from collections import namedtuple
    pair = namedtuple('TextTokenPair', ['text', 'token'])
    for line in open(path).readlines():
        line = line.strip()
        if line:
            quotation_start = line.find('\'')
            quotation_end = line.rfind('\'')
            text = line[quotation_start+1:quotation_end]
            token = line.split()[-1]
            text = text.replace('\\n', '\n')
            text = text.replace('\\t', '\t')
            yield pair(text, token)

def check_file(path, threshold, single_only):
    current_token = ''
    current_token_repeat_count = 1

    is_suspicious = False

    for value, token in unpack_file(path):
        if single_only and len(value) > 1:
            token = ''
            current_token_repeat_count = 1
            continue

        if token != current_token:
            current_token = token
            current_token_repeat_count = 1
        else:
            current_token_repeat_count += 1
        
        if current_token_repeat_count > threshold:
            is_suspicious = True
            break

    if is_suspicious:
        print(path)

    return not is_suspicious
        

def main(args):
    errors = 0
    for dir, _, files in os.walk(args.TEST_ROOT):
        for file in files:
            if not file.endswith('.output'):
                continue
            
            path = os.path.join(dir, file)
            if not check_file(path, args.threshold, args.single):
                errors += 1

    if errors > 0:
        return 1
    return 0


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('TEST_ROOT',
        help='Root directory containing the tests')
    parser.add_argument('-t', '--threshold', type=int, default=5,
        help='Warn if a token repeats itself more often then this number.')
    parser.add_argument('-s', '--single', action='store_true', default=False,
        help='Only look at tokens matching a single character')
    args = parser.parse_args()
    sys.exit(main(args))",Add a script to check for repeated tokens.,"Add a script to check for repeated tokens.
",Python,bsd-2-clause,"dscorbett/pygments,pygments/pygments,dscorbett/pygments,pygments/pygments,dscorbett/pygments,dscorbett/pygments,pygments/pygments,pygments/pygments,pygments/pygments,dscorbett/pygments,dscorbett/pygments,dscorbett/pygments,pygments/pygments,pygments/pygments,dscorbett/pygments,pygments/pygments,pygments/pygments,pygments/pygments,dscorbett/pygments,dscorbett/pygments,dscorbett/pygments,dscorbett/pygments,pygments/pygments,dscorbett/pygments,pygments/pygments,pygments/pygments,pygments/pygments,dscorbett/pygments,dscorbett/pygments,pygments/pygments,dscorbett/pygments,pygments/pygments,dscorbett/pygments,pygments/pygments,pygments/pygments,dscorbett/pygments",103,"```python
#!/usr/bin/env python
""""""
    Checker for repeated tokens
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~

    Helper script to find suspicious lexers which produce the same token
    repeatedly, i.e. for example:

    .. code::
   
      'd'           Text
      'a'           Text
      't'           Text
      'a'           Text
      'b'           Text
      'a'           Text
      's'           Text
      'e'           Text
   
    This script has two test modes: Check for tokens repeating more often than
    a given threshold, and exclude anything but single-character tokens.
    Repeated single-character tokens are quite problematic as they result in
    bloated output and are usually an indication that someone is missing a + or *
    in the regex. 

    :copyright: Copyright 2006-2021 by the Pygments team, see AUTHORS.
    :license: BSD, see LICENSE for details.
""""""
import argparse
import os
import sys



def unpack_file(path):
    """"""Unpack a file into text, token pairs.""""""
    from collections import namedtuple
    pair = namedtuple('TextTokenPair', ['text', 'token'])
    for line in open(path).readlines():
        line = line.strip()
        if line:
            quotation_start = line.find('\'')
            quotation_end = line.rfind('\'')
            text = line[quotation_start+1:quotation_end]
            token = line.split()[-1]
            text = text.replace('\\n', '\n')
            text = text.replace('\\t', '\t')
            yield pair(text, token)

def check_file(path, threshold, single_only):
    current_token = ''
    current_token_repeat_count = 1

    is_suspicious = False

    for value, token in unpack_file(path):
        if single_only and len(value) > 1:
            token = ''
            current_token_repeat_count = 1
            continue

        if token != current_token:
            current_token = token
            current_token_repeat_count = 1
        else:
            current_token_repeat_count += 1
        
        if current_token_repeat_count > threshold:
            is_suspicious = True
            break

    if is_suspicious:
        print(path)

    return not is_suspicious
        

def main(args):
    errors = 0
    for dir, _, files in os.walk(args.TEST_ROOT):
        for file in files:
            if not file.endswith('.output'):
                continue
            
            path = os.path.join(dir, file)
            if not check_file(path, args.threshold, args.single):
                errors += 1

    if errors > 0:
        return 1
    return 0


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('TEST_ROOT',
        help='Root directory containing the tests')
    parser.add_argument('-t', '--threshold', type=int, default=5,
        help='Warn if a token repeats itself more often then this number.')
    parser.add_argument('-s', '--single', action='store_true', default=False,
        help='Only look at tokens matching a single character')
    args = parser.parse_args()
    sys.exit(main(args))
```"
93ba1d34e51a9b959b63e77eae78845dd86203ed,camera-capture-test.py,camera-capture-test.py,,"#! /usr/bin/env python
# -*- coding:utf-8 -*- 


#
# Application to capture images synchronously
# from two AVT Manta cameras with the Vimba SDK
#


# External dependencies
import collections, cv2, time
import Vimba

# Number of images saved
image_count = 0

# Frame per second counter
counter = 0
fps_counter = 0
fps_buffer = collections.deque( 10*[0], 10 )

# Vimba initialization
print( 'Vimba initialization...' )
vimba = Vimba.VmbDriver()
vimba.Startup()

# Camera connection
print( 'Camera connection...' )
camera = Vimba.VmbCamera( vimba )
camera.Connect( '50-0503323406' )

# Start image acquisition
print( 'Start acquisition...' )
camera.CaptureStart()

# Live display
while True :
	
	# Initialize the clock for counting the number of frames per second
	time_start = time.clock()

	# Capture an image
	camera.CaptureFrame()
	
	# Resize image for display
	image_final = cv2.resize( camera.image, None, fx=0.3, fy=0.3 )

	# Display the image (scaled down)
	cv2.imshow( ""Camera"", image_final )
	
	# Keyboard interruption
	key = cv2.waitKey(1) & 0xFF
	
	# Escape key
	if key == 27 :
		
		# Exit live display
		break
		
	# Space key
	elif key == 32 :
		
		# Save images to disk 
		image_count += 1
		print( 'Save image {} to disk...'.format( image_count ) )
		cv2.imwrite( 'camera-{:0>2}.png'.format(image_count), camera.image )
		
	# Frames per second counter
	fps_buffer.pop()
	fps_buffer.appendleft( time.clock() - time_start )
	fps_counter = 10.0 / sum( fps_buffer )
	counter += 1
	if counter == 20 :
		print( '{:.2f} FPS'.format( fps_counter ) )
		counter = 0

# Cleanup OpenCV
cv2.destroyAllWindows()

# Stop image acquisition
print( 'Stop acquisition...' )
camera.CaptureStop()

# Camera disconnection
print( 'Camera disconnection...' )
camera.Disconnect()

# Vimba shutdown
print( 'Vimba shutdown...' )
vimba.Shutdown()
",Test to use the new Vimba module.,"Test to use the new Vimba module.
",Python,mit,"microy/VisionToolkit,microy/StereoVision,microy/VisionToolkit,microy/PyStereoVisionToolkit,microy/PyStereoVisionToolkit,microy/StereoVision",92,"```python
#! /usr/bin/env python
# -*- coding:utf-8 -*- 


#
# Application to capture images synchronously
# from two AVT Manta cameras with the Vimba SDK
#


# External dependencies
import collections, cv2, time
import Vimba

# Number of images saved
image_count = 0

# Frame per second counter
counter = 0
fps_counter = 0
fps_buffer = collections.deque( 10*[0], 10 )

# Vimba initialization
print( 'Vimba initialization...' )
vimba = Vimba.VmbDriver()
vimba.Startup()

# Camera connection
print( 'Camera connection...' )
camera = Vimba.VmbCamera( vimba )
camera.Connect( '50-0503323406' )

# Start image acquisition
print( 'Start acquisition...' )
camera.CaptureStart()

# Live display
while True :
	
	# Initialize the clock for counting the number of frames per second
	time_start = time.clock()

	# Capture an image
	camera.CaptureFrame()
	
	# Resize image for display
	image_final = cv2.resize( camera.image, None, fx=0.3, fy=0.3 )

	# Display the image (scaled down)
	cv2.imshow( ""Camera"", image_final )
	
	# Keyboard interruption
	key = cv2.waitKey(1) & 0xFF
	
	# Escape key
	if key == 27 :
		
		# Exit live display
		break
		
	# Space key
	elif key == 32 :
		
		# Save images to disk 
		image_count += 1
		print( 'Save image {} to disk...'.format( image_count ) )
		cv2.imwrite( 'camera-{:0>2}.png'.format(image_count), camera.image )
		
	# Frames per second counter
	fps_buffer.pop()
	fps_buffer.appendleft( time.clock() - time_start )
	fps_counter = 10.0 / sum( fps_buffer )
	counter += 1
	if counter == 20 :
		print( '{:.2f} FPS'.format( fps_counter ) )
		counter = 0

# Cleanup OpenCV
cv2.destroyAllWindows()

# Stop image acquisition
print( 'Stop acquisition...' )
camera.CaptureStop()

# Camera disconnection
print( 'Camera disconnection...' )
camera.Disconnect()

# Vimba shutdown
print( 'Vimba shutdown...' )
vimba.Shutdown()

```"
4084fba3d438da4a4133dfafec985d8dd4191a3a,examples/demo/basic/scatter_1d.py,examples/demo/basic/scatter_1d.py,,"""""""
Scatter plot with auxilliary 1d plots

Shows a scatter plot of a set of random points,
with auxilliary 1d plots of the data.

""""""

# Major library imports
from numpy import sort
from numpy.random import random, randint

# Enthought library imports
from enable.api import Component, ComponentEditor
from traits.api import HasTraits, Instance
from traitsui.api import Item, Group, View

# Chaco imports
from chaco.api import ArrayPlotData, Plot
from chaco.tools.api import PanTool, ZoomTool

#===============================================================================
# # Create the Chaco plot.
#===============================================================================
def _create_plot_component():

    # Create some data
    numpts = 50
    x = sort(random(numpts))
    y = random(numpts)

    # Create a plot data object and give it this data
    pd = ArrayPlotData()
    pd.set_data(""index"", x)
    pd.set_data(""value"", y)

    # Create the plot
    plot = Plot(pd, use_backbuffer=True, auto_grid=False)

    plot.plot_1d(
        'index',
        type='line_scatter_1d',
        orientation='h',
        color='lightgrey',
        line_style='dot',
    )

    plot.plot_1d(
        'index',
        type='scatter_1d',
        orientation='h',
        marker='plus',
        marker_alignment='bottom'
    )

    plot.plot_1d(
        'value',
        type='line_scatter_1d',
        orientation='v',
        color='lightgrey',
        line_style='dot',
    )

    plot.plot_1d(
        'value',
        type='scatter_1d',
        orientation='v',
        marker='plus',
        marker_alignment='left'
    )

    plot.plot((""index"", ""value""),
              type=""scatter"",
              marker=""square"",
              index_sort=""ascending"",
              color=""orange"",
              marker_size=3, #randint(1,5, numpts),
              bgcolor=""white"",
              use_backbuffer=True)


    # Tweak some of the plot properties
    plot.title = ""1D Scatter Plots""
    plot.line_width = 0.5
    plot.padding = 50

    # Attach some tools to the plot
    plot.tools.append(PanTool(plot, constrain_key=""shift""))
    zoom = ZoomTool(component=plot, tool_mode=""box"", always_on=False)
    plot.overlays.append(zoom)

    return plot

#===============================================================================
# Attributes to use for the plot view.
size = (650, 650)
title = ""1D scatter plots""
bg_color=""lightgray""

#===============================================================================
# # Demo class that is used by the demo.py application.
#===============================================================================
class Demo(HasTraits):
    plot = Instance(Component)

    traits_view = View(
                    Group(
                        Item('plot', editor=ComponentEditor(size=size,
                                                            bgcolor=bg_color),
                             show_label=False),
                        orientation = ""vertical""),
                    resizable=True, title=title
                    )

    def _plot_default(self):
         return _create_plot_component()

demo = Demo()

if __name__ == ""__main__"":
    demo.configure_traits()

#--EOF---
",Add example showing 1d scatter plots.,"Add example showing 1d scatter plots.
",Python,bsd-3-clause,"tommy-u/chaco,tommy-u/chaco,tommy-u/chaco",124,"```python
""""""
Scatter plot with auxilliary 1d plots

Shows a scatter plot of a set of random points,
with auxilliary 1d plots of the data.

""""""

# Major library imports
from numpy import sort
from numpy.random import random, randint

# Enthought library imports
from enable.api import Component, ComponentEditor
from traits.api import HasTraits, Instance
from traitsui.api import Item, Group, View

# Chaco imports
from chaco.api import ArrayPlotData, Plot
from chaco.tools.api import PanTool, ZoomTool

#===============================================================================
# # Create the Chaco plot.
#===============================================================================
def _create_plot_component():

    # Create some data
    numpts = 50
    x = sort(random(numpts))
    y = random(numpts)

    # Create a plot data object and give it this data
    pd = ArrayPlotData()
    pd.set_data(""index"", x)
    pd.set_data(""value"", y)

    # Create the plot
    plot = Plot(pd, use_backbuffer=True, auto_grid=False)

    plot.plot_1d(
        'index',
        type='line_scatter_1d',
        orientation='h',
        color='lightgrey',
        line_style='dot',
    )

    plot.plot_1d(
        'index',
        type='scatter_1d',
        orientation='h',
        marker='plus',
        marker_alignment='bottom'
    )

    plot.plot_1d(
        'value',
        type='line_scatter_1d',
        orientation='v',
        color='lightgrey',
        line_style='dot',
    )

    plot.plot_1d(
        'value',
        type='scatter_1d',
        orientation='v',
        marker='plus',
        marker_alignment='left'
    )

    plot.plot((""index"", ""value""),
              type=""scatter"",
              marker=""square"",
              index_sort=""ascending"",
              color=""orange"",
              marker_size=3, #randint(1,5, numpts),
              bgcolor=""white"",
              use_backbuffer=True)


    # Tweak some of the plot properties
    plot.title = ""1D Scatter Plots""
    plot.line_width = 0.5
    plot.padding = 50

    # Attach some tools to the plot
    plot.tools.append(PanTool(plot, constrain_key=""shift""))
    zoom = ZoomTool(component=plot, tool_mode=""box"", always_on=False)
    plot.overlays.append(zoom)

    return plot

#===============================================================================
# Attributes to use for the plot view.
size = (650, 650)
title = ""1D scatter plots""
bg_color=""lightgray""

#===============================================================================
# # Demo class that is used by the demo.py application.
#===============================================================================
class Demo(HasTraits):
    plot = Instance(Component)

    traits_view = View(
                    Group(
                        Item('plot', editor=ComponentEditor(size=size,
                                                            bgcolor=bg_color),
                             show_label=False),
                        orientation = ""vertical""),
                    resizable=True, title=title
                    )

    def _plot_default(self):
         return _create_plot_component()

demo = Demo()

if __name__ == ""__main__"":
    demo.configure_traits()

#--EOF---

```"
6dd32078aa461488872745bcbc7c43cd3988ed53,tests/api/benchmark/test_order.py,tests/api/benchmark/test_order.py,,"import pytest

from tests.api.utils import get_graphql_content


@pytest.mark.django_db
@pytest.mark.count_queries(autouse=False)
def test_user_order_details(user_api_client, order_with_lines, count_queries):
    query = """"""
        fragment OrderPrice on TaxedMoney {
          gross {
            amount
            currency
          }
          net {
            amount
            currency
          }
        }

        fragment Address on Address {
          id
          firstName
          lastName
          companyName
          streetAddress1
          streetAddress2
          city
          postalCode
          country {
            code
            country
          }
          countryArea
          phone
          isDefaultBillingAddress
          isDefaultShippingAddress
        }

        fragment Price on TaxedMoney {
          gross {
            amount
            currency
          }
          net {
            amount
            currency
          }
        }

        fragment ProductVariant on ProductVariant {
          id
          name
          pricing {
            onSale
            priceUndiscounted {
              ...Price
            }
            price {
              ...Price
            }
          }
          product {
            id
            name
            thumbnail {
              url
              alt
            }
            thumbnail2x: thumbnail(size: 510) {
              url
            }
          }
        }

        fragment OrderDetail on Order {
          userEmail
          paymentStatus
          paymentStatusDisplay
          status
          statusDisplay
          id
          number
          shippingAddress {
            ...Address
          }
          lines {
            productName
            quantity
            variant {
              ...ProductVariant
            }
            unitPrice {
              currency
              ...OrderPrice
            }
          }
          subtotal {
            ...OrderPrice
          }
          total {
            ...OrderPrice
          }
          shippingPrice {
            ...OrderPrice
          }
        }

        query OrderByToken($token: UUID!) {
          orderByToken(token: $token) {
            ...OrderDetail
          }
        }
    """"""
    variables = {
        ""token"": order_with_lines.token,
    }
    get_graphql_content(user_api_client.post_graphql(query, variables))
",Add test for order history view,"Add test for order history view
",Python,bsd-3-clause,"mociepka/saleor,mociepka/saleor,mociepka/saleor",119,"```python
import pytest

from tests.api.utils import get_graphql_content


@pytest.mark.django_db
@pytest.mark.count_queries(autouse=False)
def test_user_order_details(user_api_client, order_with_lines, count_queries):
    query = """"""
        fragment OrderPrice on TaxedMoney {
          gross {
            amount
            currency
          }
          net {
            amount
            currency
          }
        }

        fragment Address on Address {
          id
          firstName
          lastName
          companyName
          streetAddress1
          streetAddress2
          city
          postalCode
          country {
            code
            country
          }
          countryArea
          phone
          isDefaultBillingAddress
          isDefaultShippingAddress
        }

        fragment Price on TaxedMoney {
          gross {
            amount
            currency
          }
          net {
            amount
            currency
          }
        }

        fragment ProductVariant on ProductVariant {
          id
          name
          pricing {
            onSale
            priceUndiscounted {
              ...Price
            }
            price {
              ...Price
            }
          }
          product {
            id
            name
            thumbnail {
              url
              alt
            }
            thumbnail2x: thumbnail(size: 510) {
              url
            }
          }
        }

        fragment OrderDetail on Order {
          userEmail
          paymentStatus
          paymentStatusDisplay
          status
          statusDisplay
          id
          number
          shippingAddress {
            ...Address
          }
          lines {
            productName
            quantity
            variant {
              ...ProductVariant
            }
            unitPrice {
              currency
              ...OrderPrice
            }
          }
          subtotal {
            ...OrderPrice
          }
          total {
            ...OrderPrice
          }
          shippingPrice {
            ...OrderPrice
          }
        }

        query OrderByToken($token: UUID!) {
          orderByToken(token: $token) {
            ...OrderDetail
          }
        }
    """"""
    variables = {
        ""token"": order_with_lines.token,
    }
    get_graphql_content(user_api_client.post_graphql(query, variables))

```"
f771a1fa3bc7cdaeb2ef5e09d3a01701b166a009,atoman/filtering/filters/tests/test_species.py,atoman/filtering/filters/tests/test_species.py,,"
""""""
Unit tests for the species filter

""""""
import unittest

import numpy as np

from ....system import lattice
from .. import speciesFilter
from .. import base


################################################################################

class TestSpeciesFilter(unittest.TestCase):
    """"""
    Test species filter

    """"""
    def setUp(self):
        """"""
        Called before each test

        """"""
        # generate lattice
        self.lattice = lattice.Lattice()
        self.lattice.addAtom(""Au"", [0,0,3], 0)
        self.lattice.addAtom(""He"", [1,1,1], 0)
        self.lattice.addAtom(""He"", [2,3,1], 0)
        self.lattice.addAtom(""Au"", [3,0,0], 0)
        self.lattice.addAtom(""Au"", [4,1,4], 0)
        self.lattice.addAtom(""He"", [1,4,0], 0)
        self.lattice.addAtom(""He"", [2,1.8,4], 0)
        self.lattice.addAtom(""H_"", [4,4,4], 0)
        self.lattice.PBC[:] = 1

        # filter
        self.filter = speciesFilter.SpeciesFilter(""Species"")

    def tearDown(self):
        """"""
        Called after each test

        """"""
        # remove refs
        self.lattice = None
        self.filter = None

    def test_speciesFilter(self):
        """"""
        Species filter

        """"""
        # test 1
        settings = speciesFilter.SpeciesFilterSettings()
        settings.updateSetting(""visibleSpeciesList"", [""H_"", ""Au""])
        
        # filter input
        filterInput = base.FilterInput()
        filterInput.inputState = self.lattice
        visibleAtoms = np.arange(self.lattice.NAtoms, dtype=np.int32)
        filterInput.visibleAtoms = visibleAtoms
        
        # run filter
        result = self.filter.apply(filterInput, settings)
        self.assertIsInstance(result, base.FilterResult)
        
        # make sure num visible is correct
        self.assertEqual(len(visibleAtoms), 4)
        
        # make sure correct atoms selected
        self.assertTrue(0 in visibleAtoms)
        self.assertTrue(3 in visibleAtoms)
        self.assertTrue(4 in visibleAtoms)
        self.assertTrue(7 in visibleAtoms)
        
        # test 2
        settings = speciesFilter.SpeciesFilterSettings()
        settings.updateSetting(""visibleSpeciesList"", [])
        
        # filter input
        filterInput = base.FilterInput()
        filterInput.inputState = self.lattice
        visibleAtoms = np.arange(self.lattice.NAtoms, dtype=np.int32)
        filterInput.visibleAtoms = visibleAtoms
        
        # run filter
        result = self.filter.apply(filterInput, settings)
        self.assertIsInstance(result, base.FilterResult)
        
        # make sure num visible is correct
        self.assertEqual(len(visibleAtoms), 0)
",Add test for species filter.,"Add test for species filter.
",Python,mit,"chrisdjscott/Atoman,chrisdjscott/Atoman,chrisdjscott/Atoman,chrisdjscott/Atoman,chrisdjscott/Atoman",95,"```python

""""""
Unit tests for the species filter

""""""
import unittest

import numpy as np

from ....system import lattice
from .. import speciesFilter
from .. import base


################################################################################

class TestSpeciesFilter(unittest.TestCase):
    """"""
    Test species filter

    """"""
    def setUp(self):
        """"""
        Called before each test

        """"""
        # generate lattice
        self.lattice = lattice.Lattice()
        self.lattice.addAtom(""Au"", [0,0,3], 0)
        self.lattice.addAtom(""He"", [1,1,1], 0)
        self.lattice.addAtom(""He"", [2,3,1], 0)
        self.lattice.addAtom(""Au"", [3,0,0], 0)
        self.lattice.addAtom(""Au"", [4,1,4], 0)
        self.lattice.addAtom(""He"", [1,4,0], 0)
        self.lattice.addAtom(""He"", [2,1.8,4], 0)
        self.lattice.addAtom(""H_"", [4,4,4], 0)
        self.lattice.PBC[:] = 1

        # filter
        self.filter = speciesFilter.SpeciesFilter(""Species"")

    def tearDown(self):
        """"""
        Called after each test

        """"""
        # remove refs
        self.lattice = None
        self.filter = None

    def test_speciesFilter(self):
        """"""
        Species filter

        """"""
        # test 1
        settings = speciesFilter.SpeciesFilterSettings()
        settings.updateSetting(""visibleSpeciesList"", [""H_"", ""Au""])
        
        # filter input
        filterInput = base.FilterInput()
        filterInput.inputState = self.lattice
        visibleAtoms = np.arange(self.lattice.NAtoms, dtype=np.int32)
        filterInput.visibleAtoms = visibleAtoms
        
        # run filter
        result = self.filter.apply(filterInput, settings)
        self.assertIsInstance(result, base.FilterResult)
        
        # make sure num visible is correct
        self.assertEqual(len(visibleAtoms), 4)
        
        # make sure correct atoms selected
        self.assertTrue(0 in visibleAtoms)
        self.assertTrue(3 in visibleAtoms)
        self.assertTrue(4 in visibleAtoms)
        self.assertTrue(7 in visibleAtoms)
        
        # test 2
        settings = speciesFilter.SpeciesFilterSettings()
        settings.updateSetting(""visibleSpeciesList"", [])
        
        # filter input
        filterInput = base.FilterInput()
        filterInput.inputState = self.lattice
        visibleAtoms = np.arange(self.lattice.NAtoms, dtype=np.int32)
        filterInput.visibleAtoms = visibleAtoms
        
        # run filter
        result = self.filter.apply(filterInput, settings)
        self.assertIsInstance(result, base.FilterResult)
        
        # make sure num visible is correct
        self.assertEqual(len(visibleAtoms), 0)

```"
f50644484f4b05fbb25adfd6430b6207441d8b2e,src/ggrc_basic_permissions/migrations/versions/20131008124800_8f33d9bd2043_fix_system_roles.py,src/ggrc_basic_permissions/migrations/versions/20131008124800_8f33d9bd2043_fix_system_roles.py,,"""""""

Revision ID: 8f33d9bd2043
Revises: 758b4012b5f
Create Date: 2013-09-20 14:12:32.846302

""""""

# revision identifiers, used by Alembic.
revision = '8f33d9bd2043'
down_revision = '758b4012b5f'

import json
import sqlalchemy as sa
from alembic import op
from datetime import datetime
from sqlalchemy.sql import table, column

roles_table = table('roles',
    column('id', sa.Integer),
    column('name', sa.String),
    column('permissions_json', sa.Text),
    column('description', sa.Text),
    column('modified_by_id', sa.Integer),
    column('created_at', sa.DateTime),
    column('updated_at', sa.DateTime),
    column('context_id', sa.Integer),
    )


def upgrade():
  basic_objects_editable = [
      'Categorization',
      'Category',
      'Control',
      'ControlControl',
      'ControlSection',
      'Cycle',
      'DataAsset',
      'Directive',
        'Contract',
        'Policy',
        'Regulation',
      'DirectiveControl',
      'Document',
      'Facility',
      'Help',
      'Market',
      'Objective',
      'ObjectiveControl',
      'ObjectControl',
      'ObjectDocument',
      'ObjectObjective',
      'ObjectPerson',
      'ObjectSection',
      'Option',
      'OrgGroup',
      'PopulationSample',
      'Product',
      'ProgramControl',
      'ProgramDirective',
      'Project',
      'Relationship',
      'RelationshipType',
      'Section',
      'SectionObjective',
      'SystemOrProcess',
        'System',
        'Process',
      'SystemControl',
      'SystemSysetm',
      ]

  basic_objects_readable = list(basic_objects_editable)
  basic_objects_readable.extend([
      'Person',
      'Program',
      'Role',
      #'UserRole', ?? why?
      ])

  basic_objects_creatable = list(basic_objects_editable)
  basic_objects_creatable.extend([
      'Person',
      ])

  basic_objects_updateable = list(basic_objects_editable)
  basic_objects_updateable.extend([
      'Person',
      ])

  basic_objects_deletable = list(basic_objects_editable)

  op.execute(roles_table.update()\
      .where(roles_table.c.name == 'Reader')\
      .values(permissions_json=json.dumps({
            'read':   basic_objects_readable,
            })))
  op.execute(roles_table.update()\
      .where(roles_table.c.name == 'ObjectEditor')\
      .values(permissions_json=json.dumps({
            'create': basic_objects_creatable,
            'read':   basic_objects_readable,
            'update': basic_objects_updateable,
            'delete': basic_objects_deletable,
            })))

def downgrade():
  # No reason to downgrade this one
  pass
",Add migration to fix system roles,"Add migration to fix system roles

* `ObjectEditor` and `Reader` were missing `ProgramDirective`,
  `ProgramControl`, and `Person` permissions (CRUD, except `Person`,
  which is CRU.
* `ObjectControl` and `ObjectDocument` were combined due to a missing
  comma in a previous migration.
",Python,apache-2.0,"hasanalom/ggrc-core,VinnieJohns/ggrc-core,AleksNeStu/ggrc-core,plamut/ggrc-core,hasanalom/ggrc-core,jmakov/ggrc-core,jmakov/ggrc-core,kr41/ggrc-core,AleksNeStu/ggrc-core,prasannav7/ggrc-core,hasanalom/ggrc-core,j0gurt/ggrc-core,uskudnik/ggrc-core,uskudnik/ggrc-core,hyperNURb/ggrc-core,jmakov/ggrc-core,prasannav7/ggrc-core,hyperNURb/ggrc-core,uskudnik/ggrc-core,plamut/ggrc-core,j0gurt/ggrc-core,andrei-karalionak/ggrc-core,plamut/ggrc-core,hyperNURb/ggrc-core,kr41/ggrc-core,jmakov/ggrc-core,VinnieJohns/ggrc-core,josthkko/ggrc-core,hyperNURb/ggrc-core,vladan-m/ggrc-core,kr41/ggrc-core,prasannav7/ggrc-core,hasanalom/ggrc-core,selahssea/ggrc-core,vladan-m/ggrc-core,selahssea/ggrc-core,j0gurt/ggrc-core,AleksNeStu/ggrc-core,NejcZupec/ggrc-core,josthkko/ggrc-core,AleksNeStu/ggrc-core,edofic/ggrc-core,VinnieJohns/ggrc-core,vladan-m/ggrc-core,NejcZupec/ggrc-core,kr41/ggrc-core,edofic/ggrc-core,selahssea/ggrc-core,andrei-karalionak/ggrc-core,edofic/ggrc-core,andrei-karalionak/ggrc-core,josthkko/ggrc-core,NejcZupec/ggrc-core,prasannav7/ggrc-core,uskudnik/ggrc-core,jmakov/ggrc-core,selahssea/ggrc-core,edofic/ggrc-core,j0gurt/ggrc-core,uskudnik/ggrc-core,vladan-m/ggrc-core,VinnieJohns/ggrc-core,vladan-m/ggrc-core,NejcZupec/ggrc-core,hyperNURb/ggrc-core,plamut/ggrc-core,josthkko/ggrc-core,andrei-karalionak/ggrc-core,hasanalom/ggrc-core",111,"```python
""""""

Revision ID: 8f33d9bd2043
Revises: 758b4012b5f
Create Date: 2013-09-20 14:12:32.846302

""""""

# revision identifiers, used by Alembic.
revision = '8f33d9bd2043'
down_revision = '758b4012b5f'

import json
import sqlalchemy as sa
from alembic import op
from datetime import datetime
from sqlalchemy.sql import table, column

roles_table = table('roles',
    column('id', sa.Integer),
    column('name', sa.String),
    column('permissions_json', sa.Text),
    column('description', sa.Text),
    column('modified_by_id', sa.Integer),
    column('created_at', sa.DateTime),
    column('updated_at', sa.DateTime),
    column('context_id', sa.Integer),
    )


def upgrade():
  basic_objects_editable = [
      'Categorization',
      'Category',
      'Control',
      'ControlControl',
      'ControlSection',
      'Cycle',
      'DataAsset',
      'Directive',
        'Contract',
        'Policy',
        'Regulation',
      'DirectiveControl',
      'Document',
      'Facility',
      'Help',
      'Market',
      'Objective',
      'ObjectiveControl',
      'ObjectControl',
      'ObjectDocument',
      'ObjectObjective',
      'ObjectPerson',
      'ObjectSection',
      'Option',
      'OrgGroup',
      'PopulationSample',
      'Product',
      'ProgramControl',
      'ProgramDirective',
      'Project',
      'Relationship',
      'RelationshipType',
      'Section',
      'SectionObjective',
      'SystemOrProcess',
        'System',
        'Process',
      'SystemControl',
      'SystemSysetm',
      ]

  basic_objects_readable = list(basic_objects_editable)
  basic_objects_readable.extend([
      'Person',
      'Program',
      'Role',
      #'UserRole', ?? why?
      ])

  basic_objects_creatable = list(basic_objects_editable)
  basic_objects_creatable.extend([
      'Person',
      ])

  basic_objects_updateable = list(basic_objects_editable)
  basic_objects_updateable.extend([
      'Person',
      ])

  basic_objects_deletable = list(basic_objects_editable)

  op.execute(roles_table.update()\
      .where(roles_table.c.name == 'Reader')\
      .values(permissions_json=json.dumps({
            'read':   basic_objects_readable,
            })))
  op.execute(roles_table.update()\
      .where(roles_table.c.name == 'ObjectEditor')\
      .values(permissions_json=json.dumps({
            'create': basic_objects_creatable,
            'read':   basic_objects_readable,
            'update': basic_objects_updateable,
            'delete': basic_objects_deletable,
            })))

def downgrade():
  # No reason to downgrade this one
  pass

```"
1c01ff6e4510abcb7a69fe61e1a5b8ef17818529,demo/chaco_demo.py,demo/chaco_demo.py,,"# Copyright (c) 2015, Warren Weckesser.  All rights reserved.
# This software is licensed according to the ""BSD 2-clause"" license.
#
# Use chaco to display the eye diagram computed by eyediagram.grid_count.

import numpy as np

# ETS imports...
from traits.api import HasTraits, Instance
from traitsui.api import Item, Group, View
from enable.api import Component, ComponentEditor
from chaco.api import ArrayPlotData, cool, Plot, PlotGrid
from chaco.tools.api import PanTool, ZoomTool

from eyediagram.demo_data import demo_data
from eyediagram.core import grid_count


def _create_plot_component():

    # Generate some data for the eye diagram.
    num_samples = 5000
    samples_per_symbol = 24
    y = demo_data(num_samples, samples_per_symbol)

    # Compute the eye diagram array.
    ybounds = (-0.25, 1.25)
    grid = grid_count(y, 2*samples_per_symbol, offset=16, size=(480, 480),
                      bounds=ybounds).T

    # Convert the array to floating point, and replace 0 with np.nan.
    # These points will be transparent in the image plot.
    grid = grid.astype(np.float32)
    grid[grid == 0] = np.nan

    #---------------------------------------------------------------------
    # The rest of the function creates the chaco image plot.

    pd = ArrayPlotData()
    pd.set_data(""eyediagram"", grid)

    plot = Plot(pd)
    img_plot = plot.img_plot(""eyediagram"",
                             xbounds=(0, 2),
                             ybounds=ybounds,
                             bgcolor=(0, 0, 0),
                             colormap=cool)[0]

    # Tweak some of the plot properties
    plot.title = ""Eye Diagram""
    plot.padding = 50

    # Axis grids
    vgrid = PlotGrid(component=plot, mapper=plot.index_mapper,
                     orientation='vertical',
                     line_color='gray', line_style='dot')
    hgrid = PlotGrid(component=plot, mapper=plot.value_mapper,
                     orientation='horizontal',
                     line_color='gray', line_style='dot')
    plot.underlays.append(vgrid)
    plot.underlays.append(hgrid)

    # Add pan and zoom tools.
    plot.tools.append(PanTool(plot))
    zoom = ZoomTool(component=img_plot, tool_mode=""box"", always_on=False)
    img_plot.overlays.append(zoom)

    return plot


class EyeDiagramDemo(HasTraits):

    plot = Instance(Component)

    traits_view = \
        View(
            Group(
                Item('plot', editor=ComponentEditor(size=(700, 500)),
                     show_label=False),
                orientation=""vertical"",
            ),
            resizable=True, title=""Eye Diagram"",
        )

    def _plot_default(self):
        return _create_plot_component()


if __name__ == ""__main__"":
    demo = EyeDiagramDemo()
    demo.configure_traits()
",Add a demo that uses chaco to plot the eye diagram.,"ENH: Add a demo that uses chaco to plot the eye diagram.
",Python,bsd-2-clause,WarrenWeckesser/eyediagram,92,"```python
# Copyright (c) 2015, Warren Weckesser.  All rights reserved.
# This software is licensed according to the ""BSD 2-clause"" license.
#
# Use chaco to display the eye diagram computed by eyediagram.grid_count.

import numpy as np

# ETS imports...
from traits.api import HasTraits, Instance
from traitsui.api import Item, Group, View
from enable.api import Component, ComponentEditor
from chaco.api import ArrayPlotData, cool, Plot, PlotGrid
from chaco.tools.api import PanTool, ZoomTool

from eyediagram.demo_data import demo_data
from eyediagram.core import grid_count


def _create_plot_component():

    # Generate some data for the eye diagram.
    num_samples = 5000
    samples_per_symbol = 24
    y = demo_data(num_samples, samples_per_symbol)

    # Compute the eye diagram array.
    ybounds = (-0.25, 1.25)
    grid = grid_count(y, 2*samples_per_symbol, offset=16, size=(480, 480),
                      bounds=ybounds).T

    # Convert the array to floating point, and replace 0 with np.nan.
    # These points will be transparent in the image plot.
    grid = grid.astype(np.float32)
    grid[grid == 0] = np.nan

    #---------------------------------------------------------------------
    # The rest of the function creates the chaco image plot.

    pd = ArrayPlotData()
    pd.set_data(""eyediagram"", grid)

    plot = Plot(pd)
    img_plot = plot.img_plot(""eyediagram"",
                             xbounds=(0, 2),
                             ybounds=ybounds,
                             bgcolor=(0, 0, 0),
                             colormap=cool)[0]

    # Tweak some of the plot properties
    plot.title = ""Eye Diagram""
    plot.padding = 50

    # Axis grids
    vgrid = PlotGrid(component=plot, mapper=plot.index_mapper,
                     orientation='vertical',
                     line_color='gray', line_style='dot')
    hgrid = PlotGrid(component=plot, mapper=plot.value_mapper,
                     orientation='horizontal',
                     line_color='gray', line_style='dot')
    plot.underlays.append(vgrid)
    plot.underlays.append(hgrid)

    # Add pan and zoom tools.
    plot.tools.append(PanTool(plot))
    zoom = ZoomTool(component=img_plot, tool_mode=""box"", always_on=False)
    img_plot.overlays.append(zoom)

    return plot


class EyeDiagramDemo(HasTraits):

    plot = Instance(Component)

    traits_view = \
        View(
            Group(
                Item('plot', editor=ComponentEditor(size=(700, 500)),
                     show_label=False),
                orientation=""vertical"",
            ),
            resizable=True, title=""Eye Diagram"",
        )

    def _plot_default(self):
        return _create_plot_component()


if __name__ == ""__main__"":
    demo = EyeDiagramDemo()
    demo.configure_traits()

```"
1d7660282be42d96d912855eeb3bd4dfe830f224,bitvault/test/scripts/client_usage.py,bitvault/test/scripts/client_usage.py,,"import time

import bitvault


current_milli_time = lambda: int(round(time.time()))
email = '{0}@bitvault.io'.format(current_milli_time())
password = u'incredibly_secure'

## API discovery
#
# The BitVault server provides a JSON description of its API that allows
# the client to generate all necessary resource classes at runtime.

# FIXME: This is from high_level.py; why aren't we just calling authed_client?

client = bitvault.client(u'http://localhost:8998')


## User management
#
# The create action returns a User Resource which has:
#
# * action methods (get, update, reset)
# * attributes (email, first_name, etc.)
# * associated resources (applications)

client.users.create(email=email, password=password)

# Get an authenticated client representing the new user
client = bitvault.authed_client(email=email, password=password)
user = client.user


## Application management

## Fetch applications
#
# If the applications collection is not populated, it will be fetched from the
# server. The cached version will be used if it is already loaded. A refresh can
# be triggered by passing it as an option to the action.

user.applications
user.applications.refresh()


## Create an application.
#
# The optional callback_url attribute specifies a URL where BitVault
# can POST event information such as confirmed transactions.

app = user.applications.create(
    name=u'bitcoin_app',
    callback_url=u'https://someapp.com/callback')


## Wallets
#
# Wallets belong to applications, not directly to users. They require
# a passphrase to be provided on creation.

wallet = app.wallets.create(passphrase=u'very insecure', name=u'my funds')


# An application's wallet collection is enumerable

for wallet in app.wallets.values():
    print(wallet)


# And acts as a hash with names as keys

wallet = app.wallets[u'my funds']


# The passphrase is required to unlock the wallet before you can
# perform any transactions with it.

wallet.unlock(u'very insecure')


## Accounts
#
# Wallets can have multiple accounts, each represented by a path in the
# MultiWallet's deterministic trees.

account = wallet.accounts.create(name=u'office supplies')


## Payments
#
# Sending payments

# Creating addresses for receiving payments
# This is a BIP 16 ""Pay to Script Hash"" address, where the script in question
# is a BIP 11 ""multisig"".

payment_address = account.addresses.create

# TODO: Additional method ""prepare"" to obtain unsigned transaction for inspection
payment = account.pay(payees=({u'address': payment_address, u'amount': 20000},))

## Transfers

account_1 = wallet.accounts[u'rubber bands']
account_2 = wallet.accounts.create(name=u'travel expenses')

wallet.transfer(amount=10000, source=account_1, destination=account_2)
",Add high-level client usage script,"Add high-level client usage script
",Python,mit,GemHQ/round-py,109,"```python
import time

import bitvault


current_milli_time = lambda: int(round(time.time()))
email = '{0}@bitvault.io'.format(current_milli_time())
password = u'incredibly_secure'

## API discovery
#
# The BitVault server provides a JSON description of its API that allows
# the client to generate all necessary resource classes at runtime.

# FIXME: This is from high_level.py; why aren't we just calling authed_client?

client = bitvault.client(u'http://localhost:8998')


## User management
#
# The create action returns a User Resource which has:
#
# * action methods (get, update, reset)
# * attributes (email, first_name, etc.)
# * associated resources (applications)

client.users.create(email=email, password=password)

# Get an authenticated client representing the new user
client = bitvault.authed_client(email=email, password=password)
user = client.user


## Application management

## Fetch applications
#
# If the applications collection is not populated, it will be fetched from the
# server. The cached version will be used if it is already loaded. A refresh can
# be triggered by passing it as an option to the action.

user.applications
user.applications.refresh()


## Create an application.
#
# The optional callback_url attribute specifies a URL where BitVault
# can POST event information such as confirmed transactions.

app = user.applications.create(
    name=u'bitcoin_app',
    callback_url=u'https://someapp.com/callback')


## Wallets
#
# Wallets belong to applications, not directly to users. They require
# a passphrase to be provided on creation.

wallet = app.wallets.create(passphrase=u'very insecure', name=u'my funds')


# An application's wallet collection is enumerable

for wallet in app.wallets.values():
    print(wallet)


# And acts as a hash with names as keys

wallet = app.wallets[u'my funds']


# The passphrase is required to unlock the wallet before you can
# perform any transactions with it.

wallet.unlock(u'very insecure')


## Accounts
#
# Wallets can have multiple accounts, each represented by a path in the
# MultiWallet's deterministic trees.

account = wallet.accounts.create(name=u'office supplies')


## Payments
#
# Sending payments

# Creating addresses for receiving payments
# This is a BIP 16 ""Pay to Script Hash"" address, where the script in question
# is a BIP 11 ""multisig"".

payment_address = account.addresses.create

# TODO: Additional method ""prepare"" to obtain unsigned transaction for inspection
payment = account.pay(payees=({u'address': payment_address, u'amount': 20000},))

## Transfers

account_1 = wallet.accounts[u'rubber bands']
account_2 = wallet.accounts.create(name=u'travel expenses')

wallet.transfer(amount=10000, source=account_1, destination=account_2)

```"
9ed64ddd0f5ca8ec035b53b0a699ec721e6206d9,screen_manager_test.py,screen_manager_test.py,,"
from kivy.app import App
from kivy.lang import Builder
from kivy.uix.floatlayout import FloatLayout

# Example from http://robertour.com/category/kivy/page/2/

Builder.load_string(""""""

#:import FadeTransition kivy.uix.screenmanager.FadeTransition

<Phone>:
    AnchorLayout:
        anchor_x: 'right'
        anchor_y: 'center'
        ScreenManager:
            id: _screen_manager
            size_hint: .9, 1
            Screen:
                name: 'screen1'
                Label:
                    markup: True
                    text: '[size=24]Welcome to [color=dd88ff]THE APP[/color][/size]'
            Screen:
                name: 'screen2'
                GridLayout:
                    cols: 3
                    padding: 50
                    Button:
                        text: ""1""
                    Button:
                        text: ""2""
                    Button:
                        text: ""3""
                    Button:
                        text: ""4""
                    Button:
                        text: ""5""
                    Button:
                        text: ""6""
                    Button:
                        text: ""7""
                    Button:
                        text: ""8""
                    Button:
                        text: ""9""
                    Button:
                        text: ""*""
                    Button:
                        text: ""0""
                    Button:
                        text: ""#""
            Screen:
                name: 'screen3'
                BoxLayout:
                    Label:
                        markup: True
                        text: '[size=24]Welcome to [color=dd88ff]THE APP[/color][/size]'
                    Button:
                        text: 'Lampe aus'
                        on_press: _screen_manager.current = 'screen1'
    AnchorLayout:
        anchor_x: 'left'
        anchor_y: 'center'
        BoxLayout:
            orientation: 'vertical'
            size_hint: .1, 1
            spacing: 10 #spacing between children

            canvas:
                Color:
                    rgba: 1,0,0,.5
                Line:
                    rectangle: self.x+1, self.y+1, self.width-1, self.height-1
            Label:
                halign: 'center'
                text: 'SET'
            Button:
                text: 'CAM'
                size_hint: 1, .2
                on_press:
                    _screen_manager.transition = FadeTransition()
                    _screen_manager.current = 'screen1'
            Button:
                text: 'SH'
                on_press:
                    _screen_manager.transition.direction = 'right'
                    _screen_manager.current = 'screen2'
            Button:
                text: 'CL'
                on_press: _screen_manager.current = 'screen3'
            Button:
                text: 'VB'
                on_press: _screen_manager.current = 'screen2'
            Label:
                halign: 'center'
                valign: 'bottom'
                text: '29.09.2017\\n14:28:31'
"""""")

class Phone(FloatLayout):
    pass

class TestApp(App):
    def build(self):
        return Phone()

if __name__ == '__main__':
    TestApp().run()
",Add test-app which uses screen-manager,"Add test-app which uses screen-manager
",Python,apache-2.0,ThomasHangstoerfer/pyHomeCtrl,110,"```python

from kivy.app import App
from kivy.lang import Builder
from kivy.uix.floatlayout import FloatLayout

# Example from http://robertour.com/category/kivy/page/2/

Builder.load_string(""""""

#:import FadeTransition kivy.uix.screenmanager.FadeTransition

<Phone>:
    AnchorLayout:
        anchor_x: 'right'
        anchor_y: 'center'
        ScreenManager:
            id: _screen_manager
            size_hint: .9, 1
            Screen:
                name: 'screen1'
                Label:
                    markup: True
                    text: '[size=24]Welcome to [color=dd88ff]THE APP[/color][/size]'
            Screen:
                name: 'screen2'
                GridLayout:
                    cols: 3
                    padding: 50
                    Button:
                        text: ""1""
                    Button:
                        text: ""2""
                    Button:
                        text: ""3""
                    Button:
                        text: ""4""
                    Button:
                        text: ""5""
                    Button:
                        text: ""6""
                    Button:
                        text: ""7""
                    Button:
                        text: ""8""
                    Button:
                        text: ""9""
                    Button:
                        text: ""*""
                    Button:
                        text: ""0""
                    Button:
                        text: ""#""
            Screen:
                name: 'screen3'
                BoxLayout:
                    Label:
                        markup: True
                        text: '[size=24]Welcome to [color=dd88ff]THE APP[/color][/size]'
                    Button:
                        text: 'Lampe aus'
                        on_press: _screen_manager.current = 'screen1'
    AnchorLayout:
        anchor_x: 'left'
        anchor_y: 'center'
        BoxLayout:
            orientation: 'vertical'
            size_hint: .1, 1
            spacing: 10 #spacing between children

            canvas:
                Color:
                    rgba: 1,0,0,.5
                Line:
                    rectangle: self.x+1, self.y+1, self.width-1, self.height-1
            Label:
                halign: 'center'
                text: 'SET'
            Button:
                text: 'CAM'
                size_hint: 1, .2
                on_press:
                    _screen_manager.transition = FadeTransition()
                    _screen_manager.current = 'screen1'
            Button:
                text: 'SH'
                on_press:
                    _screen_manager.transition.direction = 'right'
                    _screen_manager.current = 'screen2'
            Button:
                text: 'CL'
                on_press: _screen_manager.current = 'screen3'
            Button:
                text: 'VB'
                on_press: _screen_manager.current = 'screen2'
            Label:
                halign: 'center'
                valign: 'bottom'
                text: '29.09.2017\\n14:28:31'
"""""")

class Phone(FloatLayout):
    pass

class TestApp(App):
    def build(self):
        return Phone()

if __name__ == '__main__':
    TestApp().run()

```"
ccdb1b2f87c700e283b2ca51aa94408027561b0d,api.py,api.py,,"import json

from django.conf.urls import url
from django.db.models import ObjectDoesNotExist
from django.http import Http404, HttpResponse
from django.utils.decorators import classonlymethod
from django.views import generic
from django.views.decorators.csrf import csrf_exempt


class Resource(generic.View):
    """"""
    Request-response cycle
    ======================

    - Incoming request with a certain HTTP verb
      - Standardize incoming data (PUTted JSON, POSTed multipart, whatever)

    - Process verbs
      - GET & HEAD
        - list
        - detail
      - POST
        - process
        - create
      - PUT (Complete resource)
        - replace or create
      - PATCH
        - patch, incomplete resources allowed
      - DELETE
        - obvious :-)
      - OPTIONS (unsupported)
      - TRACE (unsupported)
    """"""
    model = None
    queryset = None

    http_method_names = ['get', 'post', 'put', 'delete', 'head', 'patch']

    @classonlymethod
    def as_url(cls, prefix, **initkwargs):
        """"""
        Usage::

            urlpatterns = patterns('',
                Resource.as_url('v1/product/', model=Product),
            )
        """"""

        return url(r'^%s(?:(?P<pk>\d+)/)?$' % prefix,
            csrf_exempt(cls.as_view(**initkwargs)))

    def dispatch(self, request, *args, **kwargs):
        # Try to dispatch to the right method; if a method doesn't exist,
        # defer to the error handler. Also defer to the error handler if the
        # request method isn't on the approved list.
        if request.method.lower() in self.http_method_names:
            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)
        else:
            handler = self.http_method_not_allowed
        self.request = request
        self.args = args
        self.kwargs = kwargs
        self.unserialize_request()
        return self.serialize_response(handler(request, *self.args, **self.kwargs))

    def unserialize_request(self):
        """"""
        This method standardizes various aspects of the incoming request, f.e.
        decoding of JSON requests etc.

        The ""real"" processing methods should not have to distinguish between
        varying request types anymore.
        """"""
        pass

    def serialize_response(self, response):
        return HttpResponse(json.dumps(response), mimetype='application/json')

    def get_query_set(self):
        if self.queryset:
            return self.queryset._clone()
        elif self.model:
            return self.model._default_manager.all()

    def serialize_instance(self, instance):
        return {
            'pk': instance.pk,
            '__unicode__': unicode(instance),
            }

    def get(self, request, *args, **kwargs):
        queryset = self.get_query_set()
        if kwargs.get('pk'):
            instance = queryset.get(pk=kwargs.get('pk'))

            return self.serialize_instance(instance)

        # TODO pagination, filtering
        return [self.serialize_instance(instance) for instance in queryset]
",Add a really simple resource as beginning of a custom API implementation,"Add a really simple resource as beginning of a custom API implementation
",Python,bsd-3-clause,"matthiask/towel,matthiask/towel,matthiask/towel,matthiask/towel",101,"```python
import json

from django.conf.urls import url
from django.db.models import ObjectDoesNotExist
from django.http import Http404, HttpResponse
from django.utils.decorators import classonlymethod
from django.views import generic
from django.views.decorators.csrf import csrf_exempt


class Resource(generic.View):
    """"""
    Request-response cycle
    ======================

    - Incoming request with a certain HTTP verb
      - Standardize incoming data (PUTted JSON, POSTed multipart, whatever)

    - Process verbs
      - GET & HEAD
        - list
        - detail
      - POST
        - process
        - create
      - PUT (Complete resource)
        - replace or create
      - PATCH
        - patch, incomplete resources allowed
      - DELETE
        - obvious :-)
      - OPTIONS (unsupported)
      - TRACE (unsupported)
    """"""
    model = None
    queryset = None

    http_method_names = ['get', 'post', 'put', 'delete', 'head', 'patch']

    @classonlymethod
    def as_url(cls, prefix, **initkwargs):
        """"""
        Usage::

            urlpatterns = patterns('',
                Resource.as_url('v1/product/', model=Product),
            )
        """"""

        return url(r'^%s(?:(?P<pk>\d+)/)?$' % prefix,
            csrf_exempt(cls.as_view(**initkwargs)))

    def dispatch(self, request, *args, **kwargs):
        # Try to dispatch to the right method; if a method doesn't exist,
        # defer to the error handler. Also defer to the error handler if the
        # request method isn't on the approved list.
        if request.method.lower() in self.http_method_names:
            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)
        else:
            handler = self.http_method_not_allowed
        self.request = request
        self.args = args
        self.kwargs = kwargs
        self.unserialize_request()
        return self.serialize_response(handler(request, *self.args, **self.kwargs))

    def unserialize_request(self):
        """"""
        This method standardizes various aspects of the incoming request, f.e.
        decoding of JSON requests etc.

        The ""real"" processing methods should not have to distinguish between
        varying request types anymore.
        """"""
        pass

    def serialize_response(self, response):
        return HttpResponse(json.dumps(response), mimetype='application/json')

    def get_query_set(self):
        if self.queryset:
            return self.queryset._clone()
        elif self.model:
            return self.model._default_manager.all()

    def serialize_instance(self, instance):
        return {
            'pk': instance.pk,
            '__unicode__': unicode(instance),
            }

    def get(self, request, *args, **kwargs):
        queryset = self.get_query_set()
        if kwargs.get('pk'):
            instance = queryset.get(pk=kwargs.get('pk'))

            return self.serialize_instance(instance)

        # TODO pagination, filtering
        return [self.serialize_instance(instance) for instance in queryset]

```"
10070529a5b5954095deca0a7653ab46c83d10c4,numba/typesystem/exttypes/vtabtype.py,numba/typesystem/exttypes/vtabtype.py,,"# -*- coding: utf-8 -*-

""""""
Virtual method table types and ordering.
""""""

from numba import error
from numba.typesystem import *

#------------------------------------------------------------------------
# Virtual Method Ordering
#------------------------------------------------------------------------

def unordered(parent_vtables, methoddict):
    return methoddict.itervalues()

def extending(parent_vtables, methoddict):
    """"""
    Order the virtual methods according to the given parent vtables, i.e.
    we can only extend existing vtables.
    """"""
    if not parent_vtables:
        return unordered(parent_vtables, methoddict)

    parents = sorted(parent_vtables, key=lambda vtab: len(vtab.methoddict))
    biggest_vtab = parents[-1]

    appending_methods = set(methoddict) - set(biggest_vtab.methodnames)
    return biggest_vtab.methodnames + list(appending_methods)

# ______________________________________________________________________
# Validate Virtual Method Order

def validate_vtab_compatibility(parent_vtables, vtab):
    parents = sorted(parent_vtables, key=lambda vtab: len(vtab.methoddict))
    vtabs = parents + [vtab]

    for vtab_smaller, vtab_bigger in zip(vtabs, vtabs[1:]):
        names1 = vtab_smaller.methodnames
        names2 = vtab_bigger.methodnames[len(vtab_smaller.methodnames)]

        if names1 != names2:
            raise error.NumbaError(
                ""Cannot create compatible method ordering for ""
                ""base classes '%s' and '%s'"" % (
                                            vtab_smaller.py_class.__name__,
                                            vtab_bigger.py_class.__name__))

#------------------------------------------------------------------------
# Virtual Method Table Type
#------------------------------------------------------------------------

class VTabType(NumbaType):
    """"""
    Virtual method table type.
    """"""

    def __init__(self, py_class, parents):
        self.py_class = py_class

        # List of parent vtable types
        self.parents = parents

        # method_name -> Method
        self.methoddict = {}

        # Set once create_method_ordering is called,
        # list of ordered method names
        self.methodnames = None

    def create_method_ordering(self, ordering=unordered):
        """"""
        Create a consistent method ordering with the base types.

            ordering ∈ { unordered, extending, ... }
        """"""
        self.methodnames = ordering(self.parents, self.methoddict)

    def add_method(self, method):
        """"""
        Add a method to the vtab type and verify it with any parent
        method signatures.
        """"""
        if method.name in self.methoddict:
            # Patch current signature after type inference
            signature = self.get_signature(method.name)
            assert method.signature.args == signature.args
            if signature.return_type is None:
                signature.return_type = method.signature.return_type
            else:
                assert signature.return_type == method.signature.return_type, \
                                                            method.signature

        self.methoddict[method.name] = method

    def get_signature(self, method_name):
        ""Get the signature for the given method name. Returns ExtMethodType""
        method = self.methoddict[method_name]
        return method.signature
",Add virtual method table type and ordering,"Add virtual method table type and ordering
",Python,bsd-2-clause,"shiquanwang/numba,sklam/numba,IntelLabs/numba,stefanseefeld/numba,GaZ3ll3/numba,ssarangi/numba,sklam/numba,stonebig/numba,sklam/numba,numba/numba,seibert/numba,pitrou/numba,numba/numba,ssarangi/numba,pitrou/numba,jriehl/numba,numba/numba,seibert/numba,stonebig/numba,pombredanne/numba,cpcloud/numba,seibert/numba,gmarkall/numba,gmarkall/numba,seibert/numba,stonebig/numba,stuartarchibald/numba,pitrou/numba,cpcloud/numba,pombredanne/numba,gmarkall/numba,gdementen/numba,stefanseefeld/numba,jriehl/numba,IntelLabs/numba,jriehl/numba,stuartarchibald/numba,stuartarchibald/numba,stonebig/numba,gdementen/numba,seibert/numba,sklam/numba,pombredanne/numba,GaZ3ll3/numba,cpcloud/numba,stuartarchibald/numba,gdementen/numba,gdementen/numba,stefanseefeld/numba,GaZ3ll3/numba,numba/numba,pombredanne/numba,stuartarchibald/numba,shiquanwang/numba,cpcloud/numba,pombredanne/numba,shiquanwang/numba,stefanseefeld/numba,IntelLabs/numba,stonebig/numba,gmarkall/numba,gdementen/numba,gmarkall/numba,jriehl/numba,IntelLabs/numba,pitrou/numba,ssarangi/numba,jriehl/numba,ssarangi/numba,numba/numba,GaZ3ll3/numba,pitrou/numba,stefanseefeld/numba,GaZ3ll3/numba,IntelLabs/numba,sklam/numba,cpcloud/numba,ssarangi/numba",100,"```python
# -*- coding: utf-8 -*-

""""""
Virtual method table types and ordering.
""""""

from numba import error
from numba.typesystem import *

#------------------------------------------------------------------------
# Virtual Method Ordering
#------------------------------------------------------------------------

def unordered(parent_vtables, methoddict):
    return methoddict.itervalues()

def extending(parent_vtables, methoddict):
    """"""
    Order the virtual methods according to the given parent vtables, i.e.
    we can only extend existing vtables.
    """"""
    if not parent_vtables:
        return unordered(parent_vtables, methoddict)

    parents = sorted(parent_vtables, key=lambda vtab: len(vtab.methoddict))
    biggest_vtab = parents[-1]

    appending_methods = set(methoddict) - set(biggest_vtab.methodnames)
    return biggest_vtab.methodnames + list(appending_methods)

# ______________________________________________________________________
# Validate Virtual Method Order

def validate_vtab_compatibility(parent_vtables, vtab):
    parents = sorted(parent_vtables, key=lambda vtab: len(vtab.methoddict))
    vtabs = parents + [vtab]

    for vtab_smaller, vtab_bigger in zip(vtabs, vtabs[1:]):
        names1 = vtab_smaller.methodnames
        names2 = vtab_bigger.methodnames[len(vtab_smaller.methodnames)]

        if names1 != names2:
            raise error.NumbaError(
                ""Cannot create compatible method ordering for ""
                ""base classes '%s' and '%s'"" % (
                                            vtab_smaller.py_class.__name__,
                                            vtab_bigger.py_class.__name__))

#------------------------------------------------------------------------
# Virtual Method Table Type
#------------------------------------------------------------------------

class VTabType(NumbaType):
    """"""
    Virtual method table type.
    """"""

    def __init__(self, py_class, parents):
        self.py_class = py_class

        # List of parent vtable types
        self.parents = parents

        # method_name -> Method
        self.methoddict = {}

        # Set once create_method_ordering is called,
        # list of ordered method names
        self.methodnames = None

    def create_method_ordering(self, ordering=unordered):
        """"""
        Create a consistent method ordering with the base types.

            ordering ∈ { unordered, extending, ... }
        """"""
        self.methodnames = ordering(self.parents, self.methoddict)

    def add_method(self, method):
        """"""
        Add a method to the vtab type and verify it with any parent
        method signatures.
        """"""
        if method.name in self.methoddict:
            # Patch current signature after type inference
            signature = self.get_signature(method.name)
            assert method.signature.args == signature.args
            if signature.return_type is None:
                signature.return_type = method.signature.return_type
            else:
                assert signature.return_type == method.signature.return_type, \
                                                            method.signature

        self.methoddict[method.name] = method

    def get_signature(self, method_name):
        ""Get the signature for the given method name. Returns ExtMethodType""
        method = self.methoddict[method_name]
        return method.signature

```"
c94cb620292f93a4cd3cfc0bb57c5fa38d95a717,kafka/cluster.py,kafka/cluster.py,,"import logging
import random

from .conn import BrokerConnection, collect_hosts
from .protocol.metadata import MetadataRequest

logger = logging.getLogger(__name__)


class Cluster(object):
    def __init__(self, **kwargs):
        if 'bootstrap_servers' not in kwargs:
            kargs['bootstrap_servers'] = 'localhost'

        self._brokers = {}
        self._topics = {}
        self._groups = {}

        self._bootstrap(collect_hosts(kwargs['bootstrap_servers']),
                        timeout=kwargs.get('bootstrap_timeout', 2))

    def brokers(self):
        brokers = list(self._brokers.values())
        return random.sample(brokers, len(brokers))

    def random_broker(self):
        for broker in self.brokers():
            if broker.connected() or broker.connect():
                return broker
        return None

    def broker_by_id(self, broker_id):
        return self._brokers.get(broker_id)

    def topics(self):
        return list(self._topics.keys())

    def partitions_for_topic(self, topic):
        if topic not in self._topics:
            return None
        return list(self._topics[topic].keys())

    def broker_for_partition(self, topic, partition):
        if topic not in self._topics or partition not in self._topics[topic]:
            return None
        broker_id = self._topics[topic][partition]
        return self.broker_by_id(broker_id)

    def refresh_metadata(self):
        broker = self.random_broker()
        if not broker.send(MetadataRequest([])):
            return None
        metadata = broker.recv()
        if not metadata:
            return None
        self._update_metadata(metadata)
        return metadata

    def _update_metadata(self, metadata):
        self._brokers.update({
            node_id: BrokerConnection(host, port)
            for node_id, host, port in metadata.brokers
            if node_id not in self._brokers
        })

        self._topics = {
            topic: {
                partition: leader
                for _, partition, leader, _, _ in partitions
            }
            for _, topic, partitions in metadata.topics
        }

    def _bootstrap(self, hosts, timeout=2):
        for host, port in hosts:
            conn = BrokerConnection(host, port, timeout)
            if not conn.connect():
                continue
            self._brokers['bootstrap'] = conn
            if self.refresh_metadata():
                break
        else:
            raise ValueError(""Could not bootstrap kafka cluster from %s"" % hosts)

        if len(self._brokers) > 1:
            self._brokers.pop('bootstrap')
            conn.close()

    def __str__(self):
        return 'Cluster(brokers: %d, topics: %d, groups: %d)' % \
               (len(self._brokers), len(self._topics), len(self._groups))
",Add simple Cluster class to manage broker metadata,"Add simple Cluster class to manage broker metadata
",Python,apache-2.0,"Yelp/kafka-python,zackdever/kafka-python,wikimedia/operations-debs-python-kafka,DataDog/kafka-python,scrapinghub/kafka-python,dpkp/kafka-python,scrapinghub/kafka-python,ohmu/kafka-python,mumrah/kafka-python,dpkp/kafka-python,mumrah/kafka-python,gamechanger/kafka-python,zackdever/kafka-python,gamechanger/kafka-python,ohmu/kafka-python,Aloomaio/kafka-python,wikimedia/operations-debs-python-kafka,Yelp/kafka-python,Aloomaio/kafka-python",92,"```python
import logging
import random

from .conn import BrokerConnection, collect_hosts
from .protocol.metadata import MetadataRequest

logger = logging.getLogger(__name__)


class Cluster(object):
    def __init__(self, **kwargs):
        if 'bootstrap_servers' not in kwargs:
            kargs['bootstrap_servers'] = 'localhost'

        self._brokers = {}
        self._topics = {}
        self._groups = {}

        self._bootstrap(collect_hosts(kwargs['bootstrap_servers']),
                        timeout=kwargs.get('bootstrap_timeout', 2))

    def brokers(self):
        brokers = list(self._brokers.values())
        return random.sample(brokers, len(brokers))

    def random_broker(self):
        for broker in self.brokers():
            if broker.connected() or broker.connect():
                return broker
        return None

    def broker_by_id(self, broker_id):
        return self._brokers.get(broker_id)

    def topics(self):
        return list(self._topics.keys())

    def partitions_for_topic(self, topic):
        if topic not in self._topics:
            return None
        return list(self._topics[topic].keys())

    def broker_for_partition(self, topic, partition):
        if topic not in self._topics or partition not in self._topics[topic]:
            return None
        broker_id = self._topics[topic][partition]
        return self.broker_by_id(broker_id)

    def refresh_metadata(self):
        broker = self.random_broker()
        if not broker.send(MetadataRequest([])):
            return None
        metadata = broker.recv()
        if not metadata:
            return None
        self._update_metadata(metadata)
        return metadata

    def _update_metadata(self, metadata):
        self._brokers.update({
            node_id: BrokerConnection(host, port)
            for node_id, host, port in metadata.brokers
            if node_id not in self._brokers
        })

        self._topics = {
            topic: {
                partition: leader
                for _, partition, leader, _, _ in partitions
            }
            for _, topic, partitions in metadata.topics
        }

    def _bootstrap(self, hosts, timeout=2):
        for host, port in hosts:
            conn = BrokerConnection(host, port, timeout)
            if not conn.connect():
                continue
            self._brokers['bootstrap'] = conn
            if self.refresh_metadata():
                break
        else:
            raise ValueError(""Could not bootstrap kafka cluster from %s"" % hosts)

        if len(self._brokers) > 1:
            self._brokers.pop('bootstrap')
            conn.close()

    def __str__(self):
        return 'Cluster(brokers: %d, topics: %d, groups: %d)' % \
               (len(self._brokers), len(self._topics), len(self._groups))

```"
b839fd3a98dd943a19f5f852da89eae43470b89f,robot/robot/src/autonomous/hot_shoot.py,robot/robot/src/autonomous/hot_shoot.py,,"
try:
    import wpilib
except ImportError:
    from pyfrc import wpilib

class main(object):
    '''autonomous program'''
    
    DEFAULT = False
    MODE_NAME = ""Hot shoot""
    
    def __init__ (self, components):
        ''' initialize'''
        super().__init__()
        self.drive = components['drive']
        self.intake = components['intake']
        self.catapult = components['catapult']
        
        # number of seconds to drive forward, allow us to tune it via SmartDashboard
        wpilib.SmartDashboard.PutNumber('AutoDriveTime', 1.4)
        wpilib.SmartDashboard.PutNumber('AutoDriveSpeed', 0.5)
        
        wpilib.SmartDashboard.PutBoolean(""IsHot"", False)


    def on_enable(self):
        '''these are called when autonomous starts'''
        
        self.drive_time = wpilib.SmartDashboard.GetNumber('AutoDriveTime')
        self.drive_speed = wpilib.SmartDashboard.GetNumber('AutoDriveSpeed')
        
        self.decided = False
        self.started = False
        
        print(""Team 1418 autonomous code for 2014"")
        print(""-> Drive time:"", self.drive_time, ""seconds"")
        print(""-> Drive speed:"", self.drive_speed)
        
        #print(""-> Battery voltage: %.02fv"" % wpilib.DriverStation.GetInstance().GetBatteryVoltage())
        
        
    
    def on_disable(self):
         '''This function is called when autonomous mode is disabled'''
         pass

    def update(self, time_elapsed):   
        '''The actual autonomous program'''     
       
       
        # decide if it's hot or not
        if not self.decided:
            self.hot = wpilib.SmartDashboard.GetBoolean(""IsHot"")
            
            if self.hot:
                self.decided = True
                self.start_time = time_elapsed
                
            elif time_elapsed > 6:
                self.decided = True
                self.start_time = time_elapsed
       
        # always pulldown
        if time_elapsed > 0.3:
            self.catapult.pulldown()
            
       
        if time_elapsed < 0.3:
            # Get the arm down so that we can winch
            self.intake.armDown()
        
        elif time_elapsed < 1.5:
            # The arm is at least far enough down now that
            # the winch won't hit it, start winching
            self.intake.armDown()
        
        else:
            
            if not self.started and self.decided:
                self.decided = True
                self.started = True
                self.start_time = time_elapsed
            
            if self.decided:
                
                time_elapsed = time_elapsed - self.start_time
            
                
                if time_elapsed < self.drive_time:
                    # Drive slowly forward for N seconds
                    self.drive.move(0, self.drive_speed, 0)
                    
                    
                elif time_elapsed < 2.0 + self.drive_time + 1.0:
                    # Finally, fire and keep firing for 1 seconds
                    self.catapult.launchNoSensor()
",Add an autonomous mode based on the hot goal,"Add an autonomous mode based on the hot goal
",Python,bsd-3-clause,frc1418/2014,98,"```python

try:
    import wpilib
except ImportError:
    from pyfrc import wpilib

class main(object):
    '''autonomous program'''
    
    DEFAULT = False
    MODE_NAME = ""Hot shoot""
    
    def __init__ (self, components):
        ''' initialize'''
        super().__init__()
        self.drive = components['drive']
        self.intake = components['intake']
        self.catapult = components['catapult']
        
        # number of seconds to drive forward, allow us to tune it via SmartDashboard
        wpilib.SmartDashboard.PutNumber('AutoDriveTime', 1.4)
        wpilib.SmartDashboard.PutNumber('AutoDriveSpeed', 0.5)
        
        wpilib.SmartDashboard.PutBoolean(""IsHot"", False)


    def on_enable(self):
        '''these are called when autonomous starts'''
        
        self.drive_time = wpilib.SmartDashboard.GetNumber('AutoDriveTime')
        self.drive_speed = wpilib.SmartDashboard.GetNumber('AutoDriveSpeed')
        
        self.decided = False
        self.started = False
        
        print(""Team 1418 autonomous code for 2014"")
        print(""-> Drive time:"", self.drive_time, ""seconds"")
        print(""-> Drive speed:"", self.drive_speed)
        
        #print(""-> Battery voltage: %.02fv"" % wpilib.DriverStation.GetInstance().GetBatteryVoltage())
        
        
    
    def on_disable(self):
         '''This function is called when autonomous mode is disabled'''
         pass

    def update(self, time_elapsed):   
        '''The actual autonomous program'''     
       
       
        # decide if it's hot or not
        if not self.decided:
            self.hot = wpilib.SmartDashboard.GetBoolean(""IsHot"")
            
            if self.hot:
                self.decided = True
                self.start_time = time_elapsed
                
            elif time_elapsed > 6:
                self.decided = True
                self.start_time = time_elapsed
       
        # always pulldown
        if time_elapsed > 0.3:
            self.catapult.pulldown()
            
       
        if time_elapsed < 0.3:
            # Get the arm down so that we can winch
            self.intake.armDown()
        
        elif time_elapsed < 1.5:
            # The arm is at least far enough down now that
            # the winch won't hit it, start winching
            self.intake.armDown()
        
        else:
            
            if not self.started and self.decided:
                self.decided = True
                self.started = True
                self.start_time = time_elapsed
            
            if self.decided:
                
                time_elapsed = time_elapsed - self.start_time
            
                
                if time_elapsed < self.drive_time:
                    # Drive slowly forward for N seconds
                    self.drive.move(0, self.drive_speed, 0)
                    
                    
                elif time_elapsed < 2.0 + self.drive_time + 1.0:
                    # Finally, fire and keep firing for 1 seconds
                    self.catapult.launchNoSensor()

```"
4b3eb563a50a601bcb24a358f6ea63690cffeb27,raiden/network/nat.py,raiden/network/nat.py,,"import miniupnpc
from ethereum import slogging

MAX_PORT = 65535
RAIDEN_IDENTIFICATOR = ""raiden-network udp service""

log = slogging.getLogger(__name__)


def connect():
    """"""Try to connect to the router.
    Returns:
        u (miniupnc.UPnP): the connected upnp-instance
        router (string): the connection information
    """"""
    u = miniupnpc.UPnP()
    u.discoverdelay = 200
    providers = u.discover()
    if providers > 1:
        log.warning(""multiple upnp providers found"", num_providers=providers)
    elif providers < 1:
        log.error(""no upnp providers found"")
        return

    router = u.selectigd()
    log.debug(""connected"", router=router)

    if u.lanaddr == '0.0.0.0':
        log.error(""could not query your lanaddr"")
        return
    if u.externalipaddress() == '0.0.0.0' or u.externalipaddress() is None:
        log.error(""could not query your externalipaddress"")
        return
    return u, router


def open_port(internal_port, external_start_port=None):
    """"""Open a port for the raiden service (listening at `internal_port`) through
    UPnP.
    Args:
        internal_port (int): the target port of the raiden service
        external_start_port (int): query for an external port starting here (default: internal_port)
    Returns:
        external_ip_address, external_port (tuple(str, int)): if successful or None
    """"""
    if external_start_port is None:
        external_start_port = internal_port
    u, router = connect()
    if u is None:
        return

    register = lambda internal, external: u.addportmapping(internal,
                                                           'UDP',
                                                           u.lanaddr,
                                                           external,
                                                           RAIDEN_IDENTIFICATOR,
                                                           '')

    external_port = external_start_port
    success = register(internal_port, external_port)
    while not success and external_port <= MAX_PORT:
        external_port += 1
        success = register(internal_port, external_port)

    if success:
        log.info(""registered port-mapping per upnp"",
                internal=""{}:{}"".format(u.lanaddr, internal_port),
                external=""{}:{}"".format(u.externalipaddress(), external_port))
        return (u.externalipaddress(), external_port)
    else:
        log.error(""could not register a port-mapping"", router='FIXME')
        return


def release_port(internal_port):
    """"""Try to release the port mapping for `internal_port`.
    Args:
        internal_port (int): the port that was previously forwarded to.
    Returns:
        success (boolean): if the release was successful.
    """"""
    u, router = connect()
    mapping = u.getspecificportmapping(internal_port, 'UDP')
    if mapping is None:
        log.error(""could not find a port mapping"", router=router)
        return False
    if u.deleteportmapping(internal_port, 'UDP'):
        log.info(""successfully released port mapping"", router=router)
        return True
    else:
        log.warning(""could not release port mapping, check your router for stale mappings"", router=router)
        return False
",Add minimalistic UPnP NAT punching,"Add minimalistic UPnP NAT punching

This is not in use yet.
",Python,mit,"charles-cooper/raiden,hackaugusto/raiden,tomashaber/raiden,tomashaber/raiden,tomashaber/raiden,tomashaber/raiden,hackaugusto/raiden,tomaaron/raiden,tomaaron/raiden,tomaaron/raiden,tomashaber/raiden,tomaaron/raiden,charles-cooper/raiden",93,"```python
import miniupnpc
from ethereum import slogging

MAX_PORT = 65535
RAIDEN_IDENTIFICATOR = ""raiden-network udp service""

log = slogging.getLogger(__name__)


def connect():
    """"""Try to connect to the router.
    Returns:
        u (miniupnc.UPnP): the connected upnp-instance
        router (string): the connection information
    """"""
    u = miniupnpc.UPnP()
    u.discoverdelay = 200
    providers = u.discover()
    if providers > 1:
        log.warning(""multiple upnp providers found"", num_providers=providers)
    elif providers < 1:
        log.error(""no upnp providers found"")
        return

    router = u.selectigd()
    log.debug(""connected"", router=router)

    if u.lanaddr == '0.0.0.0':
        log.error(""could not query your lanaddr"")
        return
    if u.externalipaddress() == '0.0.0.0' or u.externalipaddress() is None:
        log.error(""could not query your externalipaddress"")
        return
    return u, router


def open_port(internal_port, external_start_port=None):
    """"""Open a port for the raiden service (listening at `internal_port`) through
    UPnP.
    Args:
        internal_port (int): the target port of the raiden service
        external_start_port (int): query for an external port starting here (default: internal_port)
    Returns:
        external_ip_address, external_port (tuple(str, int)): if successful or None
    """"""
    if external_start_port is None:
        external_start_port = internal_port
    u, router = connect()
    if u is None:
        return

    register = lambda internal, external: u.addportmapping(internal,
                                                           'UDP',
                                                           u.lanaddr,
                                                           external,
                                                           RAIDEN_IDENTIFICATOR,
                                                           '')

    external_port = external_start_port
    success = register(internal_port, external_port)
    while not success and external_port <= MAX_PORT:
        external_port += 1
        success = register(internal_port, external_port)

    if success:
        log.info(""registered port-mapping per upnp"",
                internal=""{}:{}"".format(u.lanaddr, internal_port),
                external=""{}:{}"".format(u.externalipaddress(), external_port))
        return (u.externalipaddress(), external_port)
    else:
        log.error(""could not register a port-mapping"", router='FIXME')
        return


def release_port(internal_port):
    """"""Try to release the port mapping for `internal_port`.
    Args:
        internal_port (int): the port that was previously forwarded to.
    Returns:
        success (boolean): if the release was successful.
    """"""
    u, router = connect()
    mapping = u.getspecificportmapping(internal_port, 'UDP')
    if mapping is None:
        log.error(""could not find a port mapping"", router=router)
        return False
    if u.deleteportmapping(internal_port, 'UDP'):
        log.info(""successfully released port mapping"", router=router)
        return True
    else:
        log.warning(""could not release port mapping, check your router for stale mappings"", router=router)
        return False

```"
1b7341748cc98fcb0505cf03081b92f955279d79,tests/test_mako_engine.py,tests/test_mako_engine.py,,"#!/usr/bin/env python

from __future__ import print_function

import unittest


import engines


HANDLE = 'mako'


class TestStringTemplate(unittest.TestCase):

    def setUp(self):
        try:
            import mako
        except ImportError:
            self.skipTest(""mako module not available"")

    def test_valid_engine(self):
        self.assertIn(HANDLE, engines.engines)
        engine = engines.engines[HANDLE]
        assert issubclass(engine, engines.Engine)

    def test_escape(self):
        engine = engines.engines[HANDLE]

        template = engine(
                '<%text>Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n</%text>',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )
    
    def test_conditional(self):
        engine = engines.engines[HANDLE]

        template = engine(
                '% if value < 10:\n'
                'less than ten\n'
                '% else:\n'
                'greater or equal\n'
                '% endif\n',
            )

        result = template.apply({
                'value':  4,
            })

        self.assertMultiLineEqual(result,
                'less than ten\n',
            )
    
    def test_curly_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                'Szegediner Gulasch mit\n'
                'Kartoffeln.\n'
            )

    def test_strict_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )

        self.assertRaises(Exception, template.apply, ({
                'random':  'value',
            }))

    def test_tolerant_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
                tolerant=True,
            )

        result = template.apply({
                'random':  'value',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                '<UNDEFINED> mit\n'
                '<UNDEFINED>.\n'
            )


if __name__ == '__main__':
    unittest.main()
",Add tests to mako engine.,"Add tests to mako engine.
",Python,mit,blubberdiblub/eztemplate,124,"```python
#!/usr/bin/env python

from __future__ import print_function

import unittest


import engines


HANDLE = 'mako'


class TestStringTemplate(unittest.TestCase):

    def setUp(self):
        try:
            import mako
        except ImportError:
            self.skipTest(""mako module not available"")

    def test_valid_engine(self):
        self.assertIn(HANDLE, engines.engines)
        engine = engines.engines[HANDLE]
        assert issubclass(engine, engines.Engine)

    def test_escape(self):
        engine = engines.engines[HANDLE]

        template = engine(
                '<%text>Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n</%text>',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )
    
    def test_conditional(self):
        engine = engines.engines[HANDLE]

        template = engine(
                '% if value < 10:\n'
                'less than ten\n'
                '% else:\n'
                'greater or equal\n'
                '% endif\n',
            )

        result = template.apply({
                'value':  4,
            })

        self.assertMultiLineEqual(result,
                'less than ten\n',
            )
    
    def test_curly_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                'Szegediner Gulasch mit\n'
                'Kartoffeln.\n'
            )

    def test_strict_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )

        self.assertRaises(Exception, template.apply, ({
                'random':  'value',
            }))

    def test_tolerant_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
                tolerant=True,
            )

        result = template.apply({
                'random':  'value',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                '<UNDEFINED> mit\n'
                '<UNDEFINED>.\n'
            )


if __name__ == '__main__':
    unittest.main()

```"
6dba942d41c38d301f225627aae318910d139eb0,scripts/create_pca_component_overlay.py,scripts/create_pca_component_overlay.py,,"# Generate overlay corresponding to 2nd PCA component
# which serves as a proxy for senescence

import csv

from collections import defaultdict

import dtoolcore

import click

import numpy as np


def calc_pca_components(all_entries):

    rgb_matrix = np.transpose(np.array(
        [
            map(float, [entry['R'], entry['G'], entry['B']])
            for entry in all_entries
        ]
    ))

    cov = np.cov(rgb_matrix)

    evalues, evectors = np.linalg.eig(cov)

    return evectors.T


def calc_senescence(entry, pca_rotation):

    c_R = pca_rotation[0] * float(entry['R'])
    c_G = pca_rotation[1] * float(entry['G'])
    c_B = pca_rotation[2] * float(entry['B'])

    return c_R + c_G + c_B


def find_senescence_values_by_plot_and_date(results):

    pca_components = calc_pca_components(results)
    pca_component_2 = pca_components[1]

    by_plot_then_date = defaultdict(dict)
    for entry in results:
        senescence = calc_senescence(entry, pca_component_2)
        by_plot_then_date[entry['plot']][entry['date']] = senescence

    return by_plot_then_date


def generate_pca_overlay(dataset, results):

    senescence_values = find_senescence_values_by_plot_and_date(results)

    plot_number_overlay = dataset.get_overlay('plot_number')
    ordering_overlay = dataset.get_overlay('ordering')
    date_overlay = dataset.get_overlay('date')

    pca_overlay = {}

    for identifier in dataset.identifiers:
        label = ""{}_{}"".format(
            plot_number_overlay[identifier],
            ordering_overlay[identifier]
        )
        date = date_overlay[identifier]

        try:
            senescence = senescence_values[label][date]
        except KeyError:
            senescence = None

        pca_overlay[identifier] = senescence

    dataset.put_overlay('pca_component_2', pca_overlay)


def load_output_csv_data(results_file):

    with open(results_file, 'r') as fh:
        reader = csv.DictReader(fh)

        all_entries = [row for row in reader]

    return all_entries


@click.command()
@click.argument('dataset_uri')
@click.argument('results_csv_file')
def main(dataset_uri, results_csv_file):
    dataset = dtoolcore.DataSet.from_uri(dataset_uri)

    results = load_output_csv_data(results_csv_file)

    generate_pca_overlay(dataset, results)


if __name__ == '__main__':
    main()
",Add script to create overlay on individual plots for senescence,"Add script to create overlay on individual plots for senescence
",Python,mit,"JIC-Image-Analysis/senescence-in-field,JIC-Image-Analysis/senescence-in-field,JIC-Image-Analysis/senescence-in-field",103,"```python
# Generate overlay corresponding to 2nd PCA component
# which serves as a proxy for senescence

import csv

from collections import defaultdict

import dtoolcore

import click

import numpy as np


def calc_pca_components(all_entries):

    rgb_matrix = np.transpose(np.array(
        [
            map(float, [entry['R'], entry['G'], entry['B']])
            for entry in all_entries
        ]
    ))

    cov = np.cov(rgb_matrix)

    evalues, evectors = np.linalg.eig(cov)

    return evectors.T


def calc_senescence(entry, pca_rotation):

    c_R = pca_rotation[0] * float(entry['R'])
    c_G = pca_rotation[1] * float(entry['G'])
    c_B = pca_rotation[2] * float(entry['B'])

    return c_R + c_G + c_B


def find_senescence_values_by_plot_and_date(results):

    pca_components = calc_pca_components(results)
    pca_component_2 = pca_components[1]

    by_plot_then_date = defaultdict(dict)
    for entry in results:
        senescence = calc_senescence(entry, pca_component_2)
        by_plot_then_date[entry['plot']][entry['date']] = senescence

    return by_plot_then_date


def generate_pca_overlay(dataset, results):

    senescence_values = find_senescence_values_by_plot_and_date(results)

    plot_number_overlay = dataset.get_overlay('plot_number')
    ordering_overlay = dataset.get_overlay('ordering')
    date_overlay = dataset.get_overlay('date')

    pca_overlay = {}

    for identifier in dataset.identifiers:
        label = ""{}_{}"".format(
            plot_number_overlay[identifier],
            ordering_overlay[identifier]
        )
        date = date_overlay[identifier]

        try:
            senescence = senescence_values[label][date]
        except KeyError:
            senescence = None

        pca_overlay[identifier] = senescence

    dataset.put_overlay('pca_component_2', pca_overlay)


def load_output_csv_data(results_file):

    with open(results_file, 'r') as fh:
        reader = csv.DictReader(fh)

        all_entries = [row for row in reader]

    return all_entries


@click.command()
@click.argument('dataset_uri')
@click.argument('results_csv_file')
def main(dataset_uri, results_csv_file):
    dataset = dtoolcore.DataSet.from_uri(dataset_uri)

    results = load_output_csv_data(results_csv_file)

    generate_pca_overlay(dataset, results)


if __name__ == '__main__':
    main()

```"
aeca35135975dbfe9bc807181252754cc08a1f16,paasta_tools/firewall.py,paasta_tools/firewall.py,,"#!/usr/bin/env python2.7
from __future__ import absolute_import
from __future__ import unicode_literals


import re
import shlex
import subprocess


class ChainDoesNotExist(Exception):
    pass


def ensure_chain(chain, rules):
    """"""Idempotently ensure a chain exists and has a set of rules.

    This function creates or updates an existing chain to match the rules
    passed in.

    This function will not reorder existing rules, but any new rules are always
    inserted at the front of the chain.
    """"""
    try:
        current_rules = list_chain(chain)
    except ChainDoesNotExist:
        create_chain(chain)
        current_rules = set()

    for rule in rules:
        if rule not in current_rules:
            print('adding rule: {}'.format(rule))
            add_rule(chain, rule)

    for rule in current_rules:
        if rule not in rules:
            print('deleting rule: {}'.format(rule))
            delete_rule(chain, rule)


def add_rule(chain, rule):
    subprocess.check_call(['iptables', '-t', 'filter', '-I', chain] + shlex.split(rule))


def delete_rule(chain, rule):
    subprocess.check_call(['iptables', '-t', 'filter', '-I', chain] + shlex.split(rule))


def create_chain(chain):
    subprocess.check_call(('iptables', '-t', 'filter', '-N', chain))


def list_chain(chain):
    """"""List rules in a chain.

    Returns a list of iptables rules, or raises ChainDoesNotExist.
    """"""
    cmd = ('iptables', '-t', 'filter', '--list-rules', chain)
    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    out, err = proc.communicate()
    if proc.returncode != 0:
        if b'No chain/target/match by that name.\n' in err:
            raise ChainDoesNotExist(chain)
        else:
            raise subprocess.CalledProcessError(proc.returncode, cmd, output=(out, err))

    # Parse rules into something usable
    rule_regex = re.compile(b'-A {chain} (.+)$'.format(chain=re.escape(chain)))
    rules = out.splitlines()
    parsed = set()
    for rule in rules:
        if rule == '-N {}'.format(chain):
            continue
        m = rule_regex.match(rule)
        if not m:
            raise ValueError(
                'Unable to parse iptables rule: {}'.format(rule),
            )
        else:
            parsed.add(m.group(1))

    return parsed



ensure_chain('ckuehl-test-service', [
    '-j REJECT --reject-with icmp-port-unreachable',
    '-d 169.254.255.254/32 -p tcp -m tcp --dport 20666 -j ACCEPT',
    '-d 169.254.255.254/32 -p tcp -m tcp --dport 20641 -j ACCEPT',
])
",Add some simple iptables managing functions,"Add some simple iptables managing functions

The most important of these is `ensure_chain` for creating and managing
the rules on an entire chain.
",Python,apache-2.0,"somic/paasta,somic/paasta,Yelp/paasta,Yelp/paasta",95,"```python
#!/usr/bin/env python2.7
from __future__ import absolute_import
from __future__ import unicode_literals


import re
import shlex
import subprocess


class ChainDoesNotExist(Exception):
    pass


def ensure_chain(chain, rules):
    """"""Idempotently ensure a chain exists and has a set of rules.

    This function creates or updates an existing chain to match the rules
    passed in.

    This function will not reorder existing rules, but any new rules are always
    inserted at the front of the chain.
    """"""
    try:
        current_rules = list_chain(chain)
    except ChainDoesNotExist:
        create_chain(chain)
        current_rules = set()

    for rule in rules:
        if rule not in current_rules:
            print('adding rule: {}'.format(rule))
            add_rule(chain, rule)

    for rule in current_rules:
        if rule not in rules:
            print('deleting rule: {}'.format(rule))
            delete_rule(chain, rule)


def add_rule(chain, rule):
    subprocess.check_call(['iptables', '-t', 'filter', '-I', chain] + shlex.split(rule))


def delete_rule(chain, rule):
    subprocess.check_call(['iptables', '-t', 'filter', '-I', chain] + shlex.split(rule))


def create_chain(chain):
    subprocess.check_call(('iptables', '-t', 'filter', '-N', chain))


def list_chain(chain):
    """"""List rules in a chain.

    Returns a list of iptables rules, or raises ChainDoesNotExist.
    """"""
    cmd = ('iptables', '-t', 'filter', '--list-rules', chain)
    proc = subprocess.Popen(
        cmd,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
    )
    out, err = proc.communicate()
    if proc.returncode != 0:
        if b'No chain/target/match by that name.\n' in err:
            raise ChainDoesNotExist(chain)
        else:
            raise subprocess.CalledProcessError(proc.returncode, cmd, output=(out, err))

    # Parse rules into something usable
    rule_regex = re.compile(b'-A {chain} (.+)$'.format(chain=re.escape(chain)))
    rules = out.splitlines()
    parsed = set()
    for rule in rules:
        if rule == '-N {}'.format(chain):
            continue
        m = rule_regex.match(rule)
        if not m:
            raise ValueError(
                'Unable to parse iptables rule: {}'.format(rule),
            )
        else:
            parsed.add(m.group(1))

    return parsed



ensure_chain('ckuehl-test-service', [
    '-j REJECT --reject-with icmp-port-unreachable',
    '-d 169.254.255.254/32 -p tcp -m tcp --dport 20666 -j ACCEPT',
    '-d 169.254.255.254/32 -p tcp -m tcp --dport 20641 -j ACCEPT',
])

```"
9a845da08d897f4a6b5a3105f6d9001a4f09b45e,salt/modules/bluez.py,salt/modules/bluez.py,,"'''
Support for Bluetooth (using Bluez in Linux)
'''

import salt.utils
import salt.modules.service


def __virtual__():
    '''
    Only load the module if bluetooth is installed
    '''
    if salt.utils.which('bluetoothd'):
        return 'bluetooth'
    return False


def version():
    '''
    Return Bluez version from bluetoothd -v

    CLI Example::

        salt '*' bluetoothd.version
    '''
    cmd = 'bluetoothd -v'
    out = __salt__['cmd.run'](cmd).split('\n')
    return out[0]


def address():
    '''
    Get the many addresses of the Bluetooth adapter

    CLI Example::

        salt '*' bluetooth.address
    '''
    cmd = ""dbus-send --system --print-reply --dest=org.bluez / org.bluez.Manager.DefaultAdapter|awk '/object path/ {print $3}' | sed 's/\""//g'""
    path = __salt__['cmd.run'](cmd).split('\n')
    devname = path[0].split('/')
    syspath = '/sys/class/bluetooth/%s/address' % devname[-1]
    sysfile = open(syspath, 'r')
    address = sysfile.read().strip()
    sysfile.close()
    return {
                'path': path[0],
                'devname': devname[-1],
                'address': address,
           }


def scan():
    '''
    Scan for bluetooth devices in the area

    CLI Example::

        salt '*' bluetooth.scan
    '''
    cmd = 'hcitool scan'
    ret = {}
    out = __salt__['cmd.run'](cmd).split('\n')
    for line in out:
        if not line:
            continue
        if 'Scanning' in line:
            continue
        comps = line.strip().split()
        devname = ' '.join(comps[1:])
        ret[comps[0]] = devname
    return ret


def pair(address, key):
    '''
    Pair the bluetooth adapter with a device

    CLI Example::

        salt '*' bluetooth.pair DE:AD:BE:EF:CA:FE 1234

    Where DE:AD:BE:EF:CA:FE is the address of the device
    to pair with, and 1234 is the passphrase.
    '''
    address = address()
    cmd = 'echo ""%s"" | bluez-simple-agent %s %s' % (address['devname'], address, key)
    out = __salt__['cmd.run'](cmd).split('\n')
    return out


def unpair(address):
    '''
    Unpair the bluetooth adapter from a device

    CLI Example::

        salt '*' bluetooth.unpair DE:AD:BE:EF:CA:FE

    Where DE:AD:BE:EF:CA:FE is the address of the device
    to unpair.
    '''
    address = address()
    cmd = 'bluez-test-device remove %s' % address
    out = __salt__['cmd.run'](cmd).split('\n')
    return out
",Add basic bluetooth support for salt,"Add basic bluetooth support for salt
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",107,"```python
'''
Support for Bluetooth (using Bluez in Linux)
'''

import salt.utils
import salt.modules.service


def __virtual__():
    '''
    Only load the module if bluetooth is installed
    '''
    if salt.utils.which('bluetoothd'):
        return 'bluetooth'
    return False


def version():
    '''
    Return Bluez version from bluetoothd -v

    CLI Example::

        salt '*' bluetoothd.version
    '''
    cmd = 'bluetoothd -v'
    out = __salt__['cmd.run'](cmd).split('\n')
    return out[0]


def address():
    '''
    Get the many addresses of the Bluetooth adapter

    CLI Example::

        salt '*' bluetooth.address
    '''
    cmd = ""dbus-send --system --print-reply --dest=org.bluez / org.bluez.Manager.DefaultAdapter|awk '/object path/ {print $3}' | sed 's/\""//g'""
    path = __salt__['cmd.run'](cmd).split('\n')
    devname = path[0].split('/')
    syspath = '/sys/class/bluetooth/%s/address' % devname[-1]
    sysfile = open(syspath, 'r')
    address = sysfile.read().strip()
    sysfile.close()
    return {
                'path': path[0],
                'devname': devname[-1],
                'address': address,
           }


def scan():
    '''
    Scan for bluetooth devices in the area

    CLI Example::

        salt '*' bluetooth.scan
    '''
    cmd = 'hcitool scan'
    ret = {}
    out = __salt__['cmd.run'](cmd).split('\n')
    for line in out:
        if not line:
            continue
        if 'Scanning' in line:
            continue
        comps = line.strip().split()
        devname = ' '.join(comps[1:])
        ret[comps[0]] = devname
    return ret


def pair(address, key):
    '''
    Pair the bluetooth adapter with a device

    CLI Example::

        salt '*' bluetooth.pair DE:AD:BE:EF:CA:FE 1234

    Where DE:AD:BE:EF:CA:FE is the address of the device
    to pair with, and 1234 is the passphrase.
    '''
    address = address()
    cmd = 'echo ""%s"" | bluez-simple-agent %s %s' % (address['devname'], address, key)
    out = __salt__['cmd.run'](cmd).split('\n')
    return out


def unpair(address):
    '''
    Unpair the bluetooth adapter from a device

    CLI Example::

        salt '*' bluetooth.unpair DE:AD:BE:EF:CA:FE

    Where DE:AD:BE:EF:CA:FE is the address of the device
    to unpair.
    '''
    address = address()
    cmd = 'bluez-test-device remove %s' % address
    out = __salt__['cmd.run'](cmd).split('\n')
    return out

```"
4dbf364177da4f06b366a68b1458ec55c8c1895f,docs/usage_example.py,docs/usage_example.py,,"from pandarus import *
import geopandas as gpd
import rasterio
import os
import json
from pprint import pprint

# Get filepaths of data used in tests
grid_fp = os.path.join('..', 'tests', 'data', 'grid.geojson')
points_fp = os.path.join('..', 'tests', 'data', 'points.geojson')
square_fp = os.path.join('..', 'tests', 'data', 'square.geojson')
lines_fp = os.path.join('..', 'tests', 'data', 'lines.geojson')
range_fp = os.path.join('..', 'tests', 'data', 'range.tif')

# Load test fixtures
grid = gpd.read_file(grid_fp)
square =  gpd.read_file(square_fp)
points = gpd.read_file(points_fp)
lines = gpd.read_file(lines_fp)


# Intersecting polygons
spatial_result, json_data = intersect(
    square_fp,
    'name',
    grid_fp,
    'name',
    compress=False
)

# Load resulting output data
print(""Output from intersecting polygons"")
pprint(json.load(open(json_data)))


# Intersecting lines
spatial_result, json_data = intersect(
    lines_fp,
    'name',
    grid_fp,
    'name',
    compress=False
)

# Load resulting output data
print(""Output from intersecting lines"")
pprint(json.load(open(json_data)))
print(""Vector file with calculated intersections written to:"", spatial_result)

# Intersecting points
spatial_result, json_data = intersect(
    points_fp,
    'name',
    grid_fp,
    'name',
    compress=False
)

# Load resulting output data
print(""Output from intersecting points"")
pprint(json.load(open(json_data)))
print(""Vector file with calculated intersections written to:"", spatial_result)


# Getting raster statistics for polygons
json_data = raster_statistics(
    grid_fp,
    'name',
    range_fp,
    compress=False
)

# Load resulting output data
print(""Output from raster statistics for polygons"")
pprint(json.load(open(json_data)))
print(""Vector file with calculated intersections written to:"", spatial_result)


# Getting raster statistics for lines
json_data = raster_statistics(
    lines_fp,
    'name',
    range_fp,
    compress=False
)

# Load resulting output data
print(""Output from raster statistics for lines"")
pprint(json.load(open(json_data)))


# Getting raster statistics for points
json_data = raster_statistics(
    points_fp,
    'name',
    range_fp,
    compress=False
)

# Load resulting output data
print(""Output from raster statistics for points"")
pprint(json.load(open(json_data)))
",Add usage example as a python file,"Add usage example as a python file
",Python,bsd-3-clause,cmutel/pandarus,103,"```python
from pandarus import *
import geopandas as gpd
import rasterio
import os
import json
from pprint import pprint

# Get filepaths of data used in tests
grid_fp = os.path.join('..', 'tests', 'data', 'grid.geojson')
points_fp = os.path.join('..', 'tests', 'data', 'points.geojson')
square_fp = os.path.join('..', 'tests', 'data', 'square.geojson')
lines_fp = os.path.join('..', 'tests', 'data', 'lines.geojson')
range_fp = os.path.join('..', 'tests', 'data', 'range.tif')

# Load test fixtures
grid = gpd.read_file(grid_fp)
square =  gpd.read_file(square_fp)
points = gpd.read_file(points_fp)
lines = gpd.read_file(lines_fp)


# Intersecting polygons
spatial_result, json_data = intersect(
    square_fp,
    'name',
    grid_fp,
    'name',
    compress=False
)

# Load resulting output data
print(""Output from intersecting polygons"")
pprint(json.load(open(json_data)))


# Intersecting lines
spatial_result, json_data = intersect(
    lines_fp,
    'name',
    grid_fp,
    'name',
    compress=False
)

# Load resulting output data
print(""Output from intersecting lines"")
pprint(json.load(open(json_data)))
print(""Vector file with calculated intersections written to:"", spatial_result)

# Intersecting points
spatial_result, json_data = intersect(
    points_fp,
    'name',
    grid_fp,
    'name',
    compress=False
)

# Load resulting output data
print(""Output from intersecting points"")
pprint(json.load(open(json_data)))
print(""Vector file with calculated intersections written to:"", spatial_result)


# Getting raster statistics for polygons
json_data = raster_statistics(
    grid_fp,
    'name',
    range_fp,
    compress=False
)

# Load resulting output data
print(""Output from raster statistics for polygons"")
pprint(json.load(open(json_data)))
print(""Vector file with calculated intersections written to:"", spatial_result)


# Getting raster statistics for lines
json_data = raster_statistics(
    lines_fp,
    'name',
    range_fp,
    compress=False
)

# Load resulting output data
print(""Output from raster statistics for lines"")
pprint(json.load(open(json_data)))


# Getting raster statistics for points
json_data = raster_statistics(
    points_fp,
    'name',
    range_fp,
    compress=False
)

# Load resulting output data
print(""Output from raster statistics for points"")
pprint(json.load(open(json_data)))

```"
f8ee77368da560d1becb6738ef4d8aca9d7e9ba8,annotation_statistics.py,annotation_statistics.py,,"""""""Count the numbers of annotated entities and emotional sentences in the
corpus that was manually annotated.

Usage: python annotation_statistics.py <dir containing the folia files with
EmbodiedEmotions annotations>
""""""
from lxml import etree
from bs4 import BeautifulSoup
from emotools.bs4_helpers import sentence, note
import argparse
import os
from collections import Counter


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('dir_name', help='the name of the dir containing the '
                        'FoLiA XML files that should be processed.')
    args = parser.parse_args()

    dir_name = args.dir_name

    entity_class = u'EmbodiedEmotions'

    act_tag = '{http://ilk.uvt.nl/folia}div'

    os.chdir(dir_name)

    folia_counter = 0
    num_sent = 0
    num_emotional = 0
    stats = Counter()

    for file_name in os.listdir(dir_name):
        folia_counter += 1
        print '({}) {}'.format(folia_counter, file_name)

        sents = set()
        # load document
        context = etree.iterparse(file_name,
                                  events=('start', 'end'),
                                  tag=act_tag,
                                  huge_tree=True)
        delete = True
        for event, elem in context:
            if event == 'start' and elem.get('class') == 'act':
                delete = False
            if event == 'end' and elem.get('class') == 'act':
                # load act into memory
                act_xml = BeautifulSoup(etree.tostring(elem), 'xml')
                sentences = act_xml.find_all(sentence)
                s = None
                for sent in sentences:
                    if not note(sent.parent):
                        # some t elements appear to be empty (this is not
                        # allowed, but it happens). So, check whether there is
                        # a string to add before adding it.
                        if sent.t:
                            if sent.t.string:
                                s = sent.t.string

                        # calculate stats only for unique sentences in text
                        if s and s not in sents:
                            sents.add(s)
                            num_sent += 1

                            entities = sent.find_all('entity')
                            emotional = False
                            for entity in entities:
                                e = entity.attrs.get('class')
                                if e.startswith(entity_class):
                                    emotional = True
                                    stats[e] += 1
                            if emotional:
                                num_emotional += 1

                delete = True

            # clear memory
            #if delete:
            #    elem.clear()
            #    while elem.getprevious() is not None:
            #        del elem.getparent()[0]
            #        del context

    # print stats
    print '{} sentences in {} files'.format(num_sent, folia_counter)
    print '{} emotional sentences'.format(num_emotional)
    for tag, freq in stats.most_common():
        print '{}\t{}'.format(tag, freq)
",Add script to calculate annotation statistics,"Add script to calculate annotation statistics

Added a script that counts the number of annotated entities and
emotional vs. non-emotional sentences.
",Python,apache-2.0,"NLeSC/embodied-emotions-scripts,NLeSC/embodied-emotions-scripts",91,"```python
""""""Count the numbers of annotated entities and emotional sentences in the
corpus that was manually annotated.

Usage: python annotation_statistics.py <dir containing the folia files with
EmbodiedEmotions annotations>
""""""
from lxml import etree
from bs4 import BeautifulSoup
from emotools.bs4_helpers import sentence, note
import argparse
import os
from collections import Counter


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('dir_name', help='the name of the dir containing the '
                        'FoLiA XML files that should be processed.')
    args = parser.parse_args()

    dir_name = args.dir_name

    entity_class = u'EmbodiedEmotions'

    act_tag = '{http://ilk.uvt.nl/folia}div'

    os.chdir(dir_name)

    folia_counter = 0
    num_sent = 0
    num_emotional = 0
    stats = Counter()

    for file_name in os.listdir(dir_name):
        folia_counter += 1
        print '({}) {}'.format(folia_counter, file_name)

        sents = set()
        # load document
        context = etree.iterparse(file_name,
                                  events=('start', 'end'),
                                  tag=act_tag,
                                  huge_tree=True)
        delete = True
        for event, elem in context:
            if event == 'start' and elem.get('class') == 'act':
                delete = False
            if event == 'end' and elem.get('class') == 'act':
                # load act into memory
                act_xml = BeautifulSoup(etree.tostring(elem), 'xml')
                sentences = act_xml.find_all(sentence)
                s = None
                for sent in sentences:
                    if not note(sent.parent):
                        # some t elements appear to be empty (this is not
                        # allowed, but it happens). So, check whether there is
                        # a string to add before adding it.
                        if sent.t:
                            if sent.t.string:
                                s = sent.t.string

                        # calculate stats only for unique sentences in text
                        if s and s not in sents:
                            sents.add(s)
                            num_sent += 1

                            entities = sent.find_all('entity')
                            emotional = False
                            for entity in entities:
                                e = entity.attrs.get('class')
                                if e.startswith(entity_class):
                                    emotional = True
                                    stats[e] += 1
                            if emotional:
                                num_emotional += 1

                delete = True

            # clear memory
            #if delete:
            #    elem.clear()
            #    while elem.getprevious() is not None:
            #        del elem.getparent()[0]
            #        del context

    # print stats
    print '{} sentences in {} files'.format(num_sent, folia_counter)
    print '{} emotional sentences'.format(num_emotional)
    for tag, freq in stats.most_common():
        print '{}\t{}'.format(tag, freq)

```"
df719f08efdbbadc5694454ffceed21c7c54e8c7,tests/test_config_gauge.py,tests/test_config_gauge.py,,"#!/usr/bin/env python3

""""""Test config parsing""""""

import logging
import os
import shutil
import tempfile
import unittest
from faucet import config_parser as cp

LOGNAME = '/dev/null'


class TestGaugeConfig(unittest.TestCase):
    """"""Test gauge.yaml config parsing.""""""

    DEFAULT_FAUCET_CONFIG = """"""
dps:
    dp1:
        dp_id: 1
        interfaces:
            1:
                native_vlan: v1
    dp2:
        dp_id: 2
        interfaces:
            1:
                native_vlan: v1
vlans:
    v1:
        vid: 1
""""""

    GAUGE_CONFIG_HEADER = """"""
faucet_configs:
    - '{}'
""""""

    def setUp(self):
        logging.disable(logging.CRITICAL)
        self.tmpdir = tempfile.mkdtemp()

    def tearDown(self):
        logging.disable(logging.NOTSET)
        shutil.rmtree(self.tmpdir)

    def conf_file_name(self, faucet=False):
        if faucet:
            return os.path.join(self.tmpdir, 'faucet.yaml')
        else:
            return os.path.join(self.tmpdir, 'gauge.yaml')

    def create_config_files(self, config, faucet_config=None):
        """"""Returns file path to file containing the config parameter.""""""
        gauge_file_name = self.conf_file_name()
        faucet_file_name = self.conf_file_name(faucet=True)
        with open(gauge_file_name, 'w') as conf_file:
            conf_file.write(config.format(faucet_file_name))
        with open(faucet_file_name, 'w') as conf_file:
            if faucet_config:
                conf_file.write(faucet_config)
            else:
                conf_file.write(self.DEFAULT_FAUCET_CONFIG)
        return (gauge_file_name, faucet_file_name)

    def get_config(self, conf_suffix):
        return self.GAUGE_CONFIG_HEADER + conf_suffix


    def test_all_dps(self):
        GAUGE_CONF = """"""
watchers:
    port_stats_poller:
        type: 'port_stats'
        all_dps: True
        interval: 10
        db: 'prometheus'
dbs:
    prometheus:
        type: 'prometheus'
""""""
        conf = self.get_config(GAUGE_CONF)
        gauge_file, faucet_file = self.create_config_files(conf)
        watcher_confs = cp.watcher_parser(gauge_file, 'gauge_config_test', None)
        self.assertEqual(len(watcher_confs), 2, 'failed to create config for each dp')
        for watcher_conf in watcher_confs:
            msg = 'all_dps config not applied to each dp'
            self.assertEqual(watcher_conf.type, 'port_stats', msg)
            self.assertEqual(watcher_conf.interval, 10, msg)
            self.assertEqual(watcher_conf.db_type, 'prometheus', msg)

if __name__ == ""__main__"":
    unittest.main()
",Add unit tests for gauge config,"Add unit tests for gauge config

    Add test coverage for https://github.com/faucetsdn/faucet/issues/1441
",Python,apache-2.0,"anarkiwi/faucet,gizmoguy/faucet,shivarammysore/faucet,wackerly/faucet,faucetsdn/faucet,trungdtbk/faucet,mwutzke/faucet,mwutzke/faucet,trentindav/faucet,wackerly/faucet,shivarammysore/faucet,trentindav/faucet,gizmoguy/faucet,REANNZ/faucet,REANNZ/faucet,anarkiwi/faucet,faucetsdn/faucet,trungdtbk/faucet",95,"```python
#!/usr/bin/env python3

""""""Test config parsing""""""

import logging
import os
import shutil
import tempfile
import unittest
from faucet import config_parser as cp

LOGNAME = '/dev/null'


class TestGaugeConfig(unittest.TestCase):
    """"""Test gauge.yaml config parsing.""""""

    DEFAULT_FAUCET_CONFIG = """"""
dps:
    dp1:
        dp_id: 1
        interfaces:
            1:
                native_vlan: v1
    dp2:
        dp_id: 2
        interfaces:
            1:
                native_vlan: v1
vlans:
    v1:
        vid: 1
""""""

    GAUGE_CONFIG_HEADER = """"""
faucet_configs:
    - '{}'
""""""

    def setUp(self):
        logging.disable(logging.CRITICAL)
        self.tmpdir = tempfile.mkdtemp()

    def tearDown(self):
        logging.disable(logging.NOTSET)
        shutil.rmtree(self.tmpdir)

    def conf_file_name(self, faucet=False):
        if faucet:
            return os.path.join(self.tmpdir, 'faucet.yaml')
        else:
            return os.path.join(self.tmpdir, 'gauge.yaml')

    def create_config_files(self, config, faucet_config=None):
        """"""Returns file path to file containing the config parameter.""""""
        gauge_file_name = self.conf_file_name()
        faucet_file_name = self.conf_file_name(faucet=True)
        with open(gauge_file_name, 'w') as conf_file:
            conf_file.write(config.format(faucet_file_name))
        with open(faucet_file_name, 'w') as conf_file:
            if faucet_config:
                conf_file.write(faucet_config)
            else:
                conf_file.write(self.DEFAULT_FAUCET_CONFIG)
        return (gauge_file_name, faucet_file_name)

    def get_config(self, conf_suffix):
        return self.GAUGE_CONFIG_HEADER + conf_suffix


    def test_all_dps(self):
        GAUGE_CONF = """"""
watchers:
    port_stats_poller:
        type: 'port_stats'
        all_dps: True
        interval: 10
        db: 'prometheus'
dbs:
    prometheus:
        type: 'prometheus'
""""""
        conf = self.get_config(GAUGE_CONF)
        gauge_file, faucet_file = self.create_config_files(conf)
        watcher_confs = cp.watcher_parser(gauge_file, 'gauge_config_test', None)
        self.assertEqual(len(watcher_confs), 2, 'failed to create config for each dp')
        for watcher_conf in watcher_confs:
            msg = 'all_dps config not applied to each dp'
            self.assertEqual(watcher_conf.type, 'port_stats', msg)
            self.assertEqual(watcher_conf.interval, 10, msg)
            self.assertEqual(watcher_conf.db_type, 'prometheus', msg)

if __name__ == ""__main__"":
    unittest.main()

```"
f2fb791df5023a39c82561ceb79b92b0b584e5af,scripts/fetch_data.py,scripts/fetch_data.py,,"import json
import requests
import dataset


def fetch_builds_data(jenkins_url):
    '''Get json data of all Jenkins builds

    JENKINS_URL/api/json?
      tree=jobs[name,builds[number,result,duration,builtOn,id,timestamp,fullDisplayName]]
    
    Return builds_data dictionary with following schema::

     [
      {
        ""name"" : <job_name>,
        ""builds"" : [
                     {
                       ""duration"" : <build_duration>,
                       ""timestamp"" : <build_timestamp>,
                       ""id"" : <build_id>,
                       ""number"" : <build_number>,
                       ""result"" : <build_result>,
                       ""builtOn"" : <node>,
                       ""fullDisplayName"": <build_display_name>,
                     }
                   ],
                   ...
      },
      ...

     ]
    '''

    params = {}
    params['tree'] = 'jobs[name,builds[number,result,duration,builtOn,id,timestamp,fullDisplayName]]'
    r = requests.get(
        ""%s/api/json"" % jenkins_url,
        params=params
    )
    builds_data = json.loads(r.text)[""jobs""]

    return builds_data

def store_builds_data(builds_data, dbname):
    '''Saves builds_data in SQLite database

    Database has one table `builds` with following columns::

       index           = 5
       builtOn         = slave01.example.com
       name            = check_shell
       timestamp       = 1432739306340
       number          = 113
       id              = 2015-05-27_15-08-26
       result          = SUCCESS
       duration        = 3796
       fullDisplayName = check_shell #113

    '''
    
    db = dataset.connect('sqlite:///%s' % dbname)
    
    table = db.get_table('builds', primary_id='index')

    db.begin()
    for job_entry in builds_data:
        for build in job_entry['builds']:

            # FIXME: Build DB is append-only, thus, for efficiency, we
            # should process only new builds with timestamp later than
            # the last one which alredy exists in db

            build['name'] = job_entry['name']
            table.upsert(build, ['name','number'])
    db.commit()
    
    return len(db['builds'])

def update_builds_db(dbname, source_file=None, source_url=None):

    if source_file:
        with open(source_file, 'r') as f:
            builds_data = json.loads(f.readline())['jobs']
    elif source_url:
        builds_data = fetch_builds_data(source_url)
    else:
        raise ValueError(""No URL and no source file specified"")

    return store_builds_data(builds_data, dbname)
    
if __name__ == '__main__':
    update_builds_db(source_file='test.data', dbname='test.db')

",Add script to fetch builds from Jenkins,"Add script to fetch builds from Jenkins

- uses dataset library to store data in sqlite database,
- supports fetching data from Jenkins via JSON API or from file.
",Python,mit,"bookwar/jenkins-report,bookwar/jenkins-report",95,"```python
import json
import requests
import dataset


def fetch_builds_data(jenkins_url):
    '''Get json data of all Jenkins builds

    JENKINS_URL/api/json?
      tree=jobs[name,builds[number,result,duration,builtOn,id,timestamp,fullDisplayName]]
    
    Return builds_data dictionary with following schema::

     [
      {
        ""name"" : <job_name>,
        ""builds"" : [
                     {
                       ""duration"" : <build_duration>,
                       ""timestamp"" : <build_timestamp>,
                       ""id"" : <build_id>,
                       ""number"" : <build_number>,
                       ""result"" : <build_result>,
                       ""builtOn"" : <node>,
                       ""fullDisplayName"": <build_display_name>,
                     }
                   ],
                   ...
      },
      ...

     ]
    '''

    params = {}
    params['tree'] = 'jobs[name,builds[number,result,duration,builtOn,id,timestamp,fullDisplayName]]'
    r = requests.get(
        ""%s/api/json"" % jenkins_url,
        params=params
    )
    builds_data = json.loads(r.text)[""jobs""]

    return builds_data

def store_builds_data(builds_data, dbname):
    '''Saves builds_data in SQLite database

    Database has one table `builds` with following columns::

       index           = 5
       builtOn         = slave01.example.com
       name            = check_shell
       timestamp       = 1432739306340
       number          = 113
       id              = 2015-05-27_15-08-26
       result          = SUCCESS
       duration        = 3796
       fullDisplayName = check_shell #113

    '''
    
    db = dataset.connect('sqlite:///%s' % dbname)
    
    table = db.get_table('builds', primary_id='index')

    db.begin()
    for job_entry in builds_data:
        for build in job_entry['builds']:

            # FIXME: Build DB is append-only, thus, for efficiency, we
            # should process only new builds with timestamp later than
            # the last one which alredy exists in db

            build['name'] = job_entry['name']
            table.upsert(build, ['name','number'])
    db.commit()
    
    return len(db['builds'])

def update_builds_db(dbname, source_file=None, source_url=None):

    if source_file:
        with open(source_file, 'r') as f:
            builds_data = json.loads(f.readline())['jobs']
    elif source_url:
        builds_data = fetch_builds_data(source_url)
    else:
        raise ValueError(""No URL and no source file specified"")

    return store_builds_data(builds_data, dbname)
    
if __name__ == '__main__':
    update_builds_db(source_file='test.data', dbname='test.db')


```"
82574e953dcb2ff3bd47b7ae1a70d956a06633de,examples/demo/basic/scatter_alpha.py,examples/demo/basic/scatter_alpha.py,,"""""""
Scatter plot with panning and zooming

Shows a scatter plot of a set of random points,
with basic Chaco panning and zooming.

Interacting with the plot:

  - Left-mouse-drag pans the plot.
  - Mouse wheel up and down zooms the plot in and out.
  - Pressing ""z"" brings up the Zoom Box, and you can click-drag a rectangular
    region to zoom. If you use a sequence of zoom boxes, pressing alt-left-arrow
    and alt-right-arrow moves you forwards and backwards through the ""zoom
    history"".
""""""

# Major library imports
from numpy import sort
from numpy.random import random

# Enthought library imports
from enable.api import Component, ComponentEditor
from traits.api import DelegatesTo, HasTraits, Instance
from traitsui.api import Item, Group, View, RangeEditor

# Chaco imports
from chaco.api import ArrayPlotData, Plot
from chaco.tools.api import PanTool, ZoomTool

#===============================================================================
# # Create the Chaco plot.
#===============================================================================
def _create_plot_component():

    # Create some data
    numpts = 5000
    x = sort(random(numpts))
    y = random(numpts)

    # Create a plot data object and give it this data
    pd = ArrayPlotData()
    pd.set_data(""index"", x)
    pd.set_data(""value"", y)

    # Create the plot
    plot = Plot(pd)
    plot.plot((""index"", ""value""),
              type=""scatter"",
              marker=""circle"",
              index_sort=""ascending"",
              color=""orange"",
              marker_size=3,
              bgcolor=""white"")

    # Tweak some of the plot properties
    plot.title = ""Scatter Plot""
    plot.line_width = 0.5
    plot.padding = 50

    # Attach some tools to the plot
    plot.tools.append(PanTool(plot, constrain_key=""shift""))
    zoom = ZoomTool(component=plot, tool_mode=""box"", always_on=False)
    plot.overlays.append(zoom)

    return plot

#===============================================================================
# Attributes to use for the plot view.
size = (650, 650)
title = ""Basic scatter plot""
bg_color=""lightgray""

#===============================================================================
# # Demo class that is used by the demo.py application.
#===============================================================================
class Demo(HasTraits):
    plot = Instance(Component)

    scatter_renderer = Instance(Component)

    alpha = DelegatesTo('scatter_renderer')

    traits_view = View(
                    Group(
                        Item('plot', editor=ComponentEditor(size=size,
                                                            bgcolor=bg_color),
                             show_label=False),
                        Group(
                            Item('alpha', editor=RangeEditor(low=0.0, high=1.0)),
                        ),
                        orientation = ""vertical""),
                    resizable=True, title=title
                    )

    def _plot_default(self):
         return _create_plot_component()

    def _scatter_renderer_default(self):
        plot = self.plot
        renderer = plot.plots.values()[0][0]
        print renderer
        return renderer

demo = Demo()

if __name__ == ""__main__"":
    demo.configure_traits()

#--EOF---
","Add example which demonstrates issue (works in this branch, fails otherwise).","Add example which demonstrates issue (works in this branch, fails otherwise).
",Python,bsd-3-clause,"tommy-u/chaco,tommy-u/chaco,tommy-u/chaco",110,"```python
""""""
Scatter plot with panning and zooming

Shows a scatter plot of a set of random points,
with basic Chaco panning and zooming.

Interacting with the plot:

  - Left-mouse-drag pans the plot.
  - Mouse wheel up and down zooms the plot in and out.
  - Pressing ""z"" brings up the Zoom Box, and you can click-drag a rectangular
    region to zoom. If you use a sequence of zoom boxes, pressing alt-left-arrow
    and alt-right-arrow moves you forwards and backwards through the ""zoom
    history"".
""""""

# Major library imports
from numpy import sort
from numpy.random import random

# Enthought library imports
from enable.api import Component, ComponentEditor
from traits.api import DelegatesTo, HasTraits, Instance
from traitsui.api import Item, Group, View, RangeEditor

# Chaco imports
from chaco.api import ArrayPlotData, Plot
from chaco.tools.api import PanTool, ZoomTool

#===============================================================================
# # Create the Chaco plot.
#===============================================================================
def _create_plot_component():

    # Create some data
    numpts = 5000
    x = sort(random(numpts))
    y = random(numpts)

    # Create a plot data object and give it this data
    pd = ArrayPlotData()
    pd.set_data(""index"", x)
    pd.set_data(""value"", y)

    # Create the plot
    plot = Plot(pd)
    plot.plot((""index"", ""value""),
              type=""scatter"",
              marker=""circle"",
              index_sort=""ascending"",
              color=""orange"",
              marker_size=3,
              bgcolor=""white"")

    # Tweak some of the plot properties
    plot.title = ""Scatter Plot""
    plot.line_width = 0.5
    plot.padding = 50

    # Attach some tools to the plot
    plot.tools.append(PanTool(plot, constrain_key=""shift""))
    zoom = ZoomTool(component=plot, tool_mode=""box"", always_on=False)
    plot.overlays.append(zoom)

    return plot

#===============================================================================
# Attributes to use for the plot view.
size = (650, 650)
title = ""Basic scatter plot""
bg_color=""lightgray""

#===============================================================================
# # Demo class that is used by the demo.py application.
#===============================================================================
class Demo(HasTraits):
    plot = Instance(Component)

    scatter_renderer = Instance(Component)

    alpha = DelegatesTo('scatter_renderer')

    traits_view = View(
                    Group(
                        Item('plot', editor=ComponentEditor(size=size,
                                                            bgcolor=bg_color),
                             show_label=False),
                        Group(
                            Item('alpha', editor=RangeEditor(low=0.0, high=1.0)),
                        ),
                        orientation = ""vertical""),
                    resizable=True, title=title
                    )

    def _plot_default(self):
         return _create_plot_component()

    def _scatter_renderer_default(self):
        plot = self.plot
        renderer = plot.plots.values()[0][0]
        print renderer
        return renderer

demo = Demo()

if __name__ == ""__main__"":
    demo.configure_traits()

#--EOF---

```"
c050cbd0f13f34915854137dced4003b8836f451,scripts/image_signing/security_test_artifact.py,scripts/image_signing/security_test_artifact.py,,"#!/usr/bin/env python3
# Copyright 2022 The ChromiumOS Authors.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Run security tests on an artifact""""""

import argparse
import os
from pathlib import Path
import subprocess
import sys

DIR = Path(__file__).resolve().parent


def exec_test(name, input, args):
    """"""Runs a given script

    Args:
        name: the name of the script to execute
        input: the input artifact
        args: list of additional arguments for the script
    """"""
    # Ensure this script can execute from any directory
    cmd_path = DIR / f""{name}.sh""

    cmd = [cmd_path, input] + args
    ret = subprocess.run(cmd, check=False)
    if ret.returncode:
        sys.exit(ret.returncode)


def get_parser():
    """"""Creates an argument parser""""""
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        ""--config"",
        ""-c"",
        help=""Security test baseline config directory"",
        required=True,
        type=Path,
    )

    parser.add_argument(
        ""--input"",
        ""-i"",
        help=""Artfact to test"",
        required=True,
        type=Path,
    )

    parser.add_argument(
        ""--keyset-is-mp"",
        action=""store_true"",
        help=""Target artifact is signed with a mass production keyset"",
        default=False,
    )

    return parser


def main(argv):
    """"""Main function, parses arguments and invokes the relevant scripts""""""
    parser = get_parser()
    opts = parser.parse_args(argv)

    # Run generic baseline tests.
    baseline_tests = [
        ""ensure_sane_lsb-release"",
    ]

    if opts.keyset_is_mp:
        baseline_tests += [
            ""ensure_no_nonrelease_files"",
            ""ensure_secure_kernelparams"",
        ]

    for test in baseline_tests:
        exec_test(
            test, opts.input, [os.path.join(opts.config, f""{test}.config"")]
        )

    # Run generic non-baseline tests.
    tests = []

    if opts.keyset_is_mp:
        tests += [
            ""ensure_not_ASAN"",
            ""ensure_not_tainted_license"",
            ""ensure_update_verification"",
        ]

    for test in tests:
        exec_test(test, opts.input, [])


if __name__ == ""__main__"":
    sys.exit(main(sys.argv[1:]))
",Add general security test script,"scripts/image_signing: Add general security test script

This commit adds a general security test script. This allows
some logic to be moved out of the signer as well as providing a single
entry point for the security tests run by the signer.

BRANCH=none
BUG=b:202397678
TEST=Verified that correct security tests ran with/without
`--keyset-is-mp`

Change-Id: Ib4c779a90d2fe9160c278f20d7ec61242f1d68cc
Signed-off-by: Robert Zieba <33e0ad3807fbfa18c105b3fe38a4090f81988052@google.com>
Reviewed-on: https://chromium-review.googlesource.com/c/chromiumos/platform/vboot_reference/+/3820999
Reviewed-by: Mike Frysinger <8f3f75c74bd5184edcfa6534cab3c13a00a2f794@chromium.org>
Commit-Queue: Mike Frysinger <8f3f75c74bd5184edcfa6534cab3c13a00a2f794@chromium.org>
",Python,bsd-3-clause,"coreboot/vboot,coreboot/vboot,coreboot/vboot,coreboot/vboot,coreboot/vboot",100,"```python
#!/usr/bin/env python3
# Copyright 2022 The ChromiumOS Authors.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Run security tests on an artifact""""""

import argparse
import os
from pathlib import Path
import subprocess
import sys

DIR = Path(__file__).resolve().parent


def exec_test(name, input, args):
    """"""Runs a given script

    Args:
        name: the name of the script to execute
        input: the input artifact
        args: list of additional arguments for the script
    """"""
    # Ensure this script can execute from any directory
    cmd_path = DIR / f""{name}.sh""

    cmd = [cmd_path, input] + args
    ret = subprocess.run(cmd, check=False)
    if ret.returncode:
        sys.exit(ret.returncode)


def get_parser():
    """"""Creates an argument parser""""""
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        ""--config"",
        ""-c"",
        help=""Security test baseline config directory"",
        required=True,
        type=Path,
    )

    parser.add_argument(
        ""--input"",
        ""-i"",
        help=""Artfact to test"",
        required=True,
        type=Path,
    )

    parser.add_argument(
        ""--keyset-is-mp"",
        action=""store_true"",
        help=""Target artifact is signed with a mass production keyset"",
        default=False,
    )

    return parser


def main(argv):
    """"""Main function, parses arguments and invokes the relevant scripts""""""
    parser = get_parser()
    opts = parser.parse_args(argv)

    # Run generic baseline tests.
    baseline_tests = [
        ""ensure_sane_lsb-release"",
    ]

    if opts.keyset_is_mp:
        baseline_tests += [
            ""ensure_no_nonrelease_files"",
            ""ensure_secure_kernelparams"",
        ]

    for test in baseline_tests:
        exec_test(
            test, opts.input, [os.path.join(opts.config, f""{test}.config"")]
        )

    # Run generic non-baseline tests.
    tests = []

    if opts.keyset_is_mp:
        tests += [
            ""ensure_not_ASAN"",
            ""ensure_not_tainted_license"",
            ""ensure_update_verification"",
        ]

    for test in tests:
        exec_test(test, opts.input, [])


if __name__ == ""__main__"":
    sys.exit(main(sys.argv[1:]))

```"
cde2263c2084b8ce91e85face95e5e85439ab7ce,scripts/annotate_rsvps.py,scripts/annotate_rsvps.py,,"""""""Utilities for annotating workshop RSVP data.

Example ::

    import pandas as pd
    from scripts import annotate_rsvps
    frame = pd.read_csv('workshop.csv')
    annotated = annotate_rsvps.process(frame)
    annotated.to_csv('workshop-annotated.csv')

""""""

import re
import logging

from dateutil.parser import parse as parse_date

from modularodm import Q
from modularodm.exceptions import ModularOdmException

from website.models import User, Node, NodeLog


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def find_by_email(email):
    try:
        return User.find_one(Q('username', 'iexact', email))
    except ModularOdmException:
        return None


def find_by_name(name):
    try:
        parts = re.split(r'\s+', name.strip())
    except:
        return None
    if len(parts) < 2:
        return None
    users = User.find(
        reduce(
            lambda acc, value: acc & value,
            [
                Q('fullname', 'icontains', part.decode('utf-8', 'ignore'))
                for part in parts
            ]
        )
    ).sort('-date_created')
    if not users:
        return None
    if len(users) > 1:
        logger.warn('Multiple users found for name {}'.format(name))
    return users[0]


def logs_since(user, date):
    return NodeLog.find(
        Q('user', 'eq', user._id) &
        Q('date', 'gt', date)
    )


def nodes_since(user, date):
    return Node.find(
        Q('creator', 'eq', user._id) &
        Q('date_created', 'gt', date)
    )


def process(frame):
    frame = frame.copy()
    frame['user_id'] = ''
    frame['user_logs'] = ''
    frame['user_nodes'] = ''
    frame['last_log'] = ''
    for idx, row in frame.iterrows():
        user = (
            find_by_email(row['Email address'].strip()) or
            find_by_name(row['Name'])
        )
        if user:
            date = parse_date(row['Workshop_date'])
            frame.loc[idx, 'user_id'] = user._id
            logs = logs_since(user, date)
            frame.loc[idx, 'user_logs'] = logs.count()
            frame.loc[idx, 'user_nodes'] = nodes_since(user, date).count()
            if logs:
                frame.loc[idx, 'last_log'] = logs.sort('-date')[0].date.strftime('%c')
    return frame
",Add script to annotate conference RSVP spreadsheets.,"Add script to annotate conference RSVP spreadsheets.

Requested by @lbanner.
",Python,apache-2.0,"caneruguz/osf.io,CenterForOpenScience/osf.io,petermalcolm/osf.io,bdyetton/prettychart,asanfilippo7/osf.io,petermalcolm/osf.io,KAsante95/osf.io,ZobairAlijan/osf.io,cwisecarver/osf.io,Ghalko/osf.io,Johnetordoff/osf.io,cosenal/osf.io,DanielSBrown/osf.io,doublebits/osf.io,jnayak1/osf.io,MerlinZhang/osf.io,rdhyee/osf.io,danielneis/osf.io,danielneis/osf.io,cslzchen/osf.io,reinaH/osf.io,kushG/osf.io,HarryRybacki/osf.io,billyhunt/osf.io,acshi/osf.io,kch8qx/osf.io,jnayak1/osf.io,lamdnhan/osf.io,HarryRybacki/osf.io,caseyrollins/osf.io,revanthkolli/osf.io,hmoco/osf.io,hmoco/osf.io,icereval/osf.io,doublebits/osf.io,adlius/osf.io,erinspace/osf.io,fabianvf/osf.io,lamdnhan/osf.io,caseyrygt/osf.io,hmoco/osf.io,amyshi188/osf.io,arpitar/osf.io,cslzchen/osf.io,wearpants/osf.io,asanfilippo7/osf.io,mattclark/osf.io,zachjanicki/osf.io,monikagrabowska/osf.io,dplorimer/osf,Ghalko/osf.io,dplorimer/osf,brianjgeiger/osf.io,erinspace/osf.io,CenterForOpenScience/osf.io,kwierman/osf.io,reinaH/osf.io,DanielSBrown/osf.io,reinaH/osf.io,caneruguz/osf.io,aaxelb/osf.io,petermalcolm/osf.io,laurenrevere/osf.io,baylee-d/osf.io,brianjgeiger/osf.io,barbour-em/osf.io,caseyrygt/osf.io,monikagrabowska/osf.io,DanielSBrown/osf.io,ticklemepierce/osf.io,leb2dg/osf.io,jolene-esposito/osf.io,barbour-em/osf.io,chennan47/osf.io,GageGaskins/osf.io,jolene-esposito/osf.io,kch8qx/osf.io,binoculars/osf.io,SSJohns/osf.io,icereval/osf.io,jnayak1/osf.io,monikagrabowska/osf.io,zachjanicki/osf.io,chennan47/osf.io,TomHeatwole/osf.io,jolene-esposito/osf.io,emetsger/osf.io,cosenal/osf.io,fabianvf/osf.io,TomBaxter/osf.io,barbour-em/osf.io,cosenal/osf.io,monikagrabowska/osf.io,mluo613/osf.io,mattclark/osf.io,alexschiller/osf.io,acshi/osf.io,brandonPurvis/osf.io,Johnetordoff/osf.io,hmoco/osf.io,kwierman/osf.io,brandonPurvis/osf.io,sbt9uc/osf.io,felliott/osf.io,fabianvf/osf.io,icereval/osf.io,TomHeatwole/osf.io,ckc6cz/osf.io,HarryRybacki/osf.io,RomanZWang/osf.io,mfraezz/osf.io,adlius/osf.io,himanshuo/osf.io,lyndsysimon/osf.io,KAsante95/osf.io,felliott/osf.io,ticklemepierce/osf.io,crcresearch/osf.io,lamdnhan/osf.io,caneruguz/osf.io,chennan47/osf.io,MerlinZhang/osf.io,cslzchen/osf.io,sloria/osf.io,CenterForOpenScience/osf.io,pattisdr/osf.io,jmcarp/osf.io,dplorimer/osf,jeffreyliu3230/osf.io,binoculars/osf.io,himanshuo/osf.io,cwisecarver/osf.io,abought/osf.io,aaxelb/osf.io,wearpants/osf.io,himanshuo/osf.io,zkraime/osf.io,wearpants/osf.io,HarryRybacki/osf.io,zamattiac/osf.io,doublebits/osf.io,kushG/osf.io,zamattiac/osf.io,amyshi188/osf.io,acshi/osf.io,cwisecarver/osf.io,jeffreyliu3230/osf.io,adlius/osf.io,caseyrygt/osf.io,mfraezz/osf.io,danielneis/osf.io,cldershem/osf.io,bdyetton/prettychart,TomHeatwole/osf.io,mfraezz/osf.io,doublebits/osf.io,revanthkolli/osf.io,billyhunt/osf.io,HalcyonChimera/osf.io,adlius/osf.io,MerlinZhang/osf.io,billyhunt/osf.io,ticklemepierce/osf.io,petermalcolm/osf.io,lyndsysimon/osf.io,mluke93/osf.io,GaryKriebel/osf.io,jmcarp/osf.io,doublebits/osf.io,chrisseto/osf.io,njantrania/osf.io,kwierman/osf.io,TomBaxter/osf.io,zkraime/osf.io,abought/osf.io,KAsante95/osf.io,asanfilippo7/osf.io,saradbowman/osf.io,felliott/osf.io,GaryKriebel/osf.io,ckc6cz/osf.io,kushG/osf.io,jinluyuan/osf.io,reinaH/osf.io,ticklemepierce/osf.io,samchrisinger/osf.io,laurenrevere/osf.io,GageGaskins/osf.io,zamattiac/osf.io,RomanZWang/osf.io,felliott/osf.io,mluo613/osf.io,leb2dg/osf.io,cldershem/osf.io,Nesiehr/osf.io,GageGaskins/osf.io,pattisdr/osf.io,ZobairAlijan/osf.io,mluke93/osf.io,samchrisinger/osf.io,zkraime/osf.io,samanehsan/osf.io,amyshi188/osf.io,leb2dg/osf.io,acshi/osf.io,arpitar/osf.io,SSJohns/osf.io,pattisdr/osf.io,SSJohns/osf.io,emetsger/osf.io,lyndsysimon/osf.io,kch8qx/osf.io,sloria/osf.io,Nesiehr/osf.io,Ghalko/osf.io,cslzchen/osf.io,cosenal/osf.io,mattclark/osf.io,Nesiehr/osf.io,danielneis/osf.io,baylee-d/osf.io,saradbowman/osf.io,njantrania/osf.io,kwierman/osf.io,GageGaskins/osf.io,jinluyuan/osf.io,binoculars/osf.io,mluo613/osf.io,kch8qx/osf.io,caseyrollins/osf.io,caneruguz/osf.io,ckc6cz/osf.io,njantrania/osf.io,mluke93/osf.io,Ghalko/osf.io,samanehsan/osf.io,samchrisinger/osf.io,alexschiller/osf.io,fabianvf/osf.io,jmcarp/osf.io,brianjgeiger/osf.io,crcresearch/osf.io,jeffreyliu3230/osf.io,HalcyonChimera/osf.io,emetsger/osf.io,emetsger/osf.io,leb2dg/osf.io,KAsante95/osf.io,haoyuchen1992/osf.io,KAsante95/osf.io,wearpants/osf.io,njantrania/osf.io,ZobairAlijan/osf.io,samanehsan/osf.io,Nesiehr/osf.io,ckc6cz/osf.io,revanthkolli/osf.io,HalcyonChimera/osf.io,brianjgeiger/osf.io,bdyetton/prettychart,acshi/osf.io,rdhyee/osf.io,barbour-em/osf.io,kch8qx/osf.io,jinluyuan/osf.io,cwisecarver/osf.io,DanielSBrown/osf.io,zachjanicki/osf.io,Johnetordoff/osf.io,kushG/osf.io,samchrisinger/osf.io,RomanZWang/osf.io,dplorimer/osf,mfraezz/osf.io,monikagrabowska/osf.io,sbt9uc/osf.io,ZobairAlijan/osf.io,zachjanicki/osf.io,revanthkolli/osf.io,GaryKriebel/osf.io,cldershem/osf.io,brandonPurvis/osf.io,sbt9uc/osf.io,GaryKriebel/osf.io,cldershem/osf.io,bdyetton/prettychart,laurenrevere/osf.io,jnayak1/osf.io,arpitar/osf.io,Johnetordoff/osf.io,SSJohns/osf.io,abought/osf.io,GageGaskins/osf.io,brandonPurvis/osf.io,rdhyee/osf.io,haoyuchen1992/osf.io,rdhyee/osf.io,jeffreyliu3230/osf.io,mluo613/osf.io,RomanZWang/osf.io,jmcarp/osf.io,caseyrollins/osf.io,erinspace/osf.io,caseyrygt/osf.io,billyhunt/osf.io,crcresearch/osf.io,aaxelb/osf.io,RomanZWang/osf.io,TomHeatwole/osf.io,alexschiller/osf.io,sloria/osf.io,CenterForOpenScience/osf.io,himanshuo/osf.io,abought/osf.io,aaxelb/osf.io,haoyuchen1992/osf.io,lamdnhan/osf.io,mluke93/osf.io,chrisseto/osf.io,zkraime/osf.io,HalcyonChimera/osf.io,jolene-esposito/osf.io,sbt9uc/osf.io,chrisseto/osf.io,TomBaxter/osf.io,alexschiller/osf.io,amyshi188/osf.io,zamattiac/osf.io,chrisseto/osf.io,lyndsysimon/osf.io,haoyuchen1992/osf.io,samanehsan/osf.io,alexschiller/osf.io,billyhunt/osf.io,asanfilippo7/osf.io,MerlinZhang/osf.io,mluo613/osf.io,jinluyuan/osf.io,baylee-d/osf.io,brandonPurvis/osf.io,arpitar/osf.io",92,"```python
""""""Utilities for annotating workshop RSVP data.

Example ::

    import pandas as pd
    from scripts import annotate_rsvps
    frame = pd.read_csv('workshop.csv')
    annotated = annotate_rsvps.process(frame)
    annotated.to_csv('workshop-annotated.csv')

""""""

import re
import logging

from dateutil.parser import parse as parse_date

from modularodm import Q
from modularodm.exceptions import ModularOdmException

from website.models import User, Node, NodeLog


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def find_by_email(email):
    try:
        return User.find_one(Q('username', 'iexact', email))
    except ModularOdmException:
        return None


def find_by_name(name):
    try:
        parts = re.split(r'\s+', name.strip())
    except:
        return None
    if len(parts) < 2:
        return None
    users = User.find(
        reduce(
            lambda acc, value: acc & value,
            [
                Q('fullname', 'icontains', part.decode('utf-8', 'ignore'))
                for part in parts
            ]
        )
    ).sort('-date_created')
    if not users:
        return None
    if len(users) > 1:
        logger.warn('Multiple users found for name {}'.format(name))
    return users[0]


def logs_since(user, date):
    return NodeLog.find(
        Q('user', 'eq', user._id) &
        Q('date', 'gt', date)
    )


def nodes_since(user, date):
    return Node.find(
        Q('creator', 'eq', user._id) &
        Q('date_created', 'gt', date)
    )


def process(frame):
    frame = frame.copy()
    frame['user_id'] = ''
    frame['user_logs'] = ''
    frame['user_nodes'] = ''
    frame['last_log'] = ''
    for idx, row in frame.iterrows():
        user = (
            find_by_email(row['Email address'].strip()) or
            find_by_name(row['Name'])
        )
        if user:
            date = parse_date(row['Workshop_date'])
            frame.loc[idx, 'user_id'] = user._id
            logs = logs_since(user, date)
            frame.loc[idx, 'user_logs'] = logs.count()
            frame.loc[idx, 'user_nodes'] = nodes_since(user, date).count()
            if logs:
                frame.loc[idx, 'last_log'] = logs.sort('-date')[0].date.strftime('%c')
    return frame

```"
52d312ad6bcfd68eb88202ef40574f10788eb70b,leetcode/reverse_string.py,leetcode/reverse_string.py,,"""""""
# Problem statement

https://leetcode.com/explore/interview/card/top-interview-questions-easy/127/strings/879/

## Algorithm description

Traverse the given input until the middle of it.
Swap the extremes of the input until reach the middle of it.

Example:

input = ""abcde""
len(input) = 5
middle = ceil(5 / 3) = 3

ii = initial_index
fi = final_index

i = 0, fi = 4, ""abcde""
i = 1, fi = 3, ""ebcda""
i = 2, fi = 2, ""edcba""

This works for odd and event inputs.

### Cases

I considered the following cases:

1. empty: """" -> """"
1. one: ""a"" -> ""a""
1. String length is odd: ""abc"" -> ""cba""
1. String length is even: ""abcd"" -> ""dcba""

### Examples:

""abcde"" -> l: 5 -> int(5 / 2) = 3 ->
i = 0 < 3
ii
0, 5 - 1 - 0 = 4
1, 5 - 1 - 1 = 3
2, 5 - 1 - 2 = 2

""abcd"" -> l: 4 -> int(4 / 2) = 2 ->
i = 0 < 2
ii
0, 4 - 1 - 0 = 3
1, 4 - 1 - 1 = 2


## Complexity

### Time

1. Traverse the given input until the middle of it: O(n)
1. Swap elements of the input: O(1)

Total = O(n) + O(1) = O(n)

### Space

Only simple variables were created: O(1)

## To improve

I prefer to avoid mutating data structures, so I would preferred to create
a new array to store the answer and the return it.

I mutated the given input because that was a constraint given in the problem
statement:
> Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.

""""""

import math

class Solution:

    def reverseString(self, string: [str]) -> None:
        """"""
        Do not return anything, modify s in-place instead.
        """"""
        length = len(string)
        if length <= 1:
            return
        limit = math.ceil(length / 2)
        for index in range(limit):
            final_index = length - 1 - index
            string[index], string[final_index] = string[final_index], string[index]


if __name__ == ""__main__"":
    solution = Solution()
    string = [""H"",""a"",""n"",""n"",""a"",""h""]
    solution.reverseString(string)
    print(string)
",Add solution for Reverse String problem,"Add solution for Reverse String problem
",Python,mit,"julianespinel/training,julianespinel/training,julianespinel/training,julianespinel/trainning,julianespinel/trainning,julianespinel/training",97,"```python
""""""
# Problem statement

https://leetcode.com/explore/interview/card/top-interview-questions-easy/127/strings/879/

## Algorithm description

Traverse the given input until the middle of it.
Swap the extremes of the input until reach the middle of it.

Example:

input = ""abcde""
len(input) = 5
middle = ceil(5 / 3) = 3

ii = initial_index
fi = final_index

i = 0, fi = 4, ""abcde""
i = 1, fi = 3, ""ebcda""
i = 2, fi = 2, ""edcba""

This works for odd and event inputs.

### Cases

I considered the following cases:

1. empty: """" -> """"
1. one: ""a"" -> ""a""
1. String length is odd: ""abc"" -> ""cba""
1. String length is even: ""abcd"" -> ""dcba""

### Examples:

""abcde"" -> l: 5 -> int(5 / 2) = 3 ->
i = 0 < 3
ii
0, 5 - 1 - 0 = 4
1, 5 - 1 - 1 = 3
2, 5 - 1 - 2 = 2

""abcd"" -> l: 4 -> int(4 / 2) = 2 ->
i = 0 < 2
ii
0, 4 - 1 - 0 = 3
1, 4 - 1 - 1 = 2


## Complexity

### Time

1. Traverse the given input until the middle of it: O(n)
1. Swap elements of the input: O(1)

Total = O(n) + O(1) = O(n)

### Space

Only simple variables were created: O(1)

## To improve

I prefer to avoid mutating data structures, so I would preferred to create
a new array to store the answer and the return it.

I mutated the given input because that was a constraint given in the problem
statement:
> Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory.

""""""

import math

class Solution:

    def reverseString(self, string: [str]) -> None:
        """"""
        Do not return anything, modify s in-place instead.
        """"""
        length = len(string)
        if length <= 1:
            return
        limit = math.ceil(length / 2)
        for index in range(limit):
            final_index = length - 1 - index
            string[index], string[final_index] = string[final_index], string[index]


if __name__ == ""__main__"":
    solution = Solution()
    string = [""H"",""a"",""n"",""n"",""a"",""h""]
    solution.reverseString(string)
    print(string)

```"
dae41df9835a83a71b71b5e4b64561761c404bf5,mp3-formatter/url_scrape_div.py,mp3-formatter/url_scrape_div.py,,"#!/usr/bin/python3

# sudo apt-get install python3-pip
# pip3 install requests

import lxml.html
import requests
import sys

def validate_url(url):
    if not url:
        raise SystemError(""validate_url() was given an empty URL"")

    protocol = ""http://""
    protocol_error_message = ValueError(""A URL beginning with "" \
        ""'http://' is required"")

    if len(url) < len(protocol):
        raise protocol_error_message

    if url[:len(protocol)] != protocol:
        raise protocol_error_message

def scrape_div(url, div_id):
    div_id_lookup_string = '//div[contains(@id, ""' + div_id + '"")]'

    try:
        html_page = requests.get(url)
    except:
        e = sys.exc_info()[0]
        raise ValueError(""Request could not be completed. Perhaps the "" \
            ""URL provided was invalid?"")

    html_page.raise_for_status()

    html_tree = lxml.html.fromstring(html_page.content)
    content = html_tree.xpath(div_id_lookup_string)

    if len(content) < 1:
        raise LookupError(""The requested div could not be found"")
    elif len(content) > 1:
        raise LookupError(""More than one of the requested divs were found"")

    return str(content[0].text_content())

# A line is determined to be the name of a track if it begins with a number
def extract_tracklist_begin_num(content):
    tracklist = []
    for line in content.splitlines():

        # Empty line
        if not line:
            continue

        # Strip leading and trailing whitespace
        line.lstrip()
        line.rstrip()

        if line[0].isdigit():
            tracklist.append(line)

    return tracklist

# Removes leading numbers and whitespace
def strip_leading_index(tracklist):
    tracklist_new = []
    for track in tracklist:

        for i in range(len(track)):
            if track[i].isdigit() or track[i] == "" "":
                i += 1
            else:
                tracklist_new.append(track[i:])
                tracklist[tracklist.index(track)] = track[i:]
                break

if len(sys.argv) < 2:
    raise RuntimeError(""Please provide the URL to the page with ""\
        ""the target tracklist"")

url = sys.argv[1]  # sys.argv[0] is the name of this script
validate_url(url)

div_id = ""stcpDiv""

content = scrape_div(url, div_id)
tracklist = extract_tracklist_begin_num(content)
strip_leading_index(tracklist)

for track in tracklist:
    print(track)
",Add script to scrape tracklist from URL,"MP3: Add script to scrape tracklist from URL

I didn't do this in separate commits because I had such a rough time
extracting the div I needed from the page.
",Python,mit,"jleung51/scripts,jleung51/scripts,jleung51/scripts",92,"```python
#!/usr/bin/python3

# sudo apt-get install python3-pip
# pip3 install requests

import lxml.html
import requests
import sys

def validate_url(url):
    if not url:
        raise SystemError(""validate_url() was given an empty URL"")

    protocol = ""http://""
    protocol_error_message = ValueError(""A URL beginning with "" \
        ""'http://' is required"")

    if len(url) < len(protocol):
        raise protocol_error_message

    if url[:len(protocol)] != protocol:
        raise protocol_error_message

def scrape_div(url, div_id):
    div_id_lookup_string = '//div[contains(@id, ""' + div_id + '"")]'

    try:
        html_page = requests.get(url)
    except:
        e = sys.exc_info()[0]
        raise ValueError(""Request could not be completed. Perhaps the "" \
            ""URL provided was invalid?"")

    html_page.raise_for_status()

    html_tree = lxml.html.fromstring(html_page.content)
    content = html_tree.xpath(div_id_lookup_string)

    if len(content) < 1:
        raise LookupError(""The requested div could not be found"")
    elif len(content) > 1:
        raise LookupError(""More than one of the requested divs were found"")

    return str(content[0].text_content())

# A line is determined to be the name of a track if it begins with a number
def extract_tracklist_begin_num(content):
    tracklist = []
    for line in content.splitlines():

        # Empty line
        if not line:
            continue

        # Strip leading and trailing whitespace
        line.lstrip()
        line.rstrip()

        if line[0].isdigit():
            tracklist.append(line)

    return tracklist

# Removes leading numbers and whitespace
def strip_leading_index(tracklist):
    tracklist_new = []
    for track in tracklist:

        for i in range(len(track)):
            if track[i].isdigit() or track[i] == "" "":
                i += 1
            else:
                tracklist_new.append(track[i:])
                tracklist[tracklist.index(track)] = track[i:]
                break

if len(sys.argv) < 2:
    raise RuntimeError(""Please provide the URL to the page with ""\
        ""the target tracklist"")

url = sys.argv[1]  # sys.argv[0] is the name of this script
validate_url(url)

div_id = ""stcpDiv""

content = scrape_div(url, div_id)
tracklist = extract_tracklist_begin_num(content)
strip_leading_index(tracklist)

for track in tracklist:
    print(track)

```"
7e04bc41e977ef7304972cfc630cd9bf9d2c0aa2,examples/anonymized-real-case.py,examples/anonymized-real-case.py,,"#!/usr/bin/env python2
# coding: utf-8

# this example is used in production, it is depending on private libraries
# to communicate with internal APIs, but it can help you build your own
# production company-specific hook.

import sys
import json
import pprint
pp = pprint.PrettyPrinter(indent=4)

sys.path.insert(0, '/opt/python-provisioning')
from tools.verbose_logging import logging
from localcloud import compute
from api import web_hosting


# version of remote container
ALPINE_VERSION = '3.3'


def nope(msg):
    return {'allowed': False, 'message': msg}


def archify(arch):
    return {
        'arm': 'armhf',
        'x86_64': 'amd64',
    }[arch]


def auth(hosting_id, *keys):

    if len(keys) < 1:
        return nope('no ssh key')

    granted = False
    web_hosting_int = web_hosting.WebHosting(hosting_id)
    for key in keys:

        try:
            if web_hosting_int.is_valid_ssh_key(key) == True:
                granted = True
                break
        except Exception as e:
            logging.error(e)
            return nope('http error')

    if not granted:
        return nope('access denied')

    compute_int = compute.Compute()
    try:
        server = compute_int.get_server_by_webid(hosting_id)
        logging.debug(pp.pformat(server))
    except Exception as e:
        logging.error(e)
        return nope('error while trying to resolve server')

    return {

        'allowed': True,

        'remote-user': hosting_id,

        'image-name': 'local_web/alpine:{}-{}'.format(archify(server['arch']), ALPINE_VERSION),

        'docker-run-args': [
            '--name', 'ssh2docker_{}'.format(hosting_id),
            '--hostname', server['name'],
            '--rm',
            '-it',
            '-v', '/storage/users/{}/ftp:/ftp:rw'.format(hosting_id),
            '-v', '/storage/users/{}/backups:/ftp/backups:ro'.format(hosting_id),
            '-v', '/storage/users/{}/logs:/ftp/logs:ro'.format(hosting_id),
            '-v', '/storage/users/{}/websites:/ftp/websites:rw'.format(hosting_id),
            '-m', '256m',
            '--cpu-shares', '512',  # default = 1024, so ssh2docker gets half quota
            '-u', 'webuser',
        ],

        'env': {
            'DOCKER_HOST': 'tcp://{}.local:2376'.format(server['id']),
            'DOCKER_TLS_VERIFY': '1',
            'DOCKER_CERT_PATH': '/opt/docker-tls/{}/.docker/'.format(server['hostname']),
        },

        'command': ['/bin/sh', '-i', '-l'],

    }


print(json.dumps(auth(*sys.argv[1:])))
",Add anonymzed real case example,"Add anonymzed real case example
",Python,mit,"moul/ssh2docker,moul/ssh2docker",96,"```python
#!/usr/bin/env python2
# coding: utf-8

# this example is used in production, it is depending on private libraries
# to communicate with internal APIs, but it can help you build your own
# production company-specific hook.

import sys
import json
import pprint
pp = pprint.PrettyPrinter(indent=4)

sys.path.insert(0, '/opt/python-provisioning')
from tools.verbose_logging import logging
from localcloud import compute
from api import web_hosting


# version of remote container
ALPINE_VERSION = '3.3'


def nope(msg):
    return {'allowed': False, 'message': msg}


def archify(arch):
    return {
        'arm': 'armhf',
        'x86_64': 'amd64',
    }[arch]


def auth(hosting_id, *keys):

    if len(keys) < 1:
        return nope('no ssh key')

    granted = False
    web_hosting_int = web_hosting.WebHosting(hosting_id)
    for key in keys:

        try:
            if web_hosting_int.is_valid_ssh_key(key) == True:
                granted = True
                break
        except Exception as e:
            logging.error(e)
            return nope('http error')

    if not granted:
        return nope('access denied')

    compute_int = compute.Compute()
    try:
        server = compute_int.get_server_by_webid(hosting_id)
        logging.debug(pp.pformat(server))
    except Exception as e:
        logging.error(e)
        return nope('error while trying to resolve server')

    return {

        'allowed': True,

        'remote-user': hosting_id,

        'image-name': 'local_web/alpine:{}-{}'.format(archify(server['arch']), ALPINE_VERSION),

        'docker-run-args': [
            '--name', 'ssh2docker_{}'.format(hosting_id),
            '--hostname', server['name'],
            '--rm',
            '-it',
            '-v', '/storage/users/{}/ftp:/ftp:rw'.format(hosting_id),
            '-v', '/storage/users/{}/backups:/ftp/backups:ro'.format(hosting_id),
            '-v', '/storage/users/{}/logs:/ftp/logs:ro'.format(hosting_id),
            '-v', '/storage/users/{}/websites:/ftp/websites:rw'.format(hosting_id),
            '-m', '256m',
            '--cpu-shares', '512',  # default = 1024, so ssh2docker gets half quota
            '-u', 'webuser',
        ],

        'env': {
            'DOCKER_HOST': 'tcp://{}.local:2376'.format(server['id']),
            'DOCKER_TLS_VERIFY': '1',
            'DOCKER_CERT_PATH': '/opt/docker-tls/{}/.docker/'.format(server['hostname']),
        },

        'command': ['/bin/sh', '-i', '-l'],

    }


print(json.dumps(auth(*sys.argv[1:])))

```"
b025383bb5d77dd8ca9af8fd77bc2a362e64ba51,scripts/fix_names.py,scripts/fix_names.py,,"#!/usr/bin/env python3

"""""" Fix names in auth_users.

    Usage: ./fix_names data/site/epcon.db

    Uses nameparser package to do the hard work of splitting names into
    first and last name.
 
    Written by Marc-Andre Lemburg, 2019.
 
""""""
import sqlite3
import sys
import nameparser

###

def connect(file):

    db = sqlite3.connect(file)
    db.row_factory = sqlite3.Row
    return db

def get_users(db):

    c = db.cursor()
    c.execute('select * from auth_user')
    return c.fetchall()

def fix_names(users):

    """""" Fix names in user records.
    
        Yields records (first_name, last_name, id).
    
    """"""
    for user in users:
        id = user['id']
        first_name = user['first_name'].strip()
        last_name = user['last_name'].strip()
        if not first_name and not last_name:
            # Empty name: skip
            print (f'Skipping empty name in record {id}')
            continue
        elif first_name == last_name:
            full_name = first_name
        elif first_name.endswith(last_name):
            full_name = first_name
        elif not last_name:
            full_name = first_name
        elif not first_name:
            full_name = last_name
        else:
            # In this case, the user has most likely entered the name
            # correctly split, so skip
            full_name = first_name + last_name
            print (f'Skipping already split name: {first_name} / {last_name} ({id})')
            continue
            
        print (f'Working on ""{full_name}"" ({id})')

        # Handle email addresses
        if '@' in full_name:
            print (f' - fixing email address')
            # Remove domain part
            e_name = full_name[:full_name.find('@')]
            if '+' in e_name:
                # Remove alias
                e_name = e_name[:e_name.find('+')]
            # Try to split name parts
            e_name = e_name.replace('.', ' ')
            e_name = e_name.replace('_', ' ')
            e_name = e_name.strip()
            if len(e_name) < 4:
                # Probably just initials: leave email as is
                pass
            else:
                full_name = e_name
        
        # Parse name
        name = nameparser.HumanName(full_name)
        name.capitalize()
        first_name = name.first
        last_name = name.last
        print (f' - splitting name into: {first_name} / {last_name} ({id})')
        yield (first_name, last_name, id)

def update_users(db, user_data):

    c = db.cursor()
    c.executemany('update auth_user set first_name=?, last_name=? where id=?', 
                  user_data)
    db.commit()

###

if __name__ == '__main__':
    db = connect(sys.argv[1])
    users = get_users(db)
    user_data = fix_names(users)
    update_users(db, user_data)
",Add helper script to fix names in auth_user.,"Add helper script to fix names in auth_user.
",Python,bsd-2-clause,"EuroPython/epcon,EuroPython/epcon,EuroPython/epcon,EuroPython/epcon",103,"```python
#!/usr/bin/env python3

"""""" Fix names in auth_users.

    Usage: ./fix_names data/site/epcon.db

    Uses nameparser package to do the hard work of splitting names into
    first and last name.
 
    Written by Marc-Andre Lemburg, 2019.
 
""""""
import sqlite3
import sys
import nameparser

###

def connect(file):

    db = sqlite3.connect(file)
    db.row_factory = sqlite3.Row
    return db

def get_users(db):

    c = db.cursor()
    c.execute('select * from auth_user')
    return c.fetchall()

def fix_names(users):

    """""" Fix names in user records.
    
        Yields records (first_name, last_name, id).
    
    """"""
    for user in users:
        id = user['id']
        first_name = user['first_name'].strip()
        last_name = user['last_name'].strip()
        if not first_name and not last_name:
            # Empty name: skip
            print (f'Skipping empty name in record {id}')
            continue
        elif first_name == last_name:
            full_name = first_name
        elif first_name.endswith(last_name):
            full_name = first_name
        elif not last_name:
            full_name = first_name
        elif not first_name:
            full_name = last_name
        else:
            # In this case, the user has most likely entered the name
            # correctly split, so skip
            full_name = first_name + last_name
            print (f'Skipping already split name: {first_name} / {last_name} ({id})')
            continue
            
        print (f'Working on ""{full_name}"" ({id})')

        # Handle email addresses
        if '@' in full_name:
            print (f' - fixing email address')
            # Remove domain part
            e_name = full_name[:full_name.find('@')]
            if '+' in e_name:
                # Remove alias
                e_name = e_name[:e_name.find('+')]
            # Try to split name parts
            e_name = e_name.replace('.', ' ')
            e_name = e_name.replace('_', ' ')
            e_name = e_name.strip()
            if len(e_name) < 4:
                # Probably just initials: leave email as is
                pass
            else:
                full_name = e_name
        
        # Parse name
        name = nameparser.HumanName(full_name)
        name.capitalize()
        first_name = name.first
        last_name = name.last
        print (f' - splitting name into: {first_name} / {last_name} ({id})')
        yield (first_name, last_name, id)

def update_users(db, user_data):

    c = db.cursor()
    c.executemany('update auth_user set first_name=?, last_name=? where id=?', 
                  user_data)
    db.commit()

###

if __name__ == '__main__':
    db = connect(sys.argv[1])
    users = get_users(db)
    user_data = fix_names(users)
    update_users(db, user_data)

```"
b208beafa5e060b4c2effb946f6dfda94aee8423,load_data.py,load_data.py,,"""""""
Creates a nice tidy pickle file of the data in the data/ directory.
""""""

import os
import csv
from collections import defaultdict

class Position:
    """"""
    A position for fantasy football.
    """"""
    def __init__(self, title, players=[]):
        if title in [QB, RB, WR, TE, DEF, ST, K]:
            self.title = title
        else:
            raise Exception(""Position name not valid: %s"" % name)

        # a dictionary keyed on player name for quick lookups
        self.players = {}
        for player in players:
            self.players[player.name] = player



class Player:
    """"""A player/squad""""""
    def __init__(self, name, position):
        self.name = name
        self.position = position
        self.stat_categories = []
        self.seasons = defaultdict(dict)

    # stats is a dictionary keyed on the name of the stat, with a
    # value that can be converted to a float
    def add_season(self, year, stats):
        if self.stat_categories == []:
            for key in stats.iterkeys():
                key = self.clean_stat_name(key)
                if key:
                    self.stat_categories.append(key)

        for key, val in stats.iteritems():
            key = self.clean_stat_name(key)
            if key and self.stat_categories and key not in self.stat_categories:
                raise Exception(""Stat '%s' not in existing categories: %s"" % \
                                (key ,str(self.stat_categories)))
            try:
                val = float(val)
            except:
                pass # if we can't float it, it's probably text or something


            self.seasons[year][key] = val

    def clean_stat_name(self, stat_name):
        """""" for dealing with unruly headers """"""
        stat_name = stat_name.strip()
        mapping = {'Rec Tgt':'Targets',
                   'Tgt':'Targets',
                   'KR Lng': 'KR Long',
        }

        if self.position == 'QB':
            mapping['YdsL'] = 'Sack Yds'

        for key, val in mapping.iteritems():
            if stat_name == key:
                stat_name = val

        if stat_name:
            return stat_name
        else:
            return False

if __name__ == ""__main__"":

    data_root = ""./data/""

    for subdir, dirs, files in os.walk(data_root):
        if not dirs:
            year = subdir.split('/')[-1]
            for filename in files:
                if filename.split('.')[-1].lower() == 'csv':
                    position = filename.split('.')[0].upper()
                    with open(os.path.join(subdir,filename),'rU+') as csvfile:
                        reader=csv.DictReader(csvfile)
                        for obj in reader:
                            try:
                                p = Player(obj[""Name""], position)
                                p.add_season(year, obj)
                            except KeyError:
                                p = Player(obj[""Team""], position)
                                p.add_season(year, obj)
                            a, b = p.position, p.stat_categories
                            b.sort()
                            print a, b
",Add data loader script with a couple classes. Not finished yet...,"Add data loader script with a couple classes. Not finished yet...
",Python,mit,"bjlange/revenge,bjlange/revenge",98,"```python
""""""
Creates a nice tidy pickle file of the data in the data/ directory.
""""""

import os
import csv
from collections import defaultdict

class Position:
    """"""
    A position for fantasy football.
    """"""
    def __init__(self, title, players=[]):
        if title in [QB, RB, WR, TE, DEF, ST, K]:
            self.title = title
        else:
            raise Exception(""Position name not valid: %s"" % name)

        # a dictionary keyed on player name for quick lookups
        self.players = {}
        for player in players:
            self.players[player.name] = player



class Player:
    """"""A player/squad""""""
    def __init__(self, name, position):
        self.name = name
        self.position = position
        self.stat_categories = []
        self.seasons = defaultdict(dict)

    # stats is a dictionary keyed on the name of the stat, with a
    # value that can be converted to a float
    def add_season(self, year, stats):
        if self.stat_categories == []:
            for key in stats.iterkeys():
                key = self.clean_stat_name(key)
                if key:
                    self.stat_categories.append(key)

        for key, val in stats.iteritems():
            key = self.clean_stat_name(key)
            if key and self.stat_categories and key not in self.stat_categories:
                raise Exception(""Stat '%s' not in existing categories: %s"" % \
                                (key ,str(self.stat_categories)))
            try:
                val = float(val)
            except:
                pass # if we can't float it, it's probably text or something


            self.seasons[year][key] = val

    def clean_stat_name(self, stat_name):
        """""" for dealing with unruly headers """"""
        stat_name = stat_name.strip()
        mapping = {'Rec Tgt':'Targets',
                   'Tgt':'Targets',
                   'KR Lng': 'KR Long',
        }

        if self.position == 'QB':
            mapping['YdsL'] = 'Sack Yds'

        for key, val in mapping.iteritems():
            if stat_name == key:
                stat_name = val

        if stat_name:
            return stat_name
        else:
            return False

if __name__ == ""__main__"":

    data_root = ""./data/""

    for subdir, dirs, files in os.walk(data_root):
        if not dirs:
            year = subdir.split('/')[-1]
            for filename in files:
                if filename.split('.')[-1].lower() == 'csv':
                    position = filename.split('.')[0].upper()
                    with open(os.path.join(subdir,filename),'rU+') as csvfile:
                        reader=csv.DictReader(csvfile)
                        for obj in reader:
                            try:
                                p = Player(obj[""Name""], position)
                                p.add_season(year, obj)
                            except KeyError:
                                p = Player(obj[""Team""], position)
                                p.add_season(year, obj)
                            a, b = p.position, p.stat_categories
                            b.sort()
                            print a, b

```"
0323189a504f27f14d60c8c3ebdb40ea160d7f79,source/clique/collection.py,source/clique/collection.py,,"# :coding: utf-8
# :copyright: Copyright (c) 2013 Martin Pengelly-Phillips
# :license: See LICENSE.txt.

import re


class Collection(object):
    '''Represent group of items that differ only by numerical component.'''

    def __init__(self, head, tail, padding, indexes=None):
        '''Initialise collection.

        *head* is the leading common part whilst *tail* is the trailing
        common part.

        *padding* specifies the ""width"" of the numerical component. An index
        will be padded with zeros to fill this width. A *padding* of zero
        implies no padding and width may be any size so long as no leading
        zeros are present.

        *indexes* can specify a set of numerical indexes to initially populate
        the collection with.

        '''
        super(Collection, self).__init__()
        self.head = head
        self.tail = tail
        self.padding = padding
        self.indexes = set()
        if indexes is not None:
            self.indexes.update(indexes)

        self._pattern = re.compile('^{0}(?P<index>(?P<padding>0*)\d+?){1}$'
                                   .format(self.head, self.tail))

    def __iter__(self):
        '''Return iterator over items in collection.'''

    def __contains__(self, item):
        '''Return whether *item* is present in collection.'''

    def match(self, item):
        '''Return whether *item* matches this collection pattern.

        If a match is successful return data about the match otherwise return
        None.

        '''

    def add(self, item):
        '''Add *item* to collection.

        raise :py:class:`~clique.error.CollectionError` if *item* cannot be
        added to the collection.

        '''

    def remove(self, item):
        '''Remove *item* from collection.

        raise :py:class:`~clique.error.CollectionError` if *item* cannot be
        removed from the collection.

        '''

    def format(self, pattern):
        '''Return string representation as specified by *pattern*.'''

    def is_contiguous(self):
        '''Return whether entire collection is contiguous.'''

    def holes(self):
        '''Return holes in collection.

        Return as list of :py:class:`~clique.collection.Collection` instances.

        '''

    def merge(self, collection):
        '''Merge *collection* into this collection.

        If the *collection* is compatible with this collection then update
        indexes with all indexes in *collection*.

        '''

    def separate(self):
        '''Return contiguous parts of collection as separate collections.

        Return as list of :py:class:`~clique.collection.Collection` instances.

        '''
",Add initial interface for Collection class with stubs for methods.,"Add initial interface for Collection class with stubs for methods.

A Collection will represent a group of items with a common numerical
component.
",Python,apache-2.0,4degrees/clique,94,"```python
# :coding: utf-8
# :copyright: Copyright (c) 2013 Martin Pengelly-Phillips
# :license: See LICENSE.txt.

import re


class Collection(object):
    '''Represent group of items that differ only by numerical component.'''

    def __init__(self, head, tail, padding, indexes=None):
        '''Initialise collection.

        *head* is the leading common part whilst *tail* is the trailing
        common part.

        *padding* specifies the ""width"" of the numerical component. An index
        will be padded with zeros to fill this width. A *padding* of zero
        implies no padding and width may be any size so long as no leading
        zeros are present.

        *indexes* can specify a set of numerical indexes to initially populate
        the collection with.

        '''
        super(Collection, self).__init__()
        self.head = head
        self.tail = tail
        self.padding = padding
        self.indexes = set()
        if indexes is not None:
            self.indexes.update(indexes)

        self._pattern = re.compile('^{0}(?P<index>(?P<padding>0*)\d+?){1}$'
                                   .format(self.head, self.tail))

    def __iter__(self):
        '''Return iterator over items in collection.'''

    def __contains__(self, item):
        '''Return whether *item* is present in collection.'''

    def match(self, item):
        '''Return whether *item* matches this collection pattern.

        If a match is successful return data about the match otherwise return
        None.

        '''

    def add(self, item):
        '''Add *item* to collection.

        raise :py:class:`~clique.error.CollectionError` if *item* cannot be
        added to the collection.

        '''

    def remove(self, item):
        '''Remove *item* from collection.

        raise :py:class:`~clique.error.CollectionError` if *item* cannot be
        removed from the collection.

        '''

    def format(self, pattern):
        '''Return string representation as specified by *pattern*.'''

    def is_contiguous(self):
        '''Return whether entire collection is contiguous.'''

    def holes(self):
        '''Return holes in collection.

        Return as list of :py:class:`~clique.collection.Collection` instances.

        '''

    def merge(self, collection):
        '''Merge *collection* into this collection.

        If the *collection* is compatible with this collection then update
        indexes with all indexes in *collection*.

        '''

    def separate(self):
        '''Return contiguous parts of collection as separate collections.

        Return as list of :py:class:`~clique.collection.Collection` instances.

        '''

```"
4c22b9529b9a7ac13c50bbac3ca81e450297b998,maintainers/scripts/hydra-eval-failures.py,maintainers/scripts/hydra-eval-failures.py,,"#!/usr/bin/env nix-shell
#!nix-shell -i python -p pythonFull pythonPackages.requests pythonPackages.pyquery pythonPackages.click

# To use, just execute this script with --help to display help.

import subprocess
import json

import click
import requests
from pyquery import PyQuery as pq


maintainers_json = subprocess.check_output([
    'nix-instantiate',
    'lib/maintainers.nix',
    '--eval',
    '--json'])
maintainers = json.loads(maintainers_json)
MAINTAINERS = {v: k for k, v in maintainers.iteritems()}


def get_response_text(url):
    return pq(requests.get(url).text)  # IO

EVAL_FILE = {
    'nixos': 'nixos/release.nix',
    'nixpkgs': 'pkgs/top-level/release.nix',
}


def get_maintainers(attr_name):
    nixname = attr_name.split('.')
    meta_json = subprocess.check_output([
        'nix-instantiate',
        '--eval',
        '--strict',
        '-A',
        '.'.join(nixname[1:]) + '.meta',
        EVAL_FILE[nixname[0]],
        '--json'])
    meta = json.loads(meta_json)
    if meta.get('maintainers'):
        return [MAINTAINERS[name] for name in meta['maintainers'] if MAINTAINERS.get(name)]


@click.command()
@click.option(
    '--jobset',
    default=""nixos/release-16.09"",
    help='Hydra project like nixos/release-16.09')
def cli(jobset):
    """"""
    Given a Hydra project, inspect latest evaluation
    and print a summary of failed builds
    """"""

    url = ""http://hydra.nixos.org/jobset/{}"".format(jobset)

    # get the last evaluation
    click.echo(click.style(
        'Getting latest evaluation for {}'.format(url), fg='green'))
    d = get_response_text(url)
    evaluations = d('#tabs-evaluations').find('a[class=""row-link""]')
    latest_eval_url = evaluations[0].get('href')

    # parse last evaluation page
    click.echo(click.style(
        'Parsing evaluation {}'.format(latest_eval_url), fg='green'))
    d = get_response_text(latest_eval_url + '?full=1')

    # TODO: aborted evaluations
    # TODO: dependency failed without propagated builds
    for tr in d('img[alt=""Failed""]').parents('tr'):
        a = pq(tr)('a')[1]
        print ""- [ ] [{}]({})"".format(a.text, a.get('href'))

        maintainers = get_maintainers(a.text)
        if maintainers:
            print ""  - maintainers: {}"".format("", "".join(map(lambda u: '@' + u, maintainers)))
        # TODO: print last three persons that touched this file
        # TODO: pinpoint the diff that broke this build, or maybe it's transient or maybe it never worked?


if __name__ == ""__main__"":
    try:
        cli()
    except:
        import pdb;pdb.post_mortem()
",Add a script to get failures for hydra eval /cc @globin,"Add a script to get failures for hydra eval /cc @globin
",Python,mit,"NixOS/nixpkgs,NixOS/nixpkgs,NixOS/nixpkgs,SymbiFlow/nixpkgs,NixOS/nixpkgs,SymbiFlow/nixpkgs,SymbiFlow/nixpkgs,NixOS/nixpkgs,NixOS/nixpkgs,SymbiFlow/nixpkgs,SymbiFlow/nixpkgs,NixOS/nixpkgs,NixOS/nixpkgs,NixOS/nixpkgs,SymbiFlow/nixpkgs,SymbiFlow/nixpkgs,NixOS/nixpkgs,NixOS/nixpkgs,SymbiFlow/nixpkgs,SymbiFlow/nixpkgs,SymbiFlow/nixpkgs,SymbiFlow/nixpkgs,NixOS/nixpkgs,NixOS/nixpkgs,SymbiFlow/nixpkgs,NixOS/nixpkgs,NixOS/nixpkgs,SymbiFlow/nixpkgs",90,"```python
#!/usr/bin/env nix-shell
#!nix-shell -i python -p pythonFull pythonPackages.requests pythonPackages.pyquery pythonPackages.click

# To use, just execute this script with --help to display help.

import subprocess
import json

import click
import requests
from pyquery import PyQuery as pq


maintainers_json = subprocess.check_output([
    'nix-instantiate',
    'lib/maintainers.nix',
    '--eval',
    '--json'])
maintainers = json.loads(maintainers_json)
MAINTAINERS = {v: k for k, v in maintainers.iteritems()}


def get_response_text(url):
    return pq(requests.get(url).text)  # IO

EVAL_FILE = {
    'nixos': 'nixos/release.nix',
    'nixpkgs': 'pkgs/top-level/release.nix',
}


def get_maintainers(attr_name):
    nixname = attr_name.split('.')
    meta_json = subprocess.check_output([
        'nix-instantiate',
        '--eval',
        '--strict',
        '-A',
        '.'.join(nixname[1:]) + '.meta',
        EVAL_FILE[nixname[0]],
        '--json'])
    meta = json.loads(meta_json)
    if meta.get('maintainers'):
        return [MAINTAINERS[name] for name in meta['maintainers'] if MAINTAINERS.get(name)]


@click.command()
@click.option(
    '--jobset',
    default=""nixos/release-16.09"",
    help='Hydra project like nixos/release-16.09')
def cli(jobset):
    """"""
    Given a Hydra project, inspect latest evaluation
    and print a summary of failed builds
    """"""

    url = ""http://hydra.nixos.org/jobset/{}"".format(jobset)

    # get the last evaluation
    click.echo(click.style(
        'Getting latest evaluation for {}'.format(url), fg='green'))
    d = get_response_text(url)
    evaluations = d('#tabs-evaluations').find('a[class=""row-link""]')
    latest_eval_url = evaluations[0].get('href')

    # parse last evaluation page
    click.echo(click.style(
        'Parsing evaluation {}'.format(latest_eval_url), fg='green'))
    d = get_response_text(latest_eval_url + '?full=1')

    # TODO: aborted evaluations
    # TODO: dependency failed without propagated builds
    for tr in d('img[alt=""Failed""]').parents('tr'):
        a = pq(tr)('a')[1]
        print ""- [ ] [{}]({})"".format(a.text, a.get('href'))

        maintainers = get_maintainers(a.text)
        if maintainers:
            print ""  - maintainers: {}"".format("", "".join(map(lambda u: '@' + u, maintainers)))
        # TODO: print last three persons that touched this file
        # TODO: pinpoint the diff that broke this build, or maybe it's transient or maybe it never worked?


if __name__ == ""__main__"":
    try:
        cli()
    except:
        import pdb;pdb.post_mortem()

```"
9137431ef3d57363bbf6e9a5912d4ca5399c08c0,control/test/test_heading_filter.py,control/test/test_heading_filter.py,,"""""""Tests the heading Kalman Filter.""""""

import math
import numpy
import operator
import random
import unittest

from heading_filter import HeadingFilter

#pylint: disable=protected-access
#pylint: disable=too-many-public-methods


class TestHeadingFilter(unittest.TestCase):
    """"""Tests the heading Kalman filter.""""""

    def test_multiply(self):
        """"""Test the matrix multiply method.""""""
        with self.assertRaises(TypeError):
            HeadingFilter._multiply(0, 0)

        with self.assertRaises(ValueError):
            HeadingFilter._multiply(
                [[1, 1]],
                [[1, 1]]
            )

        with self.assertRaises(ValueError):
            HeadingFilter._multiply(
                [[1, 1]],
                [[1, 1], [1, 1], [1, 1]]
            )

        self.assertEqual(
            HeadingFilter._multiply(
                [[1, 2]],
                [[2, 3],
                 [5, 8]]
            ),
            [[2 + 10, 3 + 16]]
        )

        self.assertEqual(
            HeadingFilter._multiply(
                [[1, 2, 4],
                 [3, 7, 8]],
                [[2, 0, 1, 4, 6],
                 [1, 1, 1, 1, 1],
                 [5, 3, 8, 9, 7]]
            ),
            [[24, 14, 35, 42, 36],
             [53, 31, 74, 91, 81]]
        )

    def test_add(self):
        """"""test the matrix addition method.""""""
        with self.assertRaises(TypeError):
            HeadingFilter._add(0, 0)

        with self.assertRaises(ValueError):
            HeadingFilter._add(
                [[1, 1, 1]],
                [[1, 1]]
            )

        with self.assertRaises(ValueError):
            HeadingFilter._add(
                [[1, 1]],
                [[1, 1], [1, 1], [1, 1]]
            )

        self.assertEqual(
            HeadingFilter._add(
                [[1, 2]],
                [[3, 0]],
            ),
            [[4, 2]]
        )

        self.assertEqual(
            HeadingFilter._add(
                [[1, 2],
                 [3, 0]],
                [[3, 0],
                 [4, 1]]
            ),
            [[4, 2],
             [7, 1]]
        )

    def test_inverse(self):
        foo = [[2, 3],
               [1, 4]]
        identity = [[1, 0],
                    [0, 1]]
        print(HeadingFilter._inverse(foo))
        self.assertEqual(
            HeadingFilter._multiply(
                foo,
                HeadingFilter._inverse(foo)
            ),
            identity
        )
        self.assertEqual(
            HeadingFilter._multiply(
                HeadingFilter._inverse(foo),
                foo
            ),
            identity
        )
",Add Kalman filter for heading,"Add Kalman filter for heading
",Python,mit,"bskari/sparkfun-avc,bskari/sparkfun-avc,bskari/sparkfun-avc,bskari/sparkfun-avc,bskari/sparkfun-avc,bskari/sparkfun-avc",112,"```python
""""""Tests the heading Kalman Filter.""""""

import math
import numpy
import operator
import random
import unittest

from heading_filter import HeadingFilter

#pylint: disable=protected-access
#pylint: disable=too-many-public-methods


class TestHeadingFilter(unittest.TestCase):
    """"""Tests the heading Kalman filter.""""""

    def test_multiply(self):
        """"""Test the matrix multiply method.""""""
        with self.assertRaises(TypeError):
            HeadingFilter._multiply(0, 0)

        with self.assertRaises(ValueError):
            HeadingFilter._multiply(
                [[1, 1]],
                [[1, 1]]
            )

        with self.assertRaises(ValueError):
            HeadingFilter._multiply(
                [[1, 1]],
                [[1, 1], [1, 1], [1, 1]]
            )

        self.assertEqual(
            HeadingFilter._multiply(
                [[1, 2]],
                [[2, 3],
                 [5, 8]]
            ),
            [[2 + 10, 3 + 16]]
        )

        self.assertEqual(
            HeadingFilter._multiply(
                [[1, 2, 4],
                 [3, 7, 8]],
                [[2, 0, 1, 4, 6],
                 [1, 1, 1, 1, 1],
                 [5, 3, 8, 9, 7]]
            ),
            [[24, 14, 35, 42, 36],
             [53, 31, 74, 91, 81]]
        )

    def test_add(self):
        """"""test the matrix addition method.""""""
        with self.assertRaises(TypeError):
            HeadingFilter._add(0, 0)

        with self.assertRaises(ValueError):
            HeadingFilter._add(
                [[1, 1, 1]],
                [[1, 1]]
            )

        with self.assertRaises(ValueError):
            HeadingFilter._add(
                [[1, 1]],
                [[1, 1], [1, 1], [1, 1]]
            )

        self.assertEqual(
            HeadingFilter._add(
                [[1, 2]],
                [[3, 0]],
            ),
            [[4, 2]]
        )

        self.assertEqual(
            HeadingFilter._add(
                [[1, 2],
                 [3, 0]],
                [[3, 0],
                 [4, 1]]
            ),
            [[4, 2],
             [7, 1]]
        )

    def test_inverse(self):
        foo = [[2, 3],
               [1, 4]]
        identity = [[1, 0],
                    [0, 1]]
        print(HeadingFilter._inverse(foo))
        self.assertEqual(
            HeadingFilter._multiply(
                foo,
                HeadingFilter._inverse(foo)
            ),
            identity
        )
        self.assertEqual(
            HeadingFilter._multiply(
                HeadingFilter._inverse(foo),
                foo
            ),
            identity
        )

```"
1d77b6645e28f1e614502f6bd9bb5458383ecdcf,jacquard/tests/test_config.py,jacquard/tests/test_config.py,,"import io
import sys
import tempfile
import textwrap

from jacquard.config import load_config

CONFIG_FILE = """"""
[storage]
engine = dummy
url = dummy

[directory]
engine = dummy

[test_section]
test_key = test_value
""""""


def load_test_config(extra=''):
    f = io.StringIO(CONFIG_FILE + textwrap.dedent(extra))
    return load_config(f)


def test_load_config_smoke():
    load_test_config()


def test_load_config_from_file():
    with tempfile.NamedTemporaryFile('w') as f:
        f.write(CONFIG_FILE)
        f.flush()
        load_config(f.name)


def test_config_creates_storage_engine():
    config = load_test_config()

    with config.storage.transaction() as store:
        store['bees'] = 'pony'

    with config.storage.transaction() as store:
        assert store['bees'] == 'pony'


def test_config_creates_directory():
    config = load_test_config()

    assert list(config.directory.all_users()) == []


def test_config_can_iterate_over_sections():
    config = load_test_config()

    assert set(config) == {'storage', 'directory', 'test_section', 'DEFAULT'}


def test_config_can_query_subsections():
    config = load_test_config()

    assert config['test_section']['test_key'] == 'test_value'


def test_config_can_test_section_inclusion():
    config = load_test_config()

    assert 'test_section' in config
    assert 'test_section2' not in config


def test_config_section_len():
    config = load_test_config()

    assert len(config) == 4


def test_adds_extra_elements_to_path():
    try:
        sys.path.remove('/gravity')
    except ValueError:
        pass

    load_test_config(""""""
    [paths]
    a_path = /gravity
    """""")

    assert '/gravity' in sys.path
",Add tests for config loading,"Add tests for config loading
",Python,mit,"prophile/jacquard,prophile/jacquard",90,"```python
import io
import sys
import tempfile
import textwrap

from jacquard.config import load_config

CONFIG_FILE = """"""
[storage]
engine = dummy
url = dummy

[directory]
engine = dummy

[test_section]
test_key = test_value
""""""


def load_test_config(extra=''):
    f = io.StringIO(CONFIG_FILE + textwrap.dedent(extra))
    return load_config(f)


def test_load_config_smoke():
    load_test_config()


def test_load_config_from_file():
    with tempfile.NamedTemporaryFile('w') as f:
        f.write(CONFIG_FILE)
        f.flush()
        load_config(f.name)


def test_config_creates_storage_engine():
    config = load_test_config()

    with config.storage.transaction() as store:
        store['bees'] = 'pony'

    with config.storage.transaction() as store:
        assert store['bees'] == 'pony'


def test_config_creates_directory():
    config = load_test_config()

    assert list(config.directory.all_users()) == []


def test_config_can_iterate_over_sections():
    config = load_test_config()

    assert set(config) == {'storage', 'directory', 'test_section', 'DEFAULT'}


def test_config_can_query_subsections():
    config = load_test_config()

    assert config['test_section']['test_key'] == 'test_value'


def test_config_can_test_section_inclusion():
    config = load_test_config()

    assert 'test_section' in config
    assert 'test_section2' not in config


def test_config_section_len():
    config = load_test_config()

    assert len(config) == 4


def test_adds_extra_elements_to_path():
    try:
        sys.path.remove('/gravity')
    except ValueError:
        pass

    load_test_config(""""""
    [paths]
    a_path = /gravity
    """""")

    assert '/gravity' in sys.path

```"
c81fcc0a3604c59cbebcbc9618129b1f6b6007c7,example-scripts/voctolight/voctolight_async_io.py,example-scripts/voctolight/voctolight_async_io.py,,"import asyncio
from enum import Enum

class Connection(object):
  def __init__(self, interpreter):
    self.interpreter = interpreter 
    self.loop = asyncio.get_event_loop()

  def __del__(self):
    self.loop.close()

  def schedule(self, message):
    self.loop.create_task(self.connection_future(message, self.interpreter.handler))
   
  def set_host(self, host, port = '9999'):
    self.host = host
    self.port = port

  ### FIXME This logic is wrong. we must send requests, and independently wait for
  ### answers. Otherwise we will never receive answers to requests that we haven't
  ### asked for.
  @asyncio.coroutine
  def connection_future(connection, message, handler):
    reader, writer = yield from asyncio.open_connection(connection.host,
                                                        connection.port,
                                                        loop=connection.loop)
    print('Sent: %r' % message)
    writer.write(message.encode())
    writer.write('\n'.encode())
    data = yield from reader.readline()
    handler(message, data.decode().rstrip('\n'))
    writer.close()

### FIXME Duplicate from videomix.py
class CompositeModes(Enum):
    fullscreen = 0
    side_by_side_equal = 1
    side_by_side_preview = 2
    picture_in_picture = 3

class Interpreter(object):
  def __init__(self, actor):
    self.actor = actor
    self.a_or_b = False
    self.composite_mode = CompositeModes.fullscreen
    actor.reset_led()

  def compute_state(self):
    if self.composite_mode == CompositeModes.fullscreen:
      actor.enable_tally(self.a_or_b and self.primary)
    else:   
      actor.enable_tally(self.a_or_b)

  def handler(self, message, response):
    print(""got "" + response + "" for "" + message)
    words = response.split()
    signal = words[0]
    args = words[1:]
    self.__getattribute__(""handle_""+signal)(args)
    interpreter.compute_state()

  def handle_video_status(self, cams):
  ### FIXME DO NOT HARDCODE CAM NAME, READ FROM CONFIG!
    if ""cam2"" in cams:
      self.a_or_b = True
    else:
      self.a_or_b = False

    self.primary = (cams[0] == ""cam2"")
      

  def handle_composite_mode(self, mode):
    self.composite_mode = mode

class FakeLedActor:
  def __init__(self):
    pass

  def reset_led(self):
    print(""LED has been reset to off"")

  def enable_tally(self, enable):
    if enable == True:
      print(""tally on!"")
    else:
      print(""tally off!"")

if __name__ == ""__main__"":
    actor = FakeLedActor()
    interpreter = Interpreter(actor)
    conn = Connection(interpreter)
    conn.set_host(""10.73.23.3"")
    conn.schedule(""get_video"")
    conn.schedule(""get_composite_mode"")
    conn.loop.run_forever()
    conn.wait_closed()
",Add prelim. async io version,"Add prelim. async io version
",Python,mit,"voc/voctomix,voc/voctomix,h01ger/voctomix,h01ger/voctomix",97,"```python
import asyncio
from enum import Enum

class Connection(object):
  def __init__(self, interpreter):
    self.interpreter = interpreter 
    self.loop = asyncio.get_event_loop()

  def __del__(self):
    self.loop.close()

  def schedule(self, message):
    self.loop.create_task(self.connection_future(message, self.interpreter.handler))
   
  def set_host(self, host, port = '9999'):
    self.host = host
    self.port = port

  ### FIXME This logic is wrong. we must send requests, and independently wait for
  ### answers. Otherwise we will never receive answers to requests that we haven't
  ### asked for.
  @asyncio.coroutine
  def connection_future(connection, message, handler):
    reader, writer = yield from asyncio.open_connection(connection.host,
                                                        connection.port,
                                                        loop=connection.loop)
    print('Sent: %r' % message)
    writer.write(message.encode())
    writer.write('\n'.encode())
    data = yield from reader.readline()
    handler(message, data.decode().rstrip('\n'))
    writer.close()

### FIXME Duplicate from videomix.py
class CompositeModes(Enum):
    fullscreen = 0
    side_by_side_equal = 1
    side_by_side_preview = 2
    picture_in_picture = 3

class Interpreter(object):
  def __init__(self, actor):
    self.actor = actor
    self.a_or_b = False
    self.composite_mode = CompositeModes.fullscreen
    actor.reset_led()

  def compute_state(self):
    if self.composite_mode == CompositeModes.fullscreen:
      actor.enable_tally(self.a_or_b and self.primary)
    else:   
      actor.enable_tally(self.a_or_b)

  def handler(self, message, response):
    print(""got "" + response + "" for "" + message)
    words = response.split()
    signal = words[0]
    args = words[1:]
    self.__getattribute__(""handle_""+signal)(args)
    interpreter.compute_state()

  def handle_video_status(self, cams):
  ### FIXME DO NOT HARDCODE CAM NAME, READ FROM CONFIG!
    if ""cam2"" in cams:
      self.a_or_b = True
    else:
      self.a_or_b = False

    self.primary = (cams[0] == ""cam2"")
      

  def handle_composite_mode(self, mode):
    self.composite_mode = mode

class FakeLedActor:
  def __init__(self):
    pass

  def reset_led(self):
    print(""LED has been reset to off"")

  def enable_tally(self, enable):
    if enable == True:
      print(""tally on!"")
    else:
      print(""tally off!"")

if __name__ == ""__main__"":
    actor = FakeLedActor()
    interpreter = Interpreter(actor)
    conn = Connection(interpreter)
    conn.set_host(""10.73.23.3"")
    conn.schedule(""get_video"")
    conn.schedule(""get_composite_mode"")
    conn.loop.run_forever()
    conn.wait_closed()

```"
3b0ed9b42b23a18ead0f07f221cafe89f9c8463e,tools/refract-filter.py,tools/refract-filter.py,,"#!/usr/bin/env python

from __future__ import print_function
import argparse
import sys
import json
import textwrap
from collections import OrderedDict
try:
    import yaml
except ImportError:
    yaml = None

VERSION = ""0.1""


def yaml_load(stream, Loader=yaml.Loader, object_pairs_hook=OrderedDict):
    class OrderedLoader(Loader):
        pass

    def construct_mapping(loader, node):
        loader.flatten_mapping(node)
        return object_pairs_hook(loader.construct_pairs(node))

    OrderedLoader.add_constructor(
        yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,
        construct_mapping)
    return yaml.load(stream, OrderedLoader)


def walk(node, cb):

    if isinstance(node, dict):
        for key, item in node.items():
            cb(key, item)
            walk(item, cb)
    elif type(node) is list:
        for item in iter(node):
            cb(None, item)
            walk(item, cb)


def print_body(key, item):

    if type(item) is OrderedDict and \
       'element' in item.keys() and \
       item['element'] == 'asset':
        print(item['content'])


def main():
    parser = argparse.ArgumentParser(
        description=textwrap.dedent('''\
    Simple filter for refract, prints out the json and JSONSchema content
    of the datastrucutres. Input is either stdin or given files.'''),
        epilog=textwrap.dedent('''\
    Example:
        refract-filter.py -vj test/fixtures/schema/*.json
        drafter blueprint.apib | refract-filter.py'''),
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-j', '--json',
                        help='input is json format and not yaml',
                        action='store_true', default=False)
    parser.add_argument('-V', '--version', help='print version info',
                        action='store_true', default=False)
    parser.add_argument('-v', '--verbose', help='verbose (print file names)',
                        action='store_true', default=False)
    parser.add_argument('file', type=argparse.FileType('r'), nargs='*')
    args = parser.parse_args()

    if yaml is None:
        args.json = True
        if args.verbose:
            print(""Pyaml not found, only json format supported, -j in effect"",
                  file=sys.stderr)

    if args.version:
        print(VERSION + "" refract-filter.py"")
        sys.exit(0)

    if not args.file:
        args.file.append(sys.stdin)

    for f in args.file:
        if args.verbose:
            print(f.name, file=sys.stderr)
        if args.json:
            data = json.load(f, object_pairs_hook=OrderedDict)
        else:
            data = yaml_load(f)

        walk(data, print_body)


if __name__ == '__main__':
    main()
",Add tool for json and JSONschema pretty printing,"Add tool for json and JSONschema pretty printing
",Python,mit,"apiaryio/drafter,apiaryio/drafter,apiaryio/drafter,apiaryio/drafter,apiaryio/drafter",97,"```python
#!/usr/bin/env python

from __future__ import print_function
import argparse
import sys
import json
import textwrap
from collections import OrderedDict
try:
    import yaml
except ImportError:
    yaml = None

VERSION = ""0.1""


def yaml_load(stream, Loader=yaml.Loader, object_pairs_hook=OrderedDict):
    class OrderedLoader(Loader):
        pass

    def construct_mapping(loader, node):
        loader.flatten_mapping(node)
        return object_pairs_hook(loader.construct_pairs(node))

    OrderedLoader.add_constructor(
        yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG,
        construct_mapping)
    return yaml.load(stream, OrderedLoader)


def walk(node, cb):

    if isinstance(node, dict):
        for key, item in node.items():
            cb(key, item)
            walk(item, cb)
    elif type(node) is list:
        for item in iter(node):
            cb(None, item)
            walk(item, cb)


def print_body(key, item):

    if type(item) is OrderedDict and \
       'element' in item.keys() and \
       item['element'] == 'asset':
        print(item['content'])


def main():
    parser = argparse.ArgumentParser(
        description=textwrap.dedent('''\
    Simple filter for refract, prints out the json and JSONSchema content
    of the datastrucutres. Input is either stdin or given files.'''),
        epilog=textwrap.dedent('''\
    Example:
        refract-filter.py -vj test/fixtures/schema/*.json
        drafter blueprint.apib | refract-filter.py'''),
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument('-j', '--json',
                        help='input is json format and not yaml',
                        action='store_true', default=False)
    parser.add_argument('-V', '--version', help='print version info',
                        action='store_true', default=False)
    parser.add_argument('-v', '--verbose', help='verbose (print file names)',
                        action='store_true', default=False)
    parser.add_argument('file', type=argparse.FileType('r'), nargs='*')
    args = parser.parse_args()

    if yaml is None:
        args.json = True
        if args.verbose:
            print(""Pyaml not found, only json format supported, -j in effect"",
                  file=sys.stderr)

    if args.version:
        print(VERSION + "" refract-filter.py"")
        sys.exit(0)

    if not args.file:
        args.file.append(sys.stdin)

    for f in args.file:
        if args.verbose:
            print(f.name, file=sys.stderr)
        if args.json:
            data = json.load(f, object_pairs_hook=OrderedDict)
        else:
            data = yaml_load(f)

        walk(data, print_body)


if __name__ == '__main__':
    main()

```"
55ea281c96228e46b8520f2aa7305116c21c6b20,mailchimp3/entities/campaignfolder.py,mailchimp3/entities/campaignfolder.py,,"# coding=utf-8
""""""
The Campaign Folders API endpoints

Documentation: http://developer.mailchimp.com/documentation/mailchimp/reference/campaign-folders/
Schema: https://api.mailchimp.com/schema/3.0/CampaignFolders/Instance.json
""""""
from __future__ import unicode_literals

from mailchimp3.baseapi import BaseApi


class CampaignFolder(BaseApi):
    """"""
    Organize your campaigns using folders.
    """"""
    def __init__(self, *args, **kwargs):
        """"""
        Initialize the endpoint
        """"""
        super(CampaignFolder, self).__init__(*args, **kwargs)
        self.endpoint = 'campaign-folders'
        self.folder_id = None


    def create(self, data):
        """"""
        Create a new campaign folder.

        :param data: The request body parameters
        :type data: :py:class:`dict`
        data = {
            ""name"": string*
        }
        """"""
        response = self._mc_client._post(url=self._build_path(), data=data)
        self.folder_id = response['id']
        return response


    def all(self, get_all=False, **queryparams):
        """"""
        Get all folders used to organize campaigns.

        :param get_all: Should the query get all results
        :type get_all: :py:class:`bool`
        :param queryparams: The query string parameters
        queryparams['fields'] = []
        queryparams['exclude_fields'] = []
        queryparams['count'] = integer
        queryparams['offset'] = integer
        """"""
        self.folder_id = None
        if get_all:
            return self._iterate(url=self._build_path(), **queryparams)
        else:
            return self._mc_client._get(url=self._build_path(), **queryparams)


    def get(self, folder_id, **queryparams):
        """"""
        Get information about a specific folder used to organize campaigns.

        :param folder_id: The unique id for the campaign folder.
        :type folder_id: :py:class:`str`
        :param queryparams: The query string parameters
        queryparams['fields'] = []
        queryparams['exclude_fields'] = []
        """"""
        self.folder_id = folder_id
        return self._mc_client._get(url=self._build_path(folder_id), **queryparams)


    def update(self, folder_id, data):
        """"""
        Update a specific folder used to organize campaigns.

        :param folder_id: The unique id for the campaign folder.
        :type folder_id: :py:class:`str`
        :param data: The request body parameters
        :type data: :py:class:`dict`
        data = {
            ""name"": string*
        }
        """"""
        self.folder_id = folder_id
        return self._mc_client._patch(url=self._build_path(folder_id), data=data)


    def delete(self, folder_id):
        """"""
        Delete a specific campaign folder, and mark all the campaigns in the
        folder as ‘unfiled’.

        :param folder_id: The unique id for the campaign folder.
        :type folder_id: :py:class:`str`
        """"""
        self.folder_id = folder_id
        return self._mc_client._delete(url=self._build_path(folder_id))
",Implement the Campaign Folders endpoint,"Implement the Campaign Folders endpoint
",Python,mit,charlesthk/python-mailchimp,100,"```python
# coding=utf-8
""""""
The Campaign Folders API endpoints

Documentation: http://developer.mailchimp.com/documentation/mailchimp/reference/campaign-folders/
Schema: https://api.mailchimp.com/schema/3.0/CampaignFolders/Instance.json
""""""
from __future__ import unicode_literals

from mailchimp3.baseapi import BaseApi


class CampaignFolder(BaseApi):
    """"""
    Organize your campaigns using folders.
    """"""
    def __init__(self, *args, **kwargs):
        """"""
        Initialize the endpoint
        """"""
        super(CampaignFolder, self).__init__(*args, **kwargs)
        self.endpoint = 'campaign-folders'
        self.folder_id = None


    def create(self, data):
        """"""
        Create a new campaign folder.

        :param data: The request body parameters
        :type data: :py:class:`dict`
        data = {
            ""name"": string*
        }
        """"""
        response = self._mc_client._post(url=self._build_path(), data=data)
        self.folder_id = response['id']
        return response


    def all(self, get_all=False, **queryparams):
        """"""
        Get all folders used to organize campaigns.

        :param get_all: Should the query get all results
        :type get_all: :py:class:`bool`
        :param queryparams: The query string parameters
        queryparams['fields'] = []
        queryparams['exclude_fields'] = []
        queryparams['count'] = integer
        queryparams['offset'] = integer
        """"""
        self.folder_id = None
        if get_all:
            return self._iterate(url=self._build_path(), **queryparams)
        else:
            return self._mc_client._get(url=self._build_path(), **queryparams)


    def get(self, folder_id, **queryparams):
        """"""
        Get information about a specific folder used to organize campaigns.

        :param folder_id: The unique id for the campaign folder.
        :type folder_id: :py:class:`str`
        :param queryparams: The query string parameters
        queryparams['fields'] = []
        queryparams['exclude_fields'] = []
        """"""
        self.folder_id = folder_id
        return self._mc_client._get(url=self._build_path(folder_id), **queryparams)


    def update(self, folder_id, data):
        """"""
        Update a specific folder used to organize campaigns.

        :param folder_id: The unique id for the campaign folder.
        :type folder_id: :py:class:`str`
        :param data: The request body parameters
        :type data: :py:class:`dict`
        data = {
            ""name"": string*
        }
        """"""
        self.folder_id = folder_id
        return self._mc_client._patch(url=self._build_path(folder_id), data=data)


    def delete(self, folder_id):
        """"""
        Delete a specific campaign folder, and mark all the campaigns in the
        folder as ‘unfiled’.

        :param folder_id: The unique id for the campaign folder.
        :type folder_id: :py:class:`str`
        """"""
        self.folder_id = folder_id
        return self._mc_client._delete(url=self._build_path(folder_id))

```"
56788cc70d3b6f9375f70bd874cf4d31c3f8e3ec,photutils/utils/_parameters.py,photutils/utils/_parameters.py,,"# Licensed under a 3-clause BSD style license - see LICENSE.rst
""""""
This module provides parameter validation tools.
""""""

import numpy as np


def as_pair(name, value, lower_bound=None, upper_bound=None):
    """"""
    Define a pair of integer values as a 1D array.

    Parameters
    ----------
    name : str
        The name of the parameter, which is used in error messages.

    value : int or int array_like
        The input value.

    lower_bound : int or int array_like
        A tuple defining the allowed lower bound of the value. The first
        element is the bound and the second element indicates whether
        the bound is exclusive (0) or inclusive (1).

    upper_bound : int or int array_like
        A tuple defining the allowed upper bound of the value. The first
        element is the bound and the second element indicates whether
        the bound is exclusive (0) or inclusive (1).

    Returns
    -------
    result : (2,) `~numpy.ndarray`
        The pair as a 1D array of two integers.

    Examples
    --------
    >>> from photutils.utils._parameters import as_pair

    >>> as_pair('myparam', 4)
    array([4, 4])

    >>> as_pair('myparam', (3, 4))
    array([3, 4])

    >>> as_pair('myparam', 0, lower_bound=(0, 0))
    array([0, 0])
    """"""
    value = np.atleast_1d(value)

    if np.any(~np.isfinite(value)):
        raise ValueError(f'{name} must be a finite value')

    if lower_bound is not None:
        if len(lower_bound) != 2:
            raise ValueError('lower_bound must contain only 2 elements')
        bound, inclusive = lower_bound
        if inclusive == 1:
            oper = '>'
            mask = value <= bound
        else:
            oper = '>='
            mask = value < bound
        if np.any(mask):
            raise ValueError(f'{name} must be {oper} {bound}')

    if upper_bound is not None:
        if len(upper_bound) != 2:
            raise ValueError('upper_bound must contain only 2 elements')
        bound, inclusive = upper_bound
        if inclusive == 1:
            oper = '<'
            mask = value >= bound
        else:
            oper = '<='
            mask = value > bound
        if np.any(mask):
            raise ValueError(f'{name} must be {oper} {bound}')

    if len(value) == 1:
        value = np.array((value[0], value[0]))
    if len(value) != 2:
        raise ValueError(f'{name} must have 1 or 2 elements')
    if value.ndim != 1:
        raise ValueError(f'{name} must be 1D')
    if value.dtype.kind != 'i':
        raise ValueError(f'{name} must have integer values')

    return value
",Add as_pair helper function for parameter validation,"Add as_pair helper function for parameter validation
",Python,bsd-3-clause,"astropy/photutils,larrybradley/photutils",90,"```python
# Licensed under a 3-clause BSD style license - see LICENSE.rst
""""""
This module provides parameter validation tools.
""""""

import numpy as np


def as_pair(name, value, lower_bound=None, upper_bound=None):
    """"""
    Define a pair of integer values as a 1D array.

    Parameters
    ----------
    name : str
        The name of the parameter, which is used in error messages.

    value : int or int array_like
        The input value.

    lower_bound : int or int array_like
        A tuple defining the allowed lower bound of the value. The first
        element is the bound and the second element indicates whether
        the bound is exclusive (0) or inclusive (1).

    upper_bound : int or int array_like
        A tuple defining the allowed upper bound of the value. The first
        element is the bound and the second element indicates whether
        the bound is exclusive (0) or inclusive (1).

    Returns
    -------
    result : (2,) `~numpy.ndarray`
        The pair as a 1D array of two integers.

    Examples
    --------
    >>> from photutils.utils._parameters import as_pair

    >>> as_pair('myparam', 4)
    array([4, 4])

    >>> as_pair('myparam', (3, 4))
    array([3, 4])

    >>> as_pair('myparam', 0, lower_bound=(0, 0))
    array([0, 0])
    """"""
    value = np.atleast_1d(value)

    if np.any(~np.isfinite(value)):
        raise ValueError(f'{name} must be a finite value')

    if lower_bound is not None:
        if len(lower_bound) != 2:
            raise ValueError('lower_bound must contain only 2 elements')
        bound, inclusive = lower_bound
        if inclusive == 1:
            oper = '>'
            mask = value <= bound
        else:
            oper = '>='
            mask = value < bound
        if np.any(mask):
            raise ValueError(f'{name} must be {oper} {bound}')

    if upper_bound is not None:
        if len(upper_bound) != 2:
            raise ValueError('upper_bound must contain only 2 elements')
        bound, inclusive = upper_bound
        if inclusive == 1:
            oper = '<'
            mask = value >= bound
        else:
            oper = '<='
            mask = value > bound
        if np.any(mask):
            raise ValueError(f'{name} must be {oper} {bound}')

    if len(value) == 1:
        value = np.array((value[0], value[0]))
    if len(value) != 2:
        raise ValueError(f'{name} must have 1 or 2 elements')
    if value.ndim != 1:
        raise ValueError(f'{name} must be 1D')
    if value.dtype.kind != 'i':
        raise ValueError(f'{name} must have integer values')

    return value

```"
528107bad66cc2eb9e56a7964e16eb66b4beddf7,fabfile.py,fabfile.py,,"from fabric.api import (
    local,
    settings,
    task
)
from fabric.state import env


SWARM101_NETWORK = 'swarm101'


@task
def localhost():
    env.run = local


@task
def swarm_init(subnet='192.168.0.0/24'):
    env.run('docker swarm init')

    command = 'docker network create -d overlay ' + \
        '--subnet=' + subnet + ' ' + SWARM101_NETWORK
    env.run(command)


@task
def swarm_leave():
    with settings(warn_only=True):
        env.run('docker swarm leave --force')
        env.run('docker network rm ' + SWARM101_NETWORK)


@task
def build_images():
    services = [
        (
            'bangkok',
            'bangkok/Dockerfile',
            'bangkok'
        ),
        (
            'munich',
            'munich/Dockerfile',
            'munich'
        ),
        (
            'tokyo',
            'tokyo/Dockerfile',
            'tokyo'
        ),
        (
            'nyc',
            'nyc/Dockerfile',
            'nyc'
        ),
        (
            'gateway',
            'gateway/Dockerfile',
            'gateway'
        ),
    ]
    for name, dockerfile, path in services:
        command = 'docker build -t ' + name + ':unstable -f ' + \
            dockerfile + ' ' + path
        env.run(command)


@task
def create_services(tag='unstable'):
    services = [
        'bangkok',
        'munich',
        'tokyo',
        'nyc',
    ]
    for service in services:
        command = 'docker service create --name ' + \
            service + ' --network ' + SWARM101_NETWORK + \
            ' ' + service + ':' + tag
        env.run(command)

    service = 'gateway'
    command = 'docker service create --name ' + \
        service + ' --network ' + SWARM101_NETWORK + \
        ' -p 8000:8000 ' + service + ':' + tag
    env.run(command)


@task
def setup():
    swarm_init()
    build_images()
    create_services(tag='unstable')
",Use fabric to start/end a swarm,"Use fabric to start/end a swarm
",Python,mit,"zkan/microservices-with-swarm-101,zkan/microservices-with-swarm-101,zkan/microservices-with-swarm-101,zkan/microservices-with-swarm-101",94,"```python
from fabric.api import (
    local,
    settings,
    task
)
from fabric.state import env


SWARM101_NETWORK = 'swarm101'


@task
def localhost():
    env.run = local


@task
def swarm_init(subnet='192.168.0.0/24'):
    env.run('docker swarm init')

    command = 'docker network create -d overlay ' + \
        '--subnet=' + subnet + ' ' + SWARM101_NETWORK
    env.run(command)


@task
def swarm_leave():
    with settings(warn_only=True):
        env.run('docker swarm leave --force')
        env.run('docker network rm ' + SWARM101_NETWORK)


@task
def build_images():
    services = [
        (
            'bangkok',
            'bangkok/Dockerfile',
            'bangkok'
        ),
        (
            'munich',
            'munich/Dockerfile',
            'munich'
        ),
        (
            'tokyo',
            'tokyo/Dockerfile',
            'tokyo'
        ),
        (
            'nyc',
            'nyc/Dockerfile',
            'nyc'
        ),
        (
            'gateway',
            'gateway/Dockerfile',
            'gateway'
        ),
    ]
    for name, dockerfile, path in services:
        command = 'docker build -t ' + name + ':unstable -f ' + \
            dockerfile + ' ' + path
        env.run(command)


@task
def create_services(tag='unstable'):
    services = [
        'bangkok',
        'munich',
        'tokyo',
        'nyc',
    ]
    for service in services:
        command = 'docker service create --name ' + \
            service + ' --network ' + SWARM101_NETWORK + \
            ' ' + service + ':' + tag
        env.run(command)

    service = 'gateway'
    command = 'docker service create --name ' + \
        service + ' --network ' + SWARM101_NETWORK + \
        ' -p 8000:8000 ' + service + ':' + tag
    env.run(command)


@task
def setup():
    swarm_init()
    build_images()
    create_services(tag='unstable')

```"
824c89ff8e6276271d6df57f5df80fbebc097ddc,backend/api_calls_test.py,backend/api_calls_test.py,,"#!/bin/python2.7

import mock
import unittest

import api_calls


class FakeFirebase(object):

  def get(self, path, item):
    return None

  def put(self, path, item, data):
    return None

  def patch(self, path, data):
    return None


class TestApiCalls(unittest.TestCase):

  def setUp(self):
    self.db = FakeFirebase()
    self.mdb = mock.create_autospec(FakeFirebase)

  def testValidateInputs(self):
    request = {}
    api_calls.ValidateInputs(request, self.db, [], [])

    # If we have a fooId, the value must start foo-
    request = {'gunId': 'gunFoo'}
    self.assertRaises(api_calls.InvalidInputError, api_calls.ValidateInputs, request, self.db, [], [])
    request = {'gunId': 'gun-Foo'}
    api_calls.ValidateInputs(request, self.db, [], [])

  def testRegister(self):
    """"""Register does a get/put and fails when the user already exists.""""""
    self.mdb.get.return_value = None
    api_calls.Register({'userToken': 'foo'}, self.mdb)
    self.mdb.get.assert_called_once_with('/users/foo', 'a')
    self.mdb.put.assert_called_once_with('/users', 'foo', {'a': True})

    self.mdb.get.return_value = True
    self.assertRaises(api_calls.InvalidInputError, api_calls.Register, {'userToken': 'foo'}, self.mdb)

  def testCreateGame(self):
    pass

  def testUpdateGame(self):
    pass

  def testCreateGroup(self):
    pass

  def testUpdateGroup(self):
    pass

  def testCreatePlayer(self):
    pass

  def testAddGun(self):
    pass

  def testAssignGun(self):
    pass

  def testUpdatePlayer(self):
    pass

  def testAddMission(self):
    pass

  def testUpdateMission(self):
    pass

  def testCreateChatRoom(self):
    pass

  def testAddPlayerToChat(self):
    pass

  def testSendChatMessage(self):
    pass

  def testAddRewardCategory(self):
    pass

  def testUpdateRewardCategory(self):
    pass

  def testAddReward(self):
    pass

  def testClaimReward(self):
    pass


if __name__ == '__main__':
  unittest.main()


# vim:ts=2:sw=2:expandtab
",Add mostly empty unit tests,"Add mostly empty unit tests
",Python,apache-2.0,"google/playhvz,google/playhvz,google/playhvz,google/playhvz,google/playhvz,google/playhvz,google/playhvz,google/playhvz",104,"```python
#!/bin/python2.7

import mock
import unittest

import api_calls


class FakeFirebase(object):

  def get(self, path, item):
    return None

  def put(self, path, item, data):
    return None

  def patch(self, path, data):
    return None


class TestApiCalls(unittest.TestCase):

  def setUp(self):
    self.db = FakeFirebase()
    self.mdb = mock.create_autospec(FakeFirebase)

  def testValidateInputs(self):
    request = {}
    api_calls.ValidateInputs(request, self.db, [], [])

    # If we have a fooId, the value must start foo-
    request = {'gunId': 'gunFoo'}
    self.assertRaises(api_calls.InvalidInputError, api_calls.ValidateInputs, request, self.db, [], [])
    request = {'gunId': 'gun-Foo'}
    api_calls.ValidateInputs(request, self.db, [], [])

  def testRegister(self):
    """"""Register does a get/put and fails when the user already exists.""""""
    self.mdb.get.return_value = None
    api_calls.Register({'userToken': 'foo'}, self.mdb)
    self.mdb.get.assert_called_once_with('/users/foo', 'a')
    self.mdb.put.assert_called_once_with('/users', 'foo', {'a': True})

    self.mdb.get.return_value = True
    self.assertRaises(api_calls.InvalidInputError, api_calls.Register, {'userToken': 'foo'}, self.mdb)

  def testCreateGame(self):
    pass

  def testUpdateGame(self):
    pass

  def testCreateGroup(self):
    pass

  def testUpdateGroup(self):
    pass

  def testCreatePlayer(self):
    pass

  def testAddGun(self):
    pass

  def testAssignGun(self):
    pass

  def testUpdatePlayer(self):
    pass

  def testAddMission(self):
    pass

  def testUpdateMission(self):
    pass

  def testCreateChatRoom(self):
    pass

  def testAddPlayerToChat(self):
    pass

  def testSendChatMessage(self):
    pass

  def testAddRewardCategory(self):
    pass

  def testUpdateRewardCategory(self):
    pass

  def testAddReward(self):
    pass

  def testClaimReward(self):
    pass


if __name__ == '__main__':
  unittest.main()


# vim:ts=2:sw=2:expandtab

```"
a0adf63c1f942f4cdbe839ea367772c5ae08fbfc,CodeFights/fixTree.py,CodeFights/fixTree.py,,"#!/usr/local/bin/python
# Code Fights Fix Tree Problem


def fixTree(tree):
    return [s.strip().center(len(s)) for s in tree]


def main():
    tests = [
        [
            [""      *  "",
             ""    *    "",
             ""***      "",
             ""    *****"",
             ""  *******"",
             ""*********"",
             "" ***     ""],
            [""    *    "",
             ""    *    "",
             ""   ***   "",
             ""  *****  "",
             "" ******* "",
             ""*********"",
             ""   ***   ""]
        ],
        [
            [""    *    "",
             ""    *    "",
             ""   ***   "",
             ""  *****  "",
             "" ******* "",
             ""*********"",
             ""   ***   ""],
            [""    *    "",
             ""    *    "",
             ""   ***   "",
             ""  *****  "",
             "" ******* "",
             ""*********"",
             ""   ***   ""]
        ],
        [
            [""*"",
             ""*"",
             ""*"",
             ""*""],
            [""*"",
             ""*"",
             ""*"",
             ""*""]
        ],
        [
            [""   *** ""],
            [""  ***  ""]
        ],
        [
            [""         *   "",
             ""*            "",
             ""       ***   "",
             ""   *****     "",
             ""  *******    "",
             ""  *********  "",
             ""*******      "",
             ""   ********* "",
             ""  ***********"",
             ""    *********"",
             ""***********  "",
             ""*************"",
             ""        ***  "",
             ""  ***        "",
             ""       ***   ""],
            [""      *      "",
             ""      *      "",
             ""     ***     "",
             ""    *****    "",
             ""   *******   "",
             ""  *********  "",
             ""   *******   "",
             ""  *********  "",
             "" *********** "",
             ""  *********  "",
             "" *********** "",
             ""*************"",
             ""     ***     "",
             ""     ***     "",
             ""     ***     ""]
        ]
    ]

    for t in tests:
        res = fixTree(t[0])
        ans = t[1]
        if ans == res:
            print(""PASSED: fixTree({}) returned {}""
                  .format(t[0], res))
        else:
            print((""FAILED: fixTree({}) returned {},""
                   ""answer: {}"").format(t[0], res, ans))


if __name__ == '__main__':
    main()
",Solve Code Fights fix tree problem,"Solve Code Fights fix tree problem
",Python,mit,HKuz/Test_Code,104,"```python
#!/usr/local/bin/python
# Code Fights Fix Tree Problem


def fixTree(tree):
    return [s.strip().center(len(s)) for s in tree]


def main():
    tests = [
        [
            [""      *  "",
             ""    *    "",
             ""***      "",
             ""    *****"",
             ""  *******"",
             ""*********"",
             "" ***     ""],
            [""    *    "",
             ""    *    "",
             ""   ***   "",
             ""  *****  "",
             "" ******* "",
             ""*********"",
             ""   ***   ""]
        ],
        [
            [""    *    "",
             ""    *    "",
             ""   ***   "",
             ""  *****  "",
             "" ******* "",
             ""*********"",
             ""   ***   ""],
            [""    *    "",
             ""    *    "",
             ""   ***   "",
             ""  *****  "",
             "" ******* "",
             ""*********"",
             ""   ***   ""]
        ],
        [
            [""*"",
             ""*"",
             ""*"",
             ""*""],
            [""*"",
             ""*"",
             ""*"",
             ""*""]
        ],
        [
            [""   *** ""],
            [""  ***  ""]
        ],
        [
            [""         *   "",
             ""*            "",
             ""       ***   "",
             ""   *****     "",
             ""  *******    "",
             ""  *********  "",
             ""*******      "",
             ""   ********* "",
             ""  ***********"",
             ""    *********"",
             ""***********  "",
             ""*************"",
             ""        ***  "",
             ""  ***        "",
             ""       ***   ""],
            [""      *      "",
             ""      *      "",
             ""     ***     "",
             ""    *****    "",
             ""   *******   "",
             ""  *********  "",
             ""   *******   "",
             ""  *********  "",
             "" *********** "",
             ""  *********  "",
             "" *********** "",
             ""*************"",
             ""     ***     "",
             ""     ***     "",
             ""     ***     ""]
        ]
    ]

    for t in tests:
        res = fixTree(t[0])
        ans = t[1]
        if ans == res:
            print(""PASSED: fixTree({}) returned {}""
                  .format(t[0], res))
        else:
            print((""FAILED: fixTree({}) returned {},""
                   ""answer: {}"").format(t[0], res, ans))


if __name__ == '__main__':
    main()

```"
8ad29d6563fd282b18884c345451a44a71eba890,tests/test_midi_note_numbers.py,tests/test_midi_note_numbers.py,,"# TODO: test if method to convert Note to a MIDI note number works

import pytest

from music_essentials import Note

def test_midi_0():
    n = Note.from_note_string('C-1')
    mid = n.midi_note_number()
    assert mid == 0

def test_midi_1_sharp():
    n = Note.from_note_string('C-1#')
    mid = n.midi_note_number()
    assert mid == 1

def test_midi_1_flat():
    n = Note.from_note_string('D-1b')
    mid = n.midi_note_number()
    assert mid == 1

def test_midi_2():
    n = Note.from_note_string('D-1')
    mid = n.midi_note_number()
    assert mid == 2

def test_midi_3_sharp():
    n = Note.from_note_string('D-1#')
    mid = n.midi_note_number()
    assert mid == 3

def test_midi_3_flat():
    n = Note.from_note_string('E-1b')
    mid = n.midi_note_number()
    assert mid == 3

def test_midi_4():
    n = Note.from_note_string('E-1')
    mid = n.midi_note_number()
    assert mid == 4

def test_midi_5():
    n = Note.from_note_string('F-1')
    mid = n.midi_note_number()
    assert mid == 5

def test_midi_6_sharp():
    n = Note.from_note_string('F-1#')
    mid = n.midi_note_number()
    assert mid == 6

def test_midi_6_flat():
    n = Note.from_note_string('G-1b')
    mid = n.midi_note_number()
    assert mid == 6

def test_midi_7():
    n = Note.from_note_string('G-1')
    mid = n.midi_note_number()
    assert mid == 7

def test_midi_8_sharp():
    n = Note.from_note_string('G-1#')
    mid = n.midi_note_number()
    assert mid == 8

def test_midi_8_flat():
    n = Note.from_note_string('A-1b')
    mid = n.midi_note_number()
    assert mid == 8

def test_midi_9():
    n = Note.from_note_string('A-1')
    mid = n.midi_note_number()
    assert mid == 9

def test_midi_10_sharp():
    n = Note.from_note_string('A-1#')
    mid = n.midi_note_number()
    assert mid == 10

def test_midi_10_flat():
    n = Note.from_note_string('B-1b')
    mid = n.midi_note_number()
    assert mid == 10

def test_midi_11():
    n = Note.from_note_string('B-1')
    mid = n.midi_note_number()
    assert mid == 11",Add tests for calculating MIDI numbers - octave -1.,"Add tests for calculating MIDI numbers - octave -1.

Signed-off-by: Charlotte Pierce <351429ca27f6e4bff2dbb77adb5046c88cd12fae@malformed-bits.com>
",Python,mit,charlottepierce/music_essentials,90,"```python
# TODO: test if method to convert Note to a MIDI note number works

import pytest

from music_essentials import Note

def test_midi_0():
    n = Note.from_note_string('C-1')
    mid = n.midi_note_number()
    assert mid == 0

def test_midi_1_sharp():
    n = Note.from_note_string('C-1#')
    mid = n.midi_note_number()
    assert mid == 1

def test_midi_1_flat():
    n = Note.from_note_string('D-1b')
    mid = n.midi_note_number()
    assert mid == 1

def test_midi_2():
    n = Note.from_note_string('D-1')
    mid = n.midi_note_number()
    assert mid == 2

def test_midi_3_sharp():
    n = Note.from_note_string('D-1#')
    mid = n.midi_note_number()
    assert mid == 3

def test_midi_3_flat():
    n = Note.from_note_string('E-1b')
    mid = n.midi_note_number()
    assert mid == 3

def test_midi_4():
    n = Note.from_note_string('E-1')
    mid = n.midi_note_number()
    assert mid == 4

def test_midi_5():
    n = Note.from_note_string('F-1')
    mid = n.midi_note_number()
    assert mid == 5

def test_midi_6_sharp():
    n = Note.from_note_string('F-1#')
    mid = n.midi_note_number()
    assert mid == 6

def test_midi_6_flat():
    n = Note.from_note_string('G-1b')
    mid = n.midi_note_number()
    assert mid == 6

def test_midi_7():
    n = Note.from_note_string('G-1')
    mid = n.midi_note_number()
    assert mid == 7

def test_midi_8_sharp():
    n = Note.from_note_string('G-1#')
    mid = n.midi_note_number()
    assert mid == 8

def test_midi_8_flat():
    n = Note.from_note_string('A-1b')
    mid = n.midi_note_number()
    assert mid == 8

def test_midi_9():
    n = Note.from_note_string('A-1')
    mid = n.midi_note_number()
    assert mid == 9

def test_midi_10_sharp():
    n = Note.from_note_string('A-1#')
    mid = n.midi_note_number()
    assert mid == 10

def test_midi_10_flat():
    n = Note.from_note_string('B-1b')
    mid = n.midi_note_number()
    assert mid == 10

def test_midi_11():
    n = Note.from_note_string('B-1')
    mid = n.midi_note_number()
    assert mid == 11
```"
8214d8b542e2da890bc6b34372de2016e98e7767,tests/services/shop/order/test_ordered_articles_service.py,tests/services/shop/order/test_ordered_articles_service.py,,"""""""
:Copyright: 2006-2017 Jochen Kupperschmidt
:License: Modified BSD, see LICENSE for details.
""""""

from itertools import count

from byceps.services.shop.order import ordered_articles_service
from byceps.services.shop.order.models.order import PaymentState

from testfixtures.party import create_party
from testfixtures.shop_article import create_article
from testfixtures.shop_order import create_order, create_order_item
from testfixtures.user import create_user_with_detail

from tests.base import AbstractAppTestCase


class OrderedArticlesServiceTestCase(AbstractAppTestCase):

    def setUp(self):
        super().setUp()

        self.user = self.create_user(1)

        self.article = self.create_article()

    def test_count_ordered_articles(self):
        expected = {
            PaymentState.open: 12,
            PaymentState.canceled: 7,
            PaymentState.paid: 3,
        }

        order_number_sequence = count(1)
        for article_quantity, payment_state in [
            (4, PaymentState.open),
            (1, PaymentState.open),
            (5, PaymentState.canceled),
            (3, PaymentState.paid),
            (2, PaymentState.canceled),
            (7, PaymentState.open),
        ]:
            order_number = 'XY-01-B{:05d}'.format(next(order_number_sequence))
            self.create_order(order_number, article_quantity, payment_state)

        totals = ordered_articles_service.count_ordered_articles(self.article)

        self.assertDictEqual(totals, expected)

    # -------------------------------------------------------------------- #
    # helpers

    def create_party(self, party_id, title):
        party = create_party(id=party_id, title=title, brand=self.brand)

        self.db.session.add(party)
        self.db.session.commit()

        return party

    def create_user(self, number):
        user = create_user_with_detail(number)

        self.db.session.add(user)
        self.db.session.commit()

        return user

    def create_article(self):
        article = create_article(party=self.party)

        self.db.session.add(article)
        self.db.session.commit()

        return article

    def create_order(self, order_number, article_quantity, payment_state):
        order = create_order(self.party.id, self.user,
                             order_number=order_number)
        order.payment_state = payment_state
        self.db.session.add(order)

        order_item = create_order_item(order, self.article, article_quantity)
        self.db.session.add(order_item)

        self.db.session.commit()

        return order.to_tuple()
",Add test for service function that counts ordered article by payment state,"Add test for service function that counts ordered article by payment state
",Python,bsd-3-clause,"m-ober/byceps,homeworkprod/byceps,m-ober/byceps,m-ober/byceps,homeworkprod/byceps,homeworkprod/byceps",90,"```python
""""""
:Copyright: 2006-2017 Jochen Kupperschmidt
:License: Modified BSD, see LICENSE for details.
""""""

from itertools import count

from byceps.services.shop.order import ordered_articles_service
from byceps.services.shop.order.models.order import PaymentState

from testfixtures.party import create_party
from testfixtures.shop_article import create_article
from testfixtures.shop_order import create_order, create_order_item
from testfixtures.user import create_user_with_detail

from tests.base import AbstractAppTestCase


class OrderedArticlesServiceTestCase(AbstractAppTestCase):

    def setUp(self):
        super().setUp()

        self.user = self.create_user(1)

        self.article = self.create_article()

    def test_count_ordered_articles(self):
        expected = {
            PaymentState.open: 12,
            PaymentState.canceled: 7,
            PaymentState.paid: 3,
        }

        order_number_sequence = count(1)
        for article_quantity, payment_state in [
            (4, PaymentState.open),
            (1, PaymentState.open),
            (5, PaymentState.canceled),
            (3, PaymentState.paid),
            (2, PaymentState.canceled),
            (7, PaymentState.open),
        ]:
            order_number = 'XY-01-B{:05d}'.format(next(order_number_sequence))
            self.create_order(order_number, article_quantity, payment_state)

        totals = ordered_articles_service.count_ordered_articles(self.article)

        self.assertDictEqual(totals, expected)

    # -------------------------------------------------------------------- #
    # helpers

    def create_party(self, party_id, title):
        party = create_party(id=party_id, title=title, brand=self.brand)

        self.db.session.add(party)
        self.db.session.commit()

        return party

    def create_user(self, number):
        user = create_user_with_detail(number)

        self.db.session.add(user)
        self.db.session.commit()

        return user

    def create_article(self):
        article = create_article(party=self.party)

        self.db.session.add(article)
        self.db.session.commit()

        return article

    def create_order(self, order_number, article_quantity, payment_state):
        order = create_order(self.party.id, self.user,
                             order_number=order_number)
        order.payment_state = payment_state
        self.db.session.add(order)

        order_item = create_order_item(order, self.article, article_quantity)
        self.db.session.add(order_item)

        self.db.session.commit()

        return order.to_tuple()

```"
ff2c8d3f74a69dd8873019ae2e966833ef4d79fd,pombola/south_africa/management/commands/south_africa_export_committee_members.py,pombola/south_africa/management/commands/south_africa_export_committee_members.py,,"""""""Export a CSV listing committee members with term dates.""""""

import unicodecsv as csv
import os
import collections

from pombola.core.models import Person, Organisation, OrganisationKind

from django.core.management.base import BaseCommand, CommandError
from django.utils import dateformat


def formatApproxDate(date):
        if date:
            if date.future:
                return 'future'
            if date.past:
                return 'past'
            elif date.year and date.month and date.day:
                return dateformat.format(date, 'Y-m-d')
            elif date.year and date.month:
                return dateformat.format(date, 'Y-m')
            elif date.year:
                return dateformat.format(date, 'Y')
        else:
            return None


class Command(BaseCommand):
    args = 'destination'
    help = 'Export a CSV listing committee members with term dates.'

    def handle(self, *args, **options):

        if len(args) != 1:
            raise CommandError(""You must provide a destination."")

        destination = args[0]

        organisationKind = OrganisationKind.objects.filter(slug='committee').get()
        organisations = Organisation.objects.filter(kind=organisationKind)

        fields = [
            'name',
            'title',
            'given_name',
            'family_name',
            'committee',
            'position',
            'url',
            'start_date',
            'end_date',
            'parties',
        ]

        with open(os.path.join(destination), 'wb') as output_file:
            writer = csv.DictWriter(output_file, fieldnames=fields)

            writer.writeheader()

            for organisation in organisations:

                # Get the list of positions
                positions = organisation.position_set.filter(person__hidden=False)

                # Write all the outputs
                for position in positions:
                    print position
                    person = position.person

                    parties = []
                    for party in person.parties():
                        parties.append(party.name)

                    position_output = {
                        'name': person.name,
                        'title': person.title,
                        'given_name': person.given_name,
                        'family_name': person.family_name,
                        'committee': organisation.name,
                        'position': position.title,
                        'url': 'https://www.pa.org.za/person/{}/'.format(person.slug),
                        'start_date': formatApproxDate(position.start_date),
                        'end_date': formatApproxDate(position.end_date),
                        'parties': ', '.join(parties)
                    }
                    writer.writerow(position_output)

        print ""Done! Exported CSV of "" + str(len(positions)) + "" positions.""
",Add export script for committee members,"Add export script for committee members
",Python,agpl-3.0,"mysociety/pombola,mysociety/pombola,mysociety/pombola,mysociety/pombola,mysociety/pombola,mysociety/pombola",90,"```python
""""""Export a CSV listing committee members with term dates.""""""

import unicodecsv as csv
import os
import collections

from pombola.core.models import Person, Organisation, OrganisationKind

from django.core.management.base import BaseCommand, CommandError
from django.utils import dateformat


def formatApproxDate(date):
        if date:
            if date.future:
                return 'future'
            if date.past:
                return 'past'
            elif date.year and date.month and date.day:
                return dateformat.format(date, 'Y-m-d')
            elif date.year and date.month:
                return dateformat.format(date, 'Y-m')
            elif date.year:
                return dateformat.format(date, 'Y')
        else:
            return None


class Command(BaseCommand):
    args = 'destination'
    help = 'Export a CSV listing committee members with term dates.'

    def handle(self, *args, **options):

        if len(args) != 1:
            raise CommandError(""You must provide a destination."")

        destination = args[0]

        organisationKind = OrganisationKind.objects.filter(slug='committee').get()
        organisations = Organisation.objects.filter(kind=organisationKind)

        fields = [
            'name',
            'title',
            'given_name',
            'family_name',
            'committee',
            'position',
            'url',
            'start_date',
            'end_date',
            'parties',
        ]

        with open(os.path.join(destination), 'wb') as output_file:
            writer = csv.DictWriter(output_file, fieldnames=fields)

            writer.writeheader()

            for organisation in organisations:

                # Get the list of positions
                positions = organisation.position_set.filter(person__hidden=False)

                # Write all the outputs
                for position in positions:
                    print position
                    person = position.person

                    parties = []
                    for party in person.parties():
                        parties.append(party.name)

                    position_output = {
                        'name': person.name,
                        'title': person.title,
                        'given_name': person.given_name,
                        'family_name': person.family_name,
                        'committee': organisation.name,
                        'position': position.title,
                        'url': 'https://www.pa.org.za/person/{}/'.format(person.slug),
                        'start_date': formatApproxDate(position.start_date),
                        'end_date': formatApproxDate(position.end_date),
                        'parties': ', '.join(parties)
                    }
                    writer.writerow(position_output)

        print ""Done! Exported CSV of "" + str(len(positions)) + "" positions.""

```"
868b4edfbf85aaee096021580146419904476e7d,snapshot_archive.py,snapshot_archive.py,,"#!/usr/bin/env python
""""""
Crawl the SOLr indexes to get all dataset documents for a particular project
and extract enough information to create a snapshot of the current state.  

Querying SOLr directly should work better than via the esgf search api.

Currently this script assumes dataset versions are not tampered with and that
it is sufficient to extract these values from each dataset:

 1. instance_id
 2. data_node
 3. index_node
 4. size
 6. replica (True of False)
 7. timestamp

""""""

import sys
from xml.etree import ElementTree as ET
import urllib2
import json

DUMP_PROPERTIES = ['instance_id', 'data_node', 'index_node', 'size',
                   'replica', 'timestamp']
SOLR_CORE = 'datasets'

BATCH_SIZE = 500
SHARDS_XML = '/esg/config/esgf_shards_static.xml'
ESGF_WHITELIST_NS = ""http://www.esgf.org/whitelist""

def get_shards(shards_file=SHARDS_XML):
    shards_xml = ET.parse(open(shards_file))

    for elem in shards_xml.findall('.//{%s}value' % ESGF_WHITELIST_NS):
        yield elem.text

def make_query(shard, core, project, properties, start, rows=BATCH_SIZE):
    url = ('http://{shard}/{core}/select?'
           'q=project:{project}&fl={properties}'
           '&wt=json&start={start}&rows={rows}'.format(
            core=core,
            shard=shard,
            project=project,
            properties=','.join(properties),
            rows=rows, start=start))

    return url

def iter_docs(shard, project, properties, rows=BATCH_SIZE):
    start = 0
    while 1:
        url = make_query(shard, SOLR_CORE, project, properties, start, rows)
        response = urllib2.urlopen(url)
        resp_json = json.load(response)

        for doc in resp_json['response']['docs']:
            yield doc

        num_found = resp_json['response']['numFound']
        if start == 0:
            print '### Num found = %s' % num_found

        start += rows
        if start > num_found:
            return



def main(argv=sys.argv):
    project, outfile = argv[1:]

    shards = get_shards()
    # Override for debug
    #shards = ['localhost:8984/solr']


    with open(outfile, 'w') as fh:
        for shard in shards:
            print '\n## Querying shard %s' % shard
            print >>fh, '#', '\t'.join(DUMP_PROPERTIES)
            for i, result in enumerate(iter_docs(shard, project, DUMP_PROPERTIES)):
                print >>fh, '\t'.join(str(result[x]) for x in DUMP_PROPERTIES)
                if i % BATCH_SIZE == 0:
                    print '[%d]' % (i, ),
                    sys.stdout.flush()


if __name__ == '__main__':
    main()
    
",Add script for extracting SOLr data,"Add script for extracting SOLr data
",Python,bsd-3-clause,stephenpascoe/esgf-analytics,93,"```python
#!/usr/bin/env python
""""""
Crawl the SOLr indexes to get all dataset documents for a particular project
and extract enough information to create a snapshot of the current state.  

Querying SOLr directly should work better than via the esgf search api.

Currently this script assumes dataset versions are not tampered with and that
it is sufficient to extract these values from each dataset:

 1. instance_id
 2. data_node
 3. index_node
 4. size
 6. replica (True of False)
 7. timestamp

""""""

import sys
from xml.etree import ElementTree as ET
import urllib2
import json

DUMP_PROPERTIES = ['instance_id', 'data_node', 'index_node', 'size',
                   'replica', 'timestamp']
SOLR_CORE = 'datasets'

BATCH_SIZE = 500
SHARDS_XML = '/esg/config/esgf_shards_static.xml'
ESGF_WHITELIST_NS = ""http://www.esgf.org/whitelist""

def get_shards(shards_file=SHARDS_XML):
    shards_xml = ET.parse(open(shards_file))

    for elem in shards_xml.findall('.//{%s}value' % ESGF_WHITELIST_NS):
        yield elem.text

def make_query(shard, core, project, properties, start, rows=BATCH_SIZE):
    url = ('http://{shard}/{core}/select?'
           'q=project:{project}&fl={properties}'
           '&wt=json&start={start}&rows={rows}'.format(
            core=core,
            shard=shard,
            project=project,
            properties=','.join(properties),
            rows=rows, start=start))

    return url

def iter_docs(shard, project, properties, rows=BATCH_SIZE):
    start = 0
    while 1:
        url = make_query(shard, SOLR_CORE, project, properties, start, rows)
        response = urllib2.urlopen(url)
        resp_json = json.load(response)

        for doc in resp_json['response']['docs']:
            yield doc

        num_found = resp_json['response']['numFound']
        if start == 0:
            print '### Num found = %s' % num_found

        start += rows
        if start > num_found:
            return



def main(argv=sys.argv):
    project, outfile = argv[1:]

    shards = get_shards()
    # Override for debug
    #shards = ['localhost:8984/solr']


    with open(outfile, 'w') as fh:
        for shard in shards:
            print '\n## Querying shard %s' % shard
            print >>fh, '#', '\t'.join(DUMP_PROPERTIES)
            for i, result in enumerate(iter_docs(shard, project, DUMP_PROPERTIES)):
                print >>fh, '\t'.join(str(result[x]) for x in DUMP_PROPERTIES)
                if i % BATCH_SIZE == 0:
                    print '[%d]' % (i, ),
                    sys.stdout.flush()


if __name__ == '__main__':
    main()
    

```"
42a92130fc9d6f3358bb03a7ab56cdc5f20eb4d1,tests/test_config.py,tests/test_config.py,,"import os

import pytest

from vrun import config
from vrun.compat import ConfigParser


@pytest.mark.parametrize('parts, result', [
    (
        ['simple'],
        ['simple']
    ),
    (
        ['multiple', 'simple'],
        ['multiple', 'simple']
    ),
    (
        ['with', '""quotes""'],
        ['with', '""quotes""']
    ),
    (
        ['""testing', 'quote', 'support""'],
        ['testing quote support']
    ),
    (
        [""'testing"", 'quote', ""support'""],
        ['testing quote support']
    ),
    (
        ['""testing', '\'quote', 'support""'],
        ['testing \'quote support']
    ),
    (
        ['""testing', '\'quote\'', 'support""'],
        ['testing \'quote\' support']
    ),
    (
        ['""testing', '\'quote', '\'support""'],
        ['testing \'quote \'support']
    ),
    (
        ['""""'],
        ['""""']
    ),
    (
        ['"" ', ' ""'],
        ['   ']
    ),
])
def test_quoted_combine(parts, result):
    assert list(config.quoted_combine(parts)) == result


@pytest.mark.parametrize('parts', [
    ['""testing', '\'quote', '""support""'],
    ['"" ', '""""'],
    ['""test', '""ing'],
])
def test_quoted_combine_invalid(parts):
    with pytest.raises(ValueError):
        assert list(config.quoted_combine(parts))


@pytest.mark.parametrize('folder, result', [
    ('configtest', 'vrun.cfg'),
    ('configtest/vrun_ini', 'vrun.ini'),
    ('configtest/setup_cfg', 'setup.cfg'),
    ('configtest/setup_cfg_no_section', None),
])
def test_find_config(folder, result):
    curpath = os.path.dirname(os.path.realpath(__file__))

    cwd = os.path.join(curpath, folder)

    if result:
        assert config.find_config(cwd).endswith(result)
    else:
        assert config.find_config(cwd) == result


@pytest.mark.parametrize('folder, result', [
    ('configtest', 'vrun.cfg'),
    ('configtest/vrun_ini', 'vrun.ini'),
    ('configtest/setup_cfg', 'setup.cfg'),
])
def test_config_from_file(folder, result):
    curpath = os.path.dirname(os.path.realpath(__file__))
    cwd = os.path.join(curpath, folder)

    config_file = config.find_config(cwd)
    assert isinstance(config.config_from_file(config_file), ConfigParser)
",Add tests for ancillary functions,"Add tests for ancillary functions
",Python,isc,bertjwregeer/vrun,93,"```python
import os

import pytest

from vrun import config
from vrun.compat import ConfigParser


@pytest.mark.parametrize('parts, result', [
    (
        ['simple'],
        ['simple']
    ),
    (
        ['multiple', 'simple'],
        ['multiple', 'simple']
    ),
    (
        ['with', '""quotes""'],
        ['with', '""quotes""']
    ),
    (
        ['""testing', 'quote', 'support""'],
        ['testing quote support']
    ),
    (
        [""'testing"", 'quote', ""support'""],
        ['testing quote support']
    ),
    (
        ['""testing', '\'quote', 'support""'],
        ['testing \'quote support']
    ),
    (
        ['""testing', '\'quote\'', 'support""'],
        ['testing \'quote\' support']
    ),
    (
        ['""testing', '\'quote', '\'support""'],
        ['testing \'quote \'support']
    ),
    (
        ['""""'],
        ['""""']
    ),
    (
        ['"" ', ' ""'],
        ['   ']
    ),
])
def test_quoted_combine(parts, result):
    assert list(config.quoted_combine(parts)) == result


@pytest.mark.parametrize('parts', [
    ['""testing', '\'quote', '""support""'],
    ['"" ', '""""'],
    ['""test', '""ing'],
])
def test_quoted_combine_invalid(parts):
    with pytest.raises(ValueError):
        assert list(config.quoted_combine(parts))


@pytest.mark.parametrize('folder, result', [
    ('configtest', 'vrun.cfg'),
    ('configtest/vrun_ini', 'vrun.ini'),
    ('configtest/setup_cfg', 'setup.cfg'),
    ('configtest/setup_cfg_no_section', None),
])
def test_find_config(folder, result):
    curpath = os.path.dirname(os.path.realpath(__file__))

    cwd = os.path.join(curpath, folder)

    if result:
        assert config.find_config(cwd).endswith(result)
    else:
        assert config.find_config(cwd) == result


@pytest.mark.parametrize('folder, result', [
    ('configtest', 'vrun.cfg'),
    ('configtest/vrun_ini', 'vrun.ini'),
    ('configtest/setup_cfg', 'setup.cfg'),
])
def test_config_from_file(folder, result):
    curpath = os.path.dirname(os.path.realpath(__file__))
    cwd = os.path.join(curpath, folder)

    config_file = config.find_config(cwd)
    assert isinstance(config.config_from_file(config_file), ConfigParser)

```"
5d990443a3157a1e8061e81d9bb21cfcde6a4d2b,server/rest/postgres_geojson.py,server/rest/postgres_geojson.py,,"import ast

from girder.api import access
from girder.api.describe import Description
from girder.api.rest import Resource

import psycopg2

# TODO: This will be changed with girder_db_items
def connect_to_gryphon(host=""localhost"",
                       port=""5432"",
                       user=""username"",
                       password=""password"",
                       dbname=""gryphon""):

    try:
        conn = psycopg2.connect(""dbname={} user={} host={} password={} port={}"".format(dbname,
                                                                                       user,
                                                                                       host,
                                                                                       password,
                                                                                       port))
        return conn
    except:
        print ""I am unable to connect to {}"".format(dbname)


class View(object):
    def __init__(self, conn):
        self._conn = conn

    def generateQuery(self, filters):
        q = []
        for k in filters.keys():
            _ = []
            for v in ast.literal_eval(filters[k]):
                _.append("""""" ""{}"" = '{}' """""".format(k, v))
            q.append(""("" + ""or"".join(_) + "")"")
        return ""and"".join(q)


    def getDistinctValues(self, table, filters={}):
        conn = self._conn
        cur = conn.cursor()
        base_query = 'SELECT DISTINCT ""{}"" from gryphonstates'.format(table)
        if not filters:
            query = base_query + "";""
        else:
            query = base_query + "" where"" + self.generateQuery(filters) + "";""
        cur.execute(query)
        field = sorted([i[0] for i in cur.fetchall()])
        if not filters:
            field.insert(0, ""All"")
        return field

    def filter(self, filters):

        resp = {}
        resp['NAME'] = self.getDistinctValues('NAME', filters)
        resp['PRODUCTION_CATEGORY'] = self.getDistinctValues('PRODUCTION_CATEGORY', filters)
        resp['CATEGORY'] = self.getDistinctValues('CATEGORY', filters)
        resp['SUB_CATEGORY'] = self.getDistinctValues('SUB_CATEGORY', filters)
        resp['DATA_DERIVATION'] = self.getDistinctValues('DATA_DERIVATION', filters)

        return resp

class PostgresGeojson(Resource):

    def __init__(self):
        self.resourceName = 'minerva_postgres_geojson'
        self.route('GET',(), self.postgresGeojson)

    @access.user
    def postgresGeojson(self, params):
        conn = connect_to_gryphon()
        view = View(conn)
        return view.filter(params)

    postgresGeojson.description = (
        Description('Get geojson from postgres database')
        .param('NAME', 'state name or all states', required=False,
               dataType='list')
        .param('PRODUCTION_CATEGORY', 'production category', required=False,
               dataType='list')
        .param('CATEGORY', 'category', required=False,
               dataType='list')
        .param('SUB_CATEGORY', 'category', required=False,
               dataType='list')
        .param('DATA_DERIVATION', 'data_derivation', required=False,
               dataType='list')
    ) 
",Add the logic to filter views,"Add the logic to filter views
",Python,apache-2.0,"Kitware/minerva,Kitware/minerva,Kitware/minerva",91,"```python
import ast

from girder.api import access
from girder.api.describe import Description
from girder.api.rest import Resource

import psycopg2

# TODO: This will be changed with girder_db_items
def connect_to_gryphon(host=""localhost"",
                       port=""5432"",
                       user=""username"",
                       password=""password"",
                       dbname=""gryphon""):

    try:
        conn = psycopg2.connect(""dbname={} user={} host={} password={} port={}"".format(dbname,
                                                                                       user,
                                                                                       host,
                                                                                       password,
                                                                                       port))
        return conn
    except:
        print ""I am unable to connect to {}"".format(dbname)


class View(object):
    def __init__(self, conn):
        self._conn = conn

    def generateQuery(self, filters):
        q = []
        for k in filters.keys():
            _ = []
            for v in ast.literal_eval(filters[k]):
                _.append("""""" ""{}"" = '{}' """""".format(k, v))
            q.append(""("" + ""or"".join(_) + "")"")
        return ""and"".join(q)


    def getDistinctValues(self, table, filters={}):
        conn = self._conn
        cur = conn.cursor()
        base_query = 'SELECT DISTINCT ""{}"" from gryphonstates'.format(table)
        if not filters:
            query = base_query + "";""
        else:
            query = base_query + "" where"" + self.generateQuery(filters) + "";""
        cur.execute(query)
        field = sorted([i[0] for i in cur.fetchall()])
        if not filters:
            field.insert(0, ""All"")
        return field

    def filter(self, filters):

        resp = {}
        resp['NAME'] = self.getDistinctValues('NAME', filters)
        resp['PRODUCTION_CATEGORY'] = self.getDistinctValues('PRODUCTION_CATEGORY', filters)
        resp['CATEGORY'] = self.getDistinctValues('CATEGORY', filters)
        resp['SUB_CATEGORY'] = self.getDistinctValues('SUB_CATEGORY', filters)
        resp['DATA_DERIVATION'] = self.getDistinctValues('DATA_DERIVATION', filters)

        return resp

class PostgresGeojson(Resource):

    def __init__(self):
        self.resourceName = 'minerva_postgres_geojson'
        self.route('GET',(), self.postgresGeojson)

    @access.user
    def postgresGeojson(self, params):
        conn = connect_to_gryphon()
        view = View(conn)
        return view.filter(params)

    postgresGeojson.description = (
        Description('Get geojson from postgres database')
        .param('NAME', 'state name or all states', required=False,
               dataType='list')
        .param('PRODUCTION_CATEGORY', 'production category', required=False,
               dataType='list')
        .param('CATEGORY', 'category', required=False,
               dataType='list')
        .param('SUB_CATEGORY', 'category', required=False,
               dataType='list')
        .param('DATA_DERIVATION', 'data_derivation', required=False,
               dataType='list')
    ) 

```"
2c025094ef9308ba2b1a5bfee224ddcaabbf8438,dbbot.py,dbbot.py,,"#!/usr/bin/env python

import sys
import optparse
import sqlite3
from datetime import datetime
from os.path import abspath, exists, join
from xml.etree import ElementTree


def main():
    parser = _get_option_parser()
    options = _get_validated_options(parser)
    xml_tree = _get_xml_tree(options, parser)
    root_attributes = _get_root_attributes(xml_tree)
    db = RobotDatabase(options)
    db.push(('INSERT INTO test_run (generated_at, generator) VALUES (?, ?)', root_attributes))
    db.commit()

def _exit_with_help(parser, message=None):
    if message:
        sys.stderr.write('Error: %s\n\n' % message)
    parser.print_help()
    exit(1)

def _get_option_parser():
    parser = optparse.OptionParser()
    parser.add_option('--file', dest='file_path')
    parser.add_option('--db', dest='db_file_path', default='results.db')
    return parser

def _get_xml_tree(options, parser):
    try:
        xml_tree = ElementTree.parse(options.file_path)
    except ElementTree.ParseError:
        _exit_with_help(parser, 'Invalid XML file')
    return xml_tree

def _get_formatted_timestamp(root_element):
    generated_at = root_element.get('generated').split('.')[0]
    return datetime.strptime(generated_at, '%Y%m%d %H:%M:%S')

def _get_root_attributes(xml_tree):
    root_element = xml_tree.getroot()
    return (
        _get_formatted_timestamp(root_element),
        root_element.get('generator')
    )

def _get_validated_options(parser):
    if len(sys.argv) < 2:
        _exit_with_help(parser)
    options, args = parser.parse_args()
    if args:
        _exit_with_help(parser)
    if not exists(options.file_path):
        _exit_with_help(parser, 'File not found')
    return options


class RobotDatabase(object):

    def __init__(self, options):
        self.sql_statements = []
        self.options = options
        self._init_tables()

    def _init_tables(self):
        self.push(
            '''CREATE TABLE if not exists test_run (id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    generated_at TEXT,
                                                    generator TEXT)''')
        self.commit()

    def push(self, *sql_statements):
        for statement in sql_statements:
            self.sql_statements.append(statement)

    def commit(self):
        connection = sqlite3.connect(self.options.db_file_path)
        cursor = connection.cursor()
        for statement in self.sql_statements:
            if isinstance(statement, basestring):
                cursor.execute(statement)
            else:
                cursor.execute(*statement)
            connection.commit()
        self.sql_statements = []
        connection.close()

if __name__ == '__main__':
    main()
",Insert test run results to sqlite3 database,"Insert test run results to sqlite3 database
",Python,apache-2.0,robotframework/DbBot,93,"```python
#!/usr/bin/env python

import sys
import optparse
import sqlite3
from datetime import datetime
from os.path import abspath, exists, join
from xml.etree import ElementTree


def main():
    parser = _get_option_parser()
    options = _get_validated_options(parser)
    xml_tree = _get_xml_tree(options, parser)
    root_attributes = _get_root_attributes(xml_tree)
    db = RobotDatabase(options)
    db.push(('INSERT INTO test_run (generated_at, generator) VALUES (?, ?)', root_attributes))
    db.commit()

def _exit_with_help(parser, message=None):
    if message:
        sys.stderr.write('Error: %s\n\n' % message)
    parser.print_help()
    exit(1)

def _get_option_parser():
    parser = optparse.OptionParser()
    parser.add_option('--file', dest='file_path')
    parser.add_option('--db', dest='db_file_path', default='results.db')
    return parser

def _get_xml_tree(options, parser):
    try:
        xml_tree = ElementTree.parse(options.file_path)
    except ElementTree.ParseError:
        _exit_with_help(parser, 'Invalid XML file')
    return xml_tree

def _get_formatted_timestamp(root_element):
    generated_at = root_element.get('generated').split('.')[0]
    return datetime.strptime(generated_at, '%Y%m%d %H:%M:%S')

def _get_root_attributes(xml_tree):
    root_element = xml_tree.getroot()
    return (
        _get_formatted_timestamp(root_element),
        root_element.get('generator')
    )

def _get_validated_options(parser):
    if len(sys.argv) < 2:
        _exit_with_help(parser)
    options, args = parser.parse_args()
    if args:
        _exit_with_help(parser)
    if not exists(options.file_path):
        _exit_with_help(parser, 'File not found')
    return options


class RobotDatabase(object):

    def __init__(self, options):
        self.sql_statements = []
        self.options = options
        self._init_tables()

    def _init_tables(self):
        self.push(
            '''CREATE TABLE if not exists test_run (id INTEGER PRIMARY KEY AUTOINCREMENT,
                                                    generated_at TEXT,
                                                    generator TEXT)''')
        self.commit()

    def push(self, *sql_statements):
        for statement in sql_statements:
            self.sql_statements.append(statement)

    def commit(self):
        connection = sqlite3.connect(self.options.db_file_path)
        cursor = connection.cursor()
        for statement in self.sql_statements:
            if isinstance(statement, basestring):
                cursor.execute(statement)
            else:
                cursor.execute(*statement)
            connection.commit()
        self.sql_statements = []
        connection.close()

if __name__ == '__main__':
    main()

```"
e53d101827491bcee3fffa99314cbe9561884cb2,librisxl-tools/scripts/crunch-lddb-lines.py,librisxl-tools/scripts/crunch-lddb-lines.py,,"from __future__ import print_function
import json
import sys
import re

def parse_select(sel_str):
    steps = []
    match_rule = None
    for word in sel_str.strip().split(' '):
        if match_rule:
            if match_rule == '=':
                steps.append(lambda data, word=word: data == word)
            elif match_rule == '=~':
                matcher = re.compile(word)
                steps.append(lambda data: isinstance(data, unicode) and matcher.match(data))
            match_rule = None
            continue

        match_rule = None
        if word == '{':
            parent_steps, test_steps = steps, []
            steps = test_steps
        elif word == '}':
            parent_steps.append(lambda data, steps=test_steps: match_selector(steps, data))
            steps = parent_steps
            parent_steps, test_steps = None, None
        elif word in {'=', '=~'}:
            match_rule = word
        elif word.isdigit():
            steps.append(int(word))
        else:
            steps.append(word)

    return steps

def match_selector(selector, data):
    current = data
    for i, step in enumerate(selector):
        if callable(step):
            if step(current):
                continue
            else:
                return False

        if isinstance(step, int):
            current = current[step]
        else:
            if isinstance(current, list):
                selector_trail = selector[i:]
                for item in current:
                    if match_selector(selector_trail, item):
                        return item

                return False
            else:
                current = current.get(step)
        if current is None:
            return False

    return current

if __name__ == '__main__':
    args = sys.argv[1:]

    selector = parse_select(args.pop(0)) if args else None
    match_count = 0

    for i, l in enumerate(sys.stdin):
        if not l.rstrip():
            continue
        l = l.replace(b'\\\\""', b'\\""')
        if i % 100000 == 0:
            print(""At line"", i, file=sys.stderr)
        try:
            data = json.loads(l)
            try:
                data_id = data.get('@id') or data['@graph'][0]['@id']
            except KeyError:
                data_id = None
            if selector:
                result = match_selector(selector, data)
                if result:
                    match_count += 1
                    print(""Line"", i, ""id"", data_id, ""matched on"", json.dumps(result))
                    sys.stdout.flush()
        except ValueError as e:
            print(""ERROR at"", i, ""in data:"", file=sys.stderr)
            print(l, file=sys.stderr)
            print(e, file=sys.stderr)
            for char_index in re.findall(r'.+\(char (\d+)\)$', e.message):
                char_index = int(char_index)
                print(l[char_index - 20 if char_index > 20 else char_index : char_index + 20], file=sys.stderr)
                print(('-' * 20) + '^', file=sys.stderr)
                break

    if selector:
        print(""Total matches:"", match_count)
",Add simple crunch and query LDDB script,"Add simple crunch and query LDDB script
",Python,apache-2.0,"libris/librisxl,libris/librisxl,libris/librisxl",98,"```python
from __future__ import print_function
import json
import sys
import re

def parse_select(sel_str):
    steps = []
    match_rule = None
    for word in sel_str.strip().split(' '):
        if match_rule:
            if match_rule == '=':
                steps.append(lambda data, word=word: data == word)
            elif match_rule == '=~':
                matcher = re.compile(word)
                steps.append(lambda data: isinstance(data, unicode) and matcher.match(data))
            match_rule = None
            continue

        match_rule = None
        if word == '{':
            parent_steps, test_steps = steps, []
            steps = test_steps
        elif word == '}':
            parent_steps.append(lambda data, steps=test_steps: match_selector(steps, data))
            steps = parent_steps
            parent_steps, test_steps = None, None
        elif word in {'=', '=~'}:
            match_rule = word
        elif word.isdigit():
            steps.append(int(word))
        else:
            steps.append(word)

    return steps

def match_selector(selector, data):
    current = data
    for i, step in enumerate(selector):
        if callable(step):
            if step(current):
                continue
            else:
                return False

        if isinstance(step, int):
            current = current[step]
        else:
            if isinstance(current, list):
                selector_trail = selector[i:]
                for item in current:
                    if match_selector(selector_trail, item):
                        return item

                return False
            else:
                current = current.get(step)
        if current is None:
            return False

    return current

if __name__ == '__main__':
    args = sys.argv[1:]

    selector = parse_select(args.pop(0)) if args else None
    match_count = 0

    for i, l in enumerate(sys.stdin):
        if not l.rstrip():
            continue
        l = l.replace(b'\\\\""', b'\\""')
        if i % 100000 == 0:
            print(""At line"", i, file=sys.stderr)
        try:
            data = json.loads(l)
            try:
                data_id = data.get('@id') or data['@graph'][0]['@id']
            except KeyError:
                data_id = None
            if selector:
                result = match_selector(selector, data)
                if result:
                    match_count += 1
                    print(""Line"", i, ""id"", data_id, ""matched on"", json.dumps(result))
                    sys.stdout.flush()
        except ValueError as e:
            print(""ERROR at"", i, ""in data:"", file=sys.stderr)
            print(l, file=sys.stderr)
            print(e, file=sys.stderr)
            for char_index in re.findall(r'.+\(char (\d+)\)$', e.message):
                char_index = int(char_index)
                print(l[char_index - 20 if char_index > 20 else char_index : char_index + 20], file=sys.stderr)
                print(('-' * 20) + '^', file=sys.stderr)
                break

    if selector:
        print(""Total matches:"", match_count)

```"
23696a4ba6b721248d10957fb70c2b9bd6433b84,tools/gyp-explain.py,tools/gyp-explain.py,,"#!/usr/bin/env python
# Copyright (c) 2011 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Prints paths between gyp targets.
""""""

import json
import os
import sys
import time

from collections import deque

def usage():
  print """"""\
Usage:
  tools/gyp-explain.py chrome_dll gtest#
""""""


def GetPath(graph, fro, to):
  """"""Given a graph in (node -> list of successor nodes) dictionary format,
  yields all paths from |fro| to |to|, starting with the shortest.""""""
  # Storing full paths in the queue is a bit wasteful, but good enough for this.
  q = deque([(fro, [])])
  while q:
    t, path = q.popleft()
    if t == to:
      yield path + [t]
    for d in graph[t]:
      q.append((d, path + [t]))


def MatchNode(graph, substring):
  """"""Given a dictionary, returns the key that matches |substring| best. Exits
  if there's not one single best match.""""""
  candidates = []
  for target in graph:
    if substring in target:
      candidates.append(target)

  if not candidates:
    print 'No targets match ""%s""' % substring
    sys.exit(1)
  if len(candidates) > 1:
    print 'More than one target matches ""%s"": %s' % (
        substring, ' '.join(candidates))
    sys.exit(1)
  return candidates[0]


def Main(argv):
  # Check that dump.json exists and that it's not too old.
  dump_json_dirty = False
  try:
    st = os.stat('dump.json')
    file_age_s = time.time() - st.st_mtime
    if file_age_s > 2 * 60 * 60:
      print 'dump.json is more than 2 hours old.'
      dump_json_dirty = True
  except IOError:
    print 'dump.json not found.'
    dump_json_dirty = True

  if dump_json_dirty:
    print 'Run'
    print '    GYP_GENERATORS=dump_dependency_json build/gyp_chromium'
    print 'first, then try again.'
    sys.exit(1)

  g = json.load(open('dump.json'))

  if len(argv) != 3:
    usage()
    sys.exit(1)

  fro = MatchNode(g, argv[1])
  to = MatchNode(g, argv[2])

  paths = list(GetPath(g, fro, to))
  if len(paths) > 0:
    print 'These paths lead from %s to %s:' % (fro, to)
    for path in paths:
      print ' -> '.join(path)
  else:
    print 'No paths found from %s to %s.' % (fro, to)


if __name__ == '__main__':
  Main(sys.argv)
","Add a small tool to answer questions like ""Why does target A depend on target B"".","Add a small tool to answer questions like ""Why does target A depend on target B"".

BUG=none
TEST=none

Review URL: http://codereview.chromium.org/8672006

git-svn-id: de016e52bd170d2d4f2344f9bf92d50478b649e0@111430 0039d316-1c4b-4281-b951-d872f2087c98
",Python,bsd-3-clause,"M4sse/chromium.src,axinging/chromium-crosswalk,dednal/chromium.src,dushu1203/chromium.src,ltilve/chromium,jaruba/chromium.src,mogoweb/chromium-crosswalk,junmin-zhu/chromium-rivertrail,mogoweb/chromium-crosswalk,nacl-webkit/chrome_deps,Jonekee/chromium.src,TheTypoMaster/chromium-crosswalk,nacl-webkit/chrome_deps,ondra-novak/chromium.src,Chilledheart/chromium,krieger-od/nwjs_chromium.src,hgl888/chromium-crosswalk,hgl888/chromium-crosswalk-efl,M4sse/chromium.src,ondra-novak/chromium.src,Jonekee/chromium.src,dushu1203/chromium.src,keishi/chromium,TheTypoMaster/chromium-crosswalk,rogerwang/chromium,dushu1203/chromium.src,Just-D/chromium-1,anirudhSK/chromium,bright-sparks/chromium-spacewalk,timopulkkinen/BubbleFish,littlstar/chromium.src,markYoungH/chromium.src,dednal/chromium.src,mohamed--abdel-maksoud/chromium.src,pozdnyakov/chromium-crosswalk,keishi/chromium,keishi/chromium,Jonekee/chromium.src,Jonekee/chromium.src,mohamed--abdel-maksoud/chromium.src,rogerwang/chromium,Just-D/chromium-1,Chilledheart/chromium,mohamed--abdel-maksoud/chromium.src,ChromiumWebApps/chromium,M4sse/chromium.src,dushu1203/chromium.src,hujiajie/pa-chromium,Pluto-tv/chromium-crosswalk,pozdnyakov/chromium-crosswalk,jaruba/chromium.src,hujiajie/pa-chromium,patrickm/chromium.src,hujiajie/pa-chromium,PeterWangIntel/chromium-crosswalk,crosswalk-project/chromium-crosswalk-efl,nacl-webkit/chrome_deps,patrickm/chromium.src,ltilve/chromium,crosswalk-project/chromium-crosswalk-efl,ChromiumWebApps/chromium,M4sse/chromium.src,hgl888/chromium-crosswalk,anirudhSK/chromium,Chilledheart/chromium,keishi/chromium,jaruba/chromium.src,Pluto-tv/chromium-crosswalk,dushu1203/chromium.src,mohamed--abdel-maksoud/chromium.src,zcbenz/cefode-chromium,Fireblend/chromium-crosswalk,Just-D/chromium-1,nacl-webkit/chrome_deps,dednal/chromium.src,PeterWangIntel/chromium-crosswalk,markYoungH/chromium.src,PeterWangIntel/chromium-crosswalk,Jonekee/chromium.src,markYoungH/chromium.src,Fireblend/chromium-crosswalk,crosswalk-project/chromium-crosswalk-efl,markYoungH/chromium.src,dushu1203/chromium.src,krieger-od/nwjs_chromium.src,littlstar/chromium.src,crosswalk-project/chromium-crosswalk-efl,junmin-zhu/chromium-rivertrail,dushu1203/chromium.src,robclark/chromium,krieger-od/nwjs_chromium.src,hgl888/chromium-crosswalk-efl,chuan9/chromium-crosswalk,bright-sparks/chromium-spacewalk,crosswalk-project/chromium-crosswalk-efl,anirudhSK/chromium,robclark/chromium,robclark/chromium,pozdnyakov/chromium-crosswalk,timopulkkinen/BubbleFish,zcbenz/cefode-chromium,krieger-od/nwjs_chromium.src,TheTypoMaster/chromium-crosswalk,ChromiumWebApps/chromium,pozdnyakov/chromium-crosswalk,fujunwei/chromium-crosswalk,robclark/chromium,rogerwang/chromium,hgl888/chromium-crosswalk,ChromiumWebApps/chromium,Chilledheart/chromium,mohamed--abdel-maksoud/chromium.src,krieger-od/nwjs_chromium.src,anirudhSK/chromium,jaruba/chromium.src,Pluto-tv/chromium-crosswalk,Fireblend/chromium-crosswalk,hgl888/chromium-crosswalk-efl,Pluto-tv/chromium-crosswalk,rogerwang/chromium,mogoweb/chromium-crosswalk,jaruba/chromium.src,Chilledheart/chromium,M4sse/chromium.src,keishi/chromium,mohamed--abdel-maksoud/chromium.src,pozdnyakov/chromium-crosswalk,timopulkkinen/BubbleFish,nacl-webkit/chrome_deps,Fireblend/chromium-crosswalk,hgl888/chromium-crosswalk-efl,TheTypoMaster/chromium-crosswalk,markYoungH/chromium.src,chuan9/chromium-crosswalk,M4sse/chromium.src,ChromiumWebApps/chromium,dednal/chromium.src,zcbenz/cefode-chromium,pozdnyakov/chromium-crosswalk,dednal/chromium.src,bright-sparks/chromium-spacewalk,ltilve/chromium,Jonekee/chromium.src,robclark/chromium,fujunwei/chromium-crosswalk,jaruba/chromium.src,dednal/chromium.src,ondra-novak/chromium.src,patrickm/chromium.src,ondra-novak/chromium.src,PeterWangIntel/chromium-crosswalk,ondra-novak/chromium.src,junmin-zhu/chromium-rivertrail,PeterWangIntel/chromium-crosswalk,hujiajie/pa-chromium,robclark/chromium,PeterWangIntel/chromium-crosswalk,jaruba/chromium.src,jaruba/chromium.src,rogerwang/chromium,nacl-webkit/chrome_deps,hgl888/chromium-crosswalk,Jonekee/chromium.src,Jonekee/chromium.src,Fireblend/chromium-crosswalk,chuan9/chromium-crosswalk,PeterWangIntel/chromium-crosswalk,junmin-zhu/chromium-rivertrail,ondra-novak/chromium.src,junmin-zhu/chromium-rivertrail,robclark/chromium,nacl-webkit/chrome_deps,chuan9/chromium-crosswalk,ondra-novak/chromium.src,hgl888/chromium-crosswalk-efl,dushu1203/chromium.src,Pluto-tv/chromium-crosswalk,crosswalk-project/chromium-crosswalk-efl,hgl888/chromium-crosswalk,hgl888/chromium-crosswalk,zcbenz/cefode-chromium,TheTypoMaster/chromium-crosswalk,timopulkkinen/BubbleFish,axinging/chromium-crosswalk,Jonekee/chromium.src,dednal/chromium.src,zcbenz/cefode-chromium,patrickm/chromium.src,littlstar/chromium.src,zcbenz/cefode-chromium,hgl888/chromium-crosswalk-efl,hgl888/chromium-crosswalk,dednal/chromium.src,axinging/chromium-crosswalk,hgl888/chromium-crosswalk,dednal/chromium.src,Pluto-tv/chromium-crosswalk,anirudhSK/chromium,Chilledheart/chromium,M4sse/chromium.src,Just-D/chromium-1,littlstar/chromium.src,hujiajie/pa-chromium,ChromiumWebApps/chromium,Chilledheart/chromium,Pluto-tv/chromium-crosswalk,markYoungH/chromium.src,TheTypoMaster/chromium-crosswalk,ChromiumWebApps/chromium,markYoungH/chromium.src,Just-D/chromium-1,dednal/chromium.src,TheTypoMaster/chromium-crosswalk,pozdnyakov/chromium-crosswalk,M4sse/chromium.src,pozdnyakov/chromium-crosswalk,nacl-webkit/chrome_deps,ondra-novak/chromium.src,anirudhSK/chromium,timopulkkinen/BubbleFish,junmin-zhu/chromium-rivertrail,Pluto-tv/chromium-crosswalk,dednal/chromium.src,TheTypoMaster/chromium-crosswalk,axinging/chromium-crosswalk,timopulkkinen/BubbleFish,bright-sparks/chromium-spacewalk,Fireblend/chromium-crosswalk,timopulkkinen/BubbleFish,ltilve/chromium,jaruba/chromium.src,bright-sparks/chromium-spacewalk,hujiajie/pa-chromium,mohamed--abdel-maksoud/chromium.src,anirudhSK/chromium,Just-D/chromium-1,fujunwei/chromium-crosswalk,ChromiumWebApps/chromium,anirudhSK/chromium,junmin-zhu/chromium-rivertrail,rogerwang/chromium,fujunwei/chromium-crosswalk,hgl888/chromium-crosswalk-efl,junmin-zhu/chromium-rivertrail,mogoweb/chromium-crosswalk,pozdnyakov/chromium-crosswalk,hujiajie/pa-chromium,axinging/chromium-crosswalk,hgl888/chromium-crosswalk-efl,Fireblend/chromium-crosswalk,bright-sparks/chromium-spacewalk,mogoweb/chromium-crosswalk,ltilve/chromium,pozdnyakov/chromium-crosswalk,hujiajie/pa-chromium,Just-D/chromium-1,junmin-zhu/chromium-rivertrail,Chilledheart/chromium,keishi/chromium,ChromiumWebApps/chromium,hujiajie/pa-chromium,keishi/chromium,junmin-zhu/chromium-rivertrail,ChromiumWebApps/chromium,chuan9/chromium-crosswalk,markYoungH/chromium.src,fujunwei/chromium-crosswalk,krieger-od/nwjs_chromium.src,crosswalk-project/chromium-crosswalk-efl,littlstar/chromium.src,TheTypoMaster/chromium-crosswalk,mogoweb/chromium-crosswalk,nacl-webkit/chrome_deps,chuan9/chromium-crosswalk,rogerwang/chromium,robclark/chromium,junmin-zhu/chromium-rivertrail,timopulkkinen/BubbleFish,anirudhSK/chromium,anirudhSK/chromium,krieger-od/nwjs_chromium.src,keishi/chromium,patrickm/chromium.src,hujiajie/pa-chromium,krieger-od/nwjs_chromium.src,timopulkkinen/BubbleFish,jaruba/chromium.src,rogerwang/chromium,Just-D/chromium-1,mogoweb/chromium-crosswalk,bright-sparks/chromium-spacewalk,crosswalk-project/chromium-crosswalk-efl,PeterWangIntel/chromium-crosswalk,ltilve/chromium,patrickm/chromium.src,pozdnyakov/chromium-crosswalk,littlstar/chromium.src,markYoungH/chromium.src,bright-sparks/chromium-spacewalk,dushu1203/chromium.src,M4sse/chromium.src,Jonekee/chromium.src,Just-D/chromium-1,fujunwei/chromium-crosswalk,dushu1203/chromium.src,axinging/chromium-crosswalk,keishi/chromium,bright-sparks/chromium-spacewalk,zcbenz/cefode-chromium,zcbenz/cefode-chromium,markYoungH/chromium.src,hgl888/chromium-crosswalk-efl,littlstar/chromium.src,zcbenz/cefode-chromium,ChromiumWebApps/chromium,robclark/chromium,axinging/chromium-crosswalk,Chilledheart/chromium,krieger-od/nwjs_chromium.src,zcbenz/cefode-chromium,PeterWangIntel/chromium-crosswalk,Jonekee/chromium.src,axinging/chromium-crosswalk,krieger-od/nwjs_chromium.src,fujunwei/chromium-crosswalk,jaruba/chromium.src,timopulkkinen/BubbleFish,rogerwang/chromium,M4sse/chromium.src,robclark/chromium,rogerwang/chromium,hujiajie/pa-chromium,hgl888/chromium-crosswalk,ondra-novak/chromium.src,markYoungH/chromium.src,mogoweb/chromium-crosswalk,anirudhSK/chromium,M4sse/chromium.src,ltilve/chromium,mohamed--abdel-maksoud/chromium.src,timopulkkinen/BubbleFish,fujunwei/chromium-crosswalk,ChromiumWebApps/chromium,mohamed--abdel-maksoud/chromium.src,nacl-webkit/chrome_deps,patrickm/chromium.src,chuan9/chromium-crosswalk,axinging/chromium-crosswalk,anirudhSK/chromium,zcbenz/cefode-chromium,keishi/chromium,mohamed--abdel-maksoud/chromium.src,nacl-webkit/chrome_deps,axinging/chromium-crosswalk,patrickm/chromium.src,krieger-od/nwjs_chromium.src,patrickm/chromium.src,ltilve/chromium,keishi/chromium,crosswalk-project/chromium-crosswalk-efl,axinging/chromium-crosswalk,hgl888/chromium-crosswalk-efl,mogoweb/chromium-crosswalk,littlstar/chromium.src,mogoweb/chromium-crosswalk,Fireblend/chromium-crosswalk,ltilve/chromium,chuan9/chromium-crosswalk,Pluto-tv/chromium-crosswalk,fujunwei/chromium-crosswalk,dushu1203/chromium.src,Fireblend/chromium-crosswalk,mohamed--abdel-maksoud/chromium.src,chuan9/chromium-crosswalk",93,"```python
#!/usr/bin/env python
# Copyright (c) 2011 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

""""""Prints paths between gyp targets.
""""""

import json
import os
import sys
import time

from collections import deque

def usage():
  print """"""\
Usage:
  tools/gyp-explain.py chrome_dll gtest#
""""""


def GetPath(graph, fro, to):
  """"""Given a graph in (node -> list of successor nodes) dictionary format,
  yields all paths from |fro| to |to|, starting with the shortest.""""""
  # Storing full paths in the queue is a bit wasteful, but good enough for this.
  q = deque([(fro, [])])
  while q:
    t, path = q.popleft()
    if t == to:
      yield path + [t]
    for d in graph[t]:
      q.append((d, path + [t]))


def MatchNode(graph, substring):
  """"""Given a dictionary, returns the key that matches |substring| best. Exits
  if there's not one single best match.""""""
  candidates = []
  for target in graph:
    if substring in target:
      candidates.append(target)

  if not candidates:
    print 'No targets match ""%s""' % substring
    sys.exit(1)
  if len(candidates) > 1:
    print 'More than one target matches ""%s"": %s' % (
        substring, ' '.join(candidates))
    sys.exit(1)
  return candidates[0]


def Main(argv):
  # Check that dump.json exists and that it's not too old.
  dump_json_dirty = False
  try:
    st = os.stat('dump.json')
    file_age_s = time.time() - st.st_mtime
    if file_age_s > 2 * 60 * 60:
      print 'dump.json is more than 2 hours old.'
      dump_json_dirty = True
  except IOError:
    print 'dump.json not found.'
    dump_json_dirty = True

  if dump_json_dirty:
    print 'Run'
    print '    GYP_GENERATORS=dump_dependency_json build/gyp_chromium'
    print 'first, then try again.'
    sys.exit(1)

  g = json.load(open('dump.json'))

  if len(argv) != 3:
    usage()
    sys.exit(1)

  fro = MatchNode(g, argv[1])
  to = MatchNode(g, argv[2])

  paths = list(GetPath(g, fro, to))
  if len(paths) > 0:
    print 'These paths lead from %s to %s:' % (fro, to)
    for path in paths:
      print ' -> '.join(path)
  else:
    print 'No paths found from %s to %s.' % (fro, to)


if __name__ == '__main__':
  Main(sys.argv)

```"
4c3f4b5a19832d965714997652ad245ac8abf310,animal.py,animal.py,,"#!/usr/bin/python3

import sys
import json

TEXT_FILE_PATH = './text.json'
messages = None

def traverse(parent, path):
    node = parent[path]

    if (isinstance(node, str)):
        if not ask_yes_no_question('{}{}'.format(messages.get('isItA'), node)):
            animal = input(messages.get('itWas'))
            question = input('{}{}{}{}: '.format(
                messages.get('differ'),
                animal,
                messages.get('fromA'),
                node
            ))

            parent[path] = [question, animal, node]

        print(messages.get('again'))
        return
    else:
        if ask_yes_no_question(node[0]):
            traverse(node, 1)
        else:
            traverse(node, 2)

def show_known_animals():
    nodes_to_visit = [messages.get('data')]
    result = set()

    while nodes_to_visit:
        node = nodes_to_visit.pop(0)

        if isinstance(node[1], str):
            result.add(node[1])
        else:
            nodes_to_visit.append(node[1])

        if isinstance(node[2], str):
            result.add(node[2])
        else:
            nodes_to_visit.append(node[2])

    print(messages.get('known'), ', '.join(result))

def exit_game():
    print(messages.get('exit'))
    sys.exit()

def ask_yes_no_question(question):
    while True:
      answer = input('{} ? '.format(question)).upper()
      if answer in ['Y', 'YES', 'TRUE']:
        return True
      elif answer in ['N', 'NO', 'FALSE']:
        return False

def handle_top_command(answer):
    if answer in ['L', 'LIST']:
        show_known_animals()
    elif answer in ['Y', 'YES']:
        traverse(messages, 'data')
    elif answer in ['N', 'NO']:
        exit_game()

def start():
    print(messages.get('start'))

    while True:
        answer = input(messages.get('mood'))
        handle_top_command(answer.upper())

def load_text():
    global messages
    with open(TEXT_FILE_PATH) as f:
        messages = json.load(f);

def main():
    load_text()
    start()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        sys.exit()
",Implement 'Guess The Animal' game in Python 3,"Implement 'Guess The Animal' game in Python 3

Resolves #6
",Python,mit,"drom/animal,drom/animal,drom/animal,drom/animal,drom/animal,drom/animal",92,"```python
#!/usr/bin/python3

import sys
import json

TEXT_FILE_PATH = './text.json'
messages = None

def traverse(parent, path):
    node = parent[path]

    if (isinstance(node, str)):
        if not ask_yes_no_question('{}{}'.format(messages.get('isItA'), node)):
            animal = input(messages.get('itWas'))
            question = input('{}{}{}{}: '.format(
                messages.get('differ'),
                animal,
                messages.get('fromA'),
                node
            ))

            parent[path] = [question, animal, node]

        print(messages.get('again'))
        return
    else:
        if ask_yes_no_question(node[0]):
            traverse(node, 1)
        else:
            traverse(node, 2)

def show_known_animals():
    nodes_to_visit = [messages.get('data')]
    result = set()

    while nodes_to_visit:
        node = nodes_to_visit.pop(0)

        if isinstance(node[1], str):
            result.add(node[1])
        else:
            nodes_to_visit.append(node[1])

        if isinstance(node[2], str):
            result.add(node[2])
        else:
            nodes_to_visit.append(node[2])

    print(messages.get('known'), ', '.join(result))

def exit_game():
    print(messages.get('exit'))
    sys.exit()

def ask_yes_no_question(question):
    while True:
      answer = input('{} ? '.format(question)).upper()
      if answer in ['Y', 'YES', 'TRUE']:
        return True
      elif answer in ['N', 'NO', 'FALSE']:
        return False

def handle_top_command(answer):
    if answer in ['L', 'LIST']:
        show_known_animals()
    elif answer in ['Y', 'YES']:
        traverse(messages, 'data')
    elif answer in ['N', 'NO']:
        exit_game()

def start():
    print(messages.get('start'))

    while True:
        answer = input(messages.get('mood'))
        handle_top_command(answer.upper())

def load_text():
    global messages
    with open(TEXT_FILE_PATH) as f:
        messages = json.load(f);

def main():
    load_text()
    start()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        sys.exit()

```"
a79c82b58443082827d8817ec15ae1283e5c0e21,tests/test_string_template.py,tests/test_string_template.py,,"#!/usr/bin/env python

from __future__ import print_function

import unittest


import engines


HANDLE = 'string.Template'


class TestStringTemplate(unittest.TestCase):

    def test_valid_engine(self):
        self.assertIn(HANDLE, engines.engines)
        engine = engines.engines[HANDLE]
        assert issubclass(engine, engines.Engine)

    def test_escape(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Several escaped dollar signs:\n'
                '$$ $$ $$$$$$\n',
            )

        result = template.apply({
                'random':  'value',
                '$':       'provocation',
            })

        self.assertMultiLineEqual(result,
                'Several escaped dollar signs:\n'
                '$ $ $$$\n'
            )
    
    def test_plain_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '$essen mit\n'
                '$beilage.\n',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                'Szegediner Gulasch mit\n'
                'Kartoffeln.\n'
            )
    
    def test_curly_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                'Szegediner Gulasch mit\n'
                'Kartoffeln.\n'
            )

    def test_strict_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '$essen mit\n'
                '${beilage}.\n',
            )

        self.assertRaises(Exception, template.apply, ({
                'random':  'value',
            }))

    def test_tolerant_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '$essen mit\n'
                '${beilage}.\n',
                tolerant=True,
            )

        result = template.apply({
                'random':  'value',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                '$essen mit\n'
                '${beilage}.\n'
            )


if __name__ == '__main__':
    unittest.main()
",Add tests for string.Template engine.,"Add tests for string.Template engine.
",Python,mit,blubberdiblub/eztemplate,117,"```python
#!/usr/bin/env python

from __future__ import print_function

import unittest


import engines


HANDLE = 'string.Template'


class TestStringTemplate(unittest.TestCase):

    def test_valid_engine(self):
        self.assertIn(HANDLE, engines.engines)
        engine = engines.engines[HANDLE]
        assert issubclass(engine, engines.Engine)

    def test_escape(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Several escaped dollar signs:\n'
                '$$ $$ $$$$$$\n',
            )

        result = template.apply({
                'random':  'value',
                '$':       'provocation',
            })

        self.assertMultiLineEqual(result,
                'Several escaped dollar signs:\n'
                '$ $ $$$\n'
            )
    
    def test_plain_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '$essen mit\n'
                '$beilage.\n',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                'Szegediner Gulasch mit\n'
                'Kartoffeln.\n'
            )
    
    def test_curly_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '${essen} mit\n'
                '${beilage}.\n',
            )

        result = template.apply({
                'random':  'value',
                'essen':   'Szegediner Gulasch',
                'beilage': 'Kartoffeln',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                'Szegediner Gulasch mit\n'
                'Kartoffeln.\n'
            )

    def test_strict_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '$essen mit\n'
                '${beilage}.\n',
            )

        self.assertRaises(Exception, template.apply, ({
                'random':  'value',
            }))

    def test_tolerant_template_missing_identifier(self):
        engine = engines.engines[HANDLE]

        template = engine(
                'Heute gibt es\n'
                '$essen mit\n'
                '${beilage}.\n',
                tolerant=True,
            )

        result = template.apply({
                'random':  'value',
            })

        self.assertMultiLineEqual(result,
                'Heute gibt es\n'
                '$essen mit\n'
                '${beilage}.\n'
            )


if __name__ == '__main__':
    unittest.main()

```"
2e7bff01ce3b15b943691a82e766ceffda0cc355,fourth_edition/ch2_linked_lists/python/2.1.py,fourth_edition/ch2_linked_lists/python/2.1.py,,"'''
Write code to remove duplicates from an unsorted linked list.
FOLLOW UP
How would you solve this problem if a temporary buffer is not allowed?
'''


class Node(object):
    def __init__(self, data=None, next_node=None):
        self.data = data
        self.next_node = next_node

    def get_data(self):
        return self.data

    def set_data(self, data):
        self.data = data

    def get_next_node(self):
        return self.next_node

    def set_next_node(self, next_node):
        self.next_node = next_node

dup_data = dict()


class LinkedList(object):
    def __init__(self, head=None):
        self.head = head

    def insert_node(self, data):
        new_node = Node(data, self.head)
        self.head = new_node
        # print(data, "" inserted!"")

    def traversal(self):
        curr_node = self.head
        while curr_node:
            data_ = curr_node.get_data()
            print(data_, end="" "")
            curr_node = curr_node.get_next_node()
        print("""")

    def build_dup_hashset(self):
        global dup_data
        curr_node = self.head
        while curr_node:
            data_ = curr_node.get_data()
            if data_ in dup_data:
                count = dup_data[data_]
                dup_data[data_] = count + 1
            else:
                dup_data[data_] = 1
            curr_node = curr_node.get_next_node()

    def del_node(self, data):
        curr_node = self.head
        prev_node = None

        while curr_node:
            if curr_node.get_data() == data:
                if prev_node:
                    prev_node.set_next_node(curr_node.get_next_node())
                else:
                    self.head = curr_node.get_next_node()
                print(data, "" deleted!"")
                return
            else:
                prev_node = curr_node
                curr_node = curr_node.get_next_node()

    def del_dups(self):
        global dup_data
        self.build_dup_hashset()
        for k, v in dup_data.items():
            if v > 1:
                for _ in range(v-1):
                    self.del_node(k)
                    dup_data[k] = v - 1


myLL = LinkedList()

myLL.insert_node(10)
myLL.insert_node(20)
myLL.insert_node(20)
myLL.insert_node(20)
myLL.insert_node(30)
myLL.insert_node(50)
myLL.insert_node(50)
myLL.insert_node(50)
myLL.insert_node(60)
myLL.insert_node(60)

print(""Traversing the original linked list"")
myLL.traversal()

print(""Deleting duplicate data"")
print(myLL.del_dups())

print(""Traversing the de-duplicated linked list"")
myLL.traversal()
",Remove duplicates from an unsorted linked list,"Remove duplicates from an unsorted linked list
",Python,mit,roommen/CtCI,104,"```python
'''
Write code to remove duplicates from an unsorted linked list.
FOLLOW UP
How would you solve this problem if a temporary buffer is not allowed?
'''


class Node(object):
    def __init__(self, data=None, next_node=None):
        self.data = data
        self.next_node = next_node

    def get_data(self):
        return self.data

    def set_data(self, data):
        self.data = data

    def get_next_node(self):
        return self.next_node

    def set_next_node(self, next_node):
        self.next_node = next_node

dup_data = dict()


class LinkedList(object):
    def __init__(self, head=None):
        self.head = head

    def insert_node(self, data):
        new_node = Node(data, self.head)
        self.head = new_node
        # print(data, "" inserted!"")

    def traversal(self):
        curr_node = self.head
        while curr_node:
            data_ = curr_node.get_data()
            print(data_, end="" "")
            curr_node = curr_node.get_next_node()
        print("""")

    def build_dup_hashset(self):
        global dup_data
        curr_node = self.head
        while curr_node:
            data_ = curr_node.get_data()
            if data_ in dup_data:
                count = dup_data[data_]
                dup_data[data_] = count + 1
            else:
                dup_data[data_] = 1
            curr_node = curr_node.get_next_node()

    def del_node(self, data):
        curr_node = self.head
        prev_node = None

        while curr_node:
            if curr_node.get_data() == data:
                if prev_node:
                    prev_node.set_next_node(curr_node.get_next_node())
                else:
                    self.head = curr_node.get_next_node()
                print(data, "" deleted!"")
                return
            else:
                prev_node = curr_node
                curr_node = curr_node.get_next_node()

    def del_dups(self):
        global dup_data
        self.build_dup_hashset()
        for k, v in dup_data.items():
            if v > 1:
                for _ in range(v-1):
                    self.del_node(k)
                    dup_data[k] = v - 1


myLL = LinkedList()

myLL.insert_node(10)
myLL.insert_node(20)
myLL.insert_node(20)
myLL.insert_node(20)
myLL.insert_node(30)
myLL.insert_node(50)
myLL.insert_node(50)
myLL.insert_node(50)
myLL.insert_node(60)
myLL.insert_node(60)

print(""Traversing the original linked list"")
myLL.traversal()

print(""Deleting duplicate data"")
print(myLL.del_dups())

print(""Traversing the de-duplicated linked list"")
myLL.traversal()

```"
3e51aa99f27f5e61737cc900cbbdfe53bd8c212b,docstamp/filenames.py,docstamp/filenames.py,,"# coding=utf-8
# -------------------------------------------------------------------------------
# Author: Alexandre Manhaes Savio <alexsavio@gmail.com>
# Grupo de Inteligencia Computational <www.ehu.es/ccwintco>
# Universidad del Pais Vasco UPV/EHU
#
# 2015, Alexandre Manhaes Savio
# Use this at your own risk!
# -------------------------------------------------------------------------------

import tempfile
import os.path      as op
import logging

from   .config import get_temp_dir


log = logging.getLogger(__name__)


def get_extension(filepath, check_if_exists=False):
    """"""Return the extension of fpath.

    Parameters
    ----------
    fpath: string
    File name or path

    check_if_exists: bool

    Returns
    -------
    str
    The extension of the file name or path
    """"""
    if check_if_exists:
        if not op.exists(filepath):
            err = 'File not found: ' + filepath
            log.error(err)
            raise IOError(err)

    try:
        rest, ext = op.splitext(filepath)
    except:
        raise
    else:
        return ext


def add_extension_if_needed(filepath, ext, check_if_exists=False):
    """"""Add the extension ext to fpath if it doesn't have it.

    Parameters
    ----------
    filepath: str
    File name or path

    ext: str
    File extension

    check_if_exists: bool

    Returns
    -------
    File name or path with extension added, if needed.
    """"""
    if not filepath.endswith(ext):
        filepath += ext

    if check_if_exists:
        if not op.exists(filepath):
            err = 'File not found: ' + filepath
            log.error(err)
            raise IOError(err)

    return filepath


def remove_ext(filepath):
    """"""Removes the extension of the file.

    Parameters
    ----------
    filepath: str
        File path or name

    Returns
    -------
    str
        File path or name without extension
    """"""
    return filepath[:filepath.rindex(get_extension(filepath))]


def get_tempfile(suffix='.txt'):
    """"""
    Parameters
    ----------

    Returns
    -------
    """"""
    return tempfile.NamedTemporaryFile(suffix=suffix, dir=get_temp_dir())
",Add helpers to manage file names,"Add helpers to manage file names
",Python,apache-2.0,PythonSanSebastian/docstamp,104,"```python
# coding=utf-8
# -------------------------------------------------------------------------------
# Author: Alexandre Manhaes Savio <alexsavio@gmail.com>
# Grupo de Inteligencia Computational <www.ehu.es/ccwintco>
# Universidad del Pais Vasco UPV/EHU
#
# 2015, Alexandre Manhaes Savio
# Use this at your own risk!
# -------------------------------------------------------------------------------

import tempfile
import os.path      as op
import logging

from   .config import get_temp_dir


log = logging.getLogger(__name__)


def get_extension(filepath, check_if_exists=False):
    """"""Return the extension of fpath.

    Parameters
    ----------
    fpath: string
    File name or path

    check_if_exists: bool

    Returns
    -------
    str
    The extension of the file name or path
    """"""
    if check_if_exists:
        if not op.exists(filepath):
            err = 'File not found: ' + filepath
            log.error(err)
            raise IOError(err)

    try:
        rest, ext = op.splitext(filepath)
    except:
        raise
    else:
        return ext


def add_extension_if_needed(filepath, ext, check_if_exists=False):
    """"""Add the extension ext to fpath if it doesn't have it.

    Parameters
    ----------
    filepath: str
    File name or path

    ext: str
    File extension

    check_if_exists: bool

    Returns
    -------
    File name or path with extension added, if needed.
    """"""
    if not filepath.endswith(ext):
        filepath += ext

    if check_if_exists:
        if not op.exists(filepath):
            err = 'File not found: ' + filepath
            log.error(err)
            raise IOError(err)

    return filepath


def remove_ext(filepath):
    """"""Removes the extension of the file.

    Parameters
    ----------
    filepath: str
        File path or name

    Returns
    -------
    str
        File path or name without extension
    """"""
    return filepath[:filepath.rindex(get_extension(filepath))]


def get_tempfile(suffix='.txt'):
    """"""
    Parameters
    ----------

    Returns
    -------
    """"""
    return tempfile.NamedTemporaryFile(suffix=suffix, dir=get_temp_dir())

```"
cbf1c788e2ff492c007e17be2b1efeaac42759f1,gobble/validation.py,gobble/validation.py,,"""""""Validate data-packages""""""

from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division
from __future__ import absolute_import

from json import dumps
from datetime import datetime
from future import standard_library
from collections import OrderedDict
from datapackage import DataPackage
from jsonschema.exceptions import ValidationError

from gobble.config import VALIDATION_FEEDBACK

standard_library.install_aliases()


class Validator(object):
    """"""Validate a data-package

    The :class:``Validator`` class is a thin wrapper around the
    DataPackage class that produces a tunable, human readable report.

    :type package: :class:`DataPackage`
    :type feedback: :class:`list`
    :param feedback: choose from 'message', 'cause', 'context',
                     'validator', 'validator_value', 'path',
                     'schema_path', 'instance', 'schema', 'parent'
    """"""
    valid_feedback = {
        'message', 'cause', 'context',
        'validator', 'validator_value',
        'path', 'schema_path', 'instance',
        'schema', 'parent'
    }

    def __init__(self, package, *feedback):
        bad_package = 'Package must be DataPackage class'
        bad_feedback = 'Feedback must be %s' % str(self.valid_feedback)
        assert isinstance(package, DataPackage), bad_package
        assert set(feedback).issubset(self.valid_feedback), bad_feedback

        self._feedback = feedback or VALIDATION_FEEDBACK
        self._package = package
        self._report = OrderedDict()
        self.timestamp = str(datetime.now())

        self._run()

    @property
    def report(self):
        return self._report

    @property
    def name(self):
        return self._package.metadata['name']

    @property
    def is_valid(self):
        return self._report['is_valid']

    @property
    def result(self):
        return 'success' if self.is_valid else 'fail'

    def save(self, filepath):
        if not filepath.endswith('.json'):
            raise ValueError('Reports are JSON files')
        with open(filepath, 'w+') as file:
            file.write(dumps(self.report))

    @property
    def _package_info(self):
        for attribute in self._package.required_attributes:
            value = getattr(self._package.metadata, attribute, None)
            yield attribute, value

    @property
    def _errors(self):
        for error in self._package.iter_errors():
            for choice in self._feedback:
                yield getattr(error, choice)

    def _run(self):
        self._report.update(dict(is_valid=False, timestamp=self.timestamp))
        self._report.update(dict(package_info=dict(self._package_info)))
        try:
            self._package.validate()
            self._report.update(dict(is_valid=True))
        except ValidationError:
            self._report.update(dict(errors=list(self._errors)))

    def __repr__(self):
        return '<Validator %s: %s>' % (self.result.upper(), self.name)
",Write the Validation class (wrapper around the DataPackage class).,"Write the Validation class (wrapper around the DataPackage class).
",Python,mit,openspending/gobble,97,"```python
""""""Validate data-packages""""""

from __future__ import unicode_literals
from __future__ import print_function
from __future__ import division
from __future__ import absolute_import

from json import dumps
from datetime import datetime
from future import standard_library
from collections import OrderedDict
from datapackage import DataPackage
from jsonschema.exceptions import ValidationError

from gobble.config import VALIDATION_FEEDBACK

standard_library.install_aliases()


class Validator(object):
    """"""Validate a data-package

    The :class:``Validator`` class is a thin wrapper around the
    DataPackage class that produces a tunable, human readable report.

    :type package: :class:`DataPackage`
    :type feedback: :class:`list`
    :param feedback: choose from 'message', 'cause', 'context',
                     'validator', 'validator_value', 'path',
                     'schema_path', 'instance', 'schema', 'parent'
    """"""
    valid_feedback = {
        'message', 'cause', 'context',
        'validator', 'validator_value',
        'path', 'schema_path', 'instance',
        'schema', 'parent'
    }

    def __init__(self, package, *feedback):
        bad_package = 'Package must be DataPackage class'
        bad_feedback = 'Feedback must be %s' % str(self.valid_feedback)
        assert isinstance(package, DataPackage), bad_package
        assert set(feedback).issubset(self.valid_feedback), bad_feedback

        self._feedback = feedback or VALIDATION_FEEDBACK
        self._package = package
        self._report = OrderedDict()
        self.timestamp = str(datetime.now())

        self._run()

    @property
    def report(self):
        return self._report

    @property
    def name(self):
        return self._package.metadata['name']

    @property
    def is_valid(self):
        return self._report['is_valid']

    @property
    def result(self):
        return 'success' if self.is_valid else 'fail'

    def save(self, filepath):
        if not filepath.endswith('.json'):
            raise ValueError('Reports are JSON files')
        with open(filepath, 'w+') as file:
            file.write(dumps(self.report))

    @property
    def _package_info(self):
        for attribute in self._package.required_attributes:
            value = getattr(self._package.metadata, attribute, None)
            yield attribute, value

    @property
    def _errors(self):
        for error in self._package.iter_errors():
            for choice in self._feedback:
                yield getattr(error, choice)

    def _run(self):
        self._report.update(dict(is_valid=False, timestamp=self.timestamp))
        self._report.update(dict(package_info=dict(self._package_info)))
        try:
            self._package.validate()
            self._report.update(dict(is_valid=True))
        except ValidationError:
            self._report.update(dict(errors=list(self._errors)))

    def __repr__(self):
        return '<Validator %s: %s>' % (self.result.upper(), self.name)

```"
8c9fa6d2b31cc08212cb42ca4b429ae4a2793b70,tools/shared-packs.py,tools/shared-packs.py,,"import os
import sys
import yaml

import paste.util.multidict

possible_topdir = os.path.normpath(os.path.join(os.path.abspath(sys.argv[0]),
                                   os.pardir,
                                   os.pardir))
if os.path.exists(os.path.join(possible_topdir,
                               'devstack',
                               '__init__.py')):
    sys.path.insert(0, possible_topdir)

from devstack import log
from devstack import utils


class CustomDumper(yaml.SafeDumper):
    def ignore_aliases(self, _data):
        return True


fn = sys.argv[1]
with open(fn, ""r"") as fh:
    data = fh.read()

b = yaml.load(data)

names = set()
for c in b['components']:
    names.add(c)


idf = 'packages'
pkgs = paste.util.multidict.MultiDict()
for name in names:
    data = b['components'][name]
    #print name
    for p in data.get(idf) or []:
        pname = p['name']
        pkgs.add(pname, p)

common = list()
for pkg in sorted(list(set(pkgs.keys()))):
    items = pkgs.getall(pkg)
    if len(items) > 1:
        print(""Package dupe on: %r with %s dups"" % (pkg, len(items)))
        versions = set()
        for v in items:
            if v.get('version'):
                versions.add(str(v.get('version')))
        if len(versions) > 1:
            print(""\tWith many versions: %s"" % (versions))
        else:
            print(""\tAll with the same version %s"" % (versions))
            common.append(items[0])

idf = 'pips'
pkgs = paste.util.multidict.MultiDict()
for name in names:
    data = b['components'][name]
    for p in data.get(idf) or []:
        pname = p['name']
        pkgs.add(pname, p)

print(""-"" * 20)
common_pips = list()
for pkg in sorted(list(set(pkgs.keys()))):
    items = pkgs.getall(pkg)
    if len(items) > 1:
        print(""Pip dupe on: %r with %s dups"" % (pkg, len(items)))
        versions = set()
        for v in items:
            if v.get('version'):
                versions.add(str(v.get('version')))
        if len(versions) > 1:
            print(""\tWith many versions: %s"" % (versions))
        else:
            print(""\tAll with the same version %s"" % (versions))
            common_pips.append(items[0])

#data = {'common': {'packages': common, 'pips': common_pips}}
#formatted =  yaml.dump(data,
#                    line_break=""\n"",
#                    indent=4,
#                    explicit_start=True,
#                    explicit_end=True,
#                    default_flow_style=False,
#                    Dumper=CustomDumper,
#                    )

#print formatted
",Add tool that will show the shared packages,"Add tool that will show the shared packages
",Python,apache-2.0,"stackforge/anvil,mc2014/anvil,stackforge/anvil,mc2014/anvil",94,"```python
import os
import sys
import yaml

import paste.util.multidict

possible_topdir = os.path.normpath(os.path.join(os.path.abspath(sys.argv[0]),
                                   os.pardir,
                                   os.pardir))
if os.path.exists(os.path.join(possible_topdir,
                               'devstack',
                               '__init__.py')):
    sys.path.insert(0, possible_topdir)

from devstack import log
from devstack import utils


class CustomDumper(yaml.SafeDumper):
    def ignore_aliases(self, _data):
        return True


fn = sys.argv[1]
with open(fn, ""r"") as fh:
    data = fh.read()

b = yaml.load(data)

names = set()
for c in b['components']:
    names.add(c)


idf = 'packages'
pkgs = paste.util.multidict.MultiDict()
for name in names:
    data = b['components'][name]
    #print name
    for p in data.get(idf) or []:
        pname = p['name']
        pkgs.add(pname, p)

common = list()
for pkg in sorted(list(set(pkgs.keys()))):
    items = pkgs.getall(pkg)
    if len(items) > 1:
        print(""Package dupe on: %r with %s dups"" % (pkg, len(items)))
        versions = set()
        for v in items:
            if v.get('version'):
                versions.add(str(v.get('version')))
        if len(versions) > 1:
            print(""\tWith many versions: %s"" % (versions))
        else:
            print(""\tAll with the same version %s"" % (versions))
            common.append(items[0])

idf = 'pips'
pkgs = paste.util.multidict.MultiDict()
for name in names:
    data = b['components'][name]
    for p in data.get(idf) or []:
        pname = p['name']
        pkgs.add(pname, p)

print(""-"" * 20)
common_pips = list()
for pkg in sorted(list(set(pkgs.keys()))):
    items = pkgs.getall(pkg)
    if len(items) > 1:
        print(""Pip dupe on: %r with %s dups"" % (pkg, len(items)))
        versions = set()
        for v in items:
            if v.get('version'):
                versions.add(str(v.get('version')))
        if len(versions) > 1:
            print(""\tWith many versions: %s"" % (versions))
        else:
            print(""\tAll with the same version %s"" % (versions))
            common_pips.append(items[0])

#data = {'common': {'packages': common, 'pips': common_pips}}
#formatted =  yaml.dump(data,
#                    line_break=""\n"",
#                    indent=4,
#                    explicit_start=True,
#                    explicit_end=True,
#                    default_flow_style=False,
#                    Dumper=CustomDumper,
#                    )

#print formatted

```"
c6523a03f5a000a5b04724e8750fc1b19d1dc13d,scripts/message_prs_in_range.py,scripts/message_prs_in_range.py,,"""""""
Command-line script message pull requests in a range
""""""
from os import path
import sys
import logging
import click

# Add top-level module path to sys.path before importing tubular code.
sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))

from tubular.github_api import GitHubAPI  # pylint: disable=wrong-import-position

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
LOG = logging.getLogger(__name__)


@click.command()
@click.option(
    u'--org',
    help=u'Org from the GitHub repository URL of https://github.com/<org>/<repo>',
    default=u'edx'
)
@click.option(
    u'--repo',
    required=True,
    help=u'Repo name from the GitHub repository URL of https://github.com/<org>/<repo>'
)
@click.option(
    u'--token',
    envvar=u'GIT_TOKEN',
    required=True,
    help=u'The github access token, see https://help.github.com/articles/creating-an-access-token-for-command-line-use/'
)
@click.option(
    u'--base_sha',
    required=True,
    help=u'The BASE SHA of the range',
)
@click.option(
    u'--head_sha',
    required=True,
    help=u'The HEAD SHA of the range',
)
@click.option(
    u'--release_stage', u'message_type', flag_value=u'stage'
)
@click.option(
    u'--release_prod', u'message_type', flag_value=u'prod'
)
@click.option(
    u'--release_rollback', u'message_type', flag_value=u'rollback'
)
def message_pull_requests(org,
                          repo,
                          token,
                          base_sha,
                          head_sha,
                          message_type):
    u""""""
    Message a range of Pull requests between the BASE and HEAD SHA specified.

    Message can be one of 3 types:
    - PR on stage
    - PR on prod
    - Release canceled

    Args:
        org (str): The github organization
        repo (str): The github repository
        token (str): The authentication token
        base_sha (str): The starting SHA
        head_sha (str): The ending SHA
        message_type (str): type of message to send

    Returns:
        None
    """"""
    methods = {
        u'stage': u'message_pr_deployed_stage',
        u'prod': u'message_pr_deployed_prod',
        u'rollback': u'message_pr_release_canceled'
    }

    api = GitHubAPI(org, repo, token)
    for pull_request in api.get_pr_range(base_sha, head_sha):
        getattr(api, methods[message_type])(pull_request.number)


if __name__ == u""__main__"":
    message_pull_requests()  # pylint: disable=no-value-for-parameter
",Add script to message PRs in a range,"Add script to message PRs in a range
",Python,agpl-3.0,"eltoncarr/tubular,eltoncarr/tubular",92,"```python
""""""
Command-line script message pull requests in a range
""""""
from os import path
import sys
import logging
import click

# Add top-level module path to sys.path before importing tubular code.
sys.path.append(path.dirname(path.dirname(path.abspath(__file__))))

from tubular.github_api import GitHubAPI  # pylint: disable=wrong-import-position

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
LOG = logging.getLogger(__name__)


@click.command()
@click.option(
    u'--org',
    help=u'Org from the GitHub repository URL of https://github.com/<org>/<repo>',
    default=u'edx'
)
@click.option(
    u'--repo',
    required=True,
    help=u'Repo name from the GitHub repository URL of https://github.com/<org>/<repo>'
)
@click.option(
    u'--token',
    envvar=u'GIT_TOKEN',
    required=True,
    help=u'The github access token, see https://help.github.com/articles/creating-an-access-token-for-command-line-use/'
)
@click.option(
    u'--base_sha',
    required=True,
    help=u'The BASE SHA of the range',
)
@click.option(
    u'--head_sha',
    required=True,
    help=u'The HEAD SHA of the range',
)
@click.option(
    u'--release_stage', u'message_type', flag_value=u'stage'
)
@click.option(
    u'--release_prod', u'message_type', flag_value=u'prod'
)
@click.option(
    u'--release_rollback', u'message_type', flag_value=u'rollback'
)
def message_pull_requests(org,
                          repo,
                          token,
                          base_sha,
                          head_sha,
                          message_type):
    u""""""
    Message a range of Pull requests between the BASE and HEAD SHA specified.

    Message can be one of 3 types:
    - PR on stage
    - PR on prod
    - Release canceled

    Args:
        org (str): The github organization
        repo (str): The github repository
        token (str): The authentication token
        base_sha (str): The starting SHA
        head_sha (str): The ending SHA
        message_type (str): type of message to send

    Returns:
        None
    """"""
    methods = {
        u'stage': u'message_pr_deployed_stage',
        u'prod': u'message_pr_deployed_prod',
        u'rollback': u'message_pr_release_canceled'
    }

    api = GitHubAPI(org, repo, token)
    for pull_request in api.get_pr_range(base_sha, head_sha):
        getattr(api, methods[message_type])(pull_request.number)


if __name__ == u""__main__"":
    message_pull_requests()  # pylint: disable=no-value-for-parameter

```"
00754100d3d9c962a3b106ebd296590ad5ccdaea,tests/test_errors.py,tests/test_errors.py,,"import logging
import json
import inspect
import pytest
from mappyfile.parser import Parser
from mappyfile.pprint import PrettyPrinter
from mappyfile.transformer import MapfileToDict
from lark.common import UnexpectedToken  # inherits from ParseError
# from lark.lexer import UnexpectedInput


def output(s):
    """"""
    Parse, transform, and pretty print
    the result
    """"""
    p = Parser()
    m = MapfileToDict()

    # https://stackoverflow.com/questions/900392/getting-the-caller-function-name-inside-another-function-in-python
    logging.info(inspect.stack()[1][3])

    ast = p.parse(s)
    logging.debug(ast)
    d = m.transform(ast)
    logging.debug(json.dumps(d, indent=4))
    pp = PrettyPrinter(indent=0, newlinechar="" "", quote=""'"")
    s = pp.pprint(d)
    logging.debug(s)
    return s


def test_invalid_keyword():
    """"""
    Check an invalid keyword throws a schema validation
    error
    """"""
    s = """"""
    MAP
        INVALID ""setting""
    END
    """"""
    output(s)


def test_extra_end():
    """"""
    Check an extra end keyword throws an error
    """"""
    s = """"""MAP
NAME ""test""
END
END""""""

    p = Parser()
    try:
        p.parse(s)
    except UnexpectedToken as ex:
        print(ex.__dict__)
        assert(ex.line == 4)
        assert(ex.column == 1)
        assert(str(ex.token) == 'END')


def test_missing_end():
    """"""
    Check an invalid keyword throws a schema validation
    error
    """"""
    s = """"""MAP
LAYER
NAME ""Test""
LAYER
NAME ""Test2""
END
END""""""

    p = Parser()
    try:
        p.parse(s)
    except UnexpectedToken as ex:
        print(ex.__dict__)
        assert(ex.line == 7)
        assert(ex.column == 4)
        assert(str(ex.token) == 'END')


def run_tests():
    """"""
    Need to comment out the following line in C:\VirtualEnvs\mappyfile\Lib\site-packages\pep8.py
    #stdin_get_value = sys.stdin.read
    Or get AttributeError: '_ReplInput' object has no attribute 'read'
    """"""
    # pytest.main([""tests/test_snippets.py::test_style_pattern""])
    pytest.main([""tests/test_errors.py""])


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    test_missing_end()
    # run_tests()
    print(""Done!"")
",Add tests for expected errors,"Add tests for expected errors
",Python,mit,"geographika/mappyfile,geographika/mappyfile",103,"```python
import logging
import json
import inspect
import pytest
from mappyfile.parser import Parser
from mappyfile.pprint import PrettyPrinter
from mappyfile.transformer import MapfileToDict
from lark.common import UnexpectedToken  # inherits from ParseError
# from lark.lexer import UnexpectedInput


def output(s):
    """"""
    Parse, transform, and pretty print
    the result
    """"""
    p = Parser()
    m = MapfileToDict()

    # https://stackoverflow.com/questions/900392/getting-the-caller-function-name-inside-another-function-in-python
    logging.info(inspect.stack()[1][3])

    ast = p.parse(s)
    logging.debug(ast)
    d = m.transform(ast)
    logging.debug(json.dumps(d, indent=4))
    pp = PrettyPrinter(indent=0, newlinechar="" "", quote=""'"")
    s = pp.pprint(d)
    logging.debug(s)
    return s


def test_invalid_keyword():
    """"""
    Check an invalid keyword throws a schema validation
    error
    """"""
    s = """"""
    MAP
        INVALID ""setting""
    END
    """"""
    output(s)


def test_extra_end():
    """"""
    Check an extra end keyword throws an error
    """"""
    s = """"""MAP
NAME ""test""
END
END""""""

    p = Parser()
    try:
        p.parse(s)
    except UnexpectedToken as ex:
        print(ex.__dict__)
        assert(ex.line == 4)
        assert(ex.column == 1)
        assert(str(ex.token) == 'END')


def test_missing_end():
    """"""
    Check an invalid keyword throws a schema validation
    error
    """"""
    s = """"""MAP
LAYER
NAME ""Test""
LAYER
NAME ""Test2""
END
END""""""

    p = Parser()
    try:
        p.parse(s)
    except UnexpectedToken as ex:
        print(ex.__dict__)
        assert(ex.line == 7)
        assert(ex.column == 4)
        assert(str(ex.token) == 'END')


def run_tests():
    """"""
    Need to comment out the following line in C:\VirtualEnvs\mappyfile\Lib\site-packages\pep8.py
    #stdin_get_value = sys.stdin.read
    Or get AttributeError: '_ReplInput' object has no attribute 'read'
    """"""
    # pytest.main([""tests/test_snippets.py::test_style_pattern""])
    pytest.main([""tests/test_errors.py""])


if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    test_missing_end()
    # run_tests()
    print(""Done!"")

```"
5147963a188f3c13179600d92640f0a33a78af68,algorithms/dynamic-programming/longest-increasing-subsequence/solution.py,algorithms/dynamic-programming/longest-increasing-subsequence/solution.py,,"#!/usr/bin/env python

import sys


def read(fn):
    return fn(sys.stdin.readline())


def ceil(array, value):
    """"""
    Returns the smallest index i such that array[i - 1] < value.
    """"""
    l = 0
    r = len(array) - 1
    i = r + 1

    while l <= r:
        m = l + int((r - l) / 2)
        if array[m] >= value:
            # This mid index is a candidate for the index we are searching for
            # so save it, and continue searching for a smaller candidate on the
            # left side.
            i = m
            r = m - 1
        else:
            # This mid index is not a candidate so continue searching the right
            # side.
            l = m + 1

    return i


def lis(array):
    """"""
    Returns the length of the longest increasing sub-sequence in O(NlogN) time.

    See http://www.geeksforgeeks.org/longest-monotonically-increasing-subsequence-size-n-log-n/. # nopep8
    """"""
    n = len(array)
    if n < 2:
        return n

    # Stores the last value for each candidate increasing list. We maintain
    # that last value in shorter lists is less than the last value in longer
    # lists.
    tails = [array[0]]

    for i in range(1, n):
        a = array[i]
        if a <= tails[0]:
            # Begin the shortest LIS at this new smallest element for the most
            # possible future LIS candidates.
            tails[0] = a
        elif tails[-1] < a:
            # Create a new longest LIS ending with this new largest element.
            tails.append(a)
        else:
            # Find a LIS where we can replace the last element with a.
            j = ceil(tails, a)
            tails[j] = a

    return len(tails)


def lis_dp(array):
    """"""
    Returns the length of the longest increasing sub-sequence in O(N^2) time.
    This is not fast enough to pass with the HackerRank time constraints.
    """"""
    n = len(array)
    if n == 0:
        return 0

    dp = [1] * n

    # Let F(i) be the LIS ending with array[i]. F[i] = max({1 + F[j]}) for all
    # j < i and array[j] < array[i]. If no such j exists, then F[i] = 1.
    for i in range(1, n):
        for j in range(0, i):
            if array[j] < array[i]:
                dp[i] = max(dp[i], 1 + dp[j])

    return max(dp)


def main():
    N = read(int)
    array = [0] * N
    for i in range(0, N):
        array[i] = read(int)
    print(lis(array))

if __name__ == '__main__':
    main()
",Implement Longest Increasing Subsequence in Python,"Implement Longest Increasing Subsequence in Python
",Python,mit,"andreimaximov/algorithms,andreimaximov/algorithms,andreimaximov/algorithms,andreimaximov/algorithms",96,"```python
#!/usr/bin/env python

import sys


def read(fn):
    return fn(sys.stdin.readline())


def ceil(array, value):
    """"""
    Returns the smallest index i such that array[i - 1] < value.
    """"""
    l = 0
    r = len(array) - 1
    i = r + 1

    while l <= r:
        m = l + int((r - l) / 2)
        if array[m] >= value:
            # This mid index is a candidate for the index we are searching for
            # so save it, and continue searching for a smaller candidate on the
            # left side.
            i = m
            r = m - 1
        else:
            # This mid index is not a candidate so continue searching the right
            # side.
            l = m + 1

    return i


def lis(array):
    """"""
    Returns the length of the longest increasing sub-sequence in O(NlogN) time.

    See http://www.geeksforgeeks.org/longest-monotonically-increasing-subsequence-size-n-log-n/. # nopep8
    """"""
    n = len(array)
    if n < 2:
        return n

    # Stores the last value for each candidate increasing list. We maintain
    # that last value in shorter lists is less than the last value in longer
    # lists.
    tails = [array[0]]

    for i in range(1, n):
        a = array[i]
        if a <= tails[0]:
            # Begin the shortest LIS at this new smallest element for the most
            # possible future LIS candidates.
            tails[0] = a
        elif tails[-1] < a:
            # Create a new longest LIS ending with this new largest element.
            tails.append(a)
        else:
            # Find a LIS where we can replace the last element with a.
            j = ceil(tails, a)
            tails[j] = a

    return len(tails)


def lis_dp(array):
    """"""
    Returns the length of the longest increasing sub-sequence in O(N^2) time.
    This is not fast enough to pass with the HackerRank time constraints.
    """"""
    n = len(array)
    if n == 0:
        return 0

    dp = [1] * n

    # Let F(i) be the LIS ending with array[i]. F[i] = max({1 + F[j]}) for all
    # j < i and array[j] < array[i]. If no such j exists, then F[i] = 1.
    for i in range(1, n):
        for j in range(0, i):
            if array[j] < array[i]:
                dp[i] = max(dp[i], 1 + dp[j])

    return max(dp)


def main():
    N = read(int)
    array = [0] * N
    for i in range(0, N):
        array[i] = read(int)
    print(lis(array))

if __name__ == '__main__':
    main()

```"
375e97c18daa7f12896dd3f6b9dccd179f22aa03,gym_frozen_lake.py,gym_frozen_lake.py,,"# -*- coding: utf-8 -*-

import gym
from gym import wrappers

import numpy as np

import agent

# helper function and dictionaries


def running_average(a, size=100):
    """"""calculates the running average over array a""""""
    ra = []
    ra.append(np.sum(a[:size]))
    for i in range(size, len(a)):
        ra.append(ra[-1] + a[i] - a[i - size])
    return 1. / size * np.array(ra)


translate_move_2d_to_1d = {
    tuple([-1., 0.]): 0,
    tuple([0., 1.]): 1,
    tuple([1., 0.]): 2,
    tuple([0., -1.]): 3,
}

translate_pos_1d_to_2d = {
}

i = 0
for y in np.arange(0., 4., 1.):
    for x in np.arange(0., 4., 1.):
        translate_pos_1d_to_2d[i] = np.array([x, y])
        i += 1

# define parameters

env_name = 'FrozenLake-v0'
initial_pos = [0., 0.]
env_shape = (4, 4)
possible_moves = [np.array(move) for move in translate_move_2d_to_1d.keys()]
n_episodes = 500

# set up environment and recording

env = gym.make(env_name)
# env = wrappers.Monitor(env, '/tmp/frozen-lake-experiment-2')
agent = agent.Agent(initial_pos, env_shape, possible_moves)

# run episodes

episode = 0
episode_reward = []

for episode in range(n_episodes):

    print('episode start.')
    env.reset()

    done = False
    reward = 0.

    agent.reset_position()
    agent.reset_reward()

    obs = 0
    while not done:
        env.render()

        action = agent.step(reward)
        obs, reward, done, _ = env.step(translate_move_2d_to_1d[tuple(action)])

        # since the ice is slippery, we need to correct current
        # position and last move from observation of environment
        agent._pos = translate_pos_1d_to_2d[obs]

    agent._pos = translate_pos_1d_to_2d[obs]

    agent.step(reward - 0.1)

    episode += 1
    episode_reward.append(reward)

    print('episode end. episode {episode}, reward {reward}'.format(episode=episode, reward=np.sum(episode_reward)))

ra = running_average(episode_reward)

print('solved after {n} episodes. maximal reward over 100 episodes: {max_reward}'.format(n=np.where(ra >= 0.78)[0][0], max_reward=np.max(ra)))
",Add solution for OpenAI gym frozen lake environment,"Add solution for OpenAI gym frozen lake environment
",Python,mit,jakobj/python-tdl,91,"```python
# -*- coding: utf-8 -*-

import gym
from gym import wrappers

import numpy as np

import agent

# helper function and dictionaries


def running_average(a, size=100):
    """"""calculates the running average over array a""""""
    ra = []
    ra.append(np.sum(a[:size]))
    for i in range(size, len(a)):
        ra.append(ra[-1] + a[i] - a[i - size])
    return 1. / size * np.array(ra)


translate_move_2d_to_1d = {
    tuple([-1., 0.]): 0,
    tuple([0., 1.]): 1,
    tuple([1., 0.]): 2,
    tuple([0., -1.]): 3,
}

translate_pos_1d_to_2d = {
}

i = 0
for y in np.arange(0., 4., 1.):
    for x in np.arange(0., 4., 1.):
        translate_pos_1d_to_2d[i] = np.array([x, y])
        i += 1

# define parameters

env_name = 'FrozenLake-v0'
initial_pos = [0., 0.]
env_shape = (4, 4)
possible_moves = [np.array(move) for move in translate_move_2d_to_1d.keys()]
n_episodes = 500

# set up environment and recording

env = gym.make(env_name)
# env = wrappers.Monitor(env, '/tmp/frozen-lake-experiment-2')
agent = agent.Agent(initial_pos, env_shape, possible_moves)

# run episodes

episode = 0
episode_reward = []

for episode in range(n_episodes):

    print('episode start.')
    env.reset()

    done = False
    reward = 0.

    agent.reset_position()
    agent.reset_reward()

    obs = 0
    while not done:
        env.render()

        action = agent.step(reward)
        obs, reward, done, _ = env.step(translate_move_2d_to_1d[tuple(action)])

        # since the ice is slippery, we need to correct current
        # position and last move from observation of environment
        agent._pos = translate_pos_1d_to_2d[obs]

    agent._pos = translate_pos_1d_to_2d[obs]

    agent.step(reward - 0.1)

    episode += 1
    episode_reward.append(reward)

    print('episode end. episode {episode}, reward {reward}'.format(episode=episode, reward=np.sum(episode_reward)))

ra = running_average(episode_reward)

print('solved after {n} episodes. maximal reward over 100 episodes: {max_reward}'.format(n=np.where(ra >= 0.78)[0][0], max_reward=np.max(ra)))

```"
127a9b4a71863844af4e8c573a962802d10a6403,tests/test_output.py,tests/test_output.py,,"#!/usr/bin/env python
""""""
Unit tests for the output of see for various types of object.

""""""
import itertools

try:
    import unittest2 as unittest
except ImportError:
    import unittest

import see


def union(*sets):
    return set(itertools.chain(*sets))


SIGN_OPS = set(['+obj', '-obj'])

NUMBER_OPS = set('+ - * / // % **'.split())
NUMBER_ASSIGN_OPS = set()

BITWISE_OPS = set('<< >> & ^ | ~'.split())
BITWISE_ASSIGN_OPS = set(op + '=' for op in BITWISE_OPS)

COMPARE_OPS = set('< <= == != > >='.split())

MATRIX_OPS = set(['@'])
MATRIX_ASSIGN_OPS = set(['@='])

ALL_OPS = union(
    SIGN_OPS,
    NUMBER_OPS,
    NUMBER_ASSIGN_OPS,
    BITWISE_OPS,
    BITWISE_ASSIGN_OPS,
    COMPARE_OPS,
    MATRIX_OPS,
    MATRIX_ASSIGN_OPS,
)


class TestSeeOutput(unittest.TestCase):

    def check_ops(self, obj_type, expected_ops, see_output):
        for op in ALL_OPS:
            if op in expected_ops:
                self.assertTrue(
                    op in see_output,
                    'expected %s to support %s' % (obj_type, op))
            else:
                self.assertFalse(
                    op in see_output,
                    'expected %s not to support %s' % (obj_type, op))

    def test_int(self):
        obj = 1
        lit_ops = union(
            SIGN_OPS,
            NUMBER_OPS,
            BITWISE_OPS,
            COMPARE_OPS,
        )
        obj_ops = union(
            lit_ops,
        )

        lit_see = see.see(1)
        obj_see = see.see(obj)

        self.check_ops('int literal', lit_ops, lit_see)
        self.check_ops('int object', obj_ops, obj_see)

    def test_float(self):
        obj = 1.0
        lit_ops = union(
            SIGN_OPS,
            NUMBER_OPS,
            COMPARE_OPS,
        )
        obj_ops = union(
            lit_ops,
        )

        lit_see = see.see(1.0)
        obj_see = see.see(obj)

        self.check_ops('float literal', lit_ops, lit_see)
        self.check_ops('float object', obj_ops, obj_see)


if __name__ == '__main__':
    unittest.main()
",Test some expected output of see() for int & float,"Test some expected output of see() for int & float
",Python,bsd-3-clause,araile/see,96,"```python
#!/usr/bin/env python
""""""
Unit tests for the output of see for various types of object.

""""""
import itertools

try:
    import unittest2 as unittest
except ImportError:
    import unittest

import see


def union(*sets):
    return set(itertools.chain(*sets))


SIGN_OPS = set(['+obj', '-obj'])

NUMBER_OPS = set('+ - * / // % **'.split())
NUMBER_ASSIGN_OPS = set()

BITWISE_OPS = set('<< >> & ^ | ~'.split())
BITWISE_ASSIGN_OPS = set(op + '=' for op in BITWISE_OPS)

COMPARE_OPS = set('< <= == != > >='.split())

MATRIX_OPS = set(['@'])
MATRIX_ASSIGN_OPS = set(['@='])

ALL_OPS = union(
    SIGN_OPS,
    NUMBER_OPS,
    NUMBER_ASSIGN_OPS,
    BITWISE_OPS,
    BITWISE_ASSIGN_OPS,
    COMPARE_OPS,
    MATRIX_OPS,
    MATRIX_ASSIGN_OPS,
)


class TestSeeOutput(unittest.TestCase):

    def check_ops(self, obj_type, expected_ops, see_output):
        for op in ALL_OPS:
            if op in expected_ops:
                self.assertTrue(
                    op in see_output,
                    'expected %s to support %s' % (obj_type, op))
            else:
                self.assertFalse(
                    op in see_output,
                    'expected %s not to support %s' % (obj_type, op))

    def test_int(self):
        obj = 1
        lit_ops = union(
            SIGN_OPS,
            NUMBER_OPS,
            BITWISE_OPS,
            COMPARE_OPS,
        )
        obj_ops = union(
            lit_ops,
        )

        lit_see = see.see(1)
        obj_see = see.see(obj)

        self.check_ops('int literal', lit_ops, lit_see)
        self.check_ops('int object', obj_ops, obj_see)

    def test_float(self):
        obj = 1.0
        lit_ops = union(
            SIGN_OPS,
            NUMBER_OPS,
            COMPARE_OPS,
        )
        obj_ops = union(
            lit_ops,
        )

        lit_see = see.see(1.0)
        obj_see = see.see(obj)

        self.check_ops('float literal', lit_ops, lit_see)
        self.check_ops('float object', obj_ops, obj_see)


if __name__ == '__main__':
    unittest.main()

```"
a37ec297e01316981ab3976366adc54ad00d5f51,pulses-to-data.py,pulses-to-data.py,,"import pyaudio
import struct
import math
import time

""""""
Constants and definitions
""""""
RATE = 44100
INPUT_BLOCK_TIME = 0.01 # seconds
INPUT_FRAMES_PER_BLOCK = int(RATE * INPUT_BLOCK_TIME)

PULSE_THRESHOLD = 0.6
MIN_SECONDS_BETWEEN_PULSES = 0.28

def get_rms(block):

    # RMS amplitude is defined as the square root of the 
    # mean over time of the square of the amplitude.
    # so we need to convert this string of bytes into 
    # a string of 16-bit samples...

    # we will get one short out for each 
    # two chars in the string.
    count = len(block)/2
    format = ""%dh""%(count)
    shorts = struct.unpack( format, block )

    # iterate over the block.
    sum_squares = 0.0
    for sample in shorts:
    # sample is a signed short in +/- 32768. 
    # normalize it to 1.0
        n = sample * (1.0/32768.0)
        sum_squares += n*n

    return math.sqrt( sum_squares / count )


def is_pulse(pulse_block):
    global last_pulse_timestamp

    amplitude = get_rms(pulse_block)

    if amplitude > PULSE_THRESHOLD:
        if get_current_timestamp() - last_pulse_timestamp >= MIN_SECONDS_BETWEEN_PULSES:

            return True

    return False


def get_current_timestamp():
    return time.time();


def get_pulse_stream(pyaudio_instance):

    return pyaudio_instance.open(
                format = pyaudio.paInt16,
                channels = 2,
                rate = RATE,
                input = True,
                frames_per_buffer = INPUT_FRAMES_PER_BLOCK
            )

def listen_for_pulses(pulse_stream):
    global last_pulse_timestamp, pulse_counter

    while True:
        try:
            pulse_block = pulse_stream.read(INPUT_FRAMES_PER_BLOCK)

            if is_pulse(pulse_block):
                last_pulse_timestamp = get_current_timestamp()

                pulse_counter += 1

                print 'pulse: ', pulse_counter


        except IOError, e:
            print( ""Error recording: "", e)


""""""
Program
""""""

pAudio = pyaudio.PyAudio()

pulse_stream = get_pulse_stream(pAudio)

last_pulse_timestamp = 0
pulse_counter = 0

listen_for_pulses(pulse_stream)



",Add initial code for pulse counting,"Add initial code for pulse counting

Brief intro: my stationary bicycle *had* a measurement monitor with
speed, traveled kms, etc. However, it broke down. As my stationary bike
is quite old, I was not able to find a replacement (a good thing). I
noticed that the cable going from the rotor to the monitoring device has
a 3.5mm jack ending. I put it into the microphone slot on my computer
and started pedalling. I noticed that after each finished circle (round,
or candence or how it's called), it made a beeping or *pulsing* sound.
Long story short, I made a simple Python program that monitors the
microphone and tries to detect the *pulses*. If it detect the pulse, it
simply prints out ""pulse"". When you know the distance you traveled in
each *pulse*, you can calculate almost anything (speed, distance, etc).
I'm going to create a simple React-based UI for stationary bicycles
based on this discovery.

In this commit, I'm adding a simple python program to detect the pulses
form the stationary bicycle.
",Python,mit,"lamosty/exercise-bike-ui,lamosty/exercise-bike-ui",101,"```python
import pyaudio
import struct
import math
import time

""""""
Constants and definitions
""""""
RATE = 44100
INPUT_BLOCK_TIME = 0.01 # seconds
INPUT_FRAMES_PER_BLOCK = int(RATE * INPUT_BLOCK_TIME)

PULSE_THRESHOLD = 0.6
MIN_SECONDS_BETWEEN_PULSES = 0.28

def get_rms(block):

    # RMS amplitude is defined as the square root of the 
    # mean over time of the square of the amplitude.
    # so we need to convert this string of bytes into 
    # a string of 16-bit samples...

    # we will get one short out for each 
    # two chars in the string.
    count = len(block)/2
    format = ""%dh""%(count)
    shorts = struct.unpack( format, block )

    # iterate over the block.
    sum_squares = 0.0
    for sample in shorts:
    # sample is a signed short in +/- 32768. 
    # normalize it to 1.0
        n = sample * (1.0/32768.0)
        sum_squares += n*n

    return math.sqrt( sum_squares / count )


def is_pulse(pulse_block):
    global last_pulse_timestamp

    amplitude = get_rms(pulse_block)

    if amplitude > PULSE_THRESHOLD:
        if get_current_timestamp() - last_pulse_timestamp >= MIN_SECONDS_BETWEEN_PULSES:

            return True

    return False


def get_current_timestamp():
    return time.time();


def get_pulse_stream(pyaudio_instance):

    return pyaudio_instance.open(
                format = pyaudio.paInt16,
                channels = 2,
                rate = RATE,
                input = True,
                frames_per_buffer = INPUT_FRAMES_PER_BLOCK
            )

def listen_for_pulses(pulse_stream):
    global last_pulse_timestamp, pulse_counter

    while True:
        try:
            pulse_block = pulse_stream.read(INPUT_FRAMES_PER_BLOCK)

            if is_pulse(pulse_block):
                last_pulse_timestamp = get_current_timestamp()

                pulse_counter += 1

                print 'pulse: ', pulse_counter


        except IOError, e:
            print( ""Error recording: "", e)


""""""
Program
""""""

pAudio = pyaudio.PyAudio()

pulse_stream = get_pulse_stream(pAudio)

last_pulse_timestamp = 0
pulse_counter = 0

listen_for_pulses(pulse_stream)




```"
801165049f9536840dd226c1790d41178dd6c812,rdfalchemy/py3compat.py,rdfalchemy/py3compat.py,,"""""""
Utility functions and objects to ease Python 3 compatibility.
Contributed to rdflib 3 by Thomas Kluyver, re-used here.
""""""
import sys

try:
    from functools import wraps
except ImportError:
    # No-op wraps decorator
    def wraps(f):
        def dec(newf): return newf
        return dec

def cast_bytes(s, enc='utf-8'):
    if isinstance(s, unicode):
        return s.encode(enc)
    return s

PY3 = (sys.version_info[0] >= 3)

def _modify_str_or_docstring(str_change_func):
    @wraps(str_change_func)
    def wrapper(func_or_str):
        if isinstance(func_or_str, str):
            func = None
            doc = func_or_str
        else:
            func = func_or_str
            doc = func.__doc__
        
        doc = str_change_func(doc)
        
        if func:
            func.__doc__ = doc
            return func
        return doc
    return wrapper
    
if PY3:
    # Python 3:
    # ---------
    def b(s):
        return s.encode('ascii')
    
    bytestype = bytes
    
    # Abstract u'abc' syntax:
    @_modify_str_or_docstring
    def format_doctest_out(s):
        """"""Python 2 version
        ""%(u)s'abc'"" --> ""'abc'""
        ""%(b)s'abc'"" --> ""b'abc'""
        ""55%(L)s""    --> ""55""
        
        Accepts a string or a function, so it can be used as a decorator.""""""
        return s % {'u':'', 'b':'b', 'L':''}
    
    def type_cmp(a, b):
        """"""Python 2 style comparison based on type""""""
        ta, tb = type(a).__name__, type(b).__name__
        # Ugly hack: some tests rely on tuple sorting before unicode, and I
        # don't know if that's important. Better retain it for now.
        if ta == 'str':
            ta = 'unicode'
        if tb == 'str':
            tb = 'unicode'
        # return 1 if ta > tb else -1 if ta < tb else 0
        if ta > tb:
            return 1
        elif ta < tb:
            return -1
        else:
            return 0

else:
    # Python 2
    # --------
    def b(s):
        return s
    
    bytestype = str
    
    # Abstract u'abc' syntax:
    @_modify_str_or_docstring
    def format_doctest_out(s):
        """"""Python 2 version
        ""%(u)s'abc'"" --> ""u'abc'""
        ""%(b)s'abc'"" --> ""'abc'""
        ""55%(L)s""    --> ""55L""
        
        Accepts a string or a function, so it can be used as a decorator.""""""
        return s % {'u':'u', 'b':'', 'L':'L'}
    
    def type_cmp(a, b):
        # return 1 if a > b else -1 if a < b else 0
        if a > b:
            return 1
        elif a < b:
            return -1
        else:
            return 0

",Copy Thomas Kluyver's Py3 compat from rdflib,"Copy Thomas Kluyver's Py3 compat from rdflib
",Python,mit,gjhiggins/RDFAlchemy,104,"```python
""""""
Utility functions and objects to ease Python 3 compatibility.
Contributed to rdflib 3 by Thomas Kluyver, re-used here.
""""""
import sys

try:
    from functools import wraps
except ImportError:
    # No-op wraps decorator
    def wraps(f):
        def dec(newf): return newf
        return dec

def cast_bytes(s, enc='utf-8'):
    if isinstance(s, unicode):
        return s.encode(enc)
    return s

PY3 = (sys.version_info[0] >= 3)

def _modify_str_or_docstring(str_change_func):
    @wraps(str_change_func)
    def wrapper(func_or_str):
        if isinstance(func_or_str, str):
            func = None
            doc = func_or_str
        else:
            func = func_or_str
            doc = func.__doc__
        
        doc = str_change_func(doc)
        
        if func:
            func.__doc__ = doc
            return func
        return doc
    return wrapper
    
if PY3:
    # Python 3:
    # ---------
    def b(s):
        return s.encode('ascii')
    
    bytestype = bytes
    
    # Abstract u'abc' syntax:
    @_modify_str_or_docstring
    def format_doctest_out(s):
        """"""Python 2 version
        ""%(u)s'abc'"" --> ""'abc'""
        ""%(b)s'abc'"" --> ""b'abc'""
        ""55%(L)s""    --> ""55""
        
        Accepts a string or a function, so it can be used as a decorator.""""""
        return s % {'u':'', 'b':'b', 'L':''}
    
    def type_cmp(a, b):
        """"""Python 2 style comparison based on type""""""
        ta, tb = type(a).__name__, type(b).__name__
        # Ugly hack: some tests rely on tuple sorting before unicode, and I
        # don't know if that's important. Better retain it for now.
        if ta == 'str':
            ta = 'unicode'
        if tb == 'str':
            tb = 'unicode'
        # return 1 if ta > tb else -1 if ta < tb else 0
        if ta > tb:
            return 1
        elif ta < tb:
            return -1
        else:
            return 0

else:
    # Python 2
    # --------
    def b(s):
        return s
    
    bytestype = str
    
    # Abstract u'abc' syntax:
    @_modify_str_or_docstring
    def format_doctest_out(s):
        """"""Python 2 version
        ""%(u)s'abc'"" --> ""u'abc'""
        ""%(b)s'abc'"" --> ""'abc'""
        ""55%(L)s""    --> ""55L""
        
        Accepts a string or a function, so it can be used as a decorator.""""""
        return s % {'u':'u', 'b':'', 'L':'L'}
    
    def type_cmp(a, b):
        # return 1 if a > b else -1 if a < b else 0
        if a > b:
            return 1
        elif a < b:
            return -1
        else:
            return 0


```"
b694c0da84f1a3f6b27b9d4a07c9f5cb2116d831,utils/sign_file.py,utils/sign_file.py,,"#! /usr/bin/env python

""""""
A script that creates signed Python files.

Distributing detached signatures is boring.
""""""


from __future__ import print_function

import os
import argparse
import subprocess


def get_file_encoding(filename):
    """"""
    Get the file encoding for the file with the given filename
    """"""

    with open(filename, 'rb') as fp:
        # The encoding is usually specified on the second line
        txt = fp.read().splitlines()[1]
        txt = txt.decode('utf-8')
        if 'encoding' in txt:
            encoding = txt.split()[-1]
        else:
            encoding = 'utf-8' # default

    return str(encoding)


def sign_file_and_get_sig(filename, encoding):
    """"""
    Sign the file and get the signature
    """"""

    cmd = 'gpg -bass {}'.format(filename)
    ret = subprocess.Popen(cmd, shell=True).wait()
    print ('-> %r' % cmd)

    if ret:
        raise ValueError('Could not sign the file!')

    with open('{}.asc'.format(filename), 'rb') as fp:
        sig = fp.read()

    try:
        os.remove('{}.asc'.format(filename))
    except OSError:
        pass

    sig = sig.decode(encoding)
    sig = sig.replace('\r', '').replace('\n', '\\n')
    sig = sig.encode(encoding)

    return sig


def sign_original_file(filename, encoding):
    """"""
    Sign the original file
    """"""

    sig = sign_file_and_get_sig(filename, encoding)

    with open(filename, 'ab') as outfile:
        outfile.write('#'.encode(encoding))
        outfile.write(sig)
        outfile.write('\n'.encode(encoding))


def create_signed_file(filename, encoding):
    """"""
    Create a signed file
    """"""

    sig = sign_file_and_get_sig(filename, encoding)
    name, extension = os.path.splitext(filename)

    new_file_name = '{}_signed{}'.format(name, extension)

    with open(new_file_name, 'wb') as outfile, \
         open(filename, 'rb') as infile:
        txt = infile.read()
        outfile.write(txt)
        outfile.write('#'.encode(encoding))
        outfile.write(sig)
        outfile.write('\n'.encode(encoding))


def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('filenames', action='store', nargs='+',
                        help='Files you wish to sign')

    parser.add_argument('--overwrite', action='store_true',
                        dest='overwrite', default=False, 
                        help='Overwrite the original file'
                             ' (sign the original file)')

    opts = parser.parse_args()

    return opts


if __name__ == '__main__':
    opts = parse_args()

    for filename in opts.filenames:
        encoding = get_file_encoding(filename)
        if opts.overwrite:
            sign_original_file(filename, encoding)
        else:
            create_signed_file(filename, encoding)
                        
",Write a generic script signing utility,"Write a generic script signing utility

Signed-off-by: Thomas Nagy <d86e025e60d31fa2452cf443d0fed051ea678233@gmail.com>
",Python,agpl-3.0,"hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR,hlange/LogSoCR",119,"```python
#! /usr/bin/env python

""""""
A script that creates signed Python files.

Distributing detached signatures is boring.
""""""


from __future__ import print_function

import os
import argparse
import subprocess


def get_file_encoding(filename):
    """"""
    Get the file encoding for the file with the given filename
    """"""

    with open(filename, 'rb') as fp:
        # The encoding is usually specified on the second line
        txt = fp.read().splitlines()[1]
        txt = txt.decode('utf-8')
        if 'encoding' in txt:
            encoding = txt.split()[-1]
        else:
            encoding = 'utf-8' # default

    return str(encoding)


def sign_file_and_get_sig(filename, encoding):
    """"""
    Sign the file and get the signature
    """"""

    cmd = 'gpg -bass {}'.format(filename)
    ret = subprocess.Popen(cmd, shell=True).wait()
    print ('-> %r' % cmd)

    if ret:
        raise ValueError('Could not sign the file!')

    with open('{}.asc'.format(filename), 'rb') as fp:
        sig = fp.read()

    try:
        os.remove('{}.asc'.format(filename))
    except OSError:
        pass

    sig = sig.decode(encoding)
    sig = sig.replace('\r', '').replace('\n', '\\n')
    sig = sig.encode(encoding)

    return sig


def sign_original_file(filename, encoding):
    """"""
    Sign the original file
    """"""

    sig = sign_file_and_get_sig(filename, encoding)

    with open(filename, 'ab') as outfile:
        outfile.write('#'.encode(encoding))
        outfile.write(sig)
        outfile.write('\n'.encode(encoding))


def create_signed_file(filename, encoding):
    """"""
    Create a signed file
    """"""

    sig = sign_file_and_get_sig(filename, encoding)
    name, extension = os.path.splitext(filename)

    new_file_name = '{}_signed{}'.format(name, extension)

    with open(new_file_name, 'wb') as outfile, \
         open(filename, 'rb') as infile:
        txt = infile.read()
        outfile.write(txt)
        outfile.write('#'.encode(encoding))
        outfile.write(sig)
        outfile.write('\n'.encode(encoding))


def parse_args():
    parser = argparse.ArgumentParser()

    parser.add_argument('filenames', action='store', nargs='+',
                        help='Files you wish to sign')

    parser.add_argument('--overwrite', action='store_true',
                        dest='overwrite', default=False, 
                        help='Overwrite the original file'
                             ' (sign the original file)')

    opts = parser.parse_args()

    return opts


if __name__ == '__main__':
    opts = parse_args()

    for filename in opts.filenames:
        encoding = get_file_encoding(filename)
        if opts.overwrite:
            sign_original_file(filename, encoding)
        else:
            create_signed_file(filename, encoding)
                        

```"
6c61e0a477a03dcd81d90bf828ba6c036c86b355,pydocstring/docstring.py,pydocstring/docstring.py,,"class Docstring:
    """"""Class for storing docstring information.

    Following headers are used by this class:

    * 'summary' - first line of the docstring
    * 'extended' - blocks of extended description concerning the functionality of the code
    * 'parameters' - parameters of a method
    * 'other parameters' - parameters that are not commonly used
    * 'returns' - returned value of a method
    * 'yields' - generated value of a generator
    * 'raises' - errors raised
    * 'see also' - other related code
    * 'notes' - blocks of other information about the code (e.g. implementation details)
    * 'references' - references used
    * 'examples' - examples of code usage
    * 'attributes' - attributes of a class
    * 'methods' - methods of a class/module

    Attributes
    ----------
    info : dict
        Dictionary of the headers to contents under header.

    Methods
    -------
    __init__(**headers_contents)
        Initialize.
    parse_google(docstring)
        Return instance of Docstring that corresponds to given google docstring.
    parse_numpy(docstring)
        Return instance of Docstring that corresponds to given numpy docstring.
    parse_instance(instance)
        Return instance of Docstring that corresponds to provided instance.
    make_google()
        Return corresponding google docstring
    make_numpy()
        Return corresponding numpy docstring

    Example
    -------
    For example, this docstring should be equivalent to

    info = {'Attributes': {'info': {'type': ['dict'],
                           'docs':['Dictionary of the headers to contents under header.']}}}
    info = {'Attributes': {'info': <ParamDocstring object>}}
    """"""
    pass


class ParamDocstring:
    """"""Class for storing docstring information on parameters.

    Attributes
    ----------
    name : str
        Name of the parameter.
    types : list of str
        Type of the parameters allowed.
    docs : list of str
        Documentations for the parameter.

    Methods
    -------
    __init__(name, type=None, docs=None)
        Initialize.
    parse_google(docstring)
        Return instance of Docstring that corresponds to given google docstring.
    parse_numpy(docstring)
        Return instance of Docstring that corresponds to given numpy docstring.
    """"""
    pass


class MethodDocstring:
    """"""Class for storing docstring information on methods.

    Attributes
    ----------
    name : str
        Name of the method.
    call_signature : str
        Call signature of the method.
    docs : list of str
        Documentations for the method.

    Methods
    -------
    __init__(name, call_signature, docs=None)
        Initialize.
    parse_google(docstring)
        Return instance of Docstring that corresponds to given google docstring.
    parse_numpy(docstring)
        Return instance of Docstring that corresponds to given numpy docstring.
    """"""
    pass
",Add general framework for coding,"Add general framework for coding

using documentation
",Python,mit,kimt33/pydocstring,97,"```python
class Docstring:
    """"""Class for storing docstring information.

    Following headers are used by this class:

    * 'summary' - first line of the docstring
    * 'extended' - blocks of extended description concerning the functionality of the code
    * 'parameters' - parameters of a method
    * 'other parameters' - parameters that are not commonly used
    * 'returns' - returned value of a method
    * 'yields' - generated value of a generator
    * 'raises' - errors raised
    * 'see also' - other related code
    * 'notes' - blocks of other information about the code (e.g. implementation details)
    * 'references' - references used
    * 'examples' - examples of code usage
    * 'attributes' - attributes of a class
    * 'methods' - methods of a class/module

    Attributes
    ----------
    info : dict
        Dictionary of the headers to contents under header.

    Methods
    -------
    __init__(**headers_contents)
        Initialize.
    parse_google(docstring)
        Return instance of Docstring that corresponds to given google docstring.
    parse_numpy(docstring)
        Return instance of Docstring that corresponds to given numpy docstring.
    parse_instance(instance)
        Return instance of Docstring that corresponds to provided instance.
    make_google()
        Return corresponding google docstring
    make_numpy()
        Return corresponding numpy docstring

    Example
    -------
    For example, this docstring should be equivalent to

    info = {'Attributes': {'info': {'type': ['dict'],
                           'docs':['Dictionary of the headers to contents under header.']}}}
    info = {'Attributes': {'info': <ParamDocstring object>}}
    """"""
    pass


class ParamDocstring:
    """"""Class for storing docstring information on parameters.

    Attributes
    ----------
    name : str
        Name of the parameter.
    types : list of str
        Type of the parameters allowed.
    docs : list of str
        Documentations for the parameter.

    Methods
    -------
    __init__(name, type=None, docs=None)
        Initialize.
    parse_google(docstring)
        Return instance of Docstring that corresponds to given google docstring.
    parse_numpy(docstring)
        Return instance of Docstring that corresponds to given numpy docstring.
    """"""
    pass


class MethodDocstring:
    """"""Class for storing docstring information on methods.

    Attributes
    ----------
    name : str
        Name of the method.
    call_signature : str
        Call signature of the method.
    docs : list of str
        Documentations for the method.

    Methods
    -------
    __init__(name, call_signature, docs=None)
        Initialize.
    parse_google(docstring)
        Return instance of Docstring that corresponds to given google docstring.
    parse_numpy(docstring)
        Return instance of Docstring that corresponds to given numpy docstring.
    """"""
    pass

```"
9bcd3e71f86d4a3b74c57276e65420cc8dc01fa1,test/test_decorate.py,test/test_decorate.py,,"
import py.test

from tiddlyweb.model.policy import UserRequiredError
from tiddlywebplugins.utils import (entitle, do_html, require_role, 
        require_any_user)

STATUS = ''
HEADERS = []

def start_responser(status, headers, exc_info=None):
    global STATUS
    global HEADERS
    STATUS = status
    HEADERS = headers


def setup_module(module):
    module.environ = {}


def test_entitle():

    @entitle('monkey')
    def wsgi_app(environ, start_response):
        pass

    assert 'tiddlyweb.title' not in environ

    wsgi_app(environ, start_responser)

    assert 'tiddlyweb.title' in environ
    assert environ['tiddlyweb.title'] == 'monkey'


def test_do_html():

    @do_html()
    def wsgi_app(environ, start_response):
        pass

    assert STATUS == ''

    wsgi_app(environ, start_responser)

    assert STATUS == '200 OK'
    assert ('Content-Type', 'text/html; charset=UTF-8') in HEADERS


def test_require_role():

    @require_role('ADMIN')
    def wsgi_app(environ, start_response):
        return 1

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'roles': []}

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'roles': ['fan']}

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'roles': ['ADMIN']}

    output = wsgi_app(environ, start_responser)
    assert output == 1


def test_require_any_user():

    @require_any_user()
    def wsgi_app(environ, start_response):
        return 1

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'name': 'GUEST'}

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'name': 'monkey!'}

    output = wsgi_app(environ, start_responser)
    assert output == 1
",Add a test file which tests the various decorator functions.,"Add a test file which tests the various decorator functions.
",Python,bsd-3-clause,tiddlyweb/tiddlywebplugins.utils,93,"```python

import py.test

from tiddlyweb.model.policy import UserRequiredError
from tiddlywebplugins.utils import (entitle, do_html, require_role, 
        require_any_user)

STATUS = ''
HEADERS = []

def start_responser(status, headers, exc_info=None):
    global STATUS
    global HEADERS
    STATUS = status
    HEADERS = headers


def setup_module(module):
    module.environ = {}


def test_entitle():

    @entitle('monkey')
    def wsgi_app(environ, start_response):
        pass

    assert 'tiddlyweb.title' not in environ

    wsgi_app(environ, start_responser)

    assert 'tiddlyweb.title' in environ
    assert environ['tiddlyweb.title'] == 'monkey'


def test_do_html():

    @do_html()
    def wsgi_app(environ, start_response):
        pass

    assert STATUS == ''

    wsgi_app(environ, start_responser)

    assert STATUS == '200 OK'
    assert ('Content-Type', 'text/html; charset=UTF-8') in HEADERS


def test_require_role():

    @require_role('ADMIN')
    def wsgi_app(environ, start_response):
        return 1

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'roles': []}

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'roles': ['fan']}

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'roles': ['ADMIN']}

    output = wsgi_app(environ, start_responser)
    assert output == 1


def test_require_any_user():

    @require_any_user()
    def wsgi_app(environ, start_response):
        return 1

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'name': 'GUEST'}

    with py.test.raises(UserRequiredError):
        wsgi_app(environ, start_responser)

    environ['tiddlyweb.usersign'] = {'name': 'monkey!'}

    output = wsgi_app(environ, start_responser)
    assert output == 1

```"
5afb810c19923ff7edeb41ba084b1d7b85925840,markov/markov2.py,markov/markov2.py,,"#!python3

import string
import random
import time
import re
import sys


'''
This is an implementation of a markov chain used for text generation.
Just pass a file name as an argument and it should load it up, build a markov
chain with a state for each word(s), and start walking through the chain, writing
incoherent text to the terminal.
'''


asciiset = set(string.ascii_letters)
asciiset.add(' ')
asciiset.add('.')
def strip2ascii(txt):
    return ''.join([ch for ch in txt if ch in asciiset])


def tokenize(fname):
    '''
    Generate tokens defined by
    - Sequences of characters that aren't spaces
    - Periods

    For example, 'This is a test. Ok.' => ('This', 'is', 'a', 'test', '.', 'Ok, '.')
    '''
    with open(fname, 'r') as f:
        for line in f:
            stripped = strip2ascii(line)
            for word in stripped.split():
                
                if word[-1] == '.':
                    yield word[:-1]
                    yield '.'
                else:
                    yield word


def buildtransitionmap(tokens, order):
    dct = {}
    prev = ('',)*order

    for token in tokens:
        if prev in dct:
            dct[prev].append(token)
        else:
            dct[prev] = [token]

        prev = prev[1:]+(token,)
        
    return dct


def transition(word, transmap):
    return random.choice(transmap[word])


def eternalramble(fname, order):
    '''
    Walk through the markov chain printing out words to the terminal one at a time
    '''
    transmap = buildtransitionmap(tokenize(fname), order)
    prev = random.choice(list(transmap.keys()))
    while True:
        word = transition(prev, transmap)
        print(word, end=' ')
        prev  = prev[1:]+(word,)
        sys.stdout.flush()
        time.sleep(0.25)


def printusage():
    print('Usage: markov filename order')
    print('  filename: the filename of the text to base the markov chain on.')
    print('  order: how many consecutive words make up each state (2 works well)')


def launch():
    if len(sys.argv) != 3:
        printusage()
        return

    try:
        order = int(sys.argv[2])
    except:
        printusage()
        return

    eternalramble(sys.argv[1], order)


if __name__ == '__main__':
    launch()
",Add second markov text generator v2 with order parameter,"Add second markov text generator v2 with order parameter
",Python,mit,tmerr/trevornet,100,"```python
#!python3

import string
import random
import time
import re
import sys


'''
This is an implementation of a markov chain used for text generation.
Just pass a file name as an argument and it should load it up, build a markov
chain with a state for each word(s), and start walking through the chain, writing
incoherent text to the terminal.
'''


asciiset = set(string.ascii_letters)
asciiset.add(' ')
asciiset.add('.')
def strip2ascii(txt):
    return ''.join([ch for ch in txt if ch in asciiset])


def tokenize(fname):
    '''
    Generate tokens defined by
    - Sequences of characters that aren't spaces
    - Periods

    For example, 'This is a test. Ok.' => ('This', 'is', 'a', 'test', '.', 'Ok, '.')
    '''
    with open(fname, 'r') as f:
        for line in f:
            stripped = strip2ascii(line)
            for word in stripped.split():
                
                if word[-1] == '.':
                    yield word[:-1]
                    yield '.'
                else:
                    yield word


def buildtransitionmap(tokens, order):
    dct = {}
    prev = ('',)*order

    for token in tokens:
        if prev in dct:
            dct[prev].append(token)
        else:
            dct[prev] = [token]

        prev = prev[1:]+(token,)
        
    return dct


def transition(word, transmap):
    return random.choice(transmap[word])


def eternalramble(fname, order):
    '''
    Walk through the markov chain printing out words to the terminal one at a time
    '''
    transmap = buildtransitionmap(tokenize(fname), order)
    prev = random.choice(list(transmap.keys()))
    while True:
        word = transition(prev, transmap)
        print(word, end=' ')
        prev  = prev[1:]+(word,)
        sys.stdout.flush()
        time.sleep(0.25)


def printusage():
    print('Usage: markov filename order')
    print('  filename: the filename of the text to base the markov chain on.')
    print('  order: how many consecutive words make up each state (2 works well)')


def launch():
    if len(sys.argv) != 3:
        printusage()
        return

    try:
        order = int(sys.argv[2])
    except:
        printusage()
        return

    eternalramble(sys.argv[1], order)


if __name__ == '__main__':
    launch()

```"
c245bd90dd8f1fd90da45e737ff3cbfba43707fa,run_tests.py,run_tests.py,,"#!/usr/bin/env python
#
# Copyright 2012 Ezox Systems LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
""""""Setup paths and App Engine's stubs, then run tests.""""""

import os
import sys

import argparse
import tempfile


CURRENT_PATH = os.getcwdu()

# Setup your paths here...
paths = [
    #os.path.join(CURRENT_PATH, 'lib'),
]

try:
    from dev_appserver import fix_sys_path
except ImportError:
    # Something is not setup right, maybe they're using a local .pth file.
    import site
    site.addsitedir('.')

    # Now, try again.
    from dev_appserver import fix_sys_path

fix_sys_path()

sys.path.extend(paths)

import unittest


stub_config = {
    'login_url': None,
    'require_indexes': True,
    'clear_datastore': False,
    'save_changes': False,
}


def run():
    parser = argparse.ArgumentParser(description='Run tests')
    parser.add_argument('tests', nargs='+', default='', help=""Path to tests to be run."")
    parser.add_argument('--failfast', action='store_true', default=False)
    parser.add_argument('--verbosity', '-v', type=int, default=2)

    args = parser.parse_args()

    suite = _build_suite(args.tests)

    _setup_environment()

    _run_suite(suite, args)


def _run_suite(suite, options):
    runner = unittest.TextTestRunner(
        verbosity=options.verbosity,
        failfast=options.failfast)

    return runner.run(suite)


def _build_suite(tests):
    loader = unittest.defaultTestLoader
    suite = unittest.TestSuite()

    if not tests:
        suite.addTests(loader.discover(CURRENT_PATH))
    else:
        for label in tests:
            rel_root = label.replace('.', os.path.sep)

            if os.path.exists(rel_root):
                suite.addTests(loader.discover(rel_root, top_level_dir=os.getcwdu()))
            else:
                suite.addTests(loader.loadTestsFromName(label))

    return suite


def _setup_environment():
    from google.appengine.tools import dev_appserver

    config = stub_config.copy()

    config['root_path'] = os.getcwd()
    config['blobstore_path'] = tempfile.mkdtemp()
    config['datastore_path'] = tempfile.mktemp()
    config['high_replication'] = True

    dev_appserver.SetupStubs('unittest', **config)


    import logging
    logging.getLogger().setLevel(logging.DEBUG)

if __name__ == '__main__':
    run()

",Add simple unit test runner to setup environment.,"Add simple unit test runner to setup environment.

The test runner performs setup like configuring paths and setting up App
Engine stubs needed to run tests.
",Python,apache-2.0,"andreleblanc-wf/furious,beaulyddon-wf/furious,rosshendrickson-wf/furious,mattsanders-wf/furious,Workiva/furious,beaulyddon-wf/furious,rosshendrickson-wf/furious,mattsanders-wf/furious,Workiva/furious,andreleblanc-wf/furious,robertkluin/furious",118,"```python
#!/usr/bin/env python
#
# Copyright 2012 Ezox Systems LLC
#
# Licensed under the Apache License, Version 2.0 (the ""License"");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
""""""Setup paths and App Engine's stubs, then run tests.""""""

import os
import sys

import argparse
import tempfile


CURRENT_PATH = os.getcwdu()

# Setup your paths here...
paths = [
    #os.path.join(CURRENT_PATH, 'lib'),
]

try:
    from dev_appserver import fix_sys_path
except ImportError:
    # Something is not setup right, maybe they're using a local .pth file.
    import site
    site.addsitedir('.')

    # Now, try again.
    from dev_appserver import fix_sys_path

fix_sys_path()

sys.path.extend(paths)

import unittest


stub_config = {
    'login_url': None,
    'require_indexes': True,
    'clear_datastore': False,
    'save_changes': False,
}


def run():
    parser = argparse.ArgumentParser(description='Run tests')
    parser.add_argument('tests', nargs='+', default='', help=""Path to tests to be run."")
    parser.add_argument('--failfast', action='store_true', default=False)
    parser.add_argument('--verbosity', '-v', type=int, default=2)

    args = parser.parse_args()

    suite = _build_suite(args.tests)

    _setup_environment()

    _run_suite(suite, args)


def _run_suite(suite, options):
    runner = unittest.TextTestRunner(
        verbosity=options.verbosity,
        failfast=options.failfast)

    return runner.run(suite)


def _build_suite(tests):
    loader = unittest.defaultTestLoader
    suite = unittest.TestSuite()

    if not tests:
        suite.addTests(loader.discover(CURRENT_PATH))
    else:
        for label in tests:
            rel_root = label.replace('.', os.path.sep)

            if os.path.exists(rel_root):
                suite.addTests(loader.discover(rel_root, top_level_dir=os.getcwdu()))
            else:
                suite.addTests(loader.loadTestsFromName(label))

    return suite


def _setup_environment():
    from google.appengine.tools import dev_appserver

    config = stub_config.copy()

    config['root_path'] = os.getcwd()
    config['blobstore_path'] = tempfile.mkdtemp()
    config['datastore_path'] = tempfile.mktemp()
    config['high_replication'] = True

    dev_appserver.SetupStubs('unittest', **config)


    import logging
    logging.getLogger().setLevel(logging.DEBUG)

if __name__ == '__main__':
    run()


```"
ace3ea15bc99cb384a59cc27d38a98d7aa7a2948,tests/pytests/unit/auth/test_rest.py,tests/pytests/unit/auth/test_rest.py,,"import pytest
import salt.auth.rest as rest
from tests.support.mock import MagicMock, patch


@pytest.fixture
def configure_loader_modules():
    """"""
    Rest module configuration
    """"""
    return {
        rest: {
            ""__opts__"": {
                ""external_auth"": {
                    ""rest"": {""^url"": ""https://test_url/rest"", ""fred"": ["".*"", ""@runner""]}
                }
            }
        }
    }


def test_rest_auth_config():
    ret = rest._rest_auth_setup()
    assert ret == ""https://test_url/rest""


def test_fetch_call_failed():
    with patch(""salt.utils.http.query"", MagicMock(return_value={""status"": 401})):
        ret = rest.fetch(""foo"", None)
        assert ret is False


def test_fetch_call_success_dict_none():
    with patch(
        ""salt.utils.http.query"", MagicMock(return_value={""status"": 200, ""dict"": None})
    ):
        ret = rest.fetch(""foo"", None)
        assert ret == []


def test_fetch_call_success_dict_acl():
    with patch(
        ""salt.utils.http.query"",
        MagicMock(return_value={""status"": 200, ""dict"": {""foo"": [""@wheel""]}}),
    ):
        ret = rest.fetch(""foo"", None)
        assert ret == {""foo"": [""@wheel""]}


def test_auth_nopass():
    ret = rest.auth(""foo"", None)
    assert ret is False


def test_auth_nouser():
    ret = rest.auth(None, ""foo"")
    assert ret is False


def test_auth_nouserandpass():
    ret = rest.auth(None, None)
    assert ret is False


def test_auth_ok():
    with patch(
        ""salt.utils.http.query"",
        MagicMock(return_value={""status"": 200, ""dict"": [""@wheel""]}),
    ):
        ret = rest.auth(""foo"", None)
        assert ret is True


def test_acl_without_merge():
    ret = rest.acl(""fred"", password=""password"")
    assert ret == ["".*"", ""@runner""]


def test_acl_unauthorized():
    with patch(""salt.utils.http.query"", MagicMock(return_value={""status"": 400})):
        ret = rest.acl(""foo"", password=""password"")
        assert ret is None


def test_acl_no_merge():
    with patch(
        ""salt.utils.http.query"", MagicMock(return_value={""status"": 200, ""dict"": None})
    ):
        ret = rest.acl(""fred"", password=""password"")
        assert ret == ["".*"", ""@runner""]


def test_acl_merge():
    with patch(
        ""salt.utils.http.query"",
        MagicMock(return_value={""status"": 200, ""dict"": [""@wheel""]}),
    ):
        ret = rest.acl(""fred"", password=""password"")
        assert ret == ["".*"", ""@runner"", ""@wheel""]
",Add tests for REST eauth,"Add tests for REST eauth
",Python,apache-2.0,"saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt,saltstack/salt",100,"```python
import pytest
import salt.auth.rest as rest
from tests.support.mock import MagicMock, patch


@pytest.fixture
def configure_loader_modules():
    """"""
    Rest module configuration
    """"""
    return {
        rest: {
            ""__opts__"": {
                ""external_auth"": {
                    ""rest"": {""^url"": ""https://test_url/rest"", ""fred"": ["".*"", ""@runner""]}
                }
            }
        }
    }


def test_rest_auth_config():
    ret = rest._rest_auth_setup()
    assert ret == ""https://test_url/rest""


def test_fetch_call_failed():
    with patch(""salt.utils.http.query"", MagicMock(return_value={""status"": 401})):
        ret = rest.fetch(""foo"", None)
        assert ret is False


def test_fetch_call_success_dict_none():
    with patch(
        ""salt.utils.http.query"", MagicMock(return_value={""status"": 200, ""dict"": None})
    ):
        ret = rest.fetch(""foo"", None)
        assert ret == []


def test_fetch_call_success_dict_acl():
    with patch(
        ""salt.utils.http.query"",
        MagicMock(return_value={""status"": 200, ""dict"": {""foo"": [""@wheel""]}}),
    ):
        ret = rest.fetch(""foo"", None)
        assert ret == {""foo"": [""@wheel""]}


def test_auth_nopass():
    ret = rest.auth(""foo"", None)
    assert ret is False


def test_auth_nouser():
    ret = rest.auth(None, ""foo"")
    assert ret is False


def test_auth_nouserandpass():
    ret = rest.auth(None, None)
    assert ret is False


def test_auth_ok():
    with patch(
        ""salt.utils.http.query"",
        MagicMock(return_value={""status"": 200, ""dict"": [""@wheel""]}),
    ):
        ret = rest.auth(""foo"", None)
        assert ret is True


def test_acl_without_merge():
    ret = rest.acl(""fred"", password=""password"")
    assert ret == ["".*"", ""@runner""]


def test_acl_unauthorized():
    with patch(""salt.utils.http.query"", MagicMock(return_value={""status"": 400})):
        ret = rest.acl(""foo"", password=""password"")
        assert ret is None


def test_acl_no_merge():
    with patch(
        ""salt.utils.http.query"", MagicMock(return_value={""status"": 200, ""dict"": None})
    ):
        ret = rest.acl(""fred"", password=""password"")
        assert ret == ["".*"", ""@runner""]


def test_acl_merge():
    with patch(
        ""salt.utils.http.query"",
        MagicMock(return_value={""status"": 200, ""dict"": [""@wheel""]}),
    ):
        ret = rest.acl(""fred"", password=""password"")
        assert ret == ["".*"", ""@runner"", ""@wheel""]

```"
6e658a93b9e91b30f0902f741b2f90d9ecc18021,numba/exttypes/validators.py,numba/exttypes/validators.py,,"# -*- coding: utf-8 -*-

""""""
Validate method signatures and inheritance compatiblity.
""""""

import types
import warnings
import inspect

import numba
from numba import *
from numba import error
from numba import typesystem
from numba.minivect import minitypes


#------------------------------------------------------------------------
# Method Validators
#------------------------------------------------------------------------

class MethodValidator(object):
    ""Interface for method validators""

    def validate(self, method, ext_type):
        """"""
        Validate a Method. Raise an exception for user typing errors.
        """"""

class ArgcountMethodValidator(MethodValidator):
    """"""
    Validate a signature against the number of arguments the function expects.
    """"""

    def validate(self, method, ext_type):
        """"""
        Validate a signature (which is None if not declared by the user)
        for a method.
        """"""
        if method.signature is None:
            return

        nargs = method.py_func.__code__.co_argcount - 1 + method.is_static
        if len(method.signature.args) != nargs:
            raise error.NumbaError(
                ""Expected %d argument types in function ""
                ""%s (don't include 'self')"" % (nargs, method.name))

class InitMethodValidator(MethodValidator):
    """"""
    Validate the init method of extension classes.
    """"""

    def validate(self, method, ext_type):
        if method.name == '__init__' and (method.is_class or method.is_static):
            raise error.NumbaError(""__init__ method should not be a class- ""
                                   ""or staticmethod"")

class JitInitMethodValidator(MethodValidator):
    """"""
    Validate the init method for jit functions. Issue a warning when the
    signature is omitted.
    """"""

    def validate(self, method, ext_type):
        if method.name == '__init__' and method.signature is None:
            self.check_init_args(method, ext_type)

    def check_init_args(self, method, ext_type):
        if inspect.getargspec(method.py_func).args:
            warnings.warn(
                ""Constructor for class '%s' has no signature, ""
                ""assuming arguments have type 'object'"" %
                ext_type.py_class.__name__)


jit_validators = [ArgcountMethodValidator(), InitMethodValidator(), JitInitMethodValidator()]
autojit_validators = [ArgcountMethodValidator(), InitMethodValidator()]

#------------------------------------------------------------------------
# Inheritance Validators
#------------------------------------------------------------------------

class InheritanceValidator(object):
    """"""
    Interface for validators that check for compatible inheritance trees.
    """"""

    def validate(self, ext_type, base_ext_type):
        """"""
        Validate an extension type with its parents.
        """"""

class AttributeValidator(object):

    def validate(self, ext_type):
        attr_prefix = utils.get_attributes_type(base).is_prefix(struct_type)

        if not attr_prefix or not method_prefix:
            raise error.NumbaError(
                        ""Multiple incompatible base classes found: ""
                        ""%s and %s"" % (base, bases[-1]))
",Add extension types validator module,"Add extension types validator module
",Python,bsd-2-clause,"GaZ3ll3/numba,gdementen/numba,stuartarchibald/numba,cpcloud/numba,numba/numba,pitrou/numba,sklam/numba,pitrou/numba,pitrou/numba,ssarangi/numba,shiquanwang/numba,GaZ3ll3/numba,sklam/numba,numba/numba,ssarangi/numba,gdementen/numba,jriehl/numba,seibert/numba,GaZ3ll3/numba,IntelLabs/numba,stuartarchibald/numba,cpcloud/numba,stonebig/numba,pitrou/numba,gdementen/numba,ssarangi/numba,stuartarchibald/numba,stonebig/numba,gmarkall/numba,gdementen/numba,gdementen/numba,pombredanne/numba,seibert/numba,IntelLabs/numba,stefanseefeld/numba,numba/numba,jriehl/numba,stefanseefeld/numba,pitrou/numba,stefanseefeld/numba,stonebig/numba,seibert/numba,ssarangi/numba,jriehl/numba,stefanseefeld/numba,stefanseefeld/numba,GaZ3ll3/numba,jriehl/numba,stonebig/numba,stonebig/numba,cpcloud/numba,gmarkall/numba,pombredanne/numba,pombredanne/numba,pombredanne/numba,IntelLabs/numba,gmarkall/numba,GaZ3ll3/numba,shiquanwang/numba,IntelLabs/numba,cpcloud/numba,numba/numba,IntelLabs/numba,jriehl/numba,sklam/numba,pombredanne/numba,sklam/numba,seibert/numba,stuartarchibald/numba,shiquanwang/numba,sklam/numba,stuartarchibald/numba,gmarkall/numba,seibert/numba,cpcloud/numba,ssarangi/numba,numba/numba,gmarkall/numba",103,"```python
# -*- coding: utf-8 -*-

""""""
Validate method signatures and inheritance compatiblity.
""""""

import types
import warnings
import inspect

import numba
from numba import *
from numba import error
from numba import typesystem
from numba.minivect import minitypes


#------------------------------------------------------------------------
# Method Validators
#------------------------------------------------------------------------

class MethodValidator(object):
    ""Interface for method validators""

    def validate(self, method, ext_type):
        """"""
        Validate a Method. Raise an exception for user typing errors.
        """"""

class ArgcountMethodValidator(MethodValidator):
    """"""
    Validate a signature against the number of arguments the function expects.
    """"""

    def validate(self, method, ext_type):
        """"""
        Validate a signature (which is None if not declared by the user)
        for a method.
        """"""
        if method.signature is None:
            return

        nargs = method.py_func.__code__.co_argcount - 1 + method.is_static
        if len(method.signature.args) != nargs:
            raise error.NumbaError(
                ""Expected %d argument types in function ""
                ""%s (don't include 'self')"" % (nargs, method.name))

class InitMethodValidator(MethodValidator):
    """"""
    Validate the init method of extension classes.
    """"""

    def validate(self, method, ext_type):
        if method.name == '__init__' and (method.is_class or method.is_static):
            raise error.NumbaError(""__init__ method should not be a class- ""
                                   ""or staticmethod"")

class JitInitMethodValidator(MethodValidator):
    """"""
    Validate the init method for jit functions. Issue a warning when the
    signature is omitted.
    """"""

    def validate(self, method, ext_type):
        if method.name == '__init__' and method.signature is None:
            self.check_init_args(method, ext_type)

    def check_init_args(self, method, ext_type):
        if inspect.getargspec(method.py_func).args:
            warnings.warn(
                ""Constructor for class '%s' has no signature, ""
                ""assuming arguments have type 'object'"" %
                ext_type.py_class.__name__)


jit_validators = [ArgcountMethodValidator(), InitMethodValidator(), JitInitMethodValidator()]
autojit_validators = [ArgcountMethodValidator(), InitMethodValidator()]

#------------------------------------------------------------------------
# Inheritance Validators
#------------------------------------------------------------------------

class InheritanceValidator(object):
    """"""
    Interface for validators that check for compatible inheritance trees.
    """"""

    def validate(self, ext_type, base_ext_type):
        """"""
        Validate an extension type with its parents.
        """"""

class AttributeValidator(object):

    def validate(self, ext_type):
        attr_prefix = utils.get_attributes_type(base).is_prefix(struct_type)

        if not attr_prefix or not method_prefix:
            raise error.NumbaError(
                        ""Multiple incompatible base classes found: ""
                        ""%s and %s"" % (base, bases[-1]))

```"
cf5151aecf6552faec6232ef0ba57070b546a10d,app/handlers/test_base.py,app/handlers/test_base.py,,"# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

""""""The base for the test related handlers.""""""

try:
    import simplejson as json
except ImportError:
    import json

import handlers.base as hbase
import handlers.common as hcommon
import handlers.response as hresponse
import utils.validator as validator


# pylint: disable=too-many-public-methods
class TestBaseHandler(hbase.BaseHandler):
    """"""Base class for test related API handlers.

    This class provides methods and functions common to all the test API
    endpoint.
    """"""

    def __init__(self, application, request, **kwargs):
        super(TestBaseHandler, self).__init__(application, request, **kwargs)

    @staticmethod
    def _token_validation_func():
        return hcommon.valid_token_tests

    def execute_put(self, *args, **kwargs):
        """"""Execute the PUT pre-operations.""""""
        response = None

        if self.validate_req_token(""PUT""):
            if kwargs and kwargs.get(""id"", None):
                valid_request = self._valid_post_request()

                if valid_request == 200:
                    try:
                        json_obj = json.loads(self.request.body.decode(""utf8""))

                        valid_json, j_reason = validator.is_valid_json(
                            json_obj, self._valid_keys(""PUT""))
                        if valid_json:
                            kwargs[""json_obj""] = json_obj
                            kwargs[""db_options""] = self.settings[""dboptions""]
                            kwargs[""reason""] = j_reason
                            response = self._put(*args, **kwargs)
                        else:
                            response = hresponse.HandlerResponse(400)
                            if j_reason:
                                response.reason = (
                                    ""Provided JSON is not valid: %s"" %
                                    j_reason)
                            else:
                                response.reason = ""Provided JSON is not valid""
                    except ValueError, ex:
                        self.log.exception(ex)
                        error = ""No JSON data found in the PUT request""
                        self.log.error(error)
                        response = hresponse.HandlerResponse(422)
                        response.reason = error
                else:
                    response = hresponse.HandlerResponse(valid_request)
                    response.reason = (
                        ""%s: %s"" %
                        (
                            self._get_status_message(valid_request),
                            ""Use %s as the content type"" % self.content_type
                        )
                    )
            else:
                response = hresponse.HandlerResponse(400)
                response.reason = ""No ID specified""
        else:
            response = hresponse.HandlerResponse(403)
            response.reason = hcommon.NOT_VALID_TOKEN

        return response
",Add base handler for test resources.,"Add base handler for test resources.
",Python,lgpl-2.1,"kernelci/kernelci-backend,joyxu/kernelci-backend,joyxu/kernelci-backend,joyxu/kernelci-backend,kernelci/kernelci-backend",92,"```python
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

""""""The base for the test related handlers.""""""

try:
    import simplejson as json
except ImportError:
    import json

import handlers.base as hbase
import handlers.common as hcommon
import handlers.response as hresponse
import utils.validator as validator


# pylint: disable=too-many-public-methods
class TestBaseHandler(hbase.BaseHandler):
    """"""Base class for test related API handlers.

    This class provides methods and functions common to all the test API
    endpoint.
    """"""

    def __init__(self, application, request, **kwargs):
        super(TestBaseHandler, self).__init__(application, request, **kwargs)

    @staticmethod
    def _token_validation_func():
        return hcommon.valid_token_tests

    def execute_put(self, *args, **kwargs):
        """"""Execute the PUT pre-operations.""""""
        response = None

        if self.validate_req_token(""PUT""):
            if kwargs and kwargs.get(""id"", None):
                valid_request = self._valid_post_request()

                if valid_request == 200:
                    try:
                        json_obj = json.loads(self.request.body.decode(""utf8""))

                        valid_json, j_reason = validator.is_valid_json(
                            json_obj, self._valid_keys(""PUT""))
                        if valid_json:
                            kwargs[""json_obj""] = json_obj
                            kwargs[""db_options""] = self.settings[""dboptions""]
                            kwargs[""reason""] = j_reason
                            response = self._put(*args, **kwargs)
                        else:
                            response = hresponse.HandlerResponse(400)
                            if j_reason:
                                response.reason = (
                                    ""Provided JSON is not valid: %s"" %
                                    j_reason)
                            else:
                                response.reason = ""Provided JSON is not valid""
                    except ValueError, ex:
                        self.log.exception(ex)
                        error = ""No JSON data found in the PUT request""
                        self.log.error(error)
                        response = hresponse.HandlerResponse(422)
                        response.reason = error
                else:
                    response = hresponse.HandlerResponse(valid_request)
                    response.reason = (
                        ""%s: %s"" %
                        (
                            self._get_status_message(valid_request),
                            ""Use %s as the content type"" % self.content_type
                        )
                    )
            else:
                response = hresponse.HandlerResponse(400)
                response.reason = ""No ID specified""
        else:
            response = hresponse.HandlerResponse(403)
            response.reason = hcommon.NOT_VALID_TOKEN

        return response

```"
da1e46d550669970ce0188d417af3764a046a33f,sara_flexbe_states/src/sara_flexbe_states/WonderlandAddPerson.py,sara_flexbe_states/src/sara_flexbe_states/WonderlandAddPerson.py,,"#!/usr/bin/env python
# encoding=utf8

import json

import requests
from flexbe_core import EventState, Logger

""""""
Created on 17/05/2018

@author: Lucas Maurice
""""""


class WonderlandAddPerson(EventState):
    '''
    Add a person.
    >#  entity                  sara_msgs/Entity

    <= done                     return when the add correctly append
    <= already_exit             return when the entity already exist
    <= bad_request              return when error reading data
    <= error                    return when error reading data
    '''

    def __init__(self):
        # See example_state.py for basic explanations.
        super(WonderlandAddPerson, self).__init__(input_keys=['entity'],
                                                  outcomes=['done', 'already_exit', 'bad_request', 'error'])

    def execute(self, userdata):
        # Generate URL to contact

        url = ""http://wonderland:8000/api/people/""

        entity = userdata.entity

        data = {'peopleRecognitionId': entity.face.id}

        if entity.color is not None:
            data.update({'peopleColor': entity.color})

        if entity.pose is not None:
            data.update({'peoplePose': entity.pose})

        if entity.poseProbability is not None:
            data.update({'peoplePoseAccuracy': entity.color})

        if entity.face.gender is not None:
            data.update({'peopleGender': entity.face.gender})

        if entity.face.genderProbability is not None:
            data.update({'peopleGenderAccuracy': entity.face.genderProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.face.emotionProbability:
            data.update({'peopleIsOperator': True})
        else:
            data.update({'peopleIsOperator': False})

        if len(entity.aliases) > 0:
            data.update({'peopleName': entity.aliases[0]})

        # try the request
        try:
            response = requests.post(url, data=data)
            if response.status_code == 400:
                data = json.loads(response.content)
                if 'peopleRecognitionId' in data and data['peopleRecognitionId'][
                    0] == u'people with this peopleRecognitionId already exists.':
                    return 'already_exit'
                else:
                    return 'bad_request'

            elif response.status_code == 500:
                return 'error'
            Logger.logwarn(response.status_code)
            Logger.loginfo(response.content)

        except requests.exceptions.RequestException as e:
            Logger.logerr(e)
            return 'error'

        return 'done'
",Add a step for add a person in Wonderland database.,"Add a step for add a person in Wonderland database.
",Python,bsd-3-clause,"WalkingMachine/sara_behaviors,WalkingMachine/sara_behaviors",91,"```python
#!/usr/bin/env python
# encoding=utf8

import json

import requests
from flexbe_core import EventState, Logger

""""""
Created on 17/05/2018

@author: Lucas Maurice
""""""


class WonderlandAddPerson(EventState):
    '''
    Add a person.
    >#  entity                  sara_msgs/Entity

    <= done                     return when the add correctly append
    <= already_exit             return when the entity already exist
    <= bad_request              return when error reading data
    <= error                    return when error reading data
    '''

    def __init__(self):
        # See example_state.py for basic explanations.
        super(WonderlandAddPerson, self).__init__(input_keys=['entity'],
                                                  outcomes=['done', 'already_exit', 'bad_request', 'error'])

    def execute(self, userdata):
        # Generate URL to contact

        url = ""http://wonderland:8000/api/people/""

        entity = userdata.entity

        data = {'peopleRecognitionId': entity.face.id}

        if entity.color is not None:
            data.update({'peopleColor': entity.color})

        if entity.pose is not None:
            data.update({'peoplePose': entity.pose})

        if entity.poseProbability is not None:
            data.update({'peoplePoseAccuracy': entity.color})

        if entity.face.gender is not None:
            data.update({'peopleGender': entity.face.gender})

        if entity.face.genderProbability is not None:
            data.update({'peopleGenderAccuracy': entity.face.genderProbability})

        if entity.face.emotion is not None:
            data.update({'peopleEmotion': entity.face.emotion})

        if entity.face.emotionProbability is not None:
            data.update({'peopleEmotionAccuracy': entity.face.emotionProbability})

        if entity.face.emotionProbability:
            data.update({'peopleIsOperator': True})
        else:
            data.update({'peopleIsOperator': False})

        if len(entity.aliases) > 0:
            data.update({'peopleName': entity.aliases[0]})

        # try the request
        try:
            response = requests.post(url, data=data)
            if response.status_code == 400:
                data = json.loads(response.content)
                if 'peopleRecognitionId' in data and data['peopleRecognitionId'][
                    0] == u'people with this peopleRecognitionId already exists.':
                    return 'already_exit'
                else:
                    return 'bad_request'

            elif response.status_code == 500:
                return 'error'
            Logger.logwarn(response.status_code)
            Logger.loginfo(response.content)

        except requests.exceptions.RequestException as e:
            Logger.logerr(e)
            return 'error'

        return 'done'

```"
269dbda9c702ddd4809632558a73f4e4eae31d89,scripts/python/light_rpi.py,scripts/python/light_rpi.py,,"#
# IAS Basic device framework.
#
# Author: Joeri Hermans
#

import sys
import socket
import struct
import os
import RPIO

# Global members, which are required for the communication
# with the remote IAS controller.
gDeviceIdentifier = sys.argv[1]
gControllerAddress = sys.argv[2]
gControllerPort = int(sys.argv[3])
gPin = int(sys.argv[4])
gSocket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
gSocket.connect((gControllerAddress,gControllerPort))
gRunning = True
# Light state members.
gState = False

def updateState( stateIdentifier , newValue ):
    global gSocket
    stateIdentifierLength = len(stateIdentifier)
    newValueLength = len(newValue)
    data = struct.pack(""!BBB"",0x01,stateIdentifierLength,newValueLength);
    data += str.encode(stateIdentifier)
    data += str.encode(newValue)
    gSocket.sendall(data)

def authenticate():
    global gDeviceIdentifier
    global gSocket;
    identifierLength = len(gDeviceIdentifier)
    message = struct.pack(""!BB"",0x00,identifierLength) + bytes(gDeviceIdentifier.encode(""ascii""));
    gSocket.sendall(message);

def toggle():
    global gState
    if( gState == True ):
        off()
    else:
        on()

def on():
    global gCommandOn
    global gState
    gState = True
    RPIO.output(gPin,True)
    updateState(""state"",""1"")
    RPIO.input(gPin)

def off():
    global gCommandOff
    global gState
    gState = False
    os.system(gCommandOff)
    RPIO.output(gPin,True)
    updateState(""state"",""0"")
    RPIO.input(gPin)

def processFeature(featureIdentifier,parameter):
    if( featureIdentifier == ""toggle"" ):
        toggle()
    elif( featureIdentifier == ""on"" ):
        on()
    elif( featureIdentifier == ""off"" ):
        off()

def processCommand():
    global gSocket
    global gRunning
    data = gSocket.recv(3);
    data = struct.unpack(""!BBB"",data)
    if( data[0] != 0x01 ):
        gRunning = False
        return
    featureIdentifierLength = data[1]
    parameterLength = data[2]
    featureIdentifier = gSocket.recv(featureIdentifierLength)
    featureIdentifier = featureIdentifier.decode(""ascii"")
    if( parameterLength > 0 ):
        parameter = gSocket.recv(parameterLength)
        parameter = parameter.decode(""ascii"")
    else:
        parameter = """"
    processFeature(featureIdentifier,parameter)

def processCommands():
    global gRunning
    while( gRunning ):
        try:
            processCommand()
        except Exception as e:
            print(e)
            gRunning = False

def main():
    authenticate()
    processCommands()

if( __name__ == ""__main__"" ):
    main()",Add script to control lights from Raspberry Pi.,"Add script to control lights from Raspberry Pi.
",Python,apache-2.0,"JoeriHermans/Intelligent-Automation-System,JoeriHermans/Intelligent-Automation-System,JoeriHermans/Intelligent-Automation-System,JoeriHermans/Intelligent-Automation-System",106,"```python
#
# IAS Basic device framework.
#
# Author: Joeri Hermans
#

import sys
import socket
import struct
import os
import RPIO

# Global members, which are required for the communication
# with the remote IAS controller.
gDeviceIdentifier = sys.argv[1]
gControllerAddress = sys.argv[2]
gControllerPort = int(sys.argv[3])
gPin = int(sys.argv[4])
gSocket = socket.socket(socket.AF_INET,socket.SOCK_STREAM)
gSocket.connect((gControllerAddress,gControllerPort))
gRunning = True
# Light state members.
gState = False

def updateState( stateIdentifier , newValue ):
    global gSocket
    stateIdentifierLength = len(stateIdentifier)
    newValueLength = len(newValue)
    data = struct.pack(""!BBB"",0x01,stateIdentifierLength,newValueLength);
    data += str.encode(stateIdentifier)
    data += str.encode(newValue)
    gSocket.sendall(data)

def authenticate():
    global gDeviceIdentifier
    global gSocket;
    identifierLength = len(gDeviceIdentifier)
    message = struct.pack(""!BB"",0x00,identifierLength) + bytes(gDeviceIdentifier.encode(""ascii""));
    gSocket.sendall(message);

def toggle():
    global gState
    if( gState == True ):
        off()
    else:
        on()

def on():
    global gCommandOn
    global gState
    gState = True
    RPIO.output(gPin,True)
    updateState(""state"",""1"")
    RPIO.input(gPin)

def off():
    global gCommandOff
    global gState
    gState = False
    os.system(gCommandOff)
    RPIO.output(gPin,True)
    updateState(""state"",""0"")
    RPIO.input(gPin)

def processFeature(featureIdentifier,parameter):
    if( featureIdentifier == ""toggle"" ):
        toggle()
    elif( featureIdentifier == ""on"" ):
        on()
    elif( featureIdentifier == ""off"" ):
        off()

def processCommand():
    global gSocket
    global gRunning
    data = gSocket.recv(3);
    data = struct.unpack(""!BBB"",data)
    if( data[0] != 0x01 ):
        gRunning = False
        return
    featureIdentifierLength = data[1]
    parameterLength = data[2]
    featureIdentifier = gSocket.recv(featureIdentifierLength)
    featureIdentifier = featureIdentifier.decode(""ascii"")
    if( parameterLength > 0 ):
        parameter = gSocket.recv(parameterLength)
        parameter = parameter.decode(""ascii"")
    else:
        parameter = """"
    processFeature(featureIdentifier,parameter)

def processCommands():
    global gRunning
    while( gRunning ):
        try:
            processCommand()
        except Exception as e:
            print(e)
            gRunning = False

def main():
    authenticate()
    processCommands()

if( __name__ == ""__main__"" ):
    main()
```"
ca0037e4e7d5983017c169a3d8ae0a5d7a31cddf,tests/tasks/thrift/test_multiplexed.py,tests/tasks/thrift/test_multiplexed.py,,"# Copyright (c) 2014, Facebook, Inc.  All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
#
from __future__ import absolute_import

from sparts.tests.base import MultiTaskTestCase, Skip
from sparts.thrift import compiler
from sparts.thrift.client import ThriftClient

# Make sure we have the thrift-runtime related sparts tasks
try:
    from sparts.tasks.thrift.handler import ThriftHandlerTask
    from sparts.tasks.thrift.nbserver import NBServerTask
except ImportError:
    raise Skip(""Need thrift language bindings to run this test"")

# Make sure we have the thrift compiler
try:
    compiler._require_executable('thrift1', 'thrift')
except AssertionError:
    raise Skip(""Need thrift compiler to run this test"")

# String containing .thrift file contents for some example services
EXAMPLE_SERVICES = """"""
service FooService {
    string makeFoos(1: i16 numfoos),
}

service BarService {
    string makeBars(1: i16 numbars),
}
""""""

# Compile the above service
SERVICES = compiler.CompileContext().importThriftStr(EXAMPLE_SERVICES)

class FooHandler(ThriftHandlerTask):
    MODULE = SERVICES.FooService
    def makeFoos(self, numfoos):
        return ""foo"" * numfoos

class BarHandler(ThriftHandlerTask):
    MODULE = SERVICES.BarService
    SERVICE_NAME = 'bar'

    def makeBars(self, numbars):
        return ""bar"" * numbars

class MultiplexedServer(NBServerTask):
    MULTIPLEX = True

class NonMultiplexedServer(NBServerTask):
    MULTIPLEX = False


class TestMultiplexedServer(MultiTaskTestCase):
    TASKS = [FooHandler, BarHandler, MultiplexedServer]
    
    def testClientWorks(self):
        server = self.service.requireTask(MultiplexedServer)

        # Verify the client and service for FooService/Handler
        client = ThriftClient.for_localhost(
            server.bound_port,
            module=SERVICES.FooService,
            multiplex_service='FooHandler',
        )
        self.assertEqual(
            client.makeFoos(3),
            ""foofoofoo"",
        )

        # Make sure makeBars does not work for FooService
        with self.assertRaises(Exception):
            client.makeBars(1)

        # Verify the client and service for BarService/Handler
        client = ThriftClient.for_localhost(
            server.bound_port,
            module=SERVICES.BarService,
            multiplex_service='bar',
        )
        self.assertEqual(
            client.makeBars(2),
            ""barbar"",
        )

        # Make sure makeFoos does not work for BarService
        with self.assertRaises(Exception):
            client.makeFoos(1)
",Add a test case to verify Multiplexed server/client functionality,"Add a test case to verify Multiplexed server/client functionality
",Python,bsd-3-clause,"facebook/sparts,djipko/sparts,fmoo/sparts,djipko/sparts,fmoo/sparts,facebook/sparts",94,"```python
# Copyright (c) 2014, Facebook, Inc.  All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree. An additional grant
# of patent rights can be found in the PATENTS file in the same directory.
#
from __future__ import absolute_import

from sparts.tests.base import MultiTaskTestCase, Skip
from sparts.thrift import compiler
from sparts.thrift.client import ThriftClient

# Make sure we have the thrift-runtime related sparts tasks
try:
    from sparts.tasks.thrift.handler import ThriftHandlerTask
    from sparts.tasks.thrift.nbserver import NBServerTask
except ImportError:
    raise Skip(""Need thrift language bindings to run this test"")

# Make sure we have the thrift compiler
try:
    compiler._require_executable('thrift1', 'thrift')
except AssertionError:
    raise Skip(""Need thrift compiler to run this test"")

# String containing .thrift file contents for some example services
EXAMPLE_SERVICES = """"""
service FooService {
    string makeFoos(1: i16 numfoos),
}

service BarService {
    string makeBars(1: i16 numbars),
}
""""""

# Compile the above service
SERVICES = compiler.CompileContext().importThriftStr(EXAMPLE_SERVICES)

class FooHandler(ThriftHandlerTask):
    MODULE = SERVICES.FooService
    def makeFoos(self, numfoos):
        return ""foo"" * numfoos

class BarHandler(ThriftHandlerTask):
    MODULE = SERVICES.BarService
    SERVICE_NAME = 'bar'

    def makeBars(self, numbars):
        return ""bar"" * numbars

class MultiplexedServer(NBServerTask):
    MULTIPLEX = True

class NonMultiplexedServer(NBServerTask):
    MULTIPLEX = False


class TestMultiplexedServer(MultiTaskTestCase):
    TASKS = [FooHandler, BarHandler, MultiplexedServer]
    
    def testClientWorks(self):
        server = self.service.requireTask(MultiplexedServer)

        # Verify the client and service for FooService/Handler
        client = ThriftClient.for_localhost(
            server.bound_port,
            module=SERVICES.FooService,
            multiplex_service='FooHandler',
        )
        self.assertEqual(
            client.makeFoos(3),
            ""foofoofoo"",
        )

        # Make sure makeBars does not work for FooService
        with self.assertRaises(Exception):
            client.makeBars(1)

        # Verify the client and service for BarService/Handler
        client = ThriftClient.for_localhost(
            server.bound_port,
            module=SERVICES.BarService,
            multiplex_service='bar',
        )
        self.assertEqual(
            client.makeBars(2),
            ""barbar"",
        )

        # Make sure makeFoos does not work for BarService
        with self.assertRaises(Exception):
            client.makeFoos(1)

```"
1c7a308b77f81f965a69ceead00b096375cca271,lava_server/bread_crumbs.py,lava_server/bread_crumbs.py,,"# Copyright (C) 2010, 2011 Linaro Limited
#
# Author: Zygmunt Krynicki <zygmunt.krynicki@linaro.org>
#
# This file is part of LAVA Server.
#
# LAVA Server is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License version 3
# as published by the Free Software Foundation
#
# LAVA Server is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with LAVA Server.  If not, see <http://www.gnu.org/licenses/>.


""""""
Bread crubm management for LAVA server
""""""

from django.core.urlresolvers import reverse
import logging


class BreadCrumb(object):

    def __init__(self, name, parent=None, needs=None):
        self.name = name
        self.view = None
        self.parent = parent
        self.needs = needs or []

    def __repr__(self):
        return ""<BreadCrumb name=%r view=%r parent=%r>"" % (
            self.name, self.view, self.parent)

    def __call__(self, view):
        self.view = view
        view._bread_crumb = self
        return view

    def get_name(self, kwargs):
        try:
            return self.name.format(**kwargs)
        except:
            logging.exception(""Unable to construct breadcrumb name for view %r"", self.view)
            raise

    def get_absolute_url(self, kwargs):
        try:
            return reverse(self.view, args=[kwargs[name] for name in self.needs])
        except:
            logging.exception(""Unable to construct breadcrumb URL for view %r"", self.view)
            raise


class LiveBreadCrumb(object):

    def __init__(self, bread_crumb, kwargs):
        self.bread_crumb = bread_crumb
        self.kwargs = kwargs

    def get_name(self):
        return self.bread_crumb.get_name(self.kwargs)

    def get_absolute_url(self):
        return self.bread_crumb.get_absolute_url(self.kwargs)


class BreadCrumbTrail(object):

    def __init__(self, bread_crumb_list, kwargs):
        self.bread_crumb_list = bread_crumb_list
        self.kwargs = kwargs

    def __iter__(self):
        for bread_crumb in self.bread_crumb_list:
            yield LiveBreadCrumb(bread_crumb, self.kwargs)

    @classmethod
    def leading_to(cls, view, **kwargs):
        lst = []
        while view is not None:
            lst.append(view._bread_crumb)
            view = view._bread_crumb.parent
        lst.reverse()
        return cls(lst, kwargs or {})
",Add bread crumb helpers from lava-dashboard,"Add bread crumb helpers from lava-dashboard

",Python,agpl-3.0,"OSSystems/lava-server,Linaro/lava-server,Linaro/lava-server,Linaro/lava-server,OSSystems/lava-server,OSSystems/lava-server,Linaro/lava-server",91,"```python
# Copyright (C) 2010, 2011 Linaro Limited
#
# Author: Zygmunt Krynicki <zygmunt.krynicki@linaro.org>
#
# This file is part of LAVA Server.
#
# LAVA Server is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License version 3
# as published by the Free Software Foundation
#
# LAVA Server is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with LAVA Server.  If not, see <http://www.gnu.org/licenses/>.


""""""
Bread crubm management for LAVA server
""""""

from django.core.urlresolvers import reverse
import logging


class BreadCrumb(object):

    def __init__(self, name, parent=None, needs=None):
        self.name = name
        self.view = None
        self.parent = parent
        self.needs = needs or []

    def __repr__(self):
        return ""<BreadCrumb name=%r view=%r parent=%r>"" % (
            self.name, self.view, self.parent)

    def __call__(self, view):
        self.view = view
        view._bread_crumb = self
        return view

    def get_name(self, kwargs):
        try:
            return self.name.format(**kwargs)
        except:
            logging.exception(""Unable to construct breadcrumb name for view %r"", self.view)
            raise

    def get_absolute_url(self, kwargs):
        try:
            return reverse(self.view, args=[kwargs[name] for name in self.needs])
        except:
            logging.exception(""Unable to construct breadcrumb URL for view %r"", self.view)
            raise


class LiveBreadCrumb(object):

    def __init__(self, bread_crumb, kwargs):
        self.bread_crumb = bread_crumb
        self.kwargs = kwargs

    def get_name(self):
        return self.bread_crumb.get_name(self.kwargs)

    def get_absolute_url(self):
        return self.bread_crumb.get_absolute_url(self.kwargs)


class BreadCrumbTrail(object):

    def __init__(self, bread_crumb_list, kwargs):
        self.bread_crumb_list = bread_crumb_list
        self.kwargs = kwargs

    def __iter__(self):
        for bread_crumb in self.bread_crumb_list:
            yield LiveBreadCrumb(bread_crumb, self.kwargs)

    @classmethod
    def leading_to(cls, view, **kwargs):
        lst = []
        while view is not None:
            lst.append(view._bread_crumb)
            view = view._bread_crumb.parent
        lst.reverse()
        return cls(lst, kwargs or {})

```"
4dffd31c432df77f4a65f50d76d96d27079e7fe5,link-table/link_table.py,link-table/link_table.py,,"import copy


class Node(object):

    def __init__(self, val=None):
        self.val = val
        self.next = None



def reverse_link_table1(hd):
    if hd == None or hd.next == None:
        return hd
    pre = hd
    nxt = hd.next
    while nxt.next!= None:
        tmp = nxt.next
        nxt.next = pre
        pre = nxt
        nxt = tmp
    nxt.next = pre
    hd.next = None
    return nxt


def reverse_link_table2(hd):
    cur = None
    while hd is not None:
        tmp = hd.next
        hd.next = cur
        cur = hd
        hd = tmp
    return cur


def create_link_table(vals):
    hd = Node(vals[0])
    cur = hd
    for val in vals[1:]:
        cur.next =  Node(val)
        cur = cur.next
    return hd


def print_link_table(hd):
    vals = []
    while hd != None:
        vals.append(hd.val)
        hd = hd.next
    print(vals)

def check_palindrome(hd):
    is_palindrome = True
    step = 0
    fast_ptr = hd
    slow_ptr = hd
    # use fast and slow pointer to find the central pointer of link table
    while fast_ptr and fast_ptr.next :
        step += 1
        fast_ptr = fast_ptr.next.next
        slow_ptr = slow_ptr.next
    left_hd = hd
    # reverse right-side link table and return head pointer
    right_hd = reverse_link_table2(slow_ptr)
    # save right-side link table head pointer
    right_tmp = right_hd
    # check is a palindrome link table
    for i in range(step):
        if right_hd.val != left_hd.val:
            is_palindrome = False
            break
        right_hd = right_hd.next
        left_hd = left_hd.next
    # reverse right-side link table to what it originally looks like
    tmp_ptr = None
    right_ptr = right_tmp
    while tmp_ptr != slow_ptr:
        nxt = right_ptr.next
        right_ptr.next = tmp_ptr
        tmp_ptr = right_ptr
        right_ptr = nxt
    return is_palindrome
    

def find_link_table_middle_point(hd):
    fast_ptr = hd
    slow_ptr = hd
    while fast_ptr and fast_ptr.next:
        fast_ptr = fast_ptr.next.next
        slow_ptr = slow_ptr.next
    return slow_ptr

        

hd = create_link_table('maed')
print_link_table(hd)
hd = reverse_link_table1(hd)
print_link_table(hd)
hd = reverse_link_table2(hd)
print_link_table(hd)
print(check_palindrome(hd))
print_link_table(hd)
mid_ptr = find_link_table_middle_point(hd)
print(mid_ptr.val)
",Add link list python implementation,"Add link list python implementation
",Python,apache-2.0,"free-free/algorithm,free-free/algorithm",106,"```python
import copy


class Node(object):

    def __init__(self, val=None):
        self.val = val
        self.next = None



def reverse_link_table1(hd):
    if hd == None or hd.next == None:
        return hd
    pre = hd
    nxt = hd.next
    while nxt.next!= None:
        tmp = nxt.next
        nxt.next = pre
        pre = nxt
        nxt = tmp
    nxt.next = pre
    hd.next = None
    return nxt


def reverse_link_table2(hd):
    cur = None
    while hd is not None:
        tmp = hd.next
        hd.next = cur
        cur = hd
        hd = tmp
    return cur


def create_link_table(vals):
    hd = Node(vals[0])
    cur = hd
    for val in vals[1:]:
        cur.next =  Node(val)
        cur = cur.next
    return hd


def print_link_table(hd):
    vals = []
    while hd != None:
        vals.append(hd.val)
        hd = hd.next
    print(vals)

def check_palindrome(hd):
    is_palindrome = True
    step = 0
    fast_ptr = hd
    slow_ptr = hd
    # use fast and slow pointer to find the central pointer of link table
    while fast_ptr and fast_ptr.next :
        step += 1
        fast_ptr = fast_ptr.next.next
        slow_ptr = slow_ptr.next
    left_hd = hd
    # reverse right-side link table and return head pointer
    right_hd = reverse_link_table2(slow_ptr)
    # save right-side link table head pointer
    right_tmp = right_hd
    # check is a palindrome link table
    for i in range(step):
        if right_hd.val != left_hd.val:
            is_palindrome = False
            break
        right_hd = right_hd.next
        left_hd = left_hd.next
    # reverse right-side link table to what it originally looks like
    tmp_ptr = None
    right_ptr = right_tmp
    while tmp_ptr != slow_ptr:
        nxt = right_ptr.next
        right_ptr.next = tmp_ptr
        tmp_ptr = right_ptr
        right_ptr = nxt
    return is_palindrome
    

def find_link_table_middle_point(hd):
    fast_ptr = hd
    slow_ptr = hd
    while fast_ptr and fast_ptr.next:
        fast_ptr = fast_ptr.next.next
        slow_ptr = slow_ptr.next
    return slow_ptr

        

hd = create_link_table('maed')
print_link_table(hd)
hd = reverse_link_table1(hd)
print_link_table(hd)
hd = reverse_link_table2(hd)
print_link_table(hd)
print(check_palindrome(hd))
print_link_table(hd)
mid_ptr = find_link_table_middle_point(hd)
print(mid_ptr.val)

```"
bea6560ea76bd90766aa486f74593aab26beee27,compare_real_to_dagsim.py,compare_real_to_dagsim.py,,"#!/usr/bin/env python3

## Copyright 2017 Eugenio Gianniti
##
## Licensed under the Apache License, Version 2.0 (the ""License"");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an ""AS IS"" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

import csv
import os
import sys

from collections import defaultdict
from pathlib import PurePath

if len (sys.argv) != 2:
    print (""error: wrong number of input arguments"", file = sys.stderr)
    sys.exit (2)
else:
    root = sys.argv[1]


def parse_dir_name (directory):
    experiment = query = None
    path = PurePath (directory)
    pieces = path.parts

    if len (pieces) >= 2:
        experiment = pieces[-2]
        query = pieces[-1]

    return experiment, query


def process_simulations (filename):
    results = defaultdict (dict)

    with open (filename, newline = '') as csvfile:
        reader = csv.DictReader (csvfile)

        for row in reader:
            query = row[""Query""]

            if query == row[""Run""]:
                experiment = row[""Experiment""]
                results[experiment][query] = float (row[""SimAvg""])

    return results


def process_summary (filename):
    cumsum = 0.
    count = 0

    with open (filename, newline = '') as csvfile:
        # Skip first line with application class
        try:
            next (csvfile)
        except StopIteration:
            pass

        reader = csv.DictReader (csvfile)

        for row in reader:
            cumsum += float (row[""applicationCompletionTime""]) - float (row[""applicationDeltaBeforeComputing""])
            count += 1

    return cumsum / count


avg_R = defaultdict (dict)

for directory, _, files in os.walk (root):
    for filename in files:
        full_path = os.path.join (directory, filename)

        if filename == ""summary.csv"":
            experiment, query = parse_dir_name (directory)
            result = process_summary (full_path)
            avg_R[experiment][query] = result
        elif filename == ""simulations.csv"":
            sim_R = process_simulations (full_path)

errors = {experiment: {query: (sim_R[experiment][query] - real) / real
                       for query, real in inner.items ()}
                     for experiment, inner in avg_R.items ()}

fields = [""Experiment"", ""Query"", ""Error[1]""]
rows = ([experiment, query, error] for experiment, inner in errors.items ()
        for query, error in inner.items())

writer = csv.writer (sys.stdout)
writer.writerow (fields)
writer.writerows (rows)
",Add script to compare the error between real data and DagSim simulations,"Add script to compare the error between real data and DagSim simulations
",Python,apache-2.0,"deib-polimi/Spark-Log-Parser,deib-polimi/Spark-Log-Parser",103,"```python
#!/usr/bin/env python3

## Copyright 2017 Eugenio Gianniti
##
## Licensed under the Apache License, Version 2.0 (the ""License"");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##     http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an ""AS IS"" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.

import csv
import os
import sys

from collections import defaultdict
from pathlib import PurePath

if len (sys.argv) != 2:
    print (""error: wrong number of input arguments"", file = sys.stderr)
    sys.exit (2)
else:
    root = sys.argv[1]


def parse_dir_name (directory):
    experiment = query = None
    path = PurePath (directory)
    pieces = path.parts

    if len (pieces) >= 2:
        experiment = pieces[-2]
        query = pieces[-1]

    return experiment, query


def process_simulations (filename):
    results = defaultdict (dict)

    with open (filename, newline = '') as csvfile:
        reader = csv.DictReader (csvfile)

        for row in reader:
            query = row[""Query""]

            if query == row[""Run""]:
                experiment = row[""Experiment""]
                results[experiment][query] = float (row[""SimAvg""])

    return results


def process_summary (filename):
    cumsum = 0.
    count = 0

    with open (filename, newline = '') as csvfile:
        # Skip first line with application class
        try:
            next (csvfile)
        except StopIteration:
            pass

        reader = csv.DictReader (csvfile)

        for row in reader:
            cumsum += float (row[""applicationCompletionTime""]) - float (row[""applicationDeltaBeforeComputing""])
            count += 1

    return cumsum / count


avg_R = defaultdict (dict)

for directory, _, files in os.walk (root):
    for filename in files:
        full_path = os.path.join (directory, filename)

        if filename == ""summary.csv"":
            experiment, query = parse_dir_name (directory)
            result = process_summary (full_path)
            avg_R[experiment][query] = result
        elif filename == ""simulations.csv"":
            sim_R = process_simulations (full_path)

errors = {experiment: {query: (sim_R[experiment][query] - real) / real
                       for query, real in inner.items ()}
                     for experiment, inner in avg_R.items ()}

fields = [""Experiment"", ""Query"", ""Error[1]""]
rows = ([experiment, query, error] for experiment, inner in errors.items ()
        for query, error in inner.items())

writer = csv.writer (sys.stdout)
writer.writerow (fields)
writer.writerows (rows)

```"
d3cac0f637a8667497ff311fc94bdceb19330b77,plugins/modules/dedicated_server_monitoring.py,plugins/modules/dedicated_server_monitoring.py,,"#!/usr/bin/python
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.module_utils.basic import AnsibleModule

DOCUMENTATION = '''
---
module: dedicated_server_monitoring
short_description: Enable or disable ovh monitoring on a dedicated server
description:
    - Enable or disable ovh monitoring on a dedicated server
author: Synthesio SRE Team
requirements:
    - ovh >= 0.5.0
options:
    serviceName:
        required: true
        description: The serviceName
    state:
        required: true
        description: Indicate the desired state of monitoring
        choices:
          - present
          - absent

'''

EXAMPLES = '''
synthesio.ovh.dedicated_server_monitoring:
    serviceName: ""{{ serviceName }}""
    state: ""present""
delegate_to: localhost
'''

RETURN = ''' # '''

from ansible_collections.synthesio.ovh.plugins.module_utils.ovh import ovh_api_connect, ovh_argument_spec

try:
    from ovh.exceptions import APIError
    HAS_OVH = True
except ImportError:
    HAS_OVH = False


def run_module():
    module_args = ovh_argument_spec()
    module_args.update(dict(
        serviceName=dict(required=True),
        state=dict(choices=['present', 'absent'], default='present')
    ))

    module = AnsibleModule(
        argument_spec=module_args,
        supports_check_mode=True
    )
    client = ovh_api_connect(module)

    serviceName = module.params['serviceName']
    state = module.params['state']

    if state == 'present':
        monitoring_bool = True
    elif state == 'absent':
        monitoring_bool = False

    if module.check_mode:
        module.exit_json(msg=""Monitoring is now {} for {} - (dry run mode)"".format(state, serviceName), changed=True)

    try:
        server_state = client.get('/dedicated/server/%s' % serviceName)

        if server_state['monitoring'] == monitoring_bool:
            module.exit_json(msg=""Monitoring is already {} on {}"".format(state, serviceName), changed=False)

        client.put('/dedicated/server/%s' % serviceName, monitoring=monitoring_bool)

        module.exit_json(msg=""Monitoring is now {} on {}"".format(state, serviceName), changed=True)
    except APIError as api_error:
        module.fail_json(msg=""Failed to call OVH API: {0}"".format(api_error), changed=False)


def main():
    run_module()


if __name__ == '__main__':
    main()
",Add dedicated server monitoring modulec,"INFRA-6746: Add dedicated server monitoring  modulec

- Then you can add or remove OVH monitoring
",Python,mit,synthesio/infra-ovh-ansible-module,90,"```python
#!/usr/bin/python
from __future__ import (absolute_import, division, print_function)
__metaclass__ = type

from ansible.module_utils.basic import AnsibleModule

DOCUMENTATION = '''
---
module: dedicated_server_monitoring
short_description: Enable or disable ovh monitoring on a dedicated server
description:
    - Enable or disable ovh monitoring on a dedicated server
author: Synthesio SRE Team
requirements:
    - ovh >= 0.5.0
options:
    serviceName:
        required: true
        description: The serviceName
    state:
        required: true
        description: Indicate the desired state of monitoring
        choices:
          - present
          - absent

'''

EXAMPLES = '''
synthesio.ovh.dedicated_server_monitoring:
    serviceName: ""{{ serviceName }}""
    state: ""present""
delegate_to: localhost
'''

RETURN = ''' # '''

from ansible_collections.synthesio.ovh.plugins.module_utils.ovh import ovh_api_connect, ovh_argument_spec

try:
    from ovh.exceptions import APIError
    HAS_OVH = True
except ImportError:
    HAS_OVH = False


def run_module():
    module_args = ovh_argument_spec()
    module_args.update(dict(
        serviceName=dict(required=True),
        state=dict(choices=['present', 'absent'], default='present')
    ))

    module = AnsibleModule(
        argument_spec=module_args,
        supports_check_mode=True
    )
    client = ovh_api_connect(module)

    serviceName = module.params['serviceName']
    state = module.params['state']

    if state == 'present':
        monitoring_bool = True
    elif state == 'absent':
        monitoring_bool = False

    if module.check_mode:
        module.exit_json(msg=""Monitoring is now {} for {} - (dry run mode)"".format(state, serviceName), changed=True)

    try:
        server_state = client.get('/dedicated/server/%s' % serviceName)

        if server_state['monitoring'] == monitoring_bool:
            module.exit_json(msg=""Monitoring is already {} on {}"".format(state, serviceName), changed=False)

        client.put('/dedicated/server/%s' % serviceName, monitoring=monitoring_bool)

        module.exit_json(msg=""Monitoring is now {} on {}"".format(state, serviceName), changed=True)
    except APIError as api_error:
        module.fail_json(msg=""Failed to call OVH API: {0}"".format(api_error), changed=False)


def main():
    run_module()


if __name__ == '__main__':
    main()

```"
10118cdd99a0e7b11c266ded5491099b82f5634c,src/cloudant/design_document.py,src/cloudant/design_document.py,,"#!/usr/bin/env python
""""""
_design_document_

Class representing a Cloudant design document

""""""
from .document import Document
from .views import View


class DesignDocument(Document):
    """"""
    _DesignDocument_

    Specialisation of a document to be a design doc containing
    the various views, shows, lists etc.

    """"""
    def __init__(self, cloudant_database, document_id=None):
        super(DesignDocument, self).__init__(cloudant_database, document_id)

    @property
    def views(self):
        """"""accessor property for views dictionary""""""
        return self['views']

    def add_view(self, view_name, map_func, reduce_func=None):
        """"""
        _add_view_

        Add a new view to this design document, given a map function
        and optional reduce function.

        :param view_name: Name of the view
        :param map_func: str or Code object containing js map function
        :param reduce_func: str or Code object containing js reduce function
        """"""
        v = View(self, view_name, map_func, reduce_func)
        self.views[view_name] = v
        self.save()

    def fetch(self):
        """"""
        _fetch_

        Grab the remote document and populate build the View structure

        """"""
        super(DesignDocument, self).fetch()
        for view_name, view_def in self.get('views', {}).iteritems():
            self['views'][view_name] = View(
                self,
                view_name,
                view_def.get('map'),
                view_def.get('reduce')
            )

    def iterviews(self):
        """"""
        _iterviews_

        Iterate over the view name, view instance

        """"""
        for view_name, view in self.views.iteritems():
            yield view_name, view

    def list_views(self):
        """"""
        _views_

        return a list of available views on this design doc
        """"""
        return self.views.keys()

    def get_view(self, view_name):
        """"""
        _get_view_

        Get a specific view by name.

        """"""
        return self.views.get(view_name)

    def info(self):
        """"""
        retrieve the view info data, returns dictionary

        GET databasename/_design/test/_info
        """"""
        raise NotImplemented(""info not yet implemented"")

    def cleanup(self):
        """"""

        POST /some_database/_view_cleanup

        """"""
        raise NotImplemented(""cleanup not yet implemented"")

    def compact(self):
        """"""
        POST /some_database/_compact/designname
        """"""
        raise NotImplemented(""compact not yet implemented"")
",Copy DesignDocument class to its own module,"Copy DesignDocument class to its own module
",Python,apache-2.0,cloudant/python-cloudant,107,"```python
#!/usr/bin/env python
""""""
_design_document_

Class representing a Cloudant design document

""""""
from .document import Document
from .views import View


class DesignDocument(Document):
    """"""
    _DesignDocument_

    Specialisation of a document to be a design doc containing
    the various views, shows, lists etc.

    """"""
    def __init__(self, cloudant_database, document_id=None):
        super(DesignDocument, self).__init__(cloudant_database, document_id)

    @property
    def views(self):
        """"""accessor property for views dictionary""""""
        return self['views']

    def add_view(self, view_name, map_func, reduce_func=None):
        """"""
        _add_view_

        Add a new view to this design document, given a map function
        and optional reduce function.

        :param view_name: Name of the view
        :param map_func: str or Code object containing js map function
        :param reduce_func: str or Code object containing js reduce function
        """"""
        v = View(self, view_name, map_func, reduce_func)
        self.views[view_name] = v
        self.save()

    def fetch(self):
        """"""
        _fetch_

        Grab the remote document and populate build the View structure

        """"""
        super(DesignDocument, self).fetch()
        for view_name, view_def in self.get('views', {}).iteritems():
            self['views'][view_name] = View(
                self,
                view_name,
                view_def.get('map'),
                view_def.get('reduce')
            )

    def iterviews(self):
        """"""
        _iterviews_

        Iterate over the view name, view instance

        """"""
        for view_name, view in self.views.iteritems():
            yield view_name, view

    def list_views(self):
        """"""
        _views_

        return a list of available views on this design doc
        """"""
        return self.views.keys()

    def get_view(self, view_name):
        """"""
        _get_view_

        Get a specific view by name.

        """"""
        return self.views.get(view_name)

    def info(self):
        """"""
        retrieve the view info data, returns dictionary

        GET databasename/_design/test/_info
        """"""
        raise NotImplemented(""info not yet implemented"")

    def cleanup(self):
        """"""

        POST /some_database/_view_cleanup

        """"""
        raise NotImplemented(""cleanup not yet implemented"")

    def compact(self):
        """"""
        POST /some_database/_compact/designname
        """"""
        raise NotImplemented(""compact not yet implemented"")

```"
ee8067047e86b8f6aa8581b8b8f18e45383ce03e,talkoohakemisto/migrations/versions/1ef4e5f61dac_add_finnish_collations_to_all_text_columns.py,talkoohakemisto/migrations/versions/1ef4e5f61dac_add_finnish_collations_to_all_text_columns.py,,"""""""Add Finnish collations to all text columns

Revision ID: 1ef4e5f61dac
Revises: 485b2296735
Create Date: 2014-02-09 21:51:35.842781

""""""

# revision identifiers, used by Alembic.
revision = '1ef4e5f61dac'
down_revision = '485b2296735'

from alembic import op


def upgrade():
    op.execute(
        '''
        ALTER TABLE municipality
        ALTER COLUMN name
        TYPE varchar(20)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work_type
        ALTER COLUMN name
        TYPE varchar(50)
        COLLATE ""fi_FI.utf8""
        '''
    )

    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN name
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN organizer
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN description
        TYPE text
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN street_address
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN contact_email
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )


def downgrade():
    op.execute(
        '''
        ALTER TABLE municipality
        ALTER COLUMN name
        TYPE varchar(20)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work_type
        ALTER COLUMN name
        TYPE varchar(50)
        '''
    )

    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN name
        TYPE varchar(100)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN organizer
        TYPE varchar(100)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN description
        TYPE text
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN street_address
        TYPE varchar(100)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN contact_email
        TYPE varchar(100)
        '''
    )
",Add Finnish collation to all text columns,"Add Finnish collation to all text columns
",Python,mit,talkoopaiva/talkoohakemisto-api,127,"```python
""""""Add Finnish collations to all text columns

Revision ID: 1ef4e5f61dac
Revises: 485b2296735
Create Date: 2014-02-09 21:51:35.842781

""""""

# revision identifiers, used by Alembic.
revision = '1ef4e5f61dac'
down_revision = '485b2296735'

from alembic import op


def upgrade():
    op.execute(
        '''
        ALTER TABLE municipality
        ALTER COLUMN name
        TYPE varchar(20)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work_type
        ALTER COLUMN name
        TYPE varchar(50)
        COLLATE ""fi_FI.utf8""
        '''
    )

    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN name
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN organizer
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN description
        TYPE text
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN street_address
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN contact_email
        TYPE varchar(100)
        COLLATE ""fi_FI.utf8""
        '''
    )


def downgrade():
    op.execute(
        '''
        ALTER TABLE municipality
        ALTER COLUMN name
        TYPE varchar(20)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work_type
        ALTER COLUMN name
        TYPE varchar(50)
        '''
    )

    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN name
        TYPE varchar(100)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN organizer
        TYPE varchar(100)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN description
        TYPE text
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN street_address
        TYPE varchar(100)
        '''
    )
    op.execute(
        '''
        ALTER TABLE voluntary_work
        ALTER_COLUMN contact_email
        TYPE varchar(100)
        '''
    )

```"
3c8e0de0c5e39ee773ff4860d5dff651741e31fa,plugin/tests/test_mockcontext.py,plugin/tests/test_mockcontext.py,,"from cloudify.mocks import (MockCloudifyContext,
                            MockNodeInstanceContext,
                           )


class MockNodeInstanceContextRelationships(MockNodeInstanceContext):

    def __init__(self, id=None, runtime_properties=None, relationships=None):
        super(MockNodeInstanceContextRelationships, self).__init__(
                                                        id, 
                                                        runtime_properties)
        self._relationships = []
        if relationships:
        	for i, rel in enumerate(relationships):
        		self._relationships.append(MockRelationshipContext(
        										 rel['node_id'],
        										 rel['relationship_properties'], 
        										 rel['relationship_type'])
        									)
        self._instance = MockNodeInstanceContext(id, runtime_properties)

    @property
    def relationships(self):
        return self._relationships

    @property
    def instance(self):
        return self._instance
    

class MockRelationshipContext(object):

    def __init__(self, node_id=None, runtime_properties=None, type=None):
        self._target = MockNodeInstanceContextRelationships(node_id, runtime_properties)
        self._type = type

    @property
    def target(self):
        return self._target

    @property
    def type(self):
        return self._type


class MockCloudifyContextRelationships(MockCloudifyContext):
    
    def __init__(self,
                 node_id=None,
                 node_name=None,
                 blueprint_id=None,
                 deployment_id=None,
                 execution_id=None,
                 properties=None,
                 runtime_properties=None,
                 capabilities=None,
                 related=None,
                 source=None,
                 target=None,
                 operation=None,
                 resources=None,
                 provider_context=None,
                 bootstrap_context=None,
                 relationships=None):
        super(MockCloudifyContextRelationships, self).__init__(
                            node_id,
                            node_name,
                            blueprint_id,
                            deployment_id,
                            execution_id,
                            properties,
                            capabilities,
                            related,
                            source,
                            target,
                            operation,
                            resources,
                            provider_context,
                            bootstrap_context,
                            runtime_properties)
        self._instance = MockNodeInstanceContextRelationships(
                                                node_id,
                                                runtime_properties, 
                                                relationships)


''' How to use the MockCloudifyContextRelationships :

    New Property: relationships, Mandatory: No
    Required inputs: 
        - 'node_id': the id of the node (str),
        - 'relationship_type': the type of the relationship (str),
        - 'relationship_properties': a dict of properties

    ctx = MockCloudifyContextRelationships(
				node_id='id_nod',
                node_name='mock',
                blueprint_id='id_blue',
                properties={'prop_1': 'prop_1'},
                runtime_properties={'run_prop_1': 'run_prop_1'},
                relationships=[ {'node_id': 'id_nod',
                				 'relationship_type': 'type',
                				 'relationship_properties': 
                				 	{'runtime_relation': 'runtime_relation'}
                				 },
                				 {'node_id': 'id_nod_2',
                				 'relationship_type': 'type_2',
                				 'relationship_properties': 
                				 	{'runtime_relation': 'runtime_relation_2'}
                				 }
                                ]
                )
'''",ADD (94) class MockCloudifyContextRelationships to mock relationships behaviour,"ADD (94) class MockCloudifyContextRelationships to mock relationships behaviour
",Python,apache-2.0,fastconnect/cloudify-azure-plugin,113,"```python
from cloudify.mocks import (MockCloudifyContext,
                            MockNodeInstanceContext,
                           )


class MockNodeInstanceContextRelationships(MockNodeInstanceContext):

    def __init__(self, id=None, runtime_properties=None, relationships=None):
        super(MockNodeInstanceContextRelationships, self).__init__(
                                                        id, 
                                                        runtime_properties)
        self._relationships = []
        if relationships:
        	for i, rel in enumerate(relationships):
        		self._relationships.append(MockRelationshipContext(
        										 rel['node_id'],
        										 rel['relationship_properties'], 
        										 rel['relationship_type'])
        									)
        self._instance = MockNodeInstanceContext(id, runtime_properties)

    @property
    def relationships(self):
        return self._relationships

    @property
    def instance(self):
        return self._instance
    

class MockRelationshipContext(object):

    def __init__(self, node_id=None, runtime_properties=None, type=None):
        self._target = MockNodeInstanceContextRelationships(node_id, runtime_properties)
        self._type = type

    @property
    def target(self):
        return self._target

    @property
    def type(self):
        return self._type


class MockCloudifyContextRelationships(MockCloudifyContext):
    
    def __init__(self,
                 node_id=None,
                 node_name=None,
                 blueprint_id=None,
                 deployment_id=None,
                 execution_id=None,
                 properties=None,
                 runtime_properties=None,
                 capabilities=None,
                 related=None,
                 source=None,
                 target=None,
                 operation=None,
                 resources=None,
                 provider_context=None,
                 bootstrap_context=None,
                 relationships=None):
        super(MockCloudifyContextRelationships, self).__init__(
                            node_id,
                            node_name,
                            blueprint_id,
                            deployment_id,
                            execution_id,
                            properties,
                            capabilities,
                            related,
                            source,
                            target,
                            operation,
                            resources,
                            provider_context,
                            bootstrap_context,
                            runtime_properties)
        self._instance = MockNodeInstanceContextRelationships(
                                                node_id,
                                                runtime_properties, 
                                                relationships)


''' How to use the MockCloudifyContextRelationships :

    New Property: relationships, Mandatory: No
    Required inputs: 
        - 'node_id': the id of the node (str),
        - 'relationship_type': the type of the relationship (str),
        - 'relationship_properties': a dict of properties

    ctx = MockCloudifyContextRelationships(
				node_id='id_nod',
                node_name='mock',
                blueprint_id='id_blue',
                properties={'prop_1': 'prop_1'},
                runtime_properties={'run_prop_1': 'run_prop_1'},
                relationships=[ {'node_id': 'id_nod',
                				 'relationship_type': 'type',
                				 'relationship_properties': 
                				 	{'runtime_relation': 'runtime_relation'}
                				 },
                				 {'node_id': 'id_nod_2',
                				 'relationship_type': 'type_2',
                				 'relationship_properties': 
                				 	{'runtime_relation': 'runtime_relation_2'}
                				 }
                                ]
                )
'''
```"
6c0b0ea9b6e8ecf8ea1b1185ce7d17d12e9d6976,samples/scheduled_poweroff.py,samples/scheduled_poweroff.py,,"#!/usr/bin/env python
""""""
Written by Gaël Berthaud-Müller
Github : https://github.com/blacksponge

This code is released under the terms of the Apache 2
http://www.apache.org/licenses/LICENSE-2.0.html

Example code for using the task scheduler.
""""""

import atexit
import argparse
import getpass
from datetime import datetime
from pyVmomi import vim
from pyVim import connect


def get_args():
    parser = argparse.ArgumentParser(
        description='Arguments for scheduling a poweroff of a virtual machine')
    parser.add_argument('-s', '--host', required=True, action='store',
                        help='Remote host to connect to')
    parser.add_argument('-o', '--port', type=int, default=443, action='store',
                        help='Port to connect on')
    parser.add_argument('-u', '--user', required=True, action='store',
                        help='User name to use when connecting to host')
    parser.add_argument('-p', '--password', required=False, action='store',
                        help='Password to use when connecting to host')
    parser.add_argument('-d', '--date', required=True, action='store',
                        help='Date and time used to create the scheduled task '
                        'with the format d/m/Y H:M')
    parser.add_argument('-n', '--vmname', required=True, action='store',
                        help='VM name on which the action will be performed')
    args = parser.parse_args()
    return args


def main():
    args = get_args()
    try:
        dt = datetime.strptime(args.date, '%d/%m/%Y %H:%M')
    except ValueError:
        print('Unrecognized date format')
        raise
        return -1

    if args.password:
        password = args.password
    else:
        password = getpass.getpass(prompt='Enter password for host %s and '
                                   'user %s: ' % (args.host, args.user))

    try:
        si = connect.SmartConnectNoSSL(host=args.host,
                                       user=args.user,
                                       pwd=password,
                                       port=int(args.port))
    except vim.fault.InvalidLogin:
        print(""Could not connect to the specified host using specified ""
              ""username and password"")
        return -1

    atexit.register(connect.Disconnect, si)

    view = si.content.viewManager.CreateContainerView(si.content.rootFolder,
                                                      [vim.VirtualMachine],
                                                      True)
    vms = [vm for vm in view.view if vm.name == args.vmname]

    if not vms:
        print('VM not found')
        connect.Disconnect(si)
        return -1
    vm = vms[0]

    spec = vim.scheduler.ScheduledTaskSpec()
    spec.name = 'PowerOff vm %s' % args.vmname
    spec.description = ''
    spec.scheduler = vim.scheduler.OnceTaskScheduler()
    spec.scheduler.runAt = dt
    spec.action = vim.action.MethodAction()
    spec.action.name = vim.VirtualMachine.PowerOff
    spec.enabled = True

    si.content.scheduledTaskManager.CreateScheduledTask(vm, spec)


if __name__ == ""__main__"":
    main()
",Add sample for using the task scheduler,"Add sample for using the task scheduler
",Python,apache-2.0,"pathcl/pyvmomi-community-samples,vmware/pyvmomi-community-samples,prziborowski/pyvmomi-community-samples,jm66/pyvmomi-community-samples",92,"```python
#!/usr/bin/env python
""""""
Written by Gaël Berthaud-Müller
Github : https://github.com/blacksponge

This code is released under the terms of the Apache 2
http://www.apache.org/licenses/LICENSE-2.0.html

Example code for using the task scheduler.
""""""

import atexit
import argparse
import getpass
from datetime import datetime
from pyVmomi import vim
from pyVim import connect


def get_args():
    parser = argparse.ArgumentParser(
        description='Arguments for scheduling a poweroff of a virtual machine')
    parser.add_argument('-s', '--host', required=True, action='store',
                        help='Remote host to connect to')
    parser.add_argument('-o', '--port', type=int, default=443, action='store',
                        help='Port to connect on')
    parser.add_argument('-u', '--user', required=True, action='store',
                        help='User name to use when connecting to host')
    parser.add_argument('-p', '--password', required=False, action='store',
                        help='Password to use when connecting to host')
    parser.add_argument('-d', '--date', required=True, action='store',
                        help='Date and time used to create the scheduled task '
                        'with the format d/m/Y H:M')
    parser.add_argument('-n', '--vmname', required=True, action='store',
                        help='VM name on which the action will be performed')
    args = parser.parse_args()
    return args


def main():
    args = get_args()
    try:
        dt = datetime.strptime(args.date, '%d/%m/%Y %H:%M')
    except ValueError:
        print('Unrecognized date format')
        raise
        return -1

    if args.password:
        password = args.password
    else:
        password = getpass.getpass(prompt='Enter password for host %s and '
                                   'user %s: ' % (args.host, args.user))

    try:
        si = connect.SmartConnectNoSSL(host=args.host,
                                       user=args.user,
                                       pwd=password,
                                       port=int(args.port))
    except vim.fault.InvalidLogin:
        print(""Could not connect to the specified host using specified ""
              ""username and password"")
        return -1

    atexit.register(connect.Disconnect, si)

    view = si.content.viewManager.CreateContainerView(si.content.rootFolder,
                                                      [vim.VirtualMachine],
                                                      True)
    vms = [vm for vm in view.view if vm.name == args.vmname]

    if not vms:
        print('VM not found')
        connect.Disconnect(si)
        return -1
    vm = vms[0]

    spec = vim.scheduler.ScheduledTaskSpec()
    spec.name = 'PowerOff vm %s' % args.vmname
    spec.description = ''
    spec.scheduler = vim.scheduler.OnceTaskScheduler()
    spec.scheduler.runAt = dt
    spec.action = vim.action.MethodAction()
    spec.action.name = vim.VirtualMachine.PowerOff
    spec.enabled = True

    si.content.scheduledTaskManager.CreateScheduledTask(vm, spec)


if __name__ == ""__main__"":
    main()

```"
ebfbe65c08ed8ee5d44c4c39f83f5e08bba8a1a7,tests/test_misc.py,tests/test_misc.py,,"from flask.ext.resty import Api, GenericModelView
from marshmallow import fields, Schema
import pytest
from sqlalchemy import Column, Integer

import helpers

# -----------------------------------------------------------------------------


@pytest.yield_fixture
def models(db):
    class Widget(db.Model):
        __tablename__ = 'widgets'

        id = Column(Integer, primary_key=True)

    db.create_all()

    yield {
        'widget': Widget,
    }

    db.drop_all()


@pytest.fixture
def schemas():
    class WidgetSchema(Schema):
        id = fields.Integer(as_string=True)

    return {
        'widget': WidgetSchema(),
    }


@pytest.fixture
def views(models, schemas):
    class WidgetViewBase(GenericModelView):
        model = models['widget']
        schema = schemas['widget']

    class WidgetListView(WidgetViewBase):
        def get(self):
            return self.list()

        def post(self):
            return self.create(allow_client_id=True)

    class WidgetView(WidgetViewBase):
        def get(self, id):
            return self.retrieve(id)

    return {
        'widget_list': WidgetListView,
        'widget': WidgetView,
    }


@pytest.fixture(autouse=True)
def data(db, models):
    db.session.add(models['widget']())
    db.session.commit()


# -----------------------------------------------------------------------------


def test_api_prefix(app, views, client):
    api = Api(app, '/api')
    api.add_resource('/widgets', views['widget_list'])

    response = client.get('/api/widgets')
    assert helpers.get_data(response) == [
        {
            'id': '1',
        },
    ]


def test_create_client_id(app, views, client):
    api = Api(app)
    api.add_resource('/widgets', views['widget_list'], views['widget'])

    response = helpers.request(
        client,
        'POST', '/widgets',
        {
            'id': '100',
        },
    )
    assert response.status_code == 201
    assert response.headers['Location'] == 'http://localhost/widgets/100'

    assert helpers.get_data(response) == {
        'id': '100',
    }


def test_resource_rules(app, views, client):
    api = Api(app)
    api.add_resource(
        base_rule='/widget/<id>',
        base_view=views['widget'],
        alternate_rule='/widgets',
        alternate_view=views['widget_list'],
    )

    get_response = client.get('/widget/1')
    assert get_response.status_code == 200

    assert helpers.get_data(get_response) == {
        'id': '1',
    }

    post_response = helpers.request(
        client,
        'POST', '/widgets',
        {},
    )
    assert post_response.status_code == 201
    assert post_response.headers['Location'] == 'http://localhost/widget/2'

    assert helpers.get_data(post_response) == {
        'id': '2',
    }
",Add miscellaneous tests for remaining code,"Add miscellaneous tests for remaining code
",Python,mit,"taion/flask-jsonapiview,4Catalyzer/flask-resty,4Catalyzer/flask-jsonapiview",127,"```python
from flask.ext.resty import Api, GenericModelView
from marshmallow import fields, Schema
import pytest
from sqlalchemy import Column, Integer

import helpers

# -----------------------------------------------------------------------------


@pytest.yield_fixture
def models(db):
    class Widget(db.Model):
        __tablename__ = 'widgets'

        id = Column(Integer, primary_key=True)

    db.create_all()

    yield {
        'widget': Widget,
    }

    db.drop_all()


@pytest.fixture
def schemas():
    class WidgetSchema(Schema):
        id = fields.Integer(as_string=True)

    return {
        'widget': WidgetSchema(),
    }


@pytest.fixture
def views(models, schemas):
    class WidgetViewBase(GenericModelView):
        model = models['widget']
        schema = schemas['widget']

    class WidgetListView(WidgetViewBase):
        def get(self):
            return self.list()

        def post(self):
            return self.create(allow_client_id=True)

    class WidgetView(WidgetViewBase):
        def get(self, id):
            return self.retrieve(id)

    return {
        'widget_list': WidgetListView,
        'widget': WidgetView,
    }


@pytest.fixture(autouse=True)
def data(db, models):
    db.session.add(models['widget']())
    db.session.commit()


# -----------------------------------------------------------------------------


def test_api_prefix(app, views, client):
    api = Api(app, '/api')
    api.add_resource('/widgets', views['widget_list'])

    response = client.get('/api/widgets')
    assert helpers.get_data(response) == [
        {
            'id': '1',
        },
    ]


def test_create_client_id(app, views, client):
    api = Api(app)
    api.add_resource('/widgets', views['widget_list'], views['widget'])

    response = helpers.request(
        client,
        'POST', '/widgets',
        {
            'id': '100',
        },
    )
    assert response.status_code == 201
    assert response.headers['Location'] == 'http://localhost/widgets/100'

    assert helpers.get_data(response) == {
        'id': '100',
    }


def test_resource_rules(app, views, client):
    api = Api(app)
    api.add_resource(
        base_rule='/widget/<id>',
        base_view=views['widget'],
        alternate_rule='/widgets',
        alternate_view=views['widget_list'],
    )

    get_response = client.get('/widget/1')
    assert get_response.status_code == 200

    assert helpers.get_data(get_response) == {
        'id': '1',
    }

    post_response = helpers.request(
        client,
        'POST', '/widgets',
        {},
    )
    assert post_response.status_code == 201
    assert post_response.headers['Location'] == 'http://localhost/widget/2'

    assert helpers.get_data(post_response) == {
        'id': '2',
    }

```"
896160f5291158132c670eae65b7e45dd4a8748f,pox/messenger/mux.py,pox/messenger/mux.py,,"from pox.core import core
from pox.messenger.messenger import *

log = pox.core.getLogger()

class MuxConnection (MessengerConnection):
  def __init__ (self, source, channelName, con):
    MessengerConnection.__init__(self, source, ID=str(id(self)))
    self.channelName = channelName
    self.con = con

    claimed = False
    e = core.messenger.raiseEventNoErrors(ConnectionStarted, self)
    if e is not None:
      claimed = e._claimed

    if not claimed:
      # Unclaimed events get forwarded to here too
      self.addListener(MessageRecieved, self._defaultMessageRecieved, priority=-1) # Low priority

    self._newlines = False

  def send (self, whatever, **kw):
    whatever = dict(whatever)
    whatever['_mux'] = self.channelName
    MessengerConnection.send(self, whatever, **kw)

  def sendRaw (self, data):
    self.con.sendRaw(data)


class MuxSource (object):
  def __init__ (self, con):
    self.listenTo(con)
    self.channels = {}

  def _forget (self, connection):
    if connection in self.channels:
      del self.channels[connection.channelName]
    else:
      log.warn(""Tried to forget a channel I didn't know"")

  def _handle_MessageRecieved (self, event):
    if event.con.isReadable():
      r = event.con.read()
      if type(r) is dict:
        channelName = r.get(""_mux"", None)
        del r['_mux']
        if channelName is not None:
          if channelName not in self.channels:
            # New channel
            channel = MuxConnection(self, channelName, event.con)
            self.channels[channelName] = channel
          else:
            channel = self.channels[channelName]
        elif r.get(""_mux_bye"",False):
          event.con.close()
        else:
          log.warn(""Message to demuxer didn't specify a channel or valid command"")
      else:
        log.warn(""Demuxer only handlers dictionaries"")
    else:
      self._closeAll()

  def _handle_ConnectionClosed (self, event):
    self._closeAll()

  def _closeAll (self):
    channels = self.channels.values()
    for connection in channels:
      connection._close()


class MuxHub (object):
  """"""
  """"""
  def __init__ (self):
    core.messenger.addListener(MessageRecieved, self._handle_global_MessageRecieved)#, weak=True)

  def _handle_global_MessageRecieved (self, event):
    try:
      n = event.con.read()['hello']
      if n['hello'] == 'mux':
        # It's for me!
        event.claim()
        m = MuxSource(event.con)

        print self.__class__.__name__, ""- started conversation with"", event.con
    except:
      pass


def launch ():
  #  core.register(""demux"", MessengerHub())
  global hub
  hub = MuxHub()
",Add totally untested messenger multiplexer,"Add totally untested messenger multiplexer

messenger.mux theoretically lets you use a single messenger connection to talk with
multiple messenger servers.  Send a hello:mux message to have the muxer claim the
connection.  Now you can send messages that include _mux:<conID>.  For every unique
conID, a subconnection will be created.  Messages sent by a service back down that
conID will also include _mux:<conID> key/value pairs.
",Python,apache-2.0,"adusia/pox,adusia/pox,diogommartins/pox,noxrepo/pox,kpengboy/pox-exercise,carlye566/IoT-POX,jacobq/csci5221-viro-project,kpengboy/pox-exercise,xAKLx/pox,xAKLx/pox,diogommartins/pox,kavitshah8/SDNDeveloper,xAKLx/pox,PrincetonUniversity/pox,chenyuntc/pox,kulawczukmarcin/mypox,PrincetonUniversity/pox,adusia/pox,VamsikrishnaNallabothu/pox,kpengboy/pox-exercise,diogommartins/pox,chenyuntc/pox,noxrepo/pox,pthien92/sdn,noxrepo/pox,diogommartins/pox,carlye566/IoT-POX,MurphyMc/pox,kulawczukmarcin/mypox,adusia/pox,diogommartins/pox,andiwundsam/_of_normalize,pthien92/sdn,kpengboy/pox-exercise,PrincetonUniversity/pox,VamsikrishnaNallabothu/pox,xAKLx/pox,MurphyMc/pox,jacobq/csci5221-viro-project,kavitshah8/SDNDeveloper,jacobq/csci5221-viro-project,VamsikrishnaNallabothu/pox,andiwundsam/_of_normalize,PrincetonUniversity/pox,andiwundsam/_of_normalize,carlye566/IoT-POX,waltznetworks/pox,kulawczukmarcin/mypox,VamsikrishnaNallabothu/pox,xAKLx/pox,waltznetworks/pox,MurphyMc/pox,MurphyMc/pox,jacobq/csci5221-viro-project,denovogroup/pox,denovogroup/pox,carlye566/IoT-POX,adusia/pox,PrincetonUniversity/pox,kavitshah8/SDNDeveloper,pthien92/sdn,waltznetworks/pox,MurphyMc/pox,kulawczukmarcin/mypox,chenyuntc/pox,pthien92/sdn,kulawczukmarcin/mypox,kpengboy/pox-exercise,waltznetworks/pox,noxrepo/pox,kavitshah8/SDNDeveloper,chenyuntc/pox,chenyuntc/pox,denovogroup/pox,denovogroup/pox,VamsikrishnaNallabothu/pox,andiwundsam/_of_normalize,carlye566/IoT-POX,waltznetworks/pox,jacobq/csci5221-viro-project,denovogroup/pox,pthien92/sdn",97,"```python
from pox.core import core
from pox.messenger.messenger import *

log = pox.core.getLogger()

class MuxConnection (MessengerConnection):
  def __init__ (self, source, channelName, con):
    MessengerConnection.__init__(self, source, ID=str(id(self)))
    self.channelName = channelName
    self.con = con

    claimed = False
    e = core.messenger.raiseEventNoErrors(ConnectionStarted, self)
    if e is not None:
      claimed = e._claimed

    if not claimed:
      # Unclaimed events get forwarded to here too
      self.addListener(MessageRecieved, self._defaultMessageRecieved, priority=-1) # Low priority

    self._newlines = False

  def send (self, whatever, **kw):
    whatever = dict(whatever)
    whatever['_mux'] = self.channelName
    MessengerConnection.send(self, whatever, **kw)

  def sendRaw (self, data):
    self.con.sendRaw(data)


class MuxSource (object):
  def __init__ (self, con):
    self.listenTo(con)
    self.channels = {}

  def _forget (self, connection):
    if connection in self.channels:
      del self.channels[connection.channelName]
    else:
      log.warn(""Tried to forget a channel I didn't know"")

  def _handle_MessageRecieved (self, event):
    if event.con.isReadable():
      r = event.con.read()
      if type(r) is dict:
        channelName = r.get(""_mux"", None)
        del r['_mux']
        if channelName is not None:
          if channelName not in self.channels:
            # New channel
            channel = MuxConnection(self, channelName, event.con)
            self.channels[channelName] = channel
          else:
            channel = self.channels[channelName]
        elif r.get(""_mux_bye"",False):
          event.con.close()
        else:
          log.warn(""Message to demuxer didn't specify a channel or valid command"")
      else:
        log.warn(""Demuxer only handlers dictionaries"")
    else:
      self._closeAll()

  def _handle_ConnectionClosed (self, event):
    self._closeAll()

  def _closeAll (self):
    channels = self.channels.values()
    for connection in channels:
      connection._close()


class MuxHub (object):
  """"""
  """"""
  def __init__ (self):
    core.messenger.addListener(MessageRecieved, self._handle_global_MessageRecieved)#, weak=True)

  def _handle_global_MessageRecieved (self, event):
    try:
      n = event.con.read()['hello']
      if n['hello'] == 'mux':
        # It's for me!
        event.claim()
        m = MuxSource(event.con)

        print self.__class__.__name__, ""- started conversation with"", event.con
    except:
      pass


def launch ():
  #  core.register(""demux"", MessengerHub())
  global hub
  hub = MuxHub()

```"
ff3c664896a95b119f9b5b6d46adb0f36461f218,ecpy/utils/flags.py,ecpy/utils/flags.py,,"# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# Copyright 2015 by Ecpy Authors, see AUTHORS for more details.
#
# Distributed under the terms of the BSD license.
#
# The full license is in the file LICENCE, distributed with this software.
# -----------------------------------------------------------------------------
""""""Thread safe bit flag with convenient interface.

""""""
from __future__ import (division, unicode_literals, print_function,
                        absolute_import)

from threading import Event, RLock


class BitFlag(object):
    """"""Bit flag conveniency class providing thread safety facilities.

    Parameters
    ----------
    flags : iterable[unicode]
        Name of the flags that this flag understand.

    """"""

    __slots__ = ('flags', '_lock', '_flags', '_events', '_state')

    def __new__(cls, flags):

        self = object.__new__(cls)
        self.flags = flags
        self._flags = {f: 2**i for i, f in enumerate(flags)}
        self._events = {}
        self._lock = RLock()
        self._state = 0

    def set(self, *flags):
        """"""Set specified flags.

        If a flag is already set this is a no-op. If a thread is waiting on a
        flag, it gets notified.

        """"""
        with self._lock:
            for f in flags:
                self._state |= self._flags[f]
                if f in self._events:
                    self._events[f].set()
                    del self._events[f]

    def clear(self, *flags):
        """"""Clear the specified flags.

        If a flag is already cleared this is a no-op. If a thread is waiting
        on a flag clearing, it gets notified.

        """"""
        with self._lock:
            for f in flags:
                self._state &= ~self._flags[f]

    def test(self, *flags):
        """"""Test is all specified flags are set.

        """"""
        res = False
        with self._lock:
            for f in flags:
                res &= self._state & f

        return res

    def wait(self, timeout, *flags):
        """"""Wait till some flags are set.

        Parameters
        ----------

        timeout : float|None
            Maximum time to wait. If None waits forever.

        flags : iterable[unicode]
            Flags upon which to wait.

        Returns
        -------
        result : bool
            False of the method returned because of the timeout.

        """"""
        events = []
        with self._lock:
            for f in flags:
                if not self.test(f):
                    if f not in self._events:
                        self._events[f] = Event()
                    events = self._events[f]

        res = True
        for e in events:
            res &= e.wait(timeout)

        return res
",Add a thread safe convenient bit flag class.,"Add a thread safe convenient bit flag class.
",Python,bsd-3-clause,"Ecpy/ecpy,Ecpy/ecpy",106,"```python
# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# Copyright 2015 by Ecpy Authors, see AUTHORS for more details.
#
# Distributed under the terms of the BSD license.
#
# The full license is in the file LICENCE, distributed with this software.
# -----------------------------------------------------------------------------
""""""Thread safe bit flag with convenient interface.

""""""
from __future__ import (division, unicode_literals, print_function,
                        absolute_import)

from threading import Event, RLock


class BitFlag(object):
    """"""Bit flag conveniency class providing thread safety facilities.

    Parameters
    ----------
    flags : iterable[unicode]
        Name of the flags that this flag understand.

    """"""

    __slots__ = ('flags', '_lock', '_flags', '_events', '_state')

    def __new__(cls, flags):

        self = object.__new__(cls)
        self.flags = flags
        self._flags = {f: 2**i for i, f in enumerate(flags)}
        self._events = {}
        self._lock = RLock()
        self._state = 0

    def set(self, *flags):
        """"""Set specified flags.

        If a flag is already set this is a no-op. If a thread is waiting on a
        flag, it gets notified.

        """"""
        with self._lock:
            for f in flags:
                self._state |= self._flags[f]
                if f in self._events:
                    self._events[f].set()
                    del self._events[f]

    def clear(self, *flags):
        """"""Clear the specified flags.

        If a flag is already cleared this is a no-op. If a thread is waiting
        on a flag clearing, it gets notified.

        """"""
        with self._lock:
            for f in flags:
                self._state &= ~self._flags[f]

    def test(self, *flags):
        """"""Test is all specified flags are set.

        """"""
        res = False
        with self._lock:
            for f in flags:
                res &= self._state & f

        return res

    def wait(self, timeout, *flags):
        """"""Wait till some flags are set.

        Parameters
        ----------

        timeout : float|None
            Maximum time to wait. If None waits forever.

        flags : iterable[unicode]
            Flags upon which to wait.

        Returns
        -------
        result : bool
            False of the method returned because of the timeout.

        """"""
        events = []
        with self._lock:
            for f in flags:
                if not self.test(f):
                    if f not in self._events:
                        self._events[f] = Event()
                    events = self._events[f]

        res = True
        for e in events:
            res &= e.wait(timeout)

        return res

```"
896c287ad6a5d927febaca4fa957708f783fd51a,shinken/modules/logstore_null.py,shinken/modules/logstore_null.py,,"#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""""""
This class store log broks in a black hole
It is one possibility (!) for an exchangeable storage for log broks
""""""

import os
import sys
import time
import datetime
import re
from shinken.objects.service import Service

from shinken.basemodule import BaseModule
from shinken.objects.module import Module

properties = {
    'daemons': ['livestatus'],
    'type': 'logstore_null',
    'external': False,
    'phases': ['running'],
    }


# called by the plugin manager
def get_instance(plugin):
    print ""Get an LogStore Null module for plugin %s"" % plugin.get_name()
    instance = LiveStatusLogStoreNull(plugin)
    return instance


class LiveStatusLogStoreNull(BaseModule):

    def __init__(self, modconf):
        BaseModule.__init__(self, modconf)
        self.plugins = []

        # Now sleep one second, so that won't get lineno collisions with the last second
        time.sleep(1)
        Logline.lineno = 0

    def load(self, app):
        self.app = app

    def init(self):
        pass

    def open(self):
        print ""open LiveStatusLogStoreNull ok""

    def close(self):
        pass

    def commit(self):
        pass

    def commit_and_rotate_log_db(self):
        pass

    def manage_log_brok(self, b):
        # log brok successfully stored in the black hole
        pass

    def add_filter(self, operator, attribute, reference):
        pass

    def add_filter_and(self, andnum):
        pass

    def add_filter_or(self, ornum):
        pass

    def add_filter_not(self):
        pass

    def get_live_data_log(self):
        """"""Like get_live_data, but for log objects""""""
        result = []
        return result
",Add a null LogStore backend for livestatus broker,"Add a null LogStore backend for livestatus broker
",Python,agpl-3.0,"claneys/shinken,KerkhoffTechnologies/shinken,Simage/shinken,Simage/shinken,claneys/shinken,dfranco/shinken,gst/alignak,h4wkmoon/shinken,mohierf/shinken,geektophe/shinken,KerkhoffTechnologies/shinken,geektophe/shinken,mohierf/shinken,titilambert/alignak,tal-nino/shinken,staute/shinken_package,lets-software/shinken,geektophe/shinken,h4wkmoon/shinken,lets-software/shinken,tal-nino/shinken,rednach/krill,Aimage/shinken,Aimage/shinken,naparuba/shinken,claneys/shinken,dfranco/shinken,kaji-project/shinken,h4wkmoon/shinken,xorpaul/shinken,mohierf/shinken,savoirfairelinux/shinken,ddurieux/alignak,kaji-project/shinken,claneys/shinken,naparuba/shinken,fpeyre/shinken,xorpaul/shinken,tal-nino/shinken,tal-nino/shinken,h4wkmoon/shinken,rledisez/shinken,tal-nino/shinken,lets-software/shinken,lets-software/shinken,KerkhoffTechnologies/shinken,savoirfairelinux/shinken,ddurieux/alignak,naparuba/shinken,Aimage/shinken,staute/shinken_package,geektophe/shinken,geektophe/shinken,titilambert/alignak,Simage/shinken,peeyush-tm/shinken,peeyush-tm/shinken,staute/shinken_deb,gst/alignak,Simage/shinken,KerkhoffTechnologies/shinken,savoirfairelinux/shinken,dfranco/shinken,h4wkmoon/shinken,dfranco/shinken,xorpaul/shinken,rednach/krill,tal-nino/shinken,xorpaul/shinken,kaji-project/shinken,h4wkmoon/shinken,ddurieux/alignak,Simage/shinken,staute/shinken_deb,geektophe/shinken,savoirfairelinux/shinken,dfranco/shinken,claneys/shinken,rednach/krill,lets-software/shinken,fpeyre/shinken,staute/shinken_package,peeyush-tm/shinken,xorpaul/shinken,Alignak-monitoring/alignak,rledisez/shinken,peeyush-tm/shinken,rledisez/shinken,Simage/shinken,peeyush-tm/shinken,fpeyre/shinken,staute/shinken_package,rednach/krill,kaji-project/shinken,ddurieux/alignak,staute/shinken_deb,dfranco/shinken,staute/shinken_package,fpeyre/shinken,titilambert/alignak,rledisez/shinken,kaji-project/shinken,xorpaul/shinken,ddurieux/alignak,kaji-project/shinken,staute/shinken_package,rledisez/shinken,savoirfairelinux/shinken,Aimage/shinken,KerkhoffTechnologies/shinken,mohierf/shinken,Aimage/shinken,mohierf/shinken,xorpaul/shinken,staute/shinken_deb,rledisez/shinken,titilambert/alignak,lets-software/shinken,h4wkmoon/shinken,xorpaul/shinken,naparuba/shinken,savoirfairelinux/shinken,staute/shinken_deb,mohierf/shinken,kaji-project/shinken,h4wkmoon/shinken,naparuba/shinken,gst/alignak,rednach/krill,staute/shinken_deb,claneys/shinken,Alignak-monitoring/alignak,Aimage/shinken,naparuba/shinken,peeyush-tm/shinken,fpeyre/shinken,rednach/krill,fpeyre/shinken,KerkhoffTechnologies/shinken,ddurieux/alignak,gst/alignak",104,"```python
#!/usr/bin/python

# -*- coding: utf-8 -*-

# Copyright (C) 2009-2012:
#    Gabes Jean, naparuba@gmail.com
#    Gerhard Lausser, Gerhard.Lausser@consol.de
#    Gregory Starck, g.starck@gmail.com
#    Hartmut Goebel, h.goebel@goebel-consult.de
#
# This file is part of Shinken.
#
# Shinken is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Shinken is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Shinken.  If not, see <http://www.gnu.org/licenses/>.

""""""
This class store log broks in a black hole
It is one possibility (!) for an exchangeable storage for log broks
""""""

import os
import sys
import time
import datetime
import re
from shinken.objects.service import Service

from shinken.basemodule import BaseModule
from shinken.objects.module import Module

properties = {
    'daemons': ['livestatus'],
    'type': 'logstore_null',
    'external': False,
    'phases': ['running'],
    }


# called by the plugin manager
def get_instance(plugin):
    print ""Get an LogStore Null module for plugin %s"" % plugin.get_name()
    instance = LiveStatusLogStoreNull(plugin)
    return instance


class LiveStatusLogStoreNull(BaseModule):

    def __init__(self, modconf):
        BaseModule.__init__(self, modconf)
        self.plugins = []

        # Now sleep one second, so that won't get lineno collisions with the last second
        time.sleep(1)
        Logline.lineno = 0

    def load(self, app):
        self.app = app

    def init(self):
        pass

    def open(self):
        print ""open LiveStatusLogStoreNull ok""

    def close(self):
        pass

    def commit(self):
        pass

    def commit_and_rotate_log_db(self):
        pass

    def manage_log_brok(self, b):
        # log brok successfully stored in the black hole
        pass

    def add_filter(self, operator, attribute, reference):
        pass

    def add_filter_and(self, andnum):
        pass

    def add_filter_or(self, ornum):
        pass

    def add_filter_not(self):
        pass

    def get_live_data_log(self):
        """"""Like get_live_data, but for log objects""""""
        result = []
        return result

```"
3daedd2bc822465d17f1d7ea47f313dfb7486841,tests/integration/test_demo_build.py,tests/integration/test_demo_build.py,,"""""""Test that a default build of --demo works.""""""

import io
import os
import shutil
import sys

import pytest

import nikola.plugins.command.init
from nikola import __main__
from nikola.utils import LocaleBorg

from ..base import cd

LOCALE_DEFAULT = os.environ.get('NIKOLA_LOCALE_DEFAULT', 'en')


def test_index_in_sitemap(build, output_dir):
    sitemap_path = os.path.join(output_dir, ""sitemap.xml"")
    with io.open(sitemap_path, ""r"", encoding=""utf8"") as inf:
        sitemap_data = inf.read()

    assert '<loc>https://example.com/</loc>' in sitemap_data


def test_avoid_double_slash_in_rss(build, output_dir):
    rss_path = os.path.join(output_dir, ""rss.xml"")
    with io.open(rss_path, ""r"", encoding=""utf8"") as inf:
        rss_data = inf.read()

    assert 'https://example.com//' not in rss_data


def test_archive_exists(build, output_dir):
    """"""Ensure the build did something.""""""
    index_path = os.path.join(output_dir, ""archive.html"")
    assert os.path.isfile(index_path)


@pytest.fixture
def build(target_dir):
    """"""Fill the site with demo content and build it.""""""
    init_command = nikola.plugins.command.init.CommandInit()
    init_command.copy_sample_site(target_dir)
    init_command.create_configuration(target_dir)

    src1 = os.path.join(os.path.dirname(__file__),
                        '..', 'data', '1-nolinks.rst')
    dst1 = os.path.join(target_dir, 'posts', '1.rst')
    shutil.copy(src1, dst1)
    # File for Issue #374 (empty post text)
    with io.open(os.path.join(target_dir, 'posts', 'empty.txt'), ""w+"", encoding=""utf8"") as outf:
        outf.write(
            "".. title: foobar\n""
            "".. slug: foobar\n""
            "".. date: 2013-03-06 19:08:15\n""
        )

    with cd(target_dir):
        __main__.main([""build""])


@pytest.fixture
def output_dir(target_dir):
    return os.path.join(target_dir, ""output"")


@pytest.fixture
def target_dir(tmpdir):
    tdir = os.path.join(str(tmpdir), 'target')
    yield tdir


@pytest.fixture(autouse=True)
def fixIssue438():
    try:
        yield
    finally:
        try:
            del sys.modules['conf']
        except KeyError:
            pass


@pytest.fixture(autouse=True)
def localeborg_setup():
    """"""
    Reset the LocaleBorg before and after every test.
    """"""
    LocaleBorg.reset()
    LocaleBorg.initialize({}, LOCALE_DEFAULT)
    try:
        yield
    finally:
        LocaleBorg.reset()
",Make the demo build available with pytest.,"Make the demo build available with pytest.

This is useful on it's own and for porting further tests.
",Python,mit,"getnikola/nikola,getnikola/nikola,okin/nikola,getnikola/nikola,okin/nikola,okin/nikola,okin/nikola,getnikola/nikola",97,"```python
""""""Test that a default build of --demo works.""""""

import io
import os
import shutil
import sys

import pytest

import nikola.plugins.command.init
from nikola import __main__
from nikola.utils import LocaleBorg

from ..base import cd

LOCALE_DEFAULT = os.environ.get('NIKOLA_LOCALE_DEFAULT', 'en')


def test_index_in_sitemap(build, output_dir):
    sitemap_path = os.path.join(output_dir, ""sitemap.xml"")
    with io.open(sitemap_path, ""r"", encoding=""utf8"") as inf:
        sitemap_data = inf.read()

    assert '<loc>https://example.com/</loc>' in sitemap_data


def test_avoid_double_slash_in_rss(build, output_dir):
    rss_path = os.path.join(output_dir, ""rss.xml"")
    with io.open(rss_path, ""r"", encoding=""utf8"") as inf:
        rss_data = inf.read()

    assert 'https://example.com//' not in rss_data


def test_archive_exists(build, output_dir):
    """"""Ensure the build did something.""""""
    index_path = os.path.join(output_dir, ""archive.html"")
    assert os.path.isfile(index_path)


@pytest.fixture
def build(target_dir):
    """"""Fill the site with demo content and build it.""""""
    init_command = nikola.plugins.command.init.CommandInit()
    init_command.copy_sample_site(target_dir)
    init_command.create_configuration(target_dir)

    src1 = os.path.join(os.path.dirname(__file__),
                        '..', 'data', '1-nolinks.rst')
    dst1 = os.path.join(target_dir, 'posts', '1.rst')
    shutil.copy(src1, dst1)
    # File for Issue #374 (empty post text)
    with io.open(os.path.join(target_dir, 'posts', 'empty.txt'), ""w+"", encoding=""utf8"") as outf:
        outf.write(
            "".. title: foobar\n""
            "".. slug: foobar\n""
            "".. date: 2013-03-06 19:08:15\n""
        )

    with cd(target_dir):
        __main__.main([""build""])


@pytest.fixture
def output_dir(target_dir):
    return os.path.join(target_dir, ""output"")


@pytest.fixture
def target_dir(tmpdir):
    tdir = os.path.join(str(tmpdir), 'target')
    yield tdir


@pytest.fixture(autouse=True)
def fixIssue438():
    try:
        yield
    finally:
        try:
            del sys.modules['conf']
        except KeyError:
            pass


@pytest.fixture(autouse=True)
def localeborg_setup():
    """"""
    Reset the LocaleBorg before and after every test.
    """"""
    LocaleBorg.reset()
    LocaleBorg.initialize({}, LOCALE_DEFAULT)
    try:
        yield
    finally:
        LocaleBorg.reset()

```"
513b64423999f85f0cbc9cc0a44d6ef5962dd171,examples/mdct_synth.py,examples/mdct_synth.py,,"import featureflow as ff
import zounds
from random import choice

samplerate = zounds.SR11025()
BaseDocument = zounds.stft(resample_to=samplerate)


@zounds.simple_lmdb_settings('mdct_synth', map_size=1e10)
class Document(BaseDocument):
    mdct = zounds.TimeFrequencyRepresentationFeature(
            zounds.MDCT,
            needs=BaseDocument.windowed,
            store=True)

    bark = zounds.ConstantRateTimeSeriesFeature(
            zounds.BarkBands,
            needs=BaseDocument.fft,
            store=True)


@zounds.simple_settings
class DctKmeans(ff.BaseModel):
    docs = ff.Feature(
            ff.IteratorNode,
            store=False)

    shuffle = ff.NumpyFeature(
            zounds.ReservoirSampler,
            nsamples=1e6,
            needs=docs,
            store=True)

    log = ff.PickleFeature(
            zounds.Log,
            needs=shuffle,
            store=False)

    unit_norm = ff.PickleFeature(
            zounds.UnitNorm,
            needs=log,
            store=False)

    kmeans = ff.PickleFeature(
            zounds.KMeans,
            centroids=512,
            needs=unit_norm,
            store=False)

    pipeline = ff.PickleFeature(
            zounds.PreprocessingPipeline,
            needs=(log, unit_norm, kmeans),
            store=True)


@zounds.simple_lmdb_settings('mdct_synth_with_codes', map_size=1e10)
class WithCodes(Document):
    kmeans = zounds.ConstantRateTimeSeriesFeature(
            zounds.Learned,
            learned=DctKmeans(),
            needs=Document.mdct,
            store=True)


if __name__ == '__main__':

    # stream all the audio files from the zip archive
    filename = 'FlavioGaete22.zip'
    print 'Processing Audio...'
    for zf in ff.iter_zip(filename):
        if '._' in zf.filename:
            continue
        print zf.filename
        Document.process(meta=zf)

    # learn k-means clusters for the mdct frames
    print 'learning k-means clusters'
    DctKmeans.process(docs=(doc.mdct for doc in Document))

    synth = zounds.MDCTSynthesizer()
    docs = list(doc for doc in WithCodes)
    kmeans = DctKmeans()


    def random_reconstruction():
        doc = choice(docs)
        transform_result = kmeans.pipeline.transform(doc.mdct)
        recon_mdct = transform_result.inverse_transform()
        recon_audio = synth.synthesize(recon_mdct)
        return doc.ogg, recon_audio


    app = zounds.ZoundsApp(
            model=Document,
            audio_feature=Document.ogg,
            visualization_feature=Document.bark,
            globals=globals(),
            locals=locals())
    app.start(8888)
",Add new example for a simple k-means encoded MDCT synthesizer,"Add new example for a simple k-means encoded MDCT synthesizer
",Python,mit,"JohnVinyard/zounds,JohnVinyard/zounds,JohnVinyard/zounds,JohnVinyard/zounds",100,"```python
import featureflow as ff
import zounds
from random import choice

samplerate = zounds.SR11025()
BaseDocument = zounds.stft(resample_to=samplerate)


@zounds.simple_lmdb_settings('mdct_synth', map_size=1e10)
class Document(BaseDocument):
    mdct = zounds.TimeFrequencyRepresentationFeature(
            zounds.MDCT,
            needs=BaseDocument.windowed,
            store=True)

    bark = zounds.ConstantRateTimeSeriesFeature(
            zounds.BarkBands,
            needs=BaseDocument.fft,
            store=True)


@zounds.simple_settings
class DctKmeans(ff.BaseModel):
    docs = ff.Feature(
            ff.IteratorNode,
            store=False)

    shuffle = ff.NumpyFeature(
            zounds.ReservoirSampler,
            nsamples=1e6,
            needs=docs,
            store=True)

    log = ff.PickleFeature(
            zounds.Log,
            needs=shuffle,
            store=False)

    unit_norm = ff.PickleFeature(
            zounds.UnitNorm,
            needs=log,
            store=False)

    kmeans = ff.PickleFeature(
            zounds.KMeans,
            centroids=512,
            needs=unit_norm,
            store=False)

    pipeline = ff.PickleFeature(
            zounds.PreprocessingPipeline,
            needs=(log, unit_norm, kmeans),
            store=True)


@zounds.simple_lmdb_settings('mdct_synth_with_codes', map_size=1e10)
class WithCodes(Document):
    kmeans = zounds.ConstantRateTimeSeriesFeature(
            zounds.Learned,
            learned=DctKmeans(),
            needs=Document.mdct,
            store=True)


if __name__ == '__main__':

    # stream all the audio files from the zip archive
    filename = 'FlavioGaete22.zip'
    print 'Processing Audio...'
    for zf in ff.iter_zip(filename):
        if '._' in zf.filename:
            continue
        print zf.filename
        Document.process(meta=zf)

    # learn k-means clusters for the mdct frames
    print 'learning k-means clusters'
    DctKmeans.process(docs=(doc.mdct for doc in Document))

    synth = zounds.MDCTSynthesizer()
    docs = list(doc for doc in WithCodes)
    kmeans = DctKmeans()


    def random_reconstruction():
        doc = choice(docs)
        transform_result = kmeans.pipeline.transform(doc.mdct)
        recon_mdct = transform_result.inverse_transform()
        recon_audio = synth.synthesize(recon_mdct)
        return doc.ogg, recon_audio


    app = zounds.ZoundsApp(
            model=Document,
            audio_feature=Document.ogg,
            visualization_feature=Document.bark,
            globals=globals(),
            locals=locals())
    app.start(8888)

```"
654b1dd1d9ab3d86be7ab3ca1842157c5e42b66d,tests/automated/test_model_Building.py,tests/automated/test_model_Building.py,,"from django.test import TestCase
from complaints.models import Building


class BuildingTestCase(TestCase):
    """""" Tests for the Building model.
    """"""
    def setUp(self):
        self.b1_name = 'The University of Toronto',
        self.b1_civic_address = '27 King\'s College Circle',
        self.b1_city = 'Toronto',
        self.b1_province = 'Ontario',
        self.b1_latitude = 43.6611024,
        self.b1_longitude = -79.39592909999999

        self.b2_name = '#32 27 King\'s College Circle',

        Building.objects.create(
            name=self.b1_name,
            civic_address=self.b1_civic_address,
            city=self.b1_city,
            province=self.b1_province,
            latitude=self.b1_latitude,
            longitude=self.b1_longitude,
        )

        Building.objects.create(
            name=self.b2_name,
            civic_address=self.b1_civic_address,
            city=self.b1_city,
            province=self.b1_province,
            latitude=self.b1_latitude,
            longitude=self.b1_longitude,
        )

    def test_existence(self):
        """"""
        Test if building exists in the database.

        """"""
        building = Building.objects.get(id=1)

        self.assertTrue(building)
        self.assertEqual(self.b1_name, building.name)
        self.assertEqual(self.b1_civic_address, building.civic_address)
        self.assertEqual(self.b1_city, building.city)
        self.assertEqual(self.b1_province, building.province)
        self.assertEqual(self.b1_latitude, building.latitude)
        self.assertEqual(self.b1_longitude, building.longitude)

    def test_valid_slug_simple(self):
        """"""
        Test if slug fits pattern.

        """"""
        building = Building.objects.get(id=1)
        expected = 'the-university-of-toronto'

        self.assertEqual(expected, building.slug)

    def test_valid_slug_complex(self):
        """"""
        Test if slug of name with symbols fits pattern.

        """"""
        building = Building.objects.get(id=2)
        expected = '32-27-kings-college-circle'

        self.assertEqual(expected, building.slug)

    def test_absolute_url_simple(self):
        """"""
        Test if URL of building fits pattern.

        """"""
        building = Building.objects.get(id=1)
        expected = '/building/1/the-university-of-toronto/'

        self.assertEqual(expected, building.get_absolute_url())

    def test_absolute_url_complex(self):
        """"""
        Test if URL of building that is not first and has more complicated name
        fits pattern.

        """"""
        building = Building.objects.get(id=2)
        expected = '/building/2/32-27-kings-college-circle/'

        self.assertEqual(expected, building.get_absolute_url())
",Add tests for Building model,"Add tests for Building model
",Python,mit,"CSC301H-Fall2013/healthyhome,CSC301H-Fall2013/healthyhome",91,"```python
from django.test import TestCase
from complaints.models import Building


class BuildingTestCase(TestCase):
    """""" Tests for the Building model.
    """"""
    def setUp(self):
        self.b1_name = 'The University of Toronto',
        self.b1_civic_address = '27 King\'s College Circle',
        self.b1_city = 'Toronto',
        self.b1_province = 'Ontario',
        self.b1_latitude = 43.6611024,
        self.b1_longitude = -79.39592909999999

        self.b2_name = '#32 27 King\'s College Circle',

        Building.objects.create(
            name=self.b1_name,
            civic_address=self.b1_civic_address,
            city=self.b1_city,
            province=self.b1_province,
            latitude=self.b1_latitude,
            longitude=self.b1_longitude,
        )

        Building.objects.create(
            name=self.b2_name,
            civic_address=self.b1_civic_address,
            city=self.b1_city,
            province=self.b1_province,
            latitude=self.b1_latitude,
            longitude=self.b1_longitude,
        )

    def test_existence(self):
        """"""
        Test if building exists in the database.

        """"""
        building = Building.objects.get(id=1)

        self.assertTrue(building)
        self.assertEqual(self.b1_name, building.name)
        self.assertEqual(self.b1_civic_address, building.civic_address)
        self.assertEqual(self.b1_city, building.city)
        self.assertEqual(self.b1_province, building.province)
        self.assertEqual(self.b1_latitude, building.latitude)
        self.assertEqual(self.b1_longitude, building.longitude)

    def test_valid_slug_simple(self):
        """"""
        Test if slug fits pattern.

        """"""
        building = Building.objects.get(id=1)
        expected = 'the-university-of-toronto'

        self.assertEqual(expected, building.slug)

    def test_valid_slug_complex(self):
        """"""
        Test if slug of name with symbols fits pattern.

        """"""
        building = Building.objects.get(id=2)
        expected = '32-27-kings-college-circle'

        self.assertEqual(expected, building.slug)

    def test_absolute_url_simple(self):
        """"""
        Test if URL of building fits pattern.

        """"""
        building = Building.objects.get(id=1)
        expected = '/building/1/the-university-of-toronto/'

        self.assertEqual(expected, building.get_absolute_url())

    def test_absolute_url_complex(self):
        """"""
        Test if URL of building that is not first and has more complicated name
        fits pattern.

        """"""
        building = Building.objects.get(id=2)
        expected = '/building/2/32-27-kings-college-circle/'

        self.assertEqual(expected, building.get_absolute_url())

```"
89c5d88c0fc624d0135d513694f5fdb3ed220b08,adaptive/sample_rpc.py,adaptive/sample_rpc.py,,"# Python-to-Python RPC using pickle and HTTP

import os, urllib2, pickle
from BaseHTTPServer import HTTPServer, BaseHTTPRequestHandler

# Server side

class ServiceRequestHandler(BaseHTTPRequestHandler):

    services = {}  # name -> instance

    def do_GET(self):
        res = 404
        error = ""Not found""
        body = None

        components = self.path.split(""/"")
        while components and not components[0].strip():
            components = components[1:]

        while components:
            name = components[0].strip()
            args = ""/"".join(components[1:])

            try:
                instance = ServiceRequestHandler.services[name]
            except KeyError:
                error = ""Not found: %r"" % name
                break

            try:
                command, args, kwargs = pickle.loads(args.decode(""base64""))
            except Exception:
                res = 400
                error = ""Bad Request: Failed to parse operation""
                break

            try:
                method = getattr(instance, command)
            except AttributeError:
                error = ""Not found: %s :: %r"" % (name, command)
                break

            try:
                output = True, method(*args, **kwargs)
            except Exception as exc:
                output = False, exc

            res = 200
            body = pickle.dumps(output)
            error = None
            break

        if error:
            self.send_error(res, error)
            return

        self.send_response(res)
        self.end_headers()
        if body:
            self.wfile.write(body)


def add_instance(name, instance):
    ServiceRequestHandler.services[name] = instance

def serve_forever():
    port = int(os.environ.get(""SERVER_PORT"", 8080))
    server = HTTPServer((""0.0.0.0"", port), ServiceRequestHandler)
    server.serve_forever()


# Client side

class Proxy(object):
    def __init__(self, client, name):
        self.client = client
        self.name = name

    def __call__(self, *args, **kwargs):
        url = self.client.url + ""/"" + pickle.dumps((self.name, args, kwargs)).encode(""base64"").replace(""\n"", """")
        u = urllib2.urlopen(url)
        okay, res = pickle.loads(u.read())
        if okay:
            return res
        raise res


class Client(object):
    def __init__(self, url):
        self.url = url

    def __getattr__(self, attr):
        return Proxy(self, attr)


# Sample

if __name__ == '__main__':
    class Greeting(object):
        def __init__(self):
            self.counter = 0

        def greet(self):
            self.counter += 1
            return '''{""id"":%s,""content"":""Hello, World!""}''' % self.counter

    add_instance(""Greeting"", Greeting())
    serve_forever()
",Add sample RPC client and server framework,Add sample RPC client and server framework,Python,apache-2.0,datawire/adaptive,110,"```python
# Python-to-Python RPC using pickle and HTTP

import os, urllib2, pickle
from BaseHTTPServer import HTTPServer, BaseHTTPRequestHandler

# Server side

class ServiceRequestHandler(BaseHTTPRequestHandler):

    services = {}  # name -> instance

    def do_GET(self):
        res = 404
        error = ""Not found""
        body = None

        components = self.path.split(""/"")
        while components and not components[0].strip():
            components = components[1:]

        while components:
            name = components[0].strip()
            args = ""/"".join(components[1:])

            try:
                instance = ServiceRequestHandler.services[name]
            except KeyError:
                error = ""Not found: %r"" % name
                break

            try:
                command, args, kwargs = pickle.loads(args.decode(""base64""))
            except Exception:
                res = 400
                error = ""Bad Request: Failed to parse operation""
                break

            try:
                method = getattr(instance, command)
            except AttributeError:
                error = ""Not found: %s :: %r"" % (name, command)
                break

            try:
                output = True, method(*args, **kwargs)
            except Exception as exc:
                output = False, exc

            res = 200
            body = pickle.dumps(output)
            error = None
            break

        if error:
            self.send_error(res, error)
            return

        self.send_response(res)
        self.end_headers()
        if body:
            self.wfile.write(body)


def add_instance(name, instance):
    ServiceRequestHandler.services[name] = instance

def serve_forever():
    port = int(os.environ.get(""SERVER_PORT"", 8080))
    server = HTTPServer((""0.0.0.0"", port), ServiceRequestHandler)
    server.serve_forever()


# Client side

class Proxy(object):
    def __init__(self, client, name):
        self.client = client
        self.name = name

    def __call__(self, *args, **kwargs):
        url = self.client.url + ""/"" + pickle.dumps((self.name, args, kwargs)).encode(""base64"").replace(""\n"", """")
        u = urllib2.urlopen(url)
        okay, res = pickle.loads(u.read())
        if okay:
            return res
        raise res


class Client(object):
    def __init__(self, url):
        self.url = url

    def __getattr__(self, attr):
        return Proxy(self, attr)


# Sample

if __name__ == '__main__':
    class Greeting(object):
        def __init__(self):
            self.counter = 0

        def greet(self):
            self.counter += 1
            return '''{""id"":%s,""content"":""Hello, World!""}''' % self.counter

    add_instance(""Greeting"", Greeting())
    serve_forever()

```"
9cd81da23b1c3d5ca5a9d3f805fee9ceb675ee7e,graystruct/__init__.py,graystruct/__init__.py,,"import json
import logging
import os
import socket
import zlib

from graypy.handler import SYSLOG_LEVELS, GELFHandler as BaseGELFHandler
from graypy.rabbitmq import GELFRabbitHandler as BaseGELFRabbitHandler
from structlog._frames import _find_first_app_frame_and_name
from structlog.stdlib import _NAME_TO_LEVEL


STANDARD_GELF_KEYS = (
    'version',
    'host',
    'short_message',
    'full_message',
    'timestamp',
    'level',
    'line',
    'file',
)


def _get_gelf_compatible_key(key):
    if key in STANDARD_GELF_KEYS or key.startswith('_'):
        return key
    return '_{}'.format(key)


def add_app_context(logger, method_name, event_dict):
    f, name = _find_first_app_frame_and_name(['logging', __name__])
    event_dict['file'] = f.f_code.co_filename
    event_dict['line'] = f.f_lineno
    event_dict['function'] = f.f_code.co_name
    return event_dict


class GelfJsonEncoder(object):
    def __init__(self, fqdn=True, localname=None,
                 gelf_keys=STANDARD_GELF_KEYS):
        if fqdn:
            host = socket.getfqdn()
        elif localname is not None:
            host = localname
        else:
            host = socket.gethostname()
        self.host = host
        self.gelf_keys = frozenset(gelf_keys)

    def _translate_non_gelf_keys(self, event_dict):
        return {
            _get_gelf_compatible_key(key): value
            for key, value in event_dict.items()
        }

    def __call__(self, logger, method_name, event_dict):
        levelno = _NAME_TO_LEVEL[method_name]

        gelf_dict = {
            'version': '1.1',
            'host': self.host,
            'level': SYSLOG_LEVELS.get(levelno, levelno),
        }
        if 'message' in event_dict:
            message = event_dict['short_message'] = event_dict.pop('message')
        else:
            message = ''
        if 'exception' in event_dict:
            exc = event_dict.pop('exception')
            event_dict['full_message'] = '\n'.join([message, exc])

        gelf_dict['_pid'] = os.getpid()
        gelf_dict['_logger'] = logger.name
        gelf_dict['_level_name'] = logging.getLevelName(levelno)

        gelf_dict.update(self._translate_non_gelf_keys(event_dict))

        return json.dumps(gelf_dict)


class _CompressHandler(object):
    def makePickle(self, record):
        return zlib.compress(record.msg.encode('urf-8'))


class GELFHandler(_CompressHandler, BaseGELFHandler):
    pass


class GELFRabbitHandler(_CompressHandler, BaseGELFRabbitHandler):
    pass
",Add initial attempt at graystruct,"Add initial attempt at graystruct
",Python,bsd-3-clause,enthought/graystruct,93,"```python
import json
import logging
import os
import socket
import zlib

from graypy.handler import SYSLOG_LEVELS, GELFHandler as BaseGELFHandler
from graypy.rabbitmq import GELFRabbitHandler as BaseGELFRabbitHandler
from structlog._frames import _find_first_app_frame_and_name
from structlog.stdlib import _NAME_TO_LEVEL


STANDARD_GELF_KEYS = (
    'version',
    'host',
    'short_message',
    'full_message',
    'timestamp',
    'level',
    'line',
    'file',
)


def _get_gelf_compatible_key(key):
    if key in STANDARD_GELF_KEYS or key.startswith('_'):
        return key
    return '_{}'.format(key)


def add_app_context(logger, method_name, event_dict):
    f, name = _find_first_app_frame_and_name(['logging', __name__])
    event_dict['file'] = f.f_code.co_filename
    event_dict['line'] = f.f_lineno
    event_dict['function'] = f.f_code.co_name
    return event_dict


class GelfJsonEncoder(object):
    def __init__(self, fqdn=True, localname=None,
                 gelf_keys=STANDARD_GELF_KEYS):
        if fqdn:
            host = socket.getfqdn()
        elif localname is not None:
            host = localname
        else:
            host = socket.gethostname()
        self.host = host
        self.gelf_keys = frozenset(gelf_keys)

    def _translate_non_gelf_keys(self, event_dict):
        return {
            _get_gelf_compatible_key(key): value
            for key, value in event_dict.items()
        }

    def __call__(self, logger, method_name, event_dict):
        levelno = _NAME_TO_LEVEL[method_name]

        gelf_dict = {
            'version': '1.1',
            'host': self.host,
            'level': SYSLOG_LEVELS.get(levelno, levelno),
        }
        if 'message' in event_dict:
            message = event_dict['short_message'] = event_dict.pop('message')
        else:
            message = ''
        if 'exception' in event_dict:
            exc = event_dict.pop('exception')
            event_dict['full_message'] = '\n'.join([message, exc])

        gelf_dict['_pid'] = os.getpid()
        gelf_dict['_logger'] = logger.name
        gelf_dict['_level_name'] = logging.getLevelName(levelno)

        gelf_dict.update(self._translate_non_gelf_keys(event_dict))

        return json.dumps(gelf_dict)


class _CompressHandler(object):
    def makePickle(self, record):
        return zlib.compress(record.msg.encode('urf-8'))


class GELFHandler(_CompressHandler, BaseGELFHandler):
    pass


class GELFRabbitHandler(_CompressHandler, BaseGELFRabbitHandler):
    pass

```"
f12ba3bd1c49f83de7585337518d1e9ac9fb98f7,waterbutler/s3/metadata.py,waterbutler/s3/metadata.py,"import os

from waterbutler.core import metadata


class S3FileMetadata(metadata.BaseMetadata):

    @property
    def provider(self):
        return 's3'

    @property
    def kind(self):
        return 'file'

    @property
    def name(self):
        return os.path.split(self.raw.Key.text)[1]

    @property
    def path(self):
        return self.raw.Key.text

    @property
    def size(self):
        return self.raw.Size.text

    @property
    def modified(self):
        return self.raw.LastModified.text

    @property
    def extra(self):
        return {
            'md5': self.raw.ETag.text.replace('""', '')
        }


class S3FolderMetadata(metadata.BaseMetadata):

    @property
    def provider(self):
        return 's3'

    @property
    def kind(self):
        return 'folder'

    @property
    def name(self):
        return self.raw.Prefix.text.split('/')[-2]

    @property
    def path(self):
        return self.raw.Prefix.text

    @property
    def size(self):
        return None

    @property
    def modified(self):
        return None
","import os

from waterbutler.core import metadata


class S3FileMetadata(metadata.BaseMetadata):

    @property
    def provider(self):
        return 's3'

    @property
    def kind(self):
        return 'file'

    @property
    def name(self):
        return os.path.split(self.raw.Key.text)[1]

    @property
    def path(self):
        return self.raw.Key.text

    @property
    def size(self):
        return self.raw.Size.text

    @property
    def modified(self):
        return self.raw.LastModified.text

    @property
    def extra(self):
        return {
            'md5': self.raw.ETag.text.replace('""', '')
        }


class S3FolderMetadata(metadata.BaseMetadata):

    @property
    def provider(self):
        return 's3'

    @property
    def kind(self):
        return 'folder'

    @property
    def name(self):
        return self.raw.Prefix.text.split('/')[-2]

    @property
    def path(self):
        return self.raw.Prefix.text

    @property
    def size(self):
        return None

    @property
    def modified(self):
        return None


# TODO dates!
class S3Revision(metadata.BaseRevision):

    @property
    def provider(self):
        return 's3'

    @property
    def size(self):
        return int(self.raw.Size.text)

    @property
    def revision(self):
        return self.raw.VersionId.text

    @property
    def modified(self):
        return self.raw.LastModified.text

    @property
    def extra(self):
        return {
            'md5': self.raw.ETag.text.replace('""', '')
        }
",Add an s3 revision class,"Add an s3 revision class
",Python,apache-2.0,"Johnetordoff/waterbutler,TomBaxter/waterbutler,cosenal/waterbutler,felliott/waterbutler,kwierman/waterbutler,chrisseto/waterbutler,rdhyee/waterbutler,RCOSDP/waterbutler,CenterForOpenScience/waterbutler,Ghalko/waterbutler,icereval/waterbutler,hmoco/waterbutler,rafaeldelucena/waterbutler",90,"```python
import os

from waterbutler.core import metadata


class S3FileMetadata(metadata.BaseMetadata):

    @property
    def provider(self):
        return 's3'

    @property
    def kind(self):
        return 'file'

    @property
    def name(self):
        return os.path.split(self.raw.Key.text)[1]

    @property
    def path(self):
        return self.raw.Key.text

    @property
    def size(self):
        return self.raw.Size.text

    @property
    def modified(self):
        return self.raw.LastModified.text

    @property
    def extra(self):
        return {
            'md5': self.raw.ETag.text.replace('""', '')
        }


class S3FolderMetadata(metadata.BaseMetadata):

    @property
    def provider(self):
        return 's3'

    @property
    def kind(self):
        return 'folder'

    @property
    def name(self):
        return self.raw.Prefix.text.split('/')[-2]

    @property
    def path(self):
        return self.raw.Prefix.text

    @property
    def size(self):
        return None

    @property
    def modified(self):
        return None


# TODO dates!
class S3Revision(metadata.BaseRevision):

    @property
    def provider(self):
        return 's3'

    @property
    def size(self):
        return int(self.raw.Size.text)

    @property
    def revision(self):
        return self.raw.VersionId.text

    @property
    def modified(self):
        return self.raw.LastModified.text

    @property
    def extra(self):
        return {
            'md5': self.raw.ETag.text.replace('""', '')
        }

```"
9e2eef4f246c446fbcf05ce29ae309b9a554d46b,app/views/schemas.py,app/views/schemas.py,"from dataclasses import dataclass
from datetime import datetime


@dataclass
class AuthResponse:
    email: str
    image_access: bool
    search_access: bool
    created: datetime
    modified: datetime


@dataclass
class FontResponse:
    filename: str
    id: str
    alias: str
    _self: str


@dataclass
class MemeRequest:
    template_id: str
    style: list[str]
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class CustomRequest:
    background: str
    style: str
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class MemeTemplateRequest:
    style: list[str]
    text: list[str]
    font: str
    extension: str
    redirect: bool


@dataclass
class AutomaticRequest:
    text: str
    safe: bool
    redirect: bool


@dataclass
class MemeResponse:
    url: str


@dataclass
class ExampleResponse:
    url: str
    template: str


@dataclass
class _Example:
    text: list[str]
    url: str


@dataclass
class TemplateResponse:
    id: str
    name: str
    lines: int
    overlays: int
    styles: list[str]
    blank: str
    example: _Example
    source: str
    _self: str


@dataclass
class ErrorResponse:
    error: str
","from dataclasses import dataclass
from datetime import datetime


@dataclass
class AuthResponse:
    email: str
    image_access: bool
    search_access: bool
    created: datetime
    modified: datetime


@dataclass
class FontResponse:
    filename: str
    id: str
    alias: str
    _self: str


@dataclass
class MemeRequest:
    template_id: str
    style: list[str]
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class CustomRequest:
    background: str
    style: str
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class MemeTemplateRequest:
    style: list[str]
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class AutomaticRequest:
    text: str
    safe: bool
    redirect: bool


@dataclass
class MemeResponse:
    url: str


@dataclass
class ExampleResponse:
    url: str
    template: str


@dataclass
class _Example:
    text: list[str]
    url: str


@dataclass
class TemplateResponse:
    id: str
    name: str
    lines: int
    overlays: int
    styles: list[str]
    blank: str
    example: _Example
    source: str
    _self: str


@dataclass
class ErrorResponse:
    error: str
",Support layout on template endpoints,"Support layout on template endpoints
",Python,mit,"jacebrowning/memegen,jacebrowning/memegen",94,"```python
from dataclasses import dataclass
from datetime import datetime


@dataclass
class AuthResponse:
    email: str
    image_access: bool
    search_access: bool
    created: datetime
    modified: datetime


@dataclass
class FontResponse:
    filename: str
    id: str
    alias: str
    _self: str


@dataclass
class MemeRequest:
    template_id: str
    style: list[str]
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class CustomRequest:
    background: str
    style: str
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class MemeTemplateRequest:
    style: list[str]
    text: list[str]
    layout: str
    font: str
    extension: str
    redirect: bool


@dataclass
class AutomaticRequest:
    text: str
    safe: bool
    redirect: bool


@dataclass
class MemeResponse:
    url: str


@dataclass
class ExampleResponse:
    url: str
    template: str


@dataclass
class _Example:
    text: list[str]
    url: str


@dataclass
class TemplateResponse:
    id: str
    name: str
    lines: int
    overlays: int
    styles: list[str]
    blank: str
    example: _Example
    source: str
    _self: str


@dataclass
class ErrorResponse:
    error: str

```"
eedb22b1be419130ffc4a349c3ec4b83879b44bd,client/demo_assignments/hw1_tests.py,client/demo_assignments/hw1_tests.py,"""""""Tests for hw1 demo assignment.""""""

TEST_INFO = {
    'assignment': 'hw1',
    'imports': ['from hw1 import *'],
}

TESTS = [

    # Test square
    {
        'name': ('Q1', 'q1', '1'),
        'suites': [
            [
                ['square(4)', '16'],
                ['square(-5)', '25'],
            ],
        ],
    },


    # Test double
    {
        'name': ('Q2', 'q2', '2'),
        'suites': [
            [
                ['double(4)', '8'],
                ['double(-5)', '-10'],
            ],
        ],
    },

]
","""""""Tests for hw1 demo assignment.""""""

assignment = {
  'name': 'hw1',
  'imports': ['from hw1 import *'],
  'version': '1.0',

  # Specify tests that should not be locked
  'no_lock': {
  },

  'tests': [
    # Test square
    {
      # The first name is the ""official"" name.
      'name': ['Q1', 'q1', '1'],
      # No explicit point value -- each test suite counts as 1 point
      'suites': [
        [
          {
            'type': 'code',     # Code question.
            'input': 'square(4)',
            'output': ['16'],   # List of outputs, even if only one
          },
          {
            'type': 'concept',  # Concept question.
            'input': """"""
            What type of input does the square function take?
            """""",
            'output': [
              # Denote multiple choice with a list, rather than
              # a string.
              [
                'number',         # Correct choice comes first.
                'string',
                'None',
              ]
            ],
          },
          {
            # If type is omitted, default type is 'code'.
            'input': """"""
            x = -5
            square(-5)
            """""",
            # Last line in a multiline input is used as the prompt.
            'output': ['25'],
            # Additional statuses can be included here.
            'status': {
              'lock': False,
            }
          },
        ],
      ],
    },
    # Test double
    {
      'name': ['Q2', 'q2', '2'],
      # Point value specified -- points are partitioned evenly across
      # suites.
      'points': 4,
      'suites': [
        [
          {
            'input': 'double(4)',
            'output': ['8'],
          }
        ],
        [
          {
            # Cases with multiple outputs: lines with expected output
            # are denoted by '$ '.
            'input': """"""
            x = double(4)
            $ x
            $ double(x)
            """""",
            'output': ['8', '16']
          },
          {
            'input': """"""
            x = double(2)
            $ x
            $ square(x)
            """""",
            'output': ['4', '16'],
          },
        ],
      ],
    },
  ],
}
",Make proposed testing format with demo assignment,"Make proposed testing format with demo assignment
",Python,apache-2.0,"jordonwii/ok,jackzhao-mj/ok,jackzhao-mj/ok,Cal-CS-61A-Staff/ok,jordonwii/ok,Cal-CS-61A-Staff/ok,jackzhao-mj/ok,Cal-CS-61A-Staff/ok,jordonwii/ok,Cal-CS-61A-Staff/ok,Cal-CS-61A-Staff/ok,jordonwii/ok,jackzhao-mj/ok",93,"```python
""""""Tests for hw1 demo assignment.""""""

assignment = {
  'name': 'hw1',
  'imports': ['from hw1 import *'],
  'version': '1.0',

  # Specify tests that should not be locked
  'no_lock': {
  },

  'tests': [
    # Test square
    {
      # The first name is the ""official"" name.
      'name': ['Q1', 'q1', '1'],
      # No explicit point value -- each test suite counts as 1 point
      'suites': [
        [
          {
            'type': 'code',     # Code question.
            'input': 'square(4)',
            'output': ['16'],   # List of outputs, even if only one
          },
          {
            'type': 'concept',  # Concept question.
            'input': """"""
            What type of input does the square function take?
            """""",
            'output': [
              # Denote multiple choice with a list, rather than
              # a string.
              [
                'number',         # Correct choice comes first.
                'string',
                'None',
              ]
            ],
          },
          {
            # If type is omitted, default type is 'code'.
            'input': """"""
            x = -5
            square(-5)
            """""",
            # Last line in a multiline input is used as the prompt.
            'output': ['25'],
            # Additional statuses can be included here.
            'status': {
              'lock': False,
            }
          },
        ],
      ],
    },
    # Test double
    {
      'name': ['Q2', 'q2', '2'],
      # Point value specified -- points are partitioned evenly across
      # suites.
      'points': 4,
      'suites': [
        [
          {
            'input': 'double(4)',
            'output': ['8'],
          }
        ],
        [
          {
            # Cases with multiple outputs: lines with expected output
            # are denoted by '$ '.
            'input': """"""
            x = double(4)
            $ x
            $ double(x)
            """""",
            'output': ['8', '16']
          },
          {
            'input': """"""
            x = double(2)
            $ x
            $ square(x)
            """""",
            'output': ['4', '16'],
          },
        ],
      ],
    },
  ],
}

```"
6b72e0bbb09e8f8b6d8821252e34aeca89693441,mt_core/backends/__init__.py,mt_core/backends/__init__.py,"# coding=UTF-8
","# coding=UTF-8


class GuestInfo:
    OS_WINDOWS = ""windows""
    OS_LINUX = ""linux""
    def __init__(self, username, password, os):
        self.username = username
        self.password = password
        self.os = os


class Hypervisor:
    # 完全克隆
    CLONE_FULL = 0
    # 链接克隆
    CLONE_LINKED = 1

    def clone(self, src_vm, dst_vm, type=CLONE_LINKED):
        """"""
        克隆虚拟机
        :param src_vm: 模板虚拟机路径
        :param dst_vm: 目标虚拟机路径
        :param type: 克隆类型
        :return:
        """"""
        pass

    def set_cpu_count(self, cpu_count):
        """"""
        设置CPU个数
        :param cpu_count: CPU个数
        """"""
        pass

    def set_ram(self, ram):
        """"""
        设置内存大小
        :param ram: 内存大小,以MB为单位
        """"""
        pass

    def power_on(self, vm):
        pass

    def power_off(self, vm):
        pass

    def reset(self, vm):
        pass

    def shutdown_guest(self, vm):
        pass

    def restart_guest(self, vm):
        pass

    def create_vlan(self, vm, vlan_name):
        pass

    def delete_vlan(self, vm, vlan_name):
        pass

    def add_nic(self, vm, index, vlan_name):
        pass

    def remove_nic(self, vm, index):
        pass

    def put_file(self, vm, local_path, guest_path, guest_info):
        """"""
        将本地文件放置到客户操作系统中
        :param vm: 虚拟机路径
        :param local_path: 本地文件路径
        :param guest_path: 客户操作系统路径
        :param guest_info: 客户操作系统类型
        """"""
        pass

    def get_file(self, vm, local_path, guest_path, guest_info):
        """"""
        将客户操作系统中的文件传输到本地
        :param vm: 虚拟机路径
        :param local_path: 本地文件路径
        :param guest_path: 客户操作系统路径
        :param guest_info: 客户操作系统类型
        """"""
        pass

    def exec_guest(self, vm, cmd, guest_info):
        """"""
        在虚拟机中执行指定命令
        :param vm: 虚拟机路径
        :param cmd: 命令行
        :param guest_info: 客户操作系统信息
        :return: 返回值
        """"""

    def create_snapshot(self, vm, name):
        """"""
        创建快照
        :param vm: 虚拟机路径
        :param name: 快照名称
        """"""

    def revert_snapshot(self, vm, name):
        """"""
        恢复快照
        :param vm: 虚拟机路径
        :param name: 快照名称
        """"""

",Add Hypervisor base class for workstation and virtualbox,"Add Hypervisor base class for workstation and virtualbox
",Python,mit,CADTS-Bachelor/mini-testbed,113,"```python
# coding=UTF-8


class GuestInfo:
    OS_WINDOWS = ""windows""
    OS_LINUX = ""linux""
    def __init__(self, username, password, os):
        self.username = username
        self.password = password
        self.os = os


class Hypervisor:
    # 完全克隆
    CLONE_FULL = 0
    # 链接克隆
    CLONE_LINKED = 1

    def clone(self, src_vm, dst_vm, type=CLONE_LINKED):
        """"""
        克隆虚拟机
        :param src_vm: 模板虚拟机路径
        :param dst_vm: 目标虚拟机路径
        :param type: 克隆类型
        :return:
        """"""
        pass

    def set_cpu_count(self, cpu_count):
        """"""
        设置CPU个数
        :param cpu_count: CPU个数
        """"""
        pass

    def set_ram(self, ram):
        """"""
        设置内存大小
        :param ram: 内存大小,以MB为单位
        """"""
        pass

    def power_on(self, vm):
        pass

    def power_off(self, vm):
        pass

    def reset(self, vm):
        pass

    def shutdown_guest(self, vm):
        pass

    def restart_guest(self, vm):
        pass

    def create_vlan(self, vm, vlan_name):
        pass

    def delete_vlan(self, vm, vlan_name):
        pass

    def add_nic(self, vm, index, vlan_name):
        pass

    def remove_nic(self, vm, index):
        pass

    def put_file(self, vm, local_path, guest_path, guest_info):
        """"""
        将本地文件放置到客户操作系统中
        :param vm: 虚拟机路径
        :param local_path: 本地文件路径
        :param guest_path: 客户操作系统路径
        :param guest_info: 客户操作系统类型
        """"""
        pass

    def get_file(self, vm, local_path, guest_path, guest_info):
        """"""
        将客户操作系统中的文件传输到本地
        :param vm: 虚拟机路径
        :param local_path: 本地文件路径
        :param guest_path: 客户操作系统路径
        :param guest_info: 客户操作系统类型
        """"""
        pass

    def exec_guest(self, vm, cmd, guest_info):
        """"""
        在虚拟机中执行指定命令
        :param vm: 虚拟机路径
        :param cmd: 命令行
        :param guest_info: 客户操作系统信息
        :return: 返回值
        """"""

    def create_snapshot(self, vm, name):
        """"""
        创建快照
        :param vm: 虚拟机路径
        :param name: 快照名称
        """"""

    def revert_snapshot(self, vm, name):
        """"""
        恢复快照
        :param vm: 虚拟机路径
        :param name: 快照名称
        """"""


```"
573d3d8b652527e0293321e09474f7a6e5b243f4,tests/test_dispatch.py,tests/test_dispatch.py,"import accordian
import pytest


def test_unknown_event(loop):
    """"""
    An exception should be thrown when trying to register a
    handler for an unknown event.
    """"""
    dispatch = accordian.Dispatch(loop=loop)
    with pytest.raises(ValueError):
        dispatch.on(""unknown"")


def test_clean_stop(loop):
    dispatch = accordian.Dispatch(loop=loop)
    loop.run_until_complete(dispatch.start())
    loop.run_until_complete(dispatch.stop())
","import pytest


def test_start_idempotent(loop, dispatch):
    loop.run_until_complete(dispatch.start())
    assert dispatch.running

    loop.run_until_complete(dispatch.start())
    assert dispatch.running


def test_stop_idempotent(loop, dispatch):
    loop.run_until_complete(dispatch.start())
    assert dispatch.running

    loop.run_until_complete(dispatch.stop())
    assert not dispatch.running

    loop.run_until_complete(dispatch.stop())
    assert not dispatch.running


def test_clean_stop(loop, dispatch):
    """""" Stop ensures the main dispatch loop shuts down gracefully """"""
    loop.run_until_complete(dispatch.start())
    loop.run_until_complete(dispatch.stop())


def test_unknown_event(dispatch):
    """"""
    An exception should be thrown when trying to register a
    handler for an unknown event.
    """"""
    with pytest.raises(ValueError):
        dispatch.on(""unknown"")


def test_register(dispatch):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    dispatch.register(event, params)
    assert ""my-event"" in dispatch._handlers


def test_register_twice(dispatch):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    dispatch.register(event, params)

    with pytest.raises(ValueError):
        dispatch.register(event, params)


def test_register_running(dispatch, loop):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    loop.run_until_complete(dispatch.start())

    with pytest.raises(RuntimeError):
        dispatch.register(event, params)
    loop.run_until_complete(dispatch.stop())


def test_unregister_unknown(dispatch):
    assert ""unknown-event"" not in dispatch._handlers
    dispatch.unregister(""unknown-event"")


def test_unregister_running(dispatch, loop):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    dispatch.register(event, params)

    loop.run_until_complete(dispatch.start())
    with pytest.raises(RuntimeError):
        dispatch.unregister(event)
    loop.run_until_complete(dispatch.stop())


def test_single_handler(dispatch, loop):
    event = ""my-event""
    params = {""x"": 4, ""y"": 5, ""z"": 6}
    dispatch.register(event, params.keys())

    called = False

    @dispatch.on(event)
    async def handle(x, y):
        nonlocal called
        called = True

    for task in [
        dispatch.start(),
        dispatch.trigger(event, params),
        dispatch.stop()
    ]:
        loop.run_until_complete(task)
    assert called
","Test dispatch (un)register, basic handler","Test dispatch (un)register, basic handler
",Python,mit,numberoverzero/accordian,99,"```python
import pytest


def test_start_idempotent(loop, dispatch):
    loop.run_until_complete(dispatch.start())
    assert dispatch.running

    loop.run_until_complete(dispatch.start())
    assert dispatch.running


def test_stop_idempotent(loop, dispatch):
    loop.run_until_complete(dispatch.start())
    assert dispatch.running

    loop.run_until_complete(dispatch.stop())
    assert not dispatch.running

    loop.run_until_complete(dispatch.stop())
    assert not dispatch.running


def test_clean_stop(loop, dispatch):
    """""" Stop ensures the main dispatch loop shuts down gracefully """"""
    loop.run_until_complete(dispatch.start())
    loop.run_until_complete(dispatch.stop())


def test_unknown_event(dispatch):
    """"""
    An exception should be thrown when trying to register a
    handler for an unknown event.
    """"""
    with pytest.raises(ValueError):
        dispatch.on(""unknown"")


def test_register(dispatch):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    dispatch.register(event, params)
    assert ""my-event"" in dispatch._handlers


def test_register_twice(dispatch):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    dispatch.register(event, params)

    with pytest.raises(ValueError):
        dispatch.register(event, params)


def test_register_running(dispatch, loop):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    loop.run_until_complete(dispatch.start())

    with pytest.raises(RuntimeError):
        dispatch.register(event, params)
    loop.run_until_complete(dispatch.stop())


def test_unregister_unknown(dispatch):
    assert ""unknown-event"" not in dispatch._handlers
    dispatch.unregister(""unknown-event"")


def test_unregister_running(dispatch, loop):
    event = ""my-event""
    params = [""x"", ""y"", ""z""]
    dispatch.register(event, params)

    loop.run_until_complete(dispatch.start())
    with pytest.raises(RuntimeError):
        dispatch.unregister(event)
    loop.run_until_complete(dispatch.stop())


def test_single_handler(dispatch, loop):
    event = ""my-event""
    params = {""x"": 4, ""y"": 5, ""z"": 6}
    dispatch.register(event, params.keys())

    called = False

    @dispatch.on(event)
    async def handle(x, y):
        nonlocal called
        called = True

    for task in [
        dispatch.start(),
        dispatch.trigger(event, params),
        dispatch.stop()
    ]:
        loop.run_until_complete(task)
    assert called

```"
31af6fefec9770e1ca6663fafe397465732fbf4d,lc0023_merge_k_sorted_lists.py,lc0023_merge_k_sorted_lists.py,"""""""Leetcode 23. Merge k Sorted Lists
Hard

URL: https://leetcode.com/problems/merge-k-sorted-lists/

Merge k sorted linked lists and return it as one sorted list.
Analyze and describe its complexity.

Example:
Input:
[
  1->4->5,
  1->3->4,
  2->6
]
Output: 1->1->2->3->4->4->5->6
""""""

# Definition for singly-linked list.
class ListNode(object):
    def __init__(self, val):
        self.val = val
        self.next = None


class Solution(object):
    def mergeKLists(self, lists):
        """"""
        :type lists: List[ListNode]
        :rtype: ListNode
        """"""
        pass


def main():
    pass


if __name__ == '__main__':
    main()
","""""""Leetcode 23. Merge k Sorted Lists
Hard

URL: https://leetcode.com/problems/merge-k-sorted-lists/

Merge k sorted linked lists and return it as one sorted list.
Analyze and describe its complexity.

Example:
Input:
[
  1->4->5,
  1->3->4,
  2->6
]
Output: 1->1->2->3->4->4->5->6
""""""

# Definition for singly-linked list.
class ListNode(object):
    def __init__(self, val):
        self.val = val
        self.next = None


class SolutionSort(object):
    def mergeKLists(self, lists):
        """"""
        :type lists: List[ListNode]
        :rtype: ListNode

        Time complexity: O(nk*log(nk)), where
          - n is the number of nodes,
          - k is the length of lists.
        Space complexity: O(nk).
        """"""
        # Collect all nodes from list.
        nodes = []

        for head in lists:
            current = head

            while current:
                nodes.append(current)
                current = current.next

        # Sort all nodes by their values.
        sorted_nodes = sorted(nodes, key=lambda x: x.val)

        # Link nodes in sorted_nodes.
        pre_head = ListNode(None)
        current = pre_head

        for node in sorted_nodes:
            current.next = node
            current = current.next

        return pre_head.next


def show(head):
    ls = []

    current = head
    while current:
        ls.append(current.val)
        current = current.next

    print ls



def main():
    # Input:
    # [
    #   1->4->5,
    #   1->3->4,
    #   2->6
    # ]
    # Output: 1->1->2->3->4->4->5->6
    head1 = ListNode(1)
    head1.next = ListNode(4)
    head1.next.next = ListNode(5)

    head2 = ListNode(1)
    head2.next = ListNode(3)
    head2.next.next = ListNode(4)

    head3 = ListNode(2)
    head3.next = ListNode(6)

    lists = [head1, head2, head3]

    head = SolutionSort().mergeKLists(lists)
    show(head)


if __name__ == '__main__':
    main()
",Complete sort sol w/ time/space complexity,"Complete sort sol w/ time/space complexity
",Python,bsd-2-clause,bowen0701/algorithms_data_structures,100,"```python
""""""Leetcode 23. Merge k Sorted Lists
Hard

URL: https://leetcode.com/problems/merge-k-sorted-lists/

Merge k sorted linked lists and return it as one sorted list.
Analyze and describe its complexity.

Example:
Input:
[
  1->4->5,
  1->3->4,
  2->6
]
Output: 1->1->2->3->4->4->5->6
""""""

# Definition for singly-linked list.
class ListNode(object):
    def __init__(self, val):
        self.val = val
        self.next = None


class SolutionSort(object):
    def mergeKLists(self, lists):
        """"""
        :type lists: List[ListNode]
        :rtype: ListNode

        Time complexity: O(nk*log(nk)), where
          - n is the number of nodes,
          - k is the length of lists.
        Space complexity: O(nk).
        """"""
        # Collect all nodes from list.
        nodes = []

        for head in lists:
            current = head

            while current:
                nodes.append(current)
                current = current.next

        # Sort all nodes by their values.
        sorted_nodes = sorted(nodes, key=lambda x: x.val)

        # Link nodes in sorted_nodes.
        pre_head = ListNode(None)
        current = pre_head

        for node in sorted_nodes:
            current.next = node
            current = current.next

        return pre_head.next


def show(head):
    ls = []

    current = head
    while current:
        ls.append(current.val)
        current = current.next

    print ls



def main():
    # Input:
    # [
    #   1->4->5,
    #   1->3->4,
    #   2->6
    # ]
    # Output: 1->1->2->3->4->4->5->6
    head1 = ListNode(1)
    head1.next = ListNode(4)
    head1.next.next = ListNode(5)

    head2 = ListNode(1)
    head2.next = ListNode(3)
    head2.next.next = ListNode(4)

    head3 = ListNode(2)
    head3.next = ListNode(6)

    lists = [head1, head2, head3]

    head = SolutionSort().mergeKLists(lists)
    show(head)


if __name__ == '__main__':
    main()

```"
9e3a6190b2dcfd7de03ef5c974b400a51219839e,pyof/v0x04/symmetric/hello.py,pyof/v0x04/symmetric/hello.py,"""""""Defines Hello message.""""""

# System imports

# Third-party imports

from pyof.v0x01.symmetric.hello import Hello
__all__ = ('Hello',)
","""""""Defines Hello message.""""""

# System imports

from enum import Enum

from pyof.foundation.base import GenericMessage, GenericStruct
from pyof.foundation.basic_types import BinaryData, FixedTypeList, UBInt16
from pyof.v0x04.common.header import Header, Type

# Third-party imports


__all__ = ('Hello', 'HelloElemHeader', 'HelloElemType',
           'HelloElemVersionbitmap', 'ListOfHelloElements')

# Enums


class HelloElemType(Enum):
    """"""Hello element types.""""""

    #: Bitmap of version supported.
    OFPHET_VERSIONBITMAP = 1


# Classes


class HelloElemHeader(GenericStruct):
    """"""Common header for all Hello Elements.""""""

    element_type = UBInt16(enum_ref=HelloElemType)
    length = UBInt16()

    def __init__(self, element_type=None, length=None):
        """"""The constructor just assigns parameters to object attributes.

        Args:
            element_type: One of OFPHET_*.
            length: Length in bytes of the element, including this header,
                excluding padding.
        """"""
        super().__init__()
        self.element_type = element_type
        self.length = length


class ListOfHelloElements(FixedTypeList):
    """"""List of Hello elements.

    Represented by instances of HelloElemHeader and used on Hello
    objects.
    """"""

    def __init__(self, items=None):
        """"""The constructor just assigns parameters to object attributes.

        Args:
            items (HelloElemHeader): Instance or a list of instances.
        """"""
        super().__init__(pyof_class=HelloElemHeader, items=items)


class Hello(GenericMessage):
    """"""OpenFlow Hello Message OFPT_HELLO.

    This message includes zero or more hello elements having variable size.
    Unknown element types must be ignored/skipped, to allow for future
    extensions.
    """"""

    header = Header(message_type=Type.OFPT_HELLO, length=8)
    #: Hello element list
    elements = ListOfHelloElements()

    def __init__(self, xid=None, elements=None):
        """"""The constructor takes the parameters below.

        Args:
            xid (int): xid to be used on the message header.
            elements: List of elements - 0 or more
        """"""
        super().__init__(xid)
        self.elements = elements


class HelloElemVersionbitmap(HelloElemHeader):
    """"""Version bitmap Hello Element.""""""

    #: List of bitmaps - supported versions
    bitmaps = BinaryData()

    def __init__(self, bitmaps=b''):
        """"""The constructor just assigns parameters to object attributes.

        Args:
            bitmaps: -Exactly (length - 4) bytes containing the bitmaps,then
                     -Exactly (length + 7)/8*8 - (length) (between 0 and 7)
                      bytes of all-zero bytes.
        """"""
        super().__init__(element_type=HelloElemType.OFPHET_VERSIONBITMAP,
                         length=None)
        self.bitmaps = bitmaps
",Add Hello class and related classes for v0x04,"Add Hello class and related classes for v0x04

Fix #302
Fix #303
",Python,mit,"cemsbr/python-openflow,kytos/python-openflow",105,"```python
""""""Defines Hello message.""""""

# System imports

from enum import Enum

from pyof.foundation.base import GenericMessage, GenericStruct
from pyof.foundation.basic_types import BinaryData, FixedTypeList, UBInt16
from pyof.v0x04.common.header import Header, Type

# Third-party imports


__all__ = ('Hello', 'HelloElemHeader', 'HelloElemType',
           'HelloElemVersionbitmap', 'ListOfHelloElements')

# Enums


class HelloElemType(Enum):
    """"""Hello element types.""""""

    #: Bitmap of version supported.
    OFPHET_VERSIONBITMAP = 1


# Classes


class HelloElemHeader(GenericStruct):
    """"""Common header for all Hello Elements.""""""

    element_type = UBInt16(enum_ref=HelloElemType)
    length = UBInt16()

    def __init__(self, element_type=None, length=None):
        """"""The constructor just assigns parameters to object attributes.

        Args:
            element_type: One of OFPHET_*.
            length: Length in bytes of the element, including this header,
                excluding padding.
        """"""
        super().__init__()
        self.element_type = element_type
        self.length = length


class ListOfHelloElements(FixedTypeList):
    """"""List of Hello elements.

    Represented by instances of HelloElemHeader and used on Hello
    objects.
    """"""

    def __init__(self, items=None):
        """"""The constructor just assigns parameters to object attributes.

        Args:
            items (HelloElemHeader): Instance or a list of instances.
        """"""
        super().__init__(pyof_class=HelloElemHeader, items=items)


class Hello(GenericMessage):
    """"""OpenFlow Hello Message OFPT_HELLO.

    This message includes zero or more hello elements having variable size.
    Unknown element types must be ignored/skipped, to allow for future
    extensions.
    """"""

    header = Header(message_type=Type.OFPT_HELLO, length=8)
    #: Hello element list
    elements = ListOfHelloElements()

    def __init__(self, xid=None, elements=None):
        """"""The constructor takes the parameters below.

        Args:
            xid (int): xid to be used on the message header.
            elements: List of elements - 0 or more
        """"""
        super().__init__(xid)
        self.elements = elements


class HelloElemVersionbitmap(HelloElemHeader):
    """"""Version bitmap Hello Element.""""""

    #: List of bitmaps - supported versions
    bitmaps = BinaryData()

    def __init__(self, bitmaps=b''):
        """"""The constructor just assigns parameters to object attributes.

        Args:
            bitmaps: -Exactly (length - 4) bytes containing the bitmaps,then
                     -Exactly (length + 7)/8*8 - (length) (between 0 and 7)
                      bytes of all-zero bytes.
        """"""
        super().__init__(element_type=HelloElemType.OFPHET_VERSIONBITMAP,
                         length=None)
        self.bitmaps = bitmaps

```"
55c24a4e47dfd6eab1dcceef8989a2a326322a14,osmABTS/trips.py,osmABTS/trips.py,"""""""
Trip generation
===============

""""""

","""""""
Trip generation
===============

This module can be roughtly devided into two parts, the trip description and
trip generation. The trip description part contains mostly class definitions
that can be used to describe kinds of trips, while the trip generation contains
the main driver function to generate a large list of trips based on the
travellers and places. This module is kind of at the centre of the simulation.

""""""

import random
import collections


#
# Trip description
# ----------------
#
# The trips can be roughtly described by two data structures, Location and
# Trip. A location is a location in the ways of a trip, and a trip is a series
# of locations with a mean frequency and variation.
#
# The location can be an attribute of the traveller or a random selection in a
# category of places. It is stored in the ``source`` attribute as one of the
# two constant symbols in this module. And a trip has a frequency stored in the
# ``freq`` attribute in unit of times per week, and ``var`` stores the
# corresponding variation. The list of locations are given in the ``locations``
# attribute, while the actual route is given in the route attribute as a list
# of zero-based indices in the locations list.
#

# constants for the two kinds of locations
TRAVELLER_ATTR = 1
RANDOM_FROM_CAT = 2
# Location class definition
Location = collections.namedtuple(
    'Location',
    ['source', 'value']
    )


Trip = collections.namedtuple(
    'Trip',
    ['freq', 'var', 'locations', 'route']
    )


# The default trip

DEFAULT_TRIP = [
    # Commuting to work
    Trip(
        freq=5.0, var=1.0,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=TRAVELLER_ATTR, value='work'),
            ],
        route=[0, 1, 0]
        ),
    # Go to a leisure place
    Trip(
        freq=2.0, var=0.5,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=RANDOM_FROM_CAT, value='leisure'),
            ],
        route=[0, 1, 0]
        ),
    # Go to a restaurant
    Trip(
        freq=4.0, var=1.0,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=RANDOM_FROM_CAT, value='restaurant'),
            ],
        route=[0, 1, 0]
        ),
    # Go to a church
    Trip(
        freq=1.0, var=0.5,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=TRAVELLER_ATTR, value='church'),
            ],
        route=[0, 1, 0]
        ),
]
",Implement the trip description classes,"Implement the trip description classes

And a default list for trips has also been written.
",Python,mit,tschijnmo/osmABTS,90,"```python
""""""
Trip generation
===============

This module can be roughtly devided into two parts, the trip description and
trip generation. The trip description part contains mostly class definitions
that can be used to describe kinds of trips, while the trip generation contains
the main driver function to generate a large list of trips based on the
travellers and places. This module is kind of at the centre of the simulation.

""""""

import random
import collections


#
# Trip description
# ----------------
#
# The trips can be roughtly described by two data structures, Location and
# Trip. A location is a location in the ways of a trip, and a trip is a series
# of locations with a mean frequency and variation.
#
# The location can be an attribute of the traveller or a random selection in a
# category of places. It is stored in the ``source`` attribute as one of the
# two constant symbols in this module. And a trip has a frequency stored in the
# ``freq`` attribute in unit of times per week, and ``var`` stores the
# corresponding variation. The list of locations are given in the ``locations``
# attribute, while the actual route is given in the route attribute as a list
# of zero-based indices in the locations list.
#

# constants for the two kinds of locations
TRAVELLER_ATTR = 1
RANDOM_FROM_CAT = 2
# Location class definition
Location = collections.namedtuple(
    'Location',
    ['source', 'value']
    )


Trip = collections.namedtuple(
    'Trip',
    ['freq', 'var', 'locations', 'route']
    )


# The default trip

DEFAULT_TRIP = [
    # Commuting to work
    Trip(
        freq=5.0, var=1.0,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=TRAVELLER_ATTR, value='work'),
            ],
        route=[0, 1, 0]
        ),
    # Go to a leisure place
    Trip(
        freq=2.0, var=0.5,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=RANDOM_FROM_CAT, value='leisure'),
            ],
        route=[0, 1, 0]
        ),
    # Go to a restaurant
    Trip(
        freq=4.0, var=1.0,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=RANDOM_FROM_CAT, value='restaurant'),
            ],
        route=[0, 1, 0]
        ),
    # Go to a church
    Trip(
        freq=1.0, var=0.5,
        locations=[
            Location(source=TRAVELLER_ATTR, value='home'),
            Location(source=TRAVELLER_ATTR, value='church'),
            ],
        route=[0, 1, 0]
        ),
]

```"
693ce5f8b1344f072e1f116ebf3ad79ffaad42b6,fungui.py,fungui.py,"#!/usr/bin/env python

""""""
fungui is a software to help measuring the shell of a fungi.
""""""

# Import modules
from PyQt4 import QtGui, QtCore
","#!/usr/bin/env python

""""""
fungui is a software to help measuring the shell of a fungi.
""""""

# Import modules
from PyQt4 import QtGui, QtCore
import sys


# Global variables
FRAME_WIDTH = 1020
FRAME_HEIGHT = 480

class MainWindow(QtGui.QMainWindow):
    def __init__(self):
        QtGui.QMainWindow.__init__(self)

        # create stuff
        self.wdg = Widget()
        self.setCentralWidget(self.wdg)
        self.createActions()
        self.createMenus()
        #self.createStatusBar()

         # format the main window
        self.resize(FRAME_WIDTH, FRAME_HEIGHT)
        self.center()
        self.setWindowTitle('Fungui')

        # show windows
        self.show()
        self.wdg.show()
        
    def center(self):
        qr = self.frameGeometry()
        cp = QtGui.QDesktopWidget().availableGeometry().center()
        qr.moveCenter(cp)
        self.move(qr.topLeft())        

    def about(self):
        QtGui.QMessageBox.about(self, self.tr(""About fungui""),
            self.tr(""fungui...""))

    def createActions(self):
        self.exitAct = QtGui.QAction(self.tr(""E&xit""), self)
        self.exitAct.setShortcut(self.tr(""Ctrl+Q""))
        self.exitAct.setStatusTip(self.tr(""Exit the application""))
        self.exitAct.triggered.connect(self.close)

        self.aboutAct = QtGui.QAction(self.tr(""&About""), self)
        self.aboutAct.setStatusTip(self.tr(""Show the application's About box""))
        self.aboutAct.triggered.connect(self.about)

        self.aboutQtAct = QtGui.QAction(self.tr(""About &Qt""), self)
        self.aboutQtAct.setStatusTip(self.tr(""Show the Qt library's About box""))
        self.aboutQtAct.triggered.connect(QtGui.qApp.aboutQt)

    def createMenus(self):
        self.fileMenu = self.menuBar().addMenu(self.tr(""&File""))
        self.fileMenu.addAction(self.exitAct)

        self.helpMenu = self.menuBar().addMenu(self.tr(""&Help""))
        self.helpMenu.addAction(self.aboutAct)
        self.helpMenu.addAction(self.aboutQtAct)

class Widget(QtGui.QWidget):
    
    def __init__(self):
        super(Widget, self).__init__()

        # set font for tips
        QtGui.QToolTip.setFont(QtGui.QFont('SansSerif', 10))

        self.create_frame()
        

    def create_frame(self):
        """"""The frame""""""
        self.main_frame = QtGui.QWidget()        
       

        
def main():
    app = QtGui.QApplication(sys.argv)
    mw = MainWindow()
    sys.exit(app.exec_()) 
 
if __name__ == '__main__':
    main()        ",Create a frame with a menu bar.,"Create a frame with a menu bar.

The software will have several buttons, but the idea of the menu
bar is to have redundancy on the commands and to inform the user
of the shortcuts.
",Python,bsd-3-clause,leouieda/funghi,91,"```python
#!/usr/bin/env python

""""""
fungui is a software to help measuring the shell of a fungi.
""""""

# Import modules
from PyQt4 import QtGui, QtCore
import sys


# Global variables
FRAME_WIDTH = 1020
FRAME_HEIGHT = 480

class MainWindow(QtGui.QMainWindow):
    def __init__(self):
        QtGui.QMainWindow.__init__(self)

        # create stuff
        self.wdg = Widget()
        self.setCentralWidget(self.wdg)
        self.createActions()
        self.createMenus()
        #self.createStatusBar()

         # format the main window
        self.resize(FRAME_WIDTH, FRAME_HEIGHT)
        self.center()
        self.setWindowTitle('Fungui')

        # show windows
        self.show()
        self.wdg.show()
        
    def center(self):
        qr = self.frameGeometry()
        cp = QtGui.QDesktopWidget().availableGeometry().center()
        qr.moveCenter(cp)
        self.move(qr.topLeft())        

    def about(self):
        QtGui.QMessageBox.about(self, self.tr(""About fungui""),
            self.tr(""fungui...""))

    def createActions(self):
        self.exitAct = QtGui.QAction(self.tr(""E&xit""), self)
        self.exitAct.setShortcut(self.tr(""Ctrl+Q""))
        self.exitAct.setStatusTip(self.tr(""Exit the application""))
        self.exitAct.triggered.connect(self.close)

        self.aboutAct = QtGui.QAction(self.tr(""&About""), self)
        self.aboutAct.setStatusTip(self.tr(""Show the application's About box""))
        self.aboutAct.triggered.connect(self.about)

        self.aboutQtAct = QtGui.QAction(self.tr(""About &Qt""), self)
        self.aboutQtAct.setStatusTip(self.tr(""Show the Qt library's About box""))
        self.aboutQtAct.triggered.connect(QtGui.qApp.aboutQt)

    def createMenus(self):
        self.fileMenu = self.menuBar().addMenu(self.tr(""&File""))
        self.fileMenu.addAction(self.exitAct)

        self.helpMenu = self.menuBar().addMenu(self.tr(""&Help""))
        self.helpMenu.addAction(self.aboutAct)
        self.helpMenu.addAction(self.aboutQtAct)

class Widget(QtGui.QWidget):
    
    def __init__(self):
        super(Widget, self).__init__()

        # set font for tips
        QtGui.QToolTip.setFont(QtGui.QFont('SansSerif', 10))

        self.create_frame()
        

    def create_frame(self):
        """"""The frame""""""
        self.main_frame = QtGui.QWidget()        
       

        
def main():
    app = QtGui.QApplication(sys.argv)
    mw = MainWindow()
    sys.exit(app.exec_()) 
 
if __name__ == '__main__':
    main()        
```"
ab1a2982b6a44bfcfcaff5a3469f2d85f56a86a4,src/cli/_dbus/_manager.py,src/cli/_dbus/_manager.py,"""""""
Manager interface.
""""""

class Manager(object):
    """"""
    Manager interface.
    """"""

    _INTERFACE_NAME = 'org.storage.stratis1.Manager'

    def __init__(self, dbus_object):
        """"""
        Initializer.

        :param dbus_object: the dbus object
        """"""
        self._dbus_object = dbus_object

    def CreatePool(self, pool_name, devices, num_devices):
        """"""
        Create a pool.

        :param str pool_name: the pool name
        :param devices: the component devices
        :type devices: sequence of str
        """"""
        return self._dbus_object.CreatePool(
           pool_name,
           devices,
           num_devices,
           dbus_interface=self._INTERFACE_NAME,
        )

    def DestroyPool(self, pool_name):
        """"""
        Destroy a pool.

        :param str pool_name: the name of the pool
        """"""
        return self._dbus_object.DestroyPool(
           pool_name,
           dbus_interface=self._INTERFACE_NAME
        )

    def ListPools(self):
        """"""
        List all pools.
        """"""
        return self._dbus_object.ListPools(dbus_interface=self._INTERFACE_NAME)
","""""""
Manager interface.
""""""

from ._properties import Properties


class Manager(object):
    """"""
    Manager interface.
    """"""

    _INTERFACE_NAME = 'org.storage.stratis1.Manager'

    def __init__(self, dbus_object):
        """"""
        Initializer.

        :param dbus_object: the dbus object
        """"""
        self._dbus_object = dbus_object

    def CreatePool(self, pool_name, devices, num_devices):
        """"""
        Create a pool.

        :param str pool_name: the pool name
        :param devices: the component devices
        :type devices: sequence of str
        """"""
        return self._dbus_object.CreatePool(
           pool_name,
           devices,
           num_devices,
           dbus_interface=self._INTERFACE_NAME,
        )

    def DestroyPool(self, pool_name):
        """"""
        Destroy a pool.

        :param str pool_name: the name of the pool
        """"""
        return self._dbus_object.DestroyPool(
           pool_name,
           dbus_interface=self._INTERFACE_NAME
        )

    def ListPools(self):
        """"""
        List all pools.
        """"""
        return self._dbus_object.ListPools(dbus_interface=self._INTERFACE_NAME)

    @property
    def Version(self):
        """"""
        Stratisd Version getter.

        :rtype: String
        """"""
        return Properties(self._dbus_object).Get(
           self._INTERFACE_NAME,
           'Version'
        )

    @property
    def LogLevel(self):
        """"""
        Stratisd LogLevel getter.

        :rtype: String
        """"""
        return Properties(self._dbus_object).Get(
           self._INTERFACE_NAME,
           'LogLevel'
        )

    @LogLevel.setter
    def LogLevel(self, value):
        """"""
        Stratisd LogLevel setter.

        :param str value: the value to set
        """"""
        return Properties(self._dbus_object).Set(
           self._INTERFACE_NAME,
           'LogLevel',
           value
        )
",Use Properties interface to get Manager properties.,"Use Properties interface to get Manager properties.

Signed-off-by: mulhern <7b51bcf507bcd7afb72bf8663752c0ddbeb517f6@redhat.com>
",Python,apache-2.0,"stratis-storage/stratis-cli,stratis-storage/stratis-cli",91,"```python
""""""
Manager interface.
""""""

from ._properties import Properties


class Manager(object):
    """"""
    Manager interface.
    """"""

    _INTERFACE_NAME = 'org.storage.stratis1.Manager'

    def __init__(self, dbus_object):
        """"""
        Initializer.

        :param dbus_object: the dbus object
        """"""
        self._dbus_object = dbus_object

    def CreatePool(self, pool_name, devices, num_devices):
        """"""
        Create a pool.

        :param str pool_name: the pool name
        :param devices: the component devices
        :type devices: sequence of str
        """"""
        return self._dbus_object.CreatePool(
           pool_name,
           devices,
           num_devices,
           dbus_interface=self._INTERFACE_NAME,
        )

    def DestroyPool(self, pool_name):
        """"""
        Destroy a pool.

        :param str pool_name: the name of the pool
        """"""
        return self._dbus_object.DestroyPool(
           pool_name,
           dbus_interface=self._INTERFACE_NAME
        )

    def ListPools(self):
        """"""
        List all pools.
        """"""
        return self._dbus_object.ListPools(dbus_interface=self._INTERFACE_NAME)

    @property
    def Version(self):
        """"""
        Stratisd Version getter.

        :rtype: String
        """"""
        return Properties(self._dbus_object).Get(
           self._INTERFACE_NAME,
           'Version'
        )

    @property
    def LogLevel(self):
        """"""
        Stratisd LogLevel getter.

        :rtype: String
        """"""
        return Properties(self._dbus_object).Get(
           self._INTERFACE_NAME,
           'LogLevel'
        )

    @LogLevel.setter
    def LogLevel(self, value):
        """"""
        Stratisd LogLevel setter.

        :param str value: the value to set
        """"""
        return Properties(self._dbus_object).Set(
           self._INTERFACE_NAME,
           'LogLevel',
           value
        )

```"
